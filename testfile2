



Propaganda is a concerted set of messages aimed at influencing the opinions or behavior of large numbers of people. Instead of impartially providing information, propaganda in its most basic sense presents information in order to influence its audience. The most effective propaganda is often completely truthful, but some propaganda presents facts selectively to encourage a particular synthesis, or gives loaded messages in order to produce an emotional rather than rational response to the information presented. The desired result is a change of the cognitive narrative of the subject in the target audience.



The word originates from the Latin name Congregatio de Propaganda Fide ("Congregation for the Spreading of the Faith") of a congregation founded by Pope Gregory XV in 1622, shortly after the start of the Thirty Years' War. This department of the pontifical administration was charged with the spread of Catholicism and with the regulation of ecclesiastical affairs in mission territory.

The Latin adjective propaganda, which is a form of the gerundive of the verb propago (from pro- "forth" + *pag-, root of pangere "to fasten"), means "that which is to be spread" and does not carry a connotation of information, misleading or otherwise. The modern sense dates from World War I, when the term evolved to be mainly associated with politics.



 

Propaganda shares techniques with advertising and public relations. Advertising and public relations can be thought of as propaganda that promotes a commercial product or shapes the perception of an organization, person or brand, though in post-World War II usage the word "propaganda" more typically refers to political or nationalist uses of these techniques or to the promotion of a set of ideas, since the term had gained a pejorative meaning, which commercial and government entities couldnâ€™t accept. The refusal phenomenon was eventually to be seen in politics itself by the substitution of â€˜political marketingâ€?and other designations for â€˜political propagandaâ€? 

Propaganda was often used to influence opinions and beliefs on religious issues, particularly during the split between the Catholic Church and the Protestants. Propaganda has become more common in political contexts, in particular to refer to certain efforts sponsored by governments, political groups, but also often covert interests. In the early 20th century the term propaganda was also used by the founders of the nascent public relations industry to describe their activities. This usage died out around the time of World War II, as the industry started to avoid the word, given the pejorative connotation it had acquired. 

Literally translated from the Latin gerundive as "things which must be disseminated", in some cultures the term is neutral or even positive, while in others the term has acquired a strong negative connotation. The connotations of the term "propaganda" can also vary over time. For example, in Portuguese and some Spanish language speaking countries, particularly in the Southern Cone, the word "propaganda" usually refers to the most common manipulative media â€?"advertising". 

In English, "propaganda" was originally a neutral term used to describe the dissemination of information in favor of any given cause. During the 20th century, however, the term acquired a thoroughly negative meaning in western countries, representing the intentional dissemination of often false, but certainly "compelling" claims to support or justify political actions or ideologies. This redefinition arose because both the Soviet Union and Germany's government under Hitler admitted explicitly to using propaganda favoring, respectively, communism and fascism, in all forms of public expression. As these ideologies were antipathetic to liberal western societies, the negative feelings toward them came to be projected into the word "propaganda" itself. 



Roderick Hindery argues that propaganda exists on the political left, and right, and in mainstream centrist parties. Hindery further argues that debates about most social issues can be productively revisited in the context of asking "what is or is not propaganda?" Not to be overlooked is the link between propaganda, indoctrination, and terrorism/counterterrorism. She argues that threats to destroy are often as socially disruptive as physical devastation itself.Propaganda also has much in common with public information campaigns by governments, which are intended to encourage or discourage certain forms of behavior (such as wearing seat belts, not smoking, not littering and so forth). Again, the emphasis is more political in propaganda. Propaganda can take the form of leaflets, posters, TV and radio broadcasts and can also extend to any other medium. In the case of the United States, there is also an important legal (imposed by law) distinction between advertising (a type of overt propaganda) and what the Government Accountability Office (GAO), an arm of the United States Congress, refers to as "covert propaganda." 

Journalistic theory generally holds that news items should be objective, giving the reader an accurate background and analysis of the subject at hand. On the other hand, advertisements evolved from the traditional commercial advertisements to include also a new type in the form of paid articles or broadcasts disguised as news. These generally present an issue in a very subjective and often misleading light, primarily meant to persuade rather than inform. Normally they use only subtle propaganda techniques and not the more obvious ones used in traditional commercial advertisements. If the reader believes that a paid advertisement is in fact a news item, the message the advertiser is trying to communicate will be more easily "believed" or "internalized." 

Such advertisements are considered obvious examples of "covert" propaganda because they take on the appearance of objective information rather than the appearance of propaganda, which is misleading. Federal law specifically mandates that any advertisement appearing in the format of a news item must state that the item is in fact a paid advertisement. The Bush Administration has been criticized for allegedly producing and disseminating covert propaganda in the form of television programs, aired in the United States, which appeared to be legitimate news broadcasts and did not include any information signifying that the programs were not generated by a private-sector news source. 

Propaganda, in a narrower use of the term, connotes deliberately false or misleading information that supports or furthers a political (but not only) cause or the interests of those with power. The propagandist seeks to change the way people understand an issue or situation for the purpose of changing their actions and expectations in ways that are desirable to the interest group. Propaganda, in this sense, serves as a corollary to censorship in which the same purpose is achieved, not by filling people's minds with approved information, but by preventing people from being confronted with opposing points of view. What sets propaganda apart from other forms of advocacy is the willingness of the propagandist to change people's understanding through deception and confusion rather than persuasion and understanding. The leaders of an organization know the information to be one sided or untrue, but this may not be true for the rank and file members who help to disseminate the propaganda.

More in line with the religious roots of the term, it is also used widely in the debates about new religious movements (NRMs), both by people who defend them and by people who oppose them. The latter pejoratively call these NRMs cults. Anti-cult activists and countercult activists accuse the leaders of what they consider cults of using propaganda extensively to recruit followers and keep them. Some social scientists, such as the late Jeffrey Hadden, and CESNUR affiliated scholars accuse ex-members of "cults" who became vocal critics and the anti-cult movement of making these unusual religious movements look bad without sufficient reasons.

Propaganda is a powerful weapon in war; it is used to dehumanize and create hatred toward a supposed enemy, either internal or external, by creating a false image in the mind. This can be done by using derogatory or racist terms, avoiding some words or by making allegations of enemy atrocities. Most propaganda wars require the home population to feel the enemy has inflicted an injustice, which may be fictitious or may be based on facts. The home population must also decide that the cause of their nation is just.

 Propaganda is also one of the methods used in psychological warfare, which may also involve false flag operations. The term propaganda may also refer to false information meant to reinforce the mindsets of people who already believe as the propagandist wishes. The assumption is that, if people believe something false, they will constantly be assailed by doubts. Since these doubts are unpleasant (see cognitive dissonance), people will be eager to have them extinguished, and are therefore receptive to the reassurances of those in power. For this reason propaganda is often addressed to people who are already sympathetic to the agenda. This process of reinforcement uses an individual's predisposition to self-select "agreeable" information sources as a mechanism for maintaining control.

Propaganda can be classified according to the source and nature of the message. White propaganda generally comes from an openly identified source, and is characterized by gentler methods of persuasion, such as standard public relations techniques and one-sided presentation of an argument. Black propaganda is identified as being from one source, but is in fact from another. This is most commonly to disguise the true origins of the propaganda, be it from an enemy country or from an organization with a negative public image. Grey propaganda  is propaganda without any identifiable source or author. A major application of grey propaganda is making enemies believe falsehoods using straw arguments: As phase one, to make someone believe "A", one releases as grey propaganda "B", the opposite of "A". In phase two, "B" is discredited using some strawman. The enemy will then assume "A" to be true.

In scale, these different types of propaganda can also be defined by the potential of true and correct information to compete with the propaganda. For example, opposition to white propaganda is often readily found and may slightly discredit the propaganda source. Opposition to grey propaganda, when revealed (often by an inside source), may create some level of public outcry. Opposition to black propaganda is often unavailable and may be dangerous to reveal, because public cognizance of black propaganda tactics and sources would undermine or backfire the very campaign the black propagandist supported. 

 Propaganda may be administered in insidious ways. For instance, disparaging disinformation about the history of certain groups or foreign countries may be encouraged or tolerated in the educational system. Since few people actually double-check what they learn at school, such disinformation will be repeated by journalists as well as parents, thus reinforcing the idea that the disinformation item is really a "well-known fact", even though no one repeating the myth is able to point to an authoritative source. The disinformation is then recycled in the media and in the educational system, without the need for direct governmental intervention on the media. Such permeating propaganda may be used for political goals: by giving citizens a false impression of the quality or policies of their country, they may be incited to reject certain proposals or certain remarks or ignore the experience of others. See also: black propaganda, marketing, advertising

Common media for transmitting propaganda messages include news reports, government reports, historical revision, junk science, books, leaflets, movies, radio, television, and posters. In the case of radio and television, propaganda can exist on news, current-affairs or talk-show segments, as advertising or public-service announce "spots" or as long-running advertorials. Propaganda campaigns often follow a strategic transmission pattern to indoctrinate the target group. This may begin with a simple transmission such as a leaflet dropped from a plane or an advertisement. Generally these messages will contain directions on how to obtain more information, via a web site, hot line, radio program, et cetera (as it is seen also for selling purposes among other goals). The strategy intends to initiate the individual from information recipient to information seeker through reinforcement, and then from information seeker to opinion leader through indoctrination.

A number of techniques which are based on social psychological research are used to generate propaganda. Many of these same techniques can be found under logical fallacies, since propagandists use arguments that, while sometimes convincing, are not necessarily valid. 

Some time has been spent analyzing the means by which propaganda messages are transmitted. That work is important but it is clear that information dissemination strategies only become propaganda strategies when coupled with propagandistic messages. Identifying these messages is a necessary prerequisite to study the methods by which those messages are spread. Below are a number of techniques for generating propaganda:







See also: doublespeak, cult of personality, spin, demonization, factoid

 The propaganda model is a theory advanced by Edward S. Herman and Noam Chomsky that alleges systemic biases in the mass media and seeks to explain them in terms of structural economic causes.

First presented in their 1988 book , the propaganda model views the private media as businesses selling a product â€?readers and audiences (rather than news) â€?to other businesses (advertisers) and relying primarily on government and corporate information and propaganda.

The first three (ownership, funding, and sourcing) are generally regarded by the authors as being the most important. Although the model was based mainly on the characterization of United States media, Chomsky and Herman believe the theory is equally applicable to any country that shares the basic economic structure and organizing principles which the model postulates as the cause of media biases. After the disintegration of the Soviet Union, Chomsky stated that the new filter replacing communism would be terrorism and Islam.

The epistemic merit model is a method for understanding propaganda conceived by Sheryl Tuttle Ross and detailed in her 2002 article for the Journal of Aesthetic Education entitled "Understanding Propaganda: The Epistemic Merit Model and Its Application to Art". Ross developed the Epistemic merit model due to concern about narrow, misleading definitions of propaganda. She contrasted her model with the ideas of Pope Gregory XV, the Institute for Propaganda Analysis, Alfred Lee, F.C. Bartlett, and Hans Speier. Insisting that each of their respective discussions of propaganda are too narrow, Ross proposed her own definition. 

To appropriately discuss propaganda, Ross argues that one must consider a threefold communication model: that of Sender-Message-Receiver. "That is... propaganda involve[s]... the one who is persuading (Sender) [who is] doing so intentionally, [the] target for such persuasion (Receiver) and [the] means of reaching that target (Message)." There are four conditions for a message to be considered propaganda. Propaganda involves the intention to persuade. As well, propaganda is sent on behalf of a sociopolitical institution, organization, or cause. Next, the recipient of propaganda is a socially significant group of people. Finally, propaganda is an epistemical struggle to challenge other thoughts.

Ross claims that it is misleading to say that propaganda is simply false, or that it is conditional to a lie, since often the propagandist believes in what he/she is propagandizing. In other words, it is not necessarily a lie if the person who creates the propaganda is trying to persuade you of a view that they actually hold. "The aim of the propagandist is to create the semblance of credibility." This means that they appeal to an epistemology that is weak or defective. False statements, bad arguments, immoral commands as well as inapt metaphors (and other literary tropes) are the sorts of things that are epistemically defective... Not only does epistemic defectiveness more accurately describe how propaganda endeavors to function... since many messages are in forms such as commands that do not admit to truth-values, [but it] also accounts for the role context plays in the workings of propaganda.

Throughout history those who have wished to persuade have used art to get their message out. This can be accomplished by hiring artists for the express aim of propagandizing or by investing new meanings to a previously nonpolitical work. Therefore, Ross states, it is important to consider "the conditions of its making [and] the conditions of its use."...Â 

Propaganda has been a human activity as far back as reliable recorded evidence exists. The Behistun Inscription (c. 515 BC) detailing the rise of Darius I to the Persian throne, can be seen as an early example of propaganda. The Arthashastra written by Chanakya (c. 350 - 283 BC), a professor of political science at Takshashila University and a prime minister of the Maurya Empire, discusses propaganda in detail, such as how to spread propaganda and how to apply it in warfare. His student Chandragupta Maurya (c. 340 - 293 BC), founder of the Maurya Empire, employed these methods during his rise to power. The writings of Romans such as Livy (c. 59 BC - 17 AD) are considered masterpieces of pro-Roman propaganda. Another example of early propaganda would be the 12th century work The War of the Irish with the Foreigners, written by the DÃ¡l gCais to portray themselves as legitimate rulers of Ireland.

Gabriel Tarde's Laws of Imitation (1890) and Gustave Le Bon's The Crowd: A Study of the Popular Mind (1897) were two of the first codifications of propaganda techniques, which influenced many writers afterward, including Sigmund Freud. Hitler's Mein Kampf is heavily influenced by Le Bon's theories. Journalist Walter Lippmann, in Public Opinion (1922) also worked on the subject, as well as the American advertising pioneer Edward Bernays, a nephew of Freud, early in the 20th century. 

During World War I, Lippmann and Bernays were hired by then United States President, Woodrow Wilson, to participate in the Creel Commission, the mission of which was to sway popular opinion in favor of entering the war, on the side of the United Kingdom. The Creel Commission provided themes for speeches by "four-minute men" at public functions, and also encouraged censorship of the American press. The Commission was so unpopular that after the war, Congress closed it down without providing funding to organize and archive its papers.

The war propaganda campaign of Lippmann and Bernays produced within six months such an intense anti-German hysteria as to permanently impress American business (and Adolf Hitler, among others) with the potential of large-scale propaganda to control public opinion. Bernays coined the terms "group mind" and "engineering consent", important concepts in practical propaganda work.

The current public relations industry is a direct outgrowth of Lippmann's and Bernays' work and is still used extensively by the United States government. For the first half of the 20th century Bernays and Lippmann themselves ran a very successful public relations firm. World War II saw continued use of propaganda as a weapon of war, both by Hitler's propagandist Joseph Goebbels and the British Political Warfare Executive, as well as the United States Office of War Information.

In the early 2000s, the United States government developed and freely distributed a video game known as America's Army. The stated intention of the game is to encourage players to become interested in joining the U.S. Army. According to a poll by I for I Research, 30% of young people who had a positive view of the military said that they had developed that view by playing the game.

Russian revolutionaries of the 19th and 20th centuries distinguished two different aspects covered by the English term propaganda. Their terminology included two terms:  (agitatsiya), or agitation, and , or propaganda, see agitprop (agitprop is not, however, limited to the Soviet Union, as it was considered, before the October Revolution, to be one of the fundamental activities of any Marxist activist; this importance of agit-prop in Marxist theory may also be observed today in Trotskyist circles, who insist on the importance of leaflet distribution).

Soviet propaganda meant dissemination of revolutionary ideas, teachings of Marxism, and theoretical and practical knowledge of Marxist economics, while agitation meant forming favorable public opinion and stirring up political unrest. These activities did not carry negative connotations (as they usually do in English) and were encouraged. Expanding dimensions of state propaganda, the Bolsheviks actively used transportation such as trains, aircraft and other means. 

Josef Stalin's regime built the largest fixed-wing aircraft of the 1930s, Tupolev ANT-20, exclusively for this purpose. Named after the famous Soviet writer Maxim Gorky who had recently returned from fascist Italy, it was equipped with a powerful radio set called "Voice from the sky", printing and leaflet-dropping machinery, radiostations, photographic laboratory, film projector with sound for showing movies in flight, library, etc. The aircraft could be disassembled and transported by railroad if needed. The giant aircraft set a number of world records.



Most propaganda in Germany was produced by the Ministry for Public Enlightenment and Propaganda (Propagandaministerium). Joseph Goebbels was placed in charge of this ministry shortly after Hitler took power in 1933. All journalists, writers, and artists were required to register with one of the Ministry's subordinate chambers for the press, fine arts, music, theater, film, literature, or radio.

The Nazis believed in propaganda as a vital tool in achieving their goals. Adolf Hitler, Germany's FÃ¼hrer, was impressed by the power of Allied propaganda during World War I and believed that it had been a primary cause of the collapse of morale and revolts in the German home front and Navy in 1918 (see also: DolchstoÃŸlegende). Hitler would meet nearly every day with Goebbels to discuss the news and Goebbels would obtain Hitler's thoughts on the subject; Goebbels would then meet with senior Ministry officials and pass down the official Party line on world events. Broadcasters and journalists required prior approval before their works were disseminated. Along with posters, the Nazis produced a number of films and books to spread their beliefs.



The United States and the Soviet Union both used propaganda extensively during the Cold War. Both sides used film, television, and radio programming to influence their own citizens, each other, and Third World nations. The United States Information Agency operated the Voice of America as an official government station. Radio Free Europe and Radio Liberty, which were in part supported by the Central Intelligence Agency, provided grey propaganda in news and entertainment programs to Eastern Europe and the Soviet Union respectively. The Soviet Union's official government station, Radio Moscow, broadcast white propaganda, while Radio Peace and Freedom broadcast grey propaganda. Both sides also broadcast black propaganda programs in periods of special crises. 

In 1948, the United Kingdom's Foreign Office created the IRD (Information Research Department) which took over from wartime and slightly post-war departments such as the Ministry of Information and dispensed propaganda via various media such as the BBC and publishing.

The ideological and border dispute between the Soviet Union and People's Republic of China resulted in a number of cross-border operations. One technique developed during this period was the "backwards transmission", in which the radio program was recorded and played backwards over the air. (This was done so that messages meant to be received by the other government could be heard, while the average listener could not understand the content of the program.)

When describing life in capitalist countries, in the US in particular, propaganda focused on social issues such as poverty and anti-union action by the government. Workers in capitalist countries were portrayed as "ideologically close". Propaganda claimed rich people from the US derived their income from weapons manufacturing, and claimed that there was substantial racism or neo-fascism in the US.When describing life in Communist countries, western propaganda sought to depict an image of a citizenry held captive by governments that brainwash them. The West also created a fear of the East, by depicting an aggressive Soviet Union. In the Americas, Cuba served as a major source and a target of propaganda from both black and white stations operated by the CIA and Cuban exile groups. Radio Habana Cuba, in turn, broadcast original programming, relayed Radio Moscow, and broadcast The Voice of Vietnam as well as alleged confessions from the crew of the USS Pueblo. 

George Orwell's novels Animal Farm and Nineteen Eighty-Four are virtual textbooks on the use of propaganda. Though not set in the Soviet Union, these books are about totalitarian regimes in which language is constantly corrupted for political purposes. These novels were, ironically, used for explicit propaganda. The CIA, for example, secretly commissioned an animated film adaptation of Animal Farm in the 1950s with small changes to the original story to suit its own needs.

During the democratic revolutions of 1989 in Central and Eastern Europe the propaganda poster was important weapon in the hand of the opposition. Printed and hand-made political posters appeared on the Berlin Wall, on the statue of St. Wenseslas in Prague and around the unmarked grave of Imre Nagy in Budapest and the role of them was important for the democratic change.



During the Yugoslav wars, Serb propaganda was used to create fear and hatred and particularly incite the Serb population against the other ethnicities (Bosniaks, Croats, Albanians and other non-Serbs). Serb media made a great effort in justifying, revising or denying mass war crimes committed by Serb forces during the Yugoslav wars on Bosniaks and other non-Serbs. According to the ICTY verdicts against Serb political and military leaders, during the Bosnian war, the propaganda was a part of the Strategic Plan by Serb leadership, aimed at linking Serb-populated areas in Bosnia and Herzegovina together, gaining control over these areas and creating a separate Serb state, from which most non-Serbs would be permanently removed. The Serb leadership was aware that the Strategic Plan could only be implemented by the use of force and fear, thus by the commission of war crimes.

In the 2001 invasion of Afghanistan, psychological operations tactics were employed to demoralize the Taliban and to win the sympathies of the Afghan population. At least six EC-130E Commando Solo aircraft were used to jam local radio transmissions and transmit replacement propaganda messages.Leaflets were also dropped throughout Afghanistan, offering rewards for Osama bin Laden and other individuals, portraying Americans as friends of Afghanistan and emphasizing various negative aspects of the Taliban. Another shows a picture of Mohammed Omar in a set of crosshairs with the words "We are watching." This Technique has been shown to be rather ineffective in terms of long term opinions change given current political and social conditions in Afghanistan.

The US Air Force can use cluster bombs to deliver leaflets. The LBU-30 clusterbomb is designed to allow an aircraft to deliver leaflets to a target area while minimizing wind drift.

 the 2003 invasion of Iraq, the Iraqi Information Minister Mohammed Saeed al-Sahaf repeatedly claimed Iraqi forces were decisively winning every battle. Even up to the overthrow of the Iraqi government at Baghdad, he maintained that the United States would soon be defeated, in contradiction with all other media. Due to this, he quickly became a cult figure in the West, and gained recognition on the website WeLoveTheIraqiInformationMinister.com The Iraqis who were misled by his propaganda were shocked when Iraq was defeated.

In November 2005, The Chicago Tribune and the Los Angeles Times, alleged that the United States military had manipulated news reported in Iraqi media in an effort to cast a favorable light on its actions while demoralizing the insurgency. Lt. Col. Barry Johnson, a military spokesman in Iraq, said the program is "an important part of countering misinformation in the news by insurgents", while a spokesman for former Defense Secretary Donald H. Rumsfeld said the allegations of manipulation were troubling if true. The Department of Defense has confirmed the existence of the program. The New York Times published an article about how the Pentagon has started to use contractors with little experience in journalism or public relations to plant articles in the Iraqi press. These articles are usually written by US soldiers without attribution or are attributed to a non-existent organization called the "International Information Center." Planting propaganda stories in newspapers was done by both the Allies and Central Powers in the First World War and the Axis and Allies in the Second; this is the latest version of this technique.

 

Of all the potential targets for propaganda, children are the most vulnerable because they are the most unprepared for the critical reasoning and contextual comprehension required to determine whether a message is propaganda or not. Children's vulnerability to propaganda is rooted in developmental psychology. The attention children give their environment during development, due to the process of developing their understanding of the world, will cause them to absorb propaganda indiscriminately. Also, children are highly imitative: studies by Albert Bandura, Dorothea Ross and Sheila A. Ross in the 1960s indicated that children are susceptible to filmed representations of behaviour. Therefore television is of particular interest in regard to children's vulnerability to propaganda.

Another vulnerability of children is the theoretical influence that their peers have over their behaviour. According to Judith Rich Harris's group-socialization theory, children learn the majority of what they do not receive paternally, through genes, from their peer groups. The implication then is that if peer-groups can be indoctrinated through propaganda at a young age to hold certain beliefs, the group will self-regulate the indoctrination, since new members to the group will adapt their beliefs to fit the group's.

To a degree, socialization, formal education, and standardized television programming can be seen as using propaganda for the purpose of indoctrination. Schools that utilize dogmatic, frozen world-views, often resort to propagandist curricula that indoctrinate children. The use of propaganda in schools was highly prevalent during the 1930s and 1940s in Germany, as well as in Stalinist Russia.

In Nazi Germany, the education system was thoroughly co-opted to indoctrinate the German youth with anti-Semitic ideology. This was accomplished through the National Socialist Teachersâ€?Union, of which 97% of all German teachers were members in 1937. It encouraged the teaching of â€œracial theory.â€?Picture books for children such as Donâ€™t Trust A Fox in A Green Meadow Or the Word of A Jew, The Poisonous Mushroom, and The Poodle-Pug-Dachshund-Pincher were widely circulated (over 100,000 copies of Donâ€™t Trust A Fox...Â were circulated during the late 1930s) and contained depictions of Jews as devils, child molesters, and other morally charged figures. Slogans such as â€œJudas the Jew betrayed Jesus the German to the Jewsâ€?were recited in class. The following is an example of a propagandistic math problem recommended by the National Socialist Essence of Education:The Jews are aliens in Germanyâ€”in 1933 there were 6,606,000 inhabitants in the German Reich, of whom 499,682 were Jews. What is the per cent of aliens?









<div class="references-small">















Lung cancer is a disease of uncontrolled cell growth in tissues of the lung. This growth may lead to metastasis, invasion of adjacent tissue and infiltration beyond the lungs. The vast majority of primary lung cancers are carcinomas of the lung, derived from epithelial cells. Lung cancer, the most common cause of cancer-related death in men and the second most common in women, is responsible for  worldwideannually. The most common symptoms are shortness of breath, coughing (including coughing up blood), and weight loss.

The main types of lung cancer are small cell lung carcinoma and non-small cell lung carcinoma. This distinction is important because the treatment varies; non-small cell lung carcinoma (NSCLC) is sometimes treated with surgery, while small cell lung carcinoma (SCLC) usually responds better to chemotherapy and radiation. The most common cause of lung cancer is long term exposure to tobacco smoke. The occurrence of lung cancer in non-smokers, who account for fewer than 10% of cases, appears to be due to a combination of genetic factors, radon gas, asbestos, and air pollution, including second-hand smoke.

Lung cancer may be seen on chest x-ray and computed tomography (CT scan). The diagnosis is confirmed with a biopsy. This is usually performed via bronchoscopy or CT-guided biopsy. Treatment and prognosis depend upon the histological type of cancer, the stage (degree of spread), and the patient's performance status. Possible treatments include surgery, chemotherapy, and radiotherapy. With treatment, the five-year survival rate is 14%.

The vast majority of lung cancers are carcinomasâ€”malignancies that arise from epithelial cells. There are two main types of lung carcinoma, categorized by the size and appearance of the malignant cells seen by a histopathologist under a microscope: non-small cell (80.4%) and small-cell (16.8%) lung carcinoma. This classification, based on histological criteria, has important implications for clinical management and prognosis of the disease.

The non-small cell lung carcinomas are grouped together because their prognosis and management are similar. There are three main sub-types: squamous cell lung carcinoma, adenocarcinoma and large cell lung carcinoma.

Accounting for 31.1% of lung cancers, squamous cell lung carcinoma usually starts near a central bronchus. Cavitation and necrosis within the center of the cancer is a common finding. Well-differentiated squamous cell lung cancers often grow more slowly than other cancer types.

Adenocarcinoma accounts for 29.4% of lung cancers. It usually originates in peripheral lung tissue. Most cases of adenocarcinoma are associated with smoking. However, among people who have never smoked ("never-smokers"), adenocarcinoma is the most common form of lung cancer. A subtype of adenocarcinoma, the bronchioloalveolar carcinoma, is more common in female never-smokers, and may have different responses to treatment.

Accounting for 10.7% of lung cancers, large cell lung carcinoma is a fast-growing form that develops near the surface of the lung. It is often poorly differentiated and tends to metastasize early.



Small cell lung carcinoma (SCLC, also called "oat cell carcinoma") is less common. It tends to arise in the larger airways (primary and secondary bronchi) and grows rapidly, becoming quite large. The "oat" cell contains dense neurosecretory granules (vesicles containing neuroendocrine hormones) which give this an endocrine/paraneoplastic syndrome association. While initially more sensitive to chemotherapy, it ultimately carries a worse prognosis and is often metastatic at presentation. Small cell lung cancers are divided into Limited stage and Extensive stage disease. This type of lung cancer is strongly associated with smoking.

The lung is a common place for metastasis from tumors in other parts of the body. These cancers are identified by the site of origin, thus a breast cancer metastasis to the lung is still known as breast cancer. They often have a characteristic round appearance on chest x-ray. Primary lung cancers themselves most commonly metastasize to the adrenal glands, liver, brain, and bone.



Lung cancer staging is an assessment of the degree of spread of the cancer from its original source. It is an important factor affecting the prognosis and potential treatment of lung cancer. Non-small cell lung carcinoma is staged from IA ("one A", best prognosis) to IV ("four", worst prognosis). Small cell lung carcinoma is classified as limited stage if it is confined to one half of the chest and within the scope of a single radiotherapy field. Otherwise it is extensive stage.

Symptoms that suggest lung cancer include:

If the cancer grows in the airway, it may obstruct airflow, causing breathing difficulties. This can lead to accumulation of secretions behind the blockage, predisposing the patient to pneumonia. Many lung cancers have a rich blood supply. The surface of the cancer may be fragile, leading to bleeding from the cancer into the airway. This blood may subsequently be coughed up.

Depending on the type of tumor, so-called paraneoplastic phenomena may initially attract attention to the disease. In lung cancer, these phenomena may include Lambert-Eaton myasthenic syndrome (muscle weakness due to auto-antibodies), hypercalcemia or syndrome of inappropriate antidiuretic hormone (SIADH). Tumors in the top (apex) of the lung, known as Pancoast tumors, may invade the local part of the sympathetic nervous system, leading to changed sweating patterns and eye muscle problems (a combination known as Horner's syndrome), as well as muscle weakness in the hands due to invasion of the brachial plexus.

Many of the symptoms of lung cancer (bone pain, fever, weight loss) are nonspecific; in the elderly, these may be attributed to comorbid illness. In many patients, the cancer has already spread beyond the original site by the time they have symptoms and seek medical attention. Common sites of metastasis include the bone, such as the spine (causing back pain and occasionally spinal cord compression), the liver and the brain. About 10% of people with lung cancer do not have symptoms at diagnosis; these cancers are incidentally found on routine chest x-rays.

The main causes of lung cancer (and cancer in general) include carcinogens (such as those in tobacco smoke), ionizing radiation, and viral infection.This exposure causes cumulative changes to the DNA in the tissue lining the bronchi of the lungs (the bronchial epithelium).As more tissue becomes damaged, eventually a cancer develops. 

Smoking, particularly of cigarettes, is by far the main contributor to lung cancer. In the United States, smoking is estimated to account for 87% of lung cancer cases (90% in men and 85% in women). Among male smokers, the lifetime risk of developing lung cancer is 17.2%. Among female smokers, the risk is 11.6%. This risk is significantly lower in non-smokers: 1.3% in men and 1.4% in women. Cigarette smoke contains over 60 known carcinogens including radioisotopes from the radon decay sequence, nitrosamine, and benzopyrene. Additionally, nicotine appears to depress the immune response to malignant growths in exposed tissue. The length of time a person smokes as well as the amount smoked increases the person's chance of developing lung cancer. If a person stops smoking, this chance steadily decreases as damage to the lungs is repaired and contaminant particles are gradually removed. Across the developed world, almost 90% of lung cancer deaths are caused by smoking. In addition, there is evidence that lung cancer in never-smokers has a better prognosis than in smokers, and that patients who smoke at the time of diagnosis have shorter survival than those who have quit.

Passive smokingâ€”the inhalation of smoke from another's smokingâ€”is a cause of lung cancer in non-smokers. Studies from the U.S., Europe, the UK, and Australia have consistently shown a significant increase in relative risk among those exposed to passive smoke. Recent investigation of sidestream smoke suggests it is more dangerous than direct smoke inhalation.

Radon is a colorless and odorless gas generated by the breakdown of radioactive radium, which in turn is the decay product of uranium, found in the earth's crust. The radiation decay products ionize genetic material, causing mutations that sometimes turn cancerous. Radon exposure is the second major cause of lung cancer after smoking. Radon gas levels vary by locality and the composition of the underlying soil and rocks. For example, in areas such as Cornwall in the UK (which has granite as substrata), radon gas is a major problem, and buildings have to be force-ventilated with fans to lower radon gas concentrations. The United States Environmental Protection Agency (EPA) estimates that one in 15 homes in the U.S. has radon levels above the recommended guideline of 4 picocuries per liter (pCi/L) (148 Bq/mÂ³). Iowa has the highest average radon concentration in the United States; studies performed there have demonstrated a 50% increased lung cancer risk with prolonged radon exposure above the EPA's action level of 4 pCi/L.

Asbestos can cause a variety of lung diseases, including lung cancer. There is a synergistic effect between tobacco smoking and asbestos in the formation of lung cancer. In the UK, asbestos accounts for 2â€?% of male lung cancer deaths. Asbestos can also cause cancer of the pleura, called mesothelioma (which is different from lung cancer).

Viruses are known to cause lung cancer in animals and recent evidence suggests similar potential in humans. Implicated viruses include human papillomavirus, JC virus, simian virus 40 (SV40), BK virus and cytomegalovirus. These viruses may affect the cell cycle and inhibit apoptosis, allowing uncontrolled cell division.



Similar to many other cancers, lung cancer is initiated by activation of oncogenes or inactivation of tumor suppressor genes. Oncogenes are genes that are believed to make people more susceptible to cancer. Proto-oncogenes are believed to turn into oncogenes when exposed to particular carcinogens. Mutations in the K-ras proto-oncogene are responsible for 20â€?0% of non-small cell lung cancers. Chromosomal damage can lead to loss of heterozygosity. This can cause inactivation of tumor suppressor genes. Damage to chromosomes 3p, 5q, 13q and 17p are particularly common in small cell lung carcinoma. The TP53 tumor suppressor gene, located on chromosome 17p, is often affected.

Several genetic polymorphisms are associated with lung cancer. These include polymorphisms in genes coding for interleukin-1, cytochrome P450, apoptosis promoters such as caspase-8, and DNA repair molecules such as XRCC1. People with these polymorphisms are more likely to develop lung cancer after exposure to carcinogens.



Performing a chest x-ray is the first step if a patient reports symptoms that may be suggestive of lung cancer. This may reveal an obvious mass, widening of the mediastinum (suggestive of spread to lymph nodes there), atelectasis (collapse), consolidation (pneumonia), or pleural effusion. If there are no x-ray findings but the suspicion is high (such as a heavy smoker with blood-stained sputum), bronchoscopy and/or a CT scan may provide the necessary information. Bronchoscopy or CT-guided biopsy is often used to identify the tumor type.



The differential diagnosis for patients who present with abnormalities on chest x-ray includes lung cancer, as well as nonmalignant diseases. These include infectious causes such as tuberculosis or pneumonia, or inflammatory conditions such as sarcoidosis. These diseases can result in mediastinal lymphadenopathy or lung nodules, and sometimes mimic lung cancers.



Prevention is the most cost-effective means of fighting lung cancer. While in most countries industrial and domestic carcinogens have been identified and banned, tobacco smoking is still widespread. Eliminating tobacco smoking is a primary goal in the prevention of lung cancer, and smoking cessation is an important preventative tool in this process.

Policy interventions to decrease passive smoking in public areas such as restaurants and workplaces have become more common in many Western countries, with California taking a lead in banning smoking in public establishments in 1998. Ireland played a similar role in Europe in 2004, followed by Italy and Norway in 2005, Scotland as well as several others in 2006, England in 2007, and France in 2008. New Zealand has banned smoking in public places as of 2004.

The state of Bhutan has had a complete smoking ban since 2005. In many countries, pressure groups are campaigning for similar bans. Arguments cited against such bans are criminalisation of smoking, increased risk of smuggling and the risk that such a ban cannot be enforced.

A 2008 study performed in over 75,000 middle-aged and elderly people demonstrated that the long-term use of supplemental multivitamins, such as vitamin C, vitamin E, and folate did not reduce the risk of lung cancer. To the contrary, the study indicates that the long term intake of high doses of vitamin E supplements may even increase the risk of lung cancer.



Screening refers to the use of medical tests to detect disease in asymptomatic people. Possible screening tests for lung cancer include chest x-ray or computed tomography (CT) of the chest. So far, screening programs for lung cancer have not demonstrated any clear benefit. Randomized controlled trials are underway in this area to see if decreased long-term mortality can be directly observed from CT screening.

Treatment for lung cancer depends on the cancer's specific cell type, how far it has spread, and the patient's performance status. Common treatments include surgery, chemotherapy, and radiation therapy.





If investigations confirm lung cancer, CT scan and often positron emission tomography (PET) are used to determine whether the disease is localised and amenable to surgery or whether it has spread to the point where it cannot be cured surgically.

Blood tests and spirometry (lung function testing) are also necessary to assess whether the patient is well enough to be operated on. If spirometry reveals poor respiratory reserve (often due to chronic obstructive pulmonary disease), surgery may be contraindicated.

Surgery itself has an operative death rate of about 4.4%, depending on the patient's lung function and other risk factors. Surgery is usually only an option in non-small cell lung carcinoma limited to one lung, up to stage IIIA. This is assessed with medical imaging (computed tomography, positron emission tomography). A sufficient pre-operative respiratory reserve must be present to allow adequate lung function after the tissue is removed.

Procedures include wedge resection (removal of part of a lobe), segmentectomy (removal of an anatomic division of a particular lobe of the lung), lobectomy (one lobe), bilobectomy (two lobes) or pneumonectomy (whole lung). In patients with adequate respiratory reserve, lobectomy is the preferred option, as this minimizes the chance of local recurrence. If the patient does not have enough functional lung for this, wedge resection may be performed. Radioactive iodine brachytherapy at the margins of wedge excision may reduce recurrence to that of lobectomy.

Small cell lung carcinoma is treated primarily with chemotherapy and radiation, as surgery has no demonstrable influence on survival. Primary chemotherapy is also given in metastatic non-small cell lung carcinoma.

The combination regimen depends on the tumor type. Non-small cell lung carcinoma is often treated with cisplatin or carboplatin, in combination with gemcitabine, paclitaxel, docetaxel, etoposide or vinorelbine. In small cell lung carcinoma, cisplatin and etoposide are most commonly used. Combinations with carboplatin, gemcitabine, paclitaxel, vinorelbine, topotecan and irinotecan are also used.

Adjuvant chemotherapy refers to the use of chemotherapy after surgery to improve the outcome. During surgery, samples are taken from the lymph nodes. If these samples contain cancer, then the patient has stage II or III disease. In this situation, adjuvant chemotherapy may improve survival by up to 15%. Standard practice is to offer platinum-based chemotherapy (including either cisplatin or carboplatin). 

Adjuvant chemotherapy for patients with stage IB cancer is controversial as clinical trials have not clearly demonstrated a survival benefit. Trials of preoperative chemotherapy (neoadjuvant chemotherapy) in resectable non-small cell lung carcinoma have been inconclusive.

Radiotherapy is often given together with chemotherapy, and may be used with curative intent in patients with non-small cell lung carcinoma who are not eligible for surgery. This form of high intensity radiotherapy is called radical radiotherapy. A refinement of this technique is continuous hyperfractionated accelerated radiotherapy (CHART), where a high dose of radiotherapy is given in a short time period. For small cell lung carcinoma cases that are potentially curable, in addition to chemotherapy, chest radiation is often recommended. The use of adjuvant thoracic radiotherapy following curative intent surgery for non-small cell lung carcinoma is not well established and controversial. Benefits, if any, may only be limited to those in whom the tumor has spread to the mediastinal lymph nodes.

For both non-small cell lung carcinoma and small cell lung carcinoma patients, smaller doses of radiation to the chest may be used for symptom control (palliative radiotherapy). Unlike other treatments, it is possible to deliver palliative radiotherapy without confirming the histological diagnosis of lung cancer.

Patients with limited stage small cell lung carcinoma are usually given prophylactic cranial irradiation (PCI). This is a type of radiotherapy to the brain, used to reduce the risk of metastasis. More recently, PCI has also been shown to be beneficial in those with extensive small cell lung cancer. In patients whose cancer has improved following a course of chemotherapy, PCI has been shown to reduce the cumulative risk of brain metastases within one year from 40.4% to 14.6%.

Recent improvements in targeting and imaging have led to the development of extracranial stereotactic radiation in the treatment of early-stage lung cancer. In this form of radiation therapy, very high doses are delivered in a small number of sessions using stereotactic targeting techniques. Its use is primarily in patients who are not surgical candidates due to medical comorbidities.

Radiofrequency ablation should currently be considered an investigational technique in the treatment of bronchogenic carcinoma. It is done by inserting a small heat probe into the tumor to kill the tumor cells.

In recent years, various molecular targeted therapies have been developed for the treatment of advanced lung cancer. Gefitinib (Iressa) is one such drug, which targets the tyrosine kinase domain of the epidermal growth factor receptor (EGF-R) which is expressed in many cases of non-small cell lung carcinoma. It was not shown to increase survival, although females, Asians, non-smokers and those with bronchioloalveolar carcinoma appear to derive the most benefit from gefitinib.

Erlotinib (Tarceva), another tyrosine kinase inhibitor, has been shown to increase survival in lung cancer patients and has recently been approved by the FDA for second-line treatment of advanced non-small cell lung carcinoma. Similar to gefitinib, it appeared to work best in females, Asians, non-smokers and those with bronchioloalveolar carcinoma.

The angiogenesis inhibitor bevacizumab (in combination with paclitaxel and carboplatin) improves the survival of patients with advanced non-small cell lung carcinoma. However this increases the risk of lung bleeding, particularly in patients with squamous cell carcinoma.

Advances in cytotoxic drugs, pharmacogenetics and targeted drug design show promise. A number of targeted agents are at the early stages of clinical research, such as cyclo-oxygenase-2 inhibitors, the apoptosis promoter exisulind, proteasome inhibitors, bexarotene and vaccines. Future areas of research include ras proto-oncogene inhibition, phosphoinositide 3-kinase inhibition, histone deacetylase inhibition, and tumor suppressor gene replacement.



Prognostic factors in non- small-cell lung cancer include presence or absence of pulmonary symptoms, tumor size, cell type (histology), degree of spread (stage) and metastases to multiple lymph nodes, and vascular invasion. For patients with inoperable disease, prognosis is adversely affected by poor performance status and weight loss of more than 10%. 

National Cancer Institute PDQ for Professionals.

National Cancer Institute PDQ for Professionals].

For non-small cell lung carcinoma, prognosis is generally poor. Following complete surgical resection of stage IA disease, five-year survival is 67%. With stage IB disease, five-year survival is 57%. The 5-year survival rate of patients with stage IV NSCLC is about 1%.

For small cell lung carcinoma, prognosis is also generally poor. The overall five-year survival for patients with SCLC is about 5%. Patients with extensive-stage SCLC have an average five-year survival rate of less than 1%. The median survival time for limited-stage disease is 20 months, with a five-year survival rate of 20%.

According to data provided by the National Cancer Institute, the median age of incidence of lung cancer is 70 years, and the median age of death by lung cancer 71 years.



Worldwide, lung cancer is the most common cancer in terms of both incidence and mortality with 1.35 million new cases per year and 1.18 million deaths, with the highest rates in Europe and North America. The population segment most likely to develop lung cancer is over-fifties who have a history of smoking. Lung cancer is the second most commonly occurring form of cancer in most western countries, and it is the leading cancer-related cause of death. Although the rate of men dying from lung cancer is declining in western countries, it is actually increasing for women due to the increased takeup of smoking by this group. Among lifetime non-smokers, men have higher age-standardized lung cancer death rates than women.

Not all cases of lung cancer are due to smoking, but the role of passive smoking is increasingly being recognized as a risk factor for lung cancer, leading to policy interventions to decrease undesired exposure of non-smokers to others' tobacco smoke. Emissions from automobiles, factories and power plants also pose potential risks.

Eastern Europe has the highest lung cancer mortality among men, while northern Europe and the U.S. have the highest mortality among women.Lung cancer incidence is currently less common in developing countries. With increased smoking in developing countries, the incidence is expected to increase in the next few years, notably in China and India.

Lung cancer incidence (by country) has an inverse correlation with sunlight and UVB exposure. One possible explanation is a preventative effect of vitamin D (which is produced in the skin on exposure to sunlight).

Lung cancer was extremely rare before the advent of cigarette smoking. Lung cancer was first recognized as a distinct disease in 1761. Different aspects of lung cancer were described further in 1810. Malignant lung tumors made up only 1% of all cancers seen at autopsy in 1878, but had risen to 10â€?5% by the early 1900s. Case reports in the medical literature numbered only 374 worldwide in 1912. A review of autopsies showed that the incidence of lung cancer had increased from 0.3% in 1852 to 5.66% in 1952. In Germany, in 1929 physician Fritz Lickint recognized the link between smoking and lung cancer. This led to an aggressive anti-smoking campaign. The British Doctors Study, published in the 1950s, was the first solid epidemiological evidence of the link between lung cancer and smoking. As a result, in 1964 the Surgeon General of the United States recommended that smokers should stop smoking.

The connection with radon gas was first recognized among miners in the Ore Mountains near Schneeberg, Saxony. Silver has been mined there since 1470. However these mines are rich in uranium, with accompanying radium and radon gas. Miners developed a disproportionate amount of lung disease, eventually recognized as lung cancer in the 1870s. An estimated 75% of former miners died from lung cancer. Despite this discovery, mining continued into the 1950s due to the USSR's demand for uranium.

The first successful pneumonectomy for lung cancer was carried out in 1933. Initially, pneumonectomy was the surgical treatment of choice. However with improvements in cancer staging and surgical techniques, lobectomy with lymph node dissection has now become the treatment of choice.

Palliative radiotherapy has been used since the 1940s. Radical radiotherapy, initially used in the 1950s, was an attempt to use larger radiation doses in patients with relatively early stage lung cancer, but who were otherwise unfit for surgery. In 1997, continuous hyperfractionated accelerated radiotherapy (CHART) was seen as an improvement over conventional radical radiotherapy.

With small cell lung carcinoma, initial attempts in the 1960s at surgical resection and radical radiotherapy were unsuccessful. In the 1970s, successful chemotherapy regimens were developed.













The KatyÅ„ massacre, also the KatyÅ„ Forest massacre (, 'KatyÅ„ crime'), was a mass execution of Polish citizens ordered by Soviet authorities on March 5, 1940. The estimated number of victims is about 22,000, with the most commonly cited number of 21,768.. The victims were murdered in the Katyn forest, the Kalinin (Tver) and Kharkiv prisons and elsewhere. About 8,000 were officers taken prisoner during the 1939 invasion of Poland, the rest being Poles arrested for allegedly being "intelligence agents, gendarmes, spies, saboteurs, landowners, factory owners, lawyers, priests, and officials."

Since Poland's conscription system required every unexempted university graduate to become a reserve officer, the Soviets were able to round up much of the Polish intelligentsia, and the Jewish, Ukrainian, Georgian and Belarusian intelligentsia of Polish citizenship.

Originally, "Katyn massacre" referred to the massacre at Katyn Forest, near the villages of Katyn and Gnezdovo (ca. 19Â km west of Smolensk, Russia), of Polish military officers in the Kozelsk prisoner-of-war camp. It now is applied to the simultaneous executions of POWs from geographically distant Starobelsk and Ostashkov camps, and the executions of political prisoners from West Belarus and West Ukraine, shot on Stalin's orders at Katyn Forest, at the NKVD (Narodny Komissariat Vnutrennikh Del) headquarters in Smolensk, at a Smolensk slaughterhouse, and at prisons in Kalinin (Tver), Kharkiv, Moscow, and other Russian cities.

The 1943 discovery of mass graves at Katyn Forest by Nazi Germany, after invading and occupying the place in 1941, broke diplomatic relations between the U.S.S.R. and the Polish government-in-exile in London. The Soviet Union continued denying the massacres until 1990, then acknowledged that the NKVD had done them and the cover-up. The Russian government admitted Soviet responsibility for the massacres, yet does not classify them as war crimes or as acts of genocide; that would have necessitated the  of surviving perpetrators, which is what the Polish government has requested. It also does not classify the dead as Stalinist repression victims, barring formal posthumous rehabilitation.

On September 17 1939 in violation of Polish-Soviet Non-Aggression Pact the Red Army invaded the territory of Poland from the east. This invasion took place while Poland had already sustained serious defeats in the wake of the German attack on the country that started on September 1 1939 while Great Britain and France bound by Polish-British Common Defence Pact and Franco-Polish Military Alliance to attack Germany in the case such invasion did not take any military action which is referred to as Western betrayal; thus Soviets moved to safeguard their claims in accordance with the Molotov-Ribbentrop Pact. In the wake of the Red Army's quick advance that met little resistance, between 250,000 and 454,700 Polish soldiers had become prisoners and were interned by the Soviets. About 250,000 were set free by the army almost on the spot, while 125,000 were delivered to the internal security services (the NKVD). The NKVD in turn quickly released 42,400 soldiers. The approximately 170,000 released were mostly soldiers of Ukrainian and Belarusian ethnicity serving in the Polish army. The 43,000 soldiers born in West Poland, now under German control, were transferred to the Germans. By November 19 1939, NKVD had about 40,000 Polish POWs: about 8,500 officers and warrant officers, 6,500 police officers and 25,000 soldiers and NCOs who were still being held as POWs.

As early as September 19 1939, the People's Commissar for Internal Affairs and First Rank Commissar of State Security, Lavrenty Beria, ordered the NKVD to create the Administration for Affairs of Prisoners of War and Internees to manage Polish prisoners. The NKVD took custody of Polish prisoners from the Red Army, and proceeded to organize a network of reception centers and transit camps and arrange rail transport to prisoner-of-war camps in the western USSR. The camps were located at Jukhnovo (Babynino rail station), Yuzhe (Talitsy), Kozelsk, Kozelshchyna, Oranki, Ostashkov (Stolbnyi Island on Seliger Lake near Ostashkov), Tyotkino rail station (56Â mi/90Â km from Putyvl), Starobielsk, Vologda (Zaenikevo rail station) and Gryazovets.

Kozelsk and Starobielsk were used mainly for military officers, while Ostashkov was used mainly for Boy Scouts, gendarmes, police officers and prison officers. Prisoners at these camps were not exclusively military officers or members of the other groups mentioned, but also included Polish intelligentsia. The approximate distribution of men throughout the camps was as follows: Kozelsk, 5,000; Ostashkov, 6,570; and Starobelsk, 4,000. They totalled 15,570 men.

Once at the camps, from October 1939 to February 1940, the Poles were subjected to lengthy interrogations and constant political agitation by NKVD officers such as Vasily Zarubin. The Poles were encouraged to believe they would be released, but the interviews were in effect a selection process to determine who would live and who would die. According to NKVD reports, the prisoners could not be induced to adopt a pro-Soviet attitude. They were declared "hardened and uncompromising enemies of Soviet authority."

On March 5 1940, pursuant to a note to Joseph Stalin from Lavrenty Beria, the members of the Soviet Politburo â€?Stalin, Vyacheslav Molotov, Lazar Kaganovich, Mikhail Kalinin, Kliment Voroshilov, Anastas Mikoyan and Beria â€?signed an order to execute 25,700 Polish "nationalists and counterrevolutionaries" kept at camps and prisons in occupied western Ukraine and Belarus.

The reason for the massacre, according to World War II historian Gerhard Weinberg, is that Stalin wanted to deprive a potential future Polish military of a large portion of its military talent: It has been suggested that the motive for this terrible step [the Katyn massacre] was to reassure the Germans as to the reality of Soviet anti-Polish policy. This explanation is completely unconvincing in view of the care with which the Soviet regime kept the massacre secret from the very German government it was supposed to impress... A more likely explanation is that... [the massacre] should be seen as looking forward to a future in which there might again be a Poland on the Soviet Union's western border. Since he intended to keep the eastern portion of the country in any case, Stalin could be certain that any revived Poland would be unfriendly. Under those circumstances, depriving it of a large proportion of its military and technical elite would make it weaker."

Since April 3, 1940, at least 22,436 POWs and prisoners were executed: 15,131 POWs (most or all of them from the three camps) and at least 7,305 prisoners in western parts of Belarus and Ukraine. A 1956 memo from KGB chief Alexander Shelepin to First Secretary Nikita Khrushchev contains incomplete information about the personal files of 21,857 murdered POWs and prisoners. Of them 4,421 were from Kozielsk, 3,820 from Starobielsk, 6,311 from Ostashkov, and 7,305 from Belarusian and Ukrainian prisons. Shelepin's data for prisons should be considered a minimum, because his data for POWs is incomplete (he mentions 14,552 personal files for POWs, while at least 15,131 POWs "sent to UNKVD" are mentioned in contemporary documents).

Those who died at Katyn included an admiral, two generals, 24 colonels, 79 lieutenant colonels, 258 majors, 654 captains, 17 naval captains, 3,420 NCOs, seven chaplains, three landowners, a prince, 43 officials, 85 privates, and 131 refugees. Also among the dead were 20 university professors (including Stefan Kaczmarz); 300 physicians; several hundred lawyers, engineers, and teachers; and more than 100 writers and journalists as well as about 200 pilots. In all, the NKVD executed almost half the Polish officer corps. Altogether, during the massacre the NKVD murdered 14 Polish generals: Leon Billewicz (ret.), BronisÅ‚aw Bohatyrewicz (ret.), Xawery Czernicki (admiral), StanisÅ‚aw Haller (ret.), Aleksander Kowalewski (ret.), Henryk Minkiewicz (ret.), Kazimierz Orlik-Åukoski, Konstanty Plisowski (ret.), Rudolf Prich (murdered in Lviv), Franciszek Sikorski (ret.), Leonard Skierski (ret.), Piotr Skuratowicz, MieczysÅ‚aw SmorawiÅ„ski and Alojzy Wir-Konas (promoted posthumously). A mere 395 prisoners were saved from the slaughter, among them StanisÅ‚aw Swianiewicz and JÃ³zef Czapski. They were taken to the Yukhnov camp and then down to Gryazovets. They were the only ones who escaped death.

Up to 99% of the remaining prisoners were subsequently murdered. People from Kozelsk were murdered in the usual mass murder site of Smolensk country, called Katyn forest; people from Starobilsk were murdered in the inner NKVD prison of Kharkiv and the bodies were buried near Piatykhatky; and police officers from Ostashkov were murdered in the inner NKVD prison of Kalinin (Tver) and buried in Miednoje (Mednoye). 

Detailed information on the executions in the Kalinin NKVD prison was given during the hearing by Dmitrii S. Tokarev, former head of the Board of the District NKVD in Kalinin. According to Tokarev, the shooting started in the evening and ended at dawn. The first transport on April 4, 1940, carried 390 people, and the executioners had a hard time killing so many people during one night. The following transports were no greater than 250 people. The executions were usually performed with German-made Walther PPK pistols supplied by Moscow, but Nagant M1895 revolvers were also used.

The killings were methodical. After the condemned's personal information was checked, he was handcuffed and led to a cell insulated with a felt-lined door. The sounds of the murders were also masked by the operation of loud machines (perhaps fans) throughout the night. After being taken into the cell, the victim was immediately shot in the back of the head. His body was then taken out through the opposite door and laid in one of the five or six waiting trucks, whereupon the next condemned was taken inside. The procedure went on every night, except for the May Day holiday. Near Smolensk, the Poles, with their hands tied behind their backs, were led to the graves and shot in the neck.

After the execution was carried out, there were still more than 22,000 of the former Polish soldiers in NKVD labor camps. According to Beria's report, by November 2 1940 his department had 2 generals, 39 lieutenant-colonels and colonels, 222 captains and majors, 691 lieutenants, 4022 warrant officers and NCOs and 13,321 enlisted men captured during the Polish campaign. Additional 3,300 Polish soldiers were captured during the annexation of Lithuania, where they were kept interned since September 1939.

3,000 to 4,000 Polish inmates of Ukrainian prisons and the ones from Belarus prisons in Kurapaty, were probably buried in Bykivnia. Porucznik Janina Lewandowska, daughter of Gen. JÃ³zef Dowbor-MuÅ›nicki, was the only woman executed during the massacre at Katyn.



The fate of the Polish prisoners' was raised soon after the Nazi Germans invaded the Soviet Union in June 1941, when the Polish government-in-exile and the Soviet government signed the Sikorski-Mayski Agreement for fighting Nazi Germany and form a Polish army on Soviet territory. When the Polish general WÅ‚adysÅ‚aw Anders began organizing this army, he requested information about Polish officers. During a personal meeting, Stalin assured him and WÅ‚adysÅ‚aw Sikorski, the Polish Prime Minister, that all the Poles were freed, and that not all could be accounted because the Soviets "lost track" of them in Manchuria.

In 1942 Polish railroad workers found a mass grave at Katyn, and reported it to the Polish Secret State; the news was ignored; people refused to believe the mass graves contained so many dead. The fate of the missing prisoners remained unknown until April 1943 when the German Wehrmacht (actually Rudolf Christoph Freiherr von Gersdorff) discovered the mass grave of 4,243 Polish military reserve officers in the forest on Goat Hill near Katyn. Joseph Goebbels saw this discovery as an excellent tool to drive a wedge between Poland, Western Allies, and the Soviet Union. On April 13, Berlin Radio broadcast to the world that German military forces in the Katyn forest near Smolensk had uncovered "a ditch ... 28 metres long and 16 metres wide [92Â ft by 52Â ft], in which the bodies of 3,000 Polish officers were piled up in 12 layers." The broadcast went on to charge the Soviets with carrying out the massacre in 1940.

The Germans assembled and brought in a European commission consisting of twelve forensic experts and their staffs from Belgium, Bulgaria, Finland, France, Italy, Croatia, The Netherlands, Romania, Sweden, Slovakia, and Hungary. After the war, all of the experts, save for a Bulgarian and a Czech, reaffirmed their 1943 finding of Soviet guilt.''<ref name="Bauer:>Bauer, Eddy. The Marshall Cavendish Illustrated Encyclopedia of World War II. Marshall Cavendish. 1985</ref>

The Katyn Massacre was beneficial to Nazi Germany, which used it to discredit the Soviet Union. Goebbels wrote in his diary on April 14 1943: "We are now using the discovery of 12,000 Polish officers, murdered by the GPU, for anti-Bolshevik propaganda on a grand style. We sent neutral journalists and Polish intellectuals to the spot where they were found. Their reports now reaching us from ahead are gruesome. The Fuehrer has also given permission for us to hand out a drastic news item to the German press. I gave instructions to make the widest possible use of the propaganda material. We shall be able to live on it for a couple weeks." The Germans had succeeded in discrediting the Soviet Government in the eyes of the world and briefly raised the spectre of a communist monster rampaging across the territories of Western civilization; moreover, General Sikorski's unease threatened to unravel the alliance between the Western Allies and the Soviet Union.

The Soviet government immediately denied the German charges and claimed that the Polish prisoners of war had been engaged in construction work west of Smolensk and consequently were captured and executed by invading German units in August 1941. The Soviet response on April 15 to the German initial broadcast of April 13, prepared by the Soviet Information Bureau, stated that "[...]Polish prisoners-of-war who in 1941 were engaged in country construction work west of Smolensk and who [...] fell into the hands of the German-Fascist hangmen [...]."

The Allies were aware that the Nazis had found a mass grave as the discovery transpired, via radio transmissions intercepted and decrypted by Bletchley Park. Germans and the international commission, which was invited by Germany, investigated the Katyn corpses and soon produced physical evidence that the massacre took place in early 1940, at a time when the area was still under Soviet control.

In April 1943, when the Polish government-in-exile insisted on bringing the matter to the negotiation table with the Soviets and on an investigation by the International Red Cross, Stalin accused the Polish government in exile of collaborating with Nazi Germany, broke diplomatic relations with it, and started a campaign to get the Western Allies to recognize the alternative Polish pro-Soviet government in Moscow led by Wanda Wasilewska. Sikorski, whose uncompromising stance on that issue was beginning to create a rift between the Western Allies and the Soviet Union, died suddenly two months later. The cause of his death is still disputed.



When, in September 1943, Goebbels was informed that the German Army had to withdraw from the Katyn area, he entered a prediction in his diary. His entry for September 29, 1943 reads: "Unfortunately we have had to give up Katyn. The Bolsheviks undoubtedly will soon 'find' that we shot 12,000 Polish officers. That episode is one that is going to cause us quite a little trouble in the future. The Soviets are undoubtedly going to make it their business to discover as many mass graves as possible and then blame it on us."

Indeed, having retaken the Katyn area almost immediately after the Red Army had recaptured Smolensk, the Soviet Union, led by the NKVD, began a cover-up. A cemetery the Germans had permitted the Polish Red Cross to build was destroyed and other evidence removed. In January 1944, the Soviet Union sent the "Special Commission for Determination and Investigation of the Shooting of Polish Prisoners of War by German-Fascist Invaders in Katyn Forest," (U.S.S.R. Spetsial'naya Kommissiya po Ustanovleniyu i Rassledovaniyu Obstoyatel'stv Rasstrela Nemetsko-Fashistskimi Zakhvatchikami v Katynskom Lesu) led (at least nominally) by Alexey Tolstoy to investigate the incidents again. The so-called "Burdenko Commission", headed by Nikolai Burdenko, the President of the Academy of Medical Sciences of the USSR, exhumed the bodies again and reached the conclusion that the shooting was done in 1941, when the Katyn area was under German occupation. No foreign personnel, even the Polish communists, were allowed to join the Burdenko Commission, whereas the Nazi German investigation had allowed wider access to both international press and organizations (like the Red Cross, with experts from Finland, Denmark, Slovakia etc) and even used Polish workers, like JÃ³zef Mackiewicz. Thus, the 'medico-legal experts,' as they were called, 'found out' that all the shootings were done by the 'German-Fascist' invaders. The conclusions of the commission list a number of things, from gold watches to briefs and icons allegedly found attached to the dead bodies, and the items were said to have dates from November 1940 to June 1941, thus 'rebutting' the 'Fascist lies' of the Poles being shot by the Soviets. The report can be found in pro-Soviet publication Supplement to Russia at war weekly (1944); it is also printed in Dr.Joachim Hoffmann's book Stalin's Annihilation War 1941â€?945 (original: Stalins Vernichtungskrieg 1941â€?945)

The Western Allies had an implicit, if unwilling, hand in the cover-up in their endeavour not to antagonise a then-ally, the Soviet Union. The resulting Polish-Soviet crisis was beginning to threaten the vital alliance with the Soviet Union at a time when the Poles' importance to the Allies, essential in the first years of the war, was beginning to fade, due to the entry into the conflict of the military and industrial giants, the Soviet Union and the United States. In retrospective review of records, it is clear that both British Prime Minister Winston Churchill and U.S. President Franklin D. Roosevelt were increasingly torn between their commitments to their Polish ally, the uncompromising stance of Sikorski and the demands by Stalin and his diplomats.

In private, Churchill agreed that the atrocity was likely carried out by the Soviets. According to the notes taken by Count RaczyÅ„ski, Churchill admitted on April 15, 1943 during a conversation with General Sikorski: "Alas, the German revelations are probably true. The Bolsheviks can be very cruel." However, at the same time, on April 24, 1943 Churchill assured the Soviets: "We shall certainly oppose vigorously any 'investigation' by the International Red Cross or any other body in any territory under German authority. Such investigation would be a fraud and its conclusions reached by terrorism." Unofficial or classified UK documents concluded that Soviet guilt was a "near certainty", but the alliance with the Soviets was deemed to be more important than moral issues, thus the official version supported the Soviet version, up to censoring the contradictory accounts. Churchill's own post-war account of the Katyn affair is laconic. In his memoirs, he quotes the 1944 Soviet inquiry into the massacre, which predictably found that the Germans had committed the crime, and adds, "belief seems an act of faith." In 1943 the Katyn Manifesto blaming the Soviet Union was published in London (in English) by the eccentric poet Count Geoffrey Potocki de Montalk, who was arrested by the Special Branch and imprisoned.

In the United States, a similar line was taken, notwithstanding that two official intelligence reports into the Katyn massacre were produced that contradicted the official position. In 1944 Roosevelt assigned Navy Lieutenant Commander George Earle, his special emissary to the Balkans, to compile information on Katyn, which he did using contacts in Bulgaria and Romania. He concluded that the Soviet Union had committed the massacre. After consulting with Elmer Davis, the director of the Office of War Information, Roosevelt rejected that conclusion, saying that he was convinced of Nazi Germany's responsibility, and ordered Earle's report suppressed. When Earle formally requested permission to publish his findings, the President gave him a written order to desist. Earle was reassigned and spent the rest of the war in American Samoa.

A further report in 1945, supporting the same conclusion, was produced and stifled. In 1943, two US POWs â€?Lt. Col. Donald B. Stewart and Col. John H. Van Vliet â€?had been taken by Germans to Katyn in 1943 for an international news conference. Later, in 1945, Van Vliet wrote a report concluding that the Soviets, not the Germans, were responsible. He gave the report to Maj. Gen. Clayton Bissell, Gen. George Marshall's assistant chief of staff for intelligence, who destroyed it. During the 1951â€?952 investigation, Bissell defended his action before Congress, contending that it was not in the US interest to embarrass an ally whose forces were still needed to defeat Japan.

From December 28 1945 to January 4 1946, seven servicemen of the German Wehrmacht were tried by a Soviet military court in Leningrad. One of them, Arno Diere, was charged with helping to dig the Katyn graves during the execution. Diere, who was also accused of shooting people with machine-gun in Soviet villages, "confessed" to having taken part in burial (though not the execution) of 15-20 thousand Polish POWs in Katyn. For this he was spared the execution and was given 15 years of hard labor. His confession was full of absurdities, and thus he was not used as a Soviet prosecution witness during the Nuremberg trial. In a November 29, 1954 note he recanted his confession, claiming that he was forced to confess by the investigators. Contrary to claims on several "revisionist" sites, of all the accused during the Leningrad Trial, only Diere was accused to have had a connection to the Katyn massacre. 

In 1946, the chief Soviet prosecutor at the Nuremberg Trials, Roman A. Rudenko, tried to indict Germany for the Katyn killings, stating that "one of the most important criminal acts for which the major war criminals are responsible was the mass execution of Polish prisoners of war shot in the Katyn forest near Smolensk by the German fascist invaders", but dropped the matter after the United States and United Kingdom refused to support it and German lawyers mounted an embarrassing defense.

However, the problem to be addressed by the court was not to allot the responsibility for the massacre to Germany or the Soviet Union, but to attribute the crime to at least one of the twenty-four dignitaries of the Nazi state. The task of the charge was thus to establish a link between the reproached acts and the defendants. On hearings, however, the Soviet prosecutor proved to be unable to name the person in charge for the execution of the massacre, as well as the supposed guilty among the defendants.

In spite of this bankruptcy of the charge, Nikitchenko tried to make pass in force the Soviet point of view and did not hesitate to claim the inadequacy of the statutes of the court. This failed and the name of Katyn did not appear in the verdict.

In 1951â€?2, in the background of the Korean War, a U.S. Congressional investigation chaired by Rep. Ray J. Madden and known as the Madden Committee investigated the Katyn massacre. It charged that the Poles had been killed by the Soviets and recommended that the Soviets be tried before the International Court of Justice. The committee was however less conclusive on the issue of alleged American cover up.

The question of responsibility remained controversial in the West as well as behind the Iron Curtain. For example, in the United Kingdom in the late 1970s, plans for a memorial to the victims bearing the date 1940 (rather than 1941) were condemned as provocative in the political climate of the Cold War.

It has been sometimes speculated that the choice made in 1969 for the location of the BSSR's war memorial at the former Belarusian village named Khatyn, a site of a 1943 Nazi massacre in which the entire village with its whole population was burned, have been made to cause confusion with Katyn. The two names are similar or identical in many languages, and were in fact often confused.

In Poland Communist authorities covered up the matter in concord with Soviet propaganda, deliberately censoring any sources that might shed some light on the Soviet crime. Katyn was a forbidden topic in postwar Poland. Not only did government censorship suppress all references to it, but even mentioning the atrocity was dangerous. Katyn became erased from Poland's official history, but it could not be erased from historical memory. In 1981, Polish trade union Solidarity erected a memorial with the simple inscription "Katyn, 1940" but it was confiscated by the police, to be replaced with an official monument "To the Polish soldiers â€?victims of Hitlerite fascism â€?reposing in the soil of Katyn". Nevertheless, every year on Zaduszki, similar memorial crosses were erected at PowÄ…zki cemetery and numerous other places in Poland, only to be dismantled by the police overnight. The Katyn subject remained a political taboo in Poland until the fall of the Eastern bloc in 1989.

From the late 1980s, pressure was put not only on the Polish government, but on the Soviet one as well. Polish academics tried to include Katyn in the agenda of the 1987 joint Polish-Soviet commission to investigate censored episodes of the Polish-Russian history. In 1989 Soviet scholars revealed that Joseph Stalin had indeed ordered the massacre, and in 1990 Mikhail Gorbachev admitted that the NKVD had executed the Poles and confirmed two other burial sites similar to the site at Katyn: Mednoje and Pyatikhatki.

On 30 October 1989, Gorbachev allowed a delegation of several hundred Poles, organized by a Polish association named Families of KatyÅ„ Victims, to visit the Katyn memorial. This group included former U.S. national security advisor Zbigniew Brzezinski. A mass was held and banners hailing the Solidarity movement were laid. One mourner affixed a sign reading "NKVD" on the memorial, covering the word "Nazis" in the inscription such that it read "In memory of Polish officers murdered by the NKVD in 1941." Several visitors scaled the fence of a nearby KGB compound and left burning candles on the grounds. Brzezinski commented that:

Brzezinski further stated that "The fact that the Soviet government has enabled me to be here â€?and the Soviets know my views â€?is symbolic of the breach with Stalinism that perestroika represents." His remarks were given extensive coverage on Soviet television. At the ceremony he placed a bouquet of red roses bearing a handwritten message penned in both Polish and English: "For the victims of Stalin and the NKVD. Zbigniew Brzezinski."

On 13 April 1990, the forty-seventh anniversary of the discovery of the mass graves, the USSR formally expressed "profound regret" and admitted Soviet secret police responsibility. That day is also an International Day of Katyn Victims Memorial (Åšwiatowy DzieÅ„ PamiÄ™ci Ofiar Katynia).

After Poles and Americans discovered further evidence in 1991 and 1992, Russian President Boris Yeltsin released and transferred to the new Polish president, former Solidarity leader Lech WaÅ‚Ä™sa, top-secret documents from the sealed package no. 1. Among the documents included Lavrenty Beria's March 1940 proposal to shoot 25,700 Poles from Kozelsk, Ostashkov and Starobels camps, and from certain prisons of Western Ukraine and Belarus with the signature of Stalin (among others); an excerpt from the Politburo shooting order of March 5 1940; and Aleksandr Shelepin's March 3 1959 note to Nikita Khrushchev, with information about the execution of 21,857 Poles and with the proposal to destroy their personal files.



The investigations that indicted the German state rather than the Soviet state for the killings are sometimes used to impeach the Nuremberg Trials in their entirety, often in support of Holocaust denial, or to question the legitimacy and/or wisdom of using the criminal law to prohibit Holocaust denial. Still, there are some who deny Soviet guilt, call the released documents fakes, and try to prove that Poles were shot by Germans in 1941.

On the opposing sides there are allegations that the massacre was part of wider action coordinated by Nazi Germany and Soviet Union, or that Germans at least knew of Katyn beforehand. The reason for these allegations is that Soviet Union and Nazi Germany added on 28 September, a secret supplementary protocol to the German-Soviet Boundary and Friendship Treaty, in which is stated

after which in 1939â€?940 a series of conferences by NKVD and the Gestapo were organised in the town of Zakopane. The aim of these conferences was to coordinate the killing and the deportation policy and exchange experience. A University of Cambridge professor of history George Watson believes that the fate of Polish prisoners was discussed at the conference. This theory surfaces in Polish media, where it is also pointed out that similar massacre of Polish elites (German AB-Aktion operation in Poland) were taking place in the exact time and with similar methods in German occupied Poland.

In June 1998, Yeltsin and Polish President Aleksander KwaÅ›niewski agreed to construct memorial complexes at Katyn and Mednoye, the two NKVD execution sites on Russian soil. However, in September of that year the Russians also raised the issue of Soviet POW deaths in the Camps for Russian prisoners and internees in Poland (1919-1924). About 16,000 to 20,000 POWs died in those camps due to communicable diseases; however, some Russian officials argued that it was 'a genocide comparable to KatyÅ„'. A similar claim was raised in 1994; such attempts are seen by some, particularly in Poland, as a highly provocative Russian attempt to create an 'anti-Katyn' and 'balance the historical equation'.

During KwaÅ›niewski's visit to Russia in September 2004, Russian officials announced that they are willing to transfer all the information on the Katyn Massacre to the Polish authorities as soon as it is declassified. In March 2005 Russian authorities ended the decade-long investigation with no one charged. Russian Chief Military Prosecutor Alexander Savenkov claimed that out of 14,542 Polish citizens from three Soviet camps who had been sentenced to death, only the deaths of 1,803 were confirmed absolutely. He did not address the fate of about 7,000 victims who had been not in POW camps, but in prisons. Savenkov declared that the massacre was not a genocide, a war crime, or a crime against humanity, but a military crime for which the 50-year term of limitation has expired and that consequently there is absolutely no basis to talk about this in judicial terms.  Despite earlier declarations, President Vladimir Putin's government refused to allow Polish investigators to travel to Moscow in late 2004 and 116 out of 183 volumes of files gathered during the Russian investigation, as well as the decision to put an end to it, were classified. In late 2007 and early 2008 several Russian newspapers, including Rossiyskaya Gazeta, Komsomolskaya Pravda and Nezavisimaya Gazeta printed stories that implicated the Nazis for the crime, spurning concern that this was done with the tacit approval of the Kremlin. 

Because of that, the Polish Institute of National Remembrance has decided to open its own investigation. Prosecution team head Leon Kieres said they would try to identify those involved in ordering and carrying out the killings. In addition, on March 22 2005 the Polish Sejm unanimously passed an act, requesting the Russian archives to be declassified. The Sejm also requested Russia to classify the Katyn massacre as the crime of genocide: "On the 65th anniversary of the Katyn murder the Senate pays tribute to the murdered, best sons of the homeland and those who fought for the truth about the murder to come to light, also the Russians who fought for the truth, despite harassment and persecution" â€?the resolution said. The resolution stressed that the authorities of Russia "seek to diminish the burden of this crime by refusing to acknowledge it was genocide and refuse to give access to the records of the investigation into the issue, making it difficult to determine the whole truth about the murder and its perpetrators."



Russia and Poland remained divided on the legal qualification of the Katyn crime, with the Poles considering it a case of genocide and demanding further investigations, as well as complete disclosure of Soviet documents.

The Katyn massacre is a major plot element in many works of culture, for example, in the W.E.B. Griffin novel The Lieutenants which is part of the Brotherhood of War series, as well as in the novel and film Enigma. Polish poet Jacek Kaczmarski has dedicated one of his sung poems to this event.

StanisÅ‚aw Szukalski polish painter and sculptor has dedicated some of his works to this event.

The Academy Award winning Polish film director Andrzej Wajda , whose father, Captain Jakub Wajda, was murdered in the NKVD prison of Kharkiv, has made a film depicting the event, called simply Katyn. The film recounts the fate of some of the womenâ€”mothers, wives and daughtersâ€”of the Polish officers slaughtered by the Soviets. Some Katyn Forest scenes are re-enacted. The screenplay is based on Andrzej Mularczyk's book Post mortem - the Katyn story. The film was produced by Akson Studio, and released in Poland on 21 September 2007. The film has been selected as the Polish entry for the best foreign language film for the 2008 Oscars.

A golden statue, known as the National Katyn Massacre Memorial, is located in Baltimore, Maryland at Inner Harbor East. 

At Exchange Place on the Hudson River in Jersey City, New Jersey is a statue which comemorates the massacre.

Authenticated copies of Soviet documents related to the Katyn massacre (second paper is an execution order signed by Stalin, Vyacheslav Molotov, Kliment Voroshilov, Anastas Mikoyan, Mikhail Kalinin and Lazar Kaganovich)

















Uranus () is the seventh planet from the Sun and the third-largest and fourth-most massive planet in the solar system. It is named after the ancient Greek deity of the sky (Uranus, ), the father of Kronos (Saturn) and grandfather of Zeus (Jupiter). Uranus was the first planet discovered in modern times. Though it is visible to the naked eye like the five classical planets, it was never recognized as a planet by ancient observers due to its dimness. Sir William Herschel announced its discovery on March 13, 1781, expanding the known boundaries of the solar system for the first time in modern history. This was also the first discovery of a planet made using a telescope.

Uranus and Neptune have internal and atmospheric compositions different from those of the larger gas giants Jupiter and Saturn. As such, astronomers sometimes place them in a separate category, the "ice giants". Uranus' atmosphere, while similar to Jupiter and Saturn in being composed primarily of hydrogen and helium, contains a higher proportion of "ices" such as water, ammonia and methane, along with the usual traces of hydrocarbons. It is the coldest planetary atmosphere in the Solar System, with a minimum temperature of 49Â K (âˆ?24Â Â°C). It has a complex, layered cloud structure, with water thought to make up the lowest clouds, and methane thought to make up the uppermost layer of clouds. 

Like the other giant planets, Uranus has a ring system, a magnetosphere, and numerous moons. The Uranian system has a unique configuration among the planets because its axis of rotation is tilted sideways, nearly into the plane of its revolution about the Sun; its north and south poles lie where most other planets have their equators. Seen from Earth, Uranus' rings can appear to circle the planet like an archery target and its moons revolve around it like the hands of a clock, though in 2007 and 2008 the rings appear edge-on. In 1986, images from Voyager 2 showed Uranus as a virtually featureless planet in visible light without the cloud bands or storms associated with the other giants. However, terrestrial observers have seen signs of seasonal change and increased weather activity in recent years as Uranus approached its equinox. The wind speeds on Uranus can reach 250Â meters per second.

Uranus had been observed on many occasions prior to its discovery as a planet, but it was generally mistaken for a star. The earliest recorded sighting was in 1690 when John Flamsteed catalogued Uranus as 34 Tauri and observed it at least six times. The French astronomer, Pierre Lemonnier, observed Uranus at least twelve times between 1750 and 1769, including on four consecutive nights.

Sir William Herschel observed the planet on 13 March 1781 while in the garden of his house at 19 New King Street in the town of Bath, Somerset (now the Herschel Museum of Astronomy), but initially reported it (on 26 April 1781) as a "comet". Herschel "engaged in a series of observations on the parallax of the fixed stars", using a telescope of his own design.

He recorded in his journal "In the quartile near Î¶ Tauri â€?either [a] Nebulous star or perhaps a comet". On March 17, he noted, "I looked for the Comet or Nebulous Star and found that it is a Comet, for it has changed its place". When he presented his discovery to the Royal Society, he continued to assert that he had found a comet while also implicitly comparing it to a planet:



Herschel notified the Astronomer Royal, Nevil Maskelyne, of his discovery and received this flummoxed reply from him on April 23: "I don't know what to call it. It is as likely to be a regular planet moving in an orbit nearly circular to the sun as a Comet moving in a very eccentric ellipsis. I have not yet seen any coma or tail to it".

While Herschel continued to cautiously describe his new object as a comet, other astronomers had already begun to suspect otherwise. Russian astronomer Anders Johan Lexell estimated its distance as 18 times the distance of the Sun from the Earth, and no comet had yet been observed with a perihelion of even four times the Earthâ€“Sun distance. Berlin astronomer Johann Elert Bode described Herschel's discovery as "a moving star that can be deemed a hitherto unknown planet-like object circulating beyond the orbit of Saturn". Bode concluded that its near-circular orbit was more like a planet than a comet.

The object was soon universally accepted as a new planet. By 1783, Herschel himself acknowledged this fact to Royal Society president Joseph Banks: "By the observation of the most eminent Astronomers in Europe it appears that the new star, which I had the honour of pointing out to them in March 1781, is a Primary Planet of our Solar System." In recognition of his achievement, King George III gave Herschel an annual stipend of Â£200 on the condition that he move to Windsor so the Royal Family could have a chance to look through his telescopes. 

Maskelyne asked Herschel to "do the astronomical world the faver [sic] to give a name to your planet, which is entirely your own, & which we are so much obliged to you for the discovery of." In response to Maskelyne's request, Herschel decided to name the object Georgium Sidus (George's Star), or the "Georgian Planet" in honour of his new patron, King George III. He explained this decision in a letter to Joseph Banks:



Astronomer JÃ©rÃ´me Lalande proposed the planet be named Herschel in honour of its discoverer. Bode, however, opted for Uranus, the Latinized version of the Greek god of the sky, Ouranos. Bode argued that just as Saturn was the father of Jupiter, the new planet should be named after the father of Saturn. The earliest citation of the name Uranus in an official publication is in 1823, a year after Herschel's death. The name Georgium Sidus or "the Georgian" was still used infrequently (by the British alone) for some time thereafter; the final holdout was HM Nautical Almanac Office, which did not switch to Uranus until 1850.

The preferred pronunciation of the name Uranus among astronomers is , with the first syllable stressed and a short a; this is more classically correct than the alternate, with stress on the second syllable and a "long a" ,which is often used in the English-speaking world.

Uranus is the only planet whose name is derived from a figure from Greek mythology rather than Roman mythology. (The Roman equivalent would have been Caelus.) The adjective of Uranus is "Uranian". The element uranium, discovered in 1789, was named in its honour by its discoverer, Martin Klaproth.

Its astronomical symbol is . It is a hybrid of the symbols for Mars and the Sun because Uranus was the Sky in Greek mythology, which was thought to be dominated by the combined powers of the Sun and Mars. Its astrological symbol is , suggested by Lalande in 1784. In a letter to Herschel, Lalande described it as "un globe surmontÃ© par la premiÃ¨re lettre de votre nom" ("a globe surmounted by the first letter of your name"). In the Chinese, Japanese, Korean, and Vietnamese languages, the planet's name is literally translated as the sky king star (å¤©çŽ‹æ˜?.

Uranus revolves around the Sun once every 84 Earth years. Its average distance from the Sun is roughly 3 billionÂ km (about 20 AU). The intensity of sunlight on Uranus is about 1/400 that of Earth. Its orbital elements were first calculated in 1783 by Pierre-Simon Laplace. With time, discrepancies began to appear between the predicted and observed orbits, and in 1841, John Couch Adams first proposed that the differences might be due to the gravitational tug of an unseen planet. In 1845, Urbain Le Verrier began his own independent research into Uranus' orbit. On September 23, 1846, Johann Gottfried Galle located a new planet, later named Neptune, at nearly the position predicted by Le Verrier.

The rotational period of the interior of Uranus is 17Â hours, 14Â minutes. However, as on all giant planets, its upper atmosphere experiences very strong winds in the direction of rotation. In effect, at some latitudes, such as about two-thirds of the way from the equator to the south pole, visible features of the atmosphere move much faster, making a full rotation in as little as 14 hours.

Uranus' axis of rotation lies on its side with respect to the plane of the solar system, with an axial tilt of 98 degrees. This makes its exchange of seasons completely unlike those of the other major planets. Other planets can be visualized to rotate like tilted spinning tops relative to the plane of the solar system, while Uranus rotates more like a tilted rolling ball. Near the time of Uranian solstices, one pole faces the Sun continually while the other pole faces away. Only a narrow strip around the equator experiences a rapid day-night cycle, but with the Sun very low over the horizon as in the Earth's polar regions. At the other side of Uranus' orbit the orientation of the poles towards the Sun is reversed. Each pole gets around 42Â years of continuous sunlight, followed by 42Â years of darkness. Near the time of the equinoxes, the Sun faces the equator of Uranus giving a period of day-night cycles similar to those seen on most of the other planets. Uranus reached its most recent equinox on 7 December 2007.

One result of this axis orientation is that, on average during the year, the polar regions of Uranus receive a greater energy input from the Sun than its equatorial regions. Nevertheless, Uranus is hotter at its equator than at its poles. The underlying mechanism which causes this is unknown. The reason for Uranus' unusual axial tilt is also not known with certainty, but the usual speculation is that during the formation of the Solar System, an Earth sized protoplanet collided with Uranus, causing the skewed orientation. Uranus' south pole was pointed almost directly at the Sun at the time of Voyager 2's flyby in 1986. The labeling of this pole as "south" uses the definition currently endorsed by the International Astronomical Union, namely that the north pole of a planet or satellite shall be the pole which points above the invariable plane of the solar system, regardless of the direction the planet is spinning. However, a different convention is sometimes used, where a body's north and south poles are defined according to the right-hand rule in relation to the direction of rotation. In terms of this latter coordinate system it was Uranus' north pole which was in sunlight in 1986. Astronomer Patrick Moore, commenting on the issue, summed it up by saying "Take your pick!"

From 1995 to 2006, Uranus' apparent magnitude fluctuated between +5.6 and +5.9, placing it just within the limit of naked eye visibility at +6.5. Its angular diameter is between 3.4 and 3.7Â arcseconds, compared with 16 to 20Â arcseconds for Saturn and 32 to 45Â arcseconds for Jupiter. At opposition, Uranus is visible to the naked eye in dark, un-light polluted skies, and becomes an easy target even in urban conditions with binoculars. In larger amateur telescopes with an objective diameter of between 15 and 23Â cm, the planet appears as a pale cyan disk with distinct limb darkening. With a large telescope of 25Â cm or wider, cloud patterns, as well as some of the larger satellites, such as Titania and Oberon, may be visible.



Uranus' mass is roughly 14.5Â times that of the Earth, making it the least massive of the giant planets, while its density of 1.27 g/cmÂ³ makes it the second least dense planet, after Saturn. Though having a diameter similar to Neptune (roughly four times Earth's), it is less massive. These values indicate that it is made primarily of various ices, such as water, ammonia, and methane. The total mass of ice in Uranus' interior is not precisely known, as different figures emerge depending on the model chosen; however, it must be between 9.3Â and 13.5Â Earth masses. Hydrogen and helium constitute only a small part of the total, with between 0.5 and 1.5 Earth masses. The remainder of the mass (0.5 to 3.7Â Earth masses) is accounted for by rocky material.

The standard model of Uranus' structure is that it consists of three layers: a rocky core in the center, an icy mantle in the middle and an outer gaseous hydrogen/helium envelope. The core is relatively small, with a mass of only 0.55Â Earth masses and a radius less than 20Â percent Uranus'; the mantle comprises the bulk of the planet, with around 13.4Â Earth masses, while the upper atmosphere is relatively insubstantial, weighing about 0.5Â Earth masses and extending for the last 20Â percent of Uranus' radius. Uranus' core density is around 9Â g/cmÂ³, with a pressure at the core/mantle boundary of 8Â millionÂ bars (800 GPa) and a temperature of about 5000Â K. The ice mantle is not in fact composed of ice in the conventional sense, but of a hot and dense fluid consisting of water, ammonia and other volatiles. This fluid, which has a high electrical conductivity, is sometimes called a waterâ€“ammonia ocean. The bulk compositions of Uranus and Neptune are very different from those of Jupiter and Saturn, with ice dominating over gases, hence justifying their separate classification as ice giants.

While the model considered above is more or less standard, it is not unique; other models also satisfy observations. For instance, if substantial amounts of hydrogen and rocky material are mixed in the ice mantle, the total mass of ices in the interior will be lower, and, correspondingly, the total mass of rocks and hydrogen will be higher. Presently available data does not allow us to determine which model is correct. The fluid interior structure of Uranus means that it has no solid surface. The gaseous atmosphere gradually transitions into the internal liquid layers. However for the sake of convenience an oblate spheroid of revolution, where pressure equals 1Â bar (100 kPa), is designated conditionally as a â€˜surfaceâ€? It has equatorial and polar radii of 25,559Â Â±Â 4 and 24,973Â Â±Â 20Â km, respectively. This surface will be used throughout this article as a zero point for altitudes.

Uranus' internal heat appears markedly lower than that of the other giant planets; in astronomical terms, it has a low thermal flux. Why Uranus' internal temperature is so low is still not understood. Neptune, which is Uranus' near twin in size and composition, radiates 2.61 times as much energy into space as it receives from the Sun. Uranus, by contrast, radiates hardly any excess heat at all. The total power radiated by Uranus in the far infrared (i.e. heat) part of the spectrum is  times the solar energy absorbed in its atmosphere. In fact, Uranus' heat flux is only Â W/mÂ², which is lower than the internal heat flux of Earth of about 0.075Â W/mÂ². The lowest temperature recorded in Uranus' tropopause is 49Â K (âˆ?24 Â°C), making Uranus the coldest planet in the Solar System.

Hypotheses for this discrepancy include that when Uranus was "knocked over" by the supermassive impactor which caused its extreme axial tilt, the event also caused it to expel most of its primordial heat, leaving it with a depleted core temperature. Another hypothesis is that some form of barrier exists in Uranus' upper layers which prevents the core's heat from reaching the surface. For example, convection may take place in a set of compositionally different layers, which may inhibit the upward heat transport.

Although there is no well-defined solid surface within Uranus' interior, the outermost part of Uranus' gaseous envelope that is accessible to remote sensing is called its atmosphere. Remote sensing capability extends down to roughly 300Â km below the 1 bar (100 kPa) level, with a corresponding pressure around 100Â bar (10 MPa) and temperature of 320Â K. The tenuous corona of the atmosphere extends remarkably over two planetary radii from the nominal surface at 1 bar pressure. The Uranian atmosphere can be divided into three layers: the troposphere, between altitudes of âˆ?00 and 50Â km and pressures from 100 to 0.1Â bar; (10 MPa to 10 kPa) the stratosphere, spanning altitudes between 50 and 4000Â km and pressures of between  (10 kPa to 10 ÂµPa)and the thermosphere/corona extending from 4,000Â km to as high as 50,000Â km from the surface. There is no mesosphere.

The composition of the Uranian atmosphere is different from the composition of Uranus as a whole, consisting as it does mainly of molecular hydrogen and helium. The helium molar fraction, i.e. the number of helium atoms per molecule of gas, is  in the upper troposphere, which corresponds to a mass fraction . This value is very close to the protosolar helium mass fraction of , indicating that helium has not settled in the center of the planet as it has in the gas giants. The third most abundant constituent of the Uranian atmosphere is methane . Methane possesses prominent absorption bands in the visible and near-infrared (IR) making Uranus aquamarine or cyan in color. Methane molecules account for 2.3% of the atmosphere by molar fraction below the methane cloud deck at the pressure level of 1.3Â bar (130 kPa); this represents about 20 to 30 times the carbon abundance found in the Sun. The mixing ratio is much lower in the upper atmosphere due to its extremely low temperature, which lowers the saturation level and causes excess methane to freeze out. The abundances of less volatile compounds such as ammonia, water and hydrogen sulfide in the deep atmosphere are poorly known. However they are probably also higher than solar values. In addition to methane, trace amounts of various hydrocarbons are found in the upper atmosphere of Uranus, which are thought to be produced from methane by photolysis induced by the solar ultraviolet (UV) radiation. They include ethane , acetylene , methylacetylene , diacetylene . Spectroscopy has also uncovered traces of water vapor, carbon monoxide and carbon dioxide in the upper atmosphere, which can only originate from an external source such as infalling dust and comets.

The troposphere is the lowest and densest part of the atmosphere and is characterized by a decrease in temperature with altitude. The temperature falls from about 320Â K at the base of the nominal troposphere at âˆ?00Â km to 53Â K at 50Â km. The temperatures in the coldest upper region of the troposphere (the tropopause) actually vary in the range between 49 and 57Â K depending on planetary latitude. The tropopause region is responsible for the vast majority of the planetâ€™s thermal far infrared emissions, thus determining its effective temperature of 59.1Â Â±Â 0.3Â K.

The troposphere is believed to possess a highly complex cloud structure; water clouds are hypothesised to lie in the pressure range of  (5 to 10 MPa), ammonium hydrosulfide clouds in the range of  (2 to 4 MPa), ammonia or hydrogen sulfide clouds at between 3 and 10Â bar (0.3 to 1 MPa) and finally directly detected thin methane clouds at  (0.1 to 0.2 MPa). The troposphere is a very dynamic part of the atmosphere, exhibiting strong winds, bright clouds and seasonal changes, which will be discussed below.

The middle layer of the Uranian atmosphere is the stratosphere, where temperature generally increases with altitude from 53Â K in the tropopause to between 800 and 850Â K at the base of the thermosphere. The heating of the stratosphere is caused by absorption of solar UV and IR radiation by methane and other hydrocarbons that form in this part of the atmosphere as a result of methane photolysis. Heating from the hot thermosphere may also be significant. The hydrocarbons occupy a relatively narrow layer at altitudes of between 100 and 280Â km corresponding to a pressure range of 10 to 0.1Â mbar (1000 to 10 kPa) and temperatures of between 75 and 170Â K. The most abundant hydrocarbons are acetylene and ethane with mixing ratios of around  relative to hydrogen, which is similar to the mixing ratios of methane and carbon monoxide at these altitudes. Heavier hydrocarbons and carbon dioxide have mixing ratios three orders of magnitude lower. The abudance ratio of water is around 7. Ethane and acetylene tend to condense in the colder lower part of stratosphere and tropopause forming haze layers, which may be partly responsible for the bland appearance of Uranus. However, the concentration of hydrocarbons in the Uranian stratosphere above the haze is significantly lower than in the stratospheres of the other giant planets.

The outermost layer of the Uranian atmosphere is the thermosphere and corona, which has a uniform temperature around 800 to 850Â K. The heat sources necessary to sustain such a high value are not understood, since neither solar far UV and extreme UV radiation nor auroral activity can provide the necessary energy, although weak cooling efficiency due to the lack of hydrocarbons in the upper part of the stratosphere may also contribute. In addition to molecular hydrogen, the thermosphere-corona contains a large proportion of free hydrogen atoms. Their small molecular mass together with the high temperatures may help to explain why the corona extends as far as 50,000Â km or two Uranian radii from the planet. This extended corona is a unique feature of Uranus. Its effects include a drag on small particles orbiting Uranus, causing a general depletion of dust in the Uranian rings. The Uranian thermosphere, together with the upper part of the stratosphere, corresponds to the ionosphere of Uranus. Observations show that the ionosphere occupies altitudes from 2,000Â to 10,000Â km. The Uranian ionosphere is denser than that of either Saturn or Neptune, which may arise from the low concentration of hydrocarbons in the stratosphere. The ionosphere is mainly sustained by solar UV radiation and its density depends on the solar activity. Auroral activity is not as significant as at Jupiter and Saturn.

 has a faint planetary ring system, composed of dark particulate matter up to ten meters in diameter. It was the second ring system to be discovered in the Solar System after Saturn's. Thirteen distinct rings are presently known, the brightest being the epsilon ring. Uranusâ€?rings are probably quite young; gaps in their circumference as well as differences in their opacity suggest that they did not form with Uranus. The matter in the rings may once have been part of a moon which was shattered by a high-speed impact or tidal forces.

William Herschel claimed to have seen rings at Uranus in 1789 (see below), however this is doubtful as in the two following centuries no rings were noted by other observers. The ring system was definitively discovered on March 10, 1977 by James L. Elliot, Edward W. Dunham, and Douglas J. Mink using the Kuiper Airborne Observatory. The discovery was serendipitous; they planned to use the occultation of the star SAO 158687 by Uranus to study the planet's atmosphere. However, when their observations were analyzed, they found that the star had disappeared briefly from view five times both before and after it disappeared behind the planet. They concluded that there must be a ring system around the planet. The rings were directly imaged when Voyager 2 passed Uranus in 1986. Voyager 2 also discovered two additional faint rings bringing the total number to eleven.

In December 2005, the Hubble Space Telescope detected a pair of previously unknown rings. The largest is located at twice the distance from the planet of the previously known rings. These new rings are so far from the planet that they are being called the "outer" ring system. Hubble also spotted two small satellites, one of which, Mab, shares its orbit with the outermost newly discovered ring. The new rings bring the total number of Uranian rings to 13. In April 2006, images of the new rings with the Keck Observatory yielded the colours of the outer rings: the outermost is blue and the other red.One hypothesis concerning the outer ring's blue colour is that it is composed of minute particles of water ice from the surface of Mab that are small enough to scatter blue light. The planet's inner rings appear grey.Regarding William Herschel's observations in the 18th century, the first mention of a Uranian ring system comes from his notes detailing his observations of Uranus, which include the following passage: "February 22, 1789: A ring was suspected".Herschel drew a small diagram of the ring and noted that it was "a little inclined to the red". The Keck Telescope in Hawaii has since confirmed this to be the case. Herschel's notes were published in a Royal Society journal in 1797. However, in the two centuries between 1797 and 1977 the rings are rarely mentioned, if at all. This casts serious doubt whether Herschel could have seen anything of the sort while hundreds of other astronomers saw nothing. Still, it has been claimed by some that Herschel actually gave accurate descriptions of the ring's size relative to Uranus, its changes as Uranus travelled around the Sun, and its colour.

Prior to the arrival of Voyager 2, no measurements of the Uranian magnetosphere had been taken, so its nature remained a mystery. Before 1986, astronomers had expected the magnetic field of Uranus to be in line with the solar wind, since it would then align with the planet's poles that lie in the ecliptic. 

Voyager's observations revealed that the magnetic field is peculiar, both because it does not originate from the planet's geometric center, and because it is tilted at 59Â° from the axis of rotation. In fact the magnetic dipole is shifted from the center of the planet towards the south rotational pole by as much as one third of the planetary radius. This unusual geometry results in a highly asymmetric magnetosphere, where the magnetic field strength on the surface in the southern hemisphere can be as low as 0.1Â gauss (10 ÂµT), whereas in the northern hemisphere it can be as high 1.1Â gauss (110 ÂµT). The average field at the surface is 0.23Â gauss (23 ÂµT). In comparison, the magnetic field of Earth is roughly as strong at either pole, and its "magnetic equator" is roughly parallel with its physical equator. The dipole moment of Uranus is 50 times that of Earth. Neptune has a similarly displaced and tilted magnetic field, suggesting that this may be a common feature of ice giants. One hypothesis is that, unlike the magnetic fields of the terrestrial and gas giant planets, which are generated within their cores, the ice giants' magnetic fields are generated by motion at relatively shallow depths, for instance, in the waterâ€“ammonia ocean.

Despite its curious alignment, in other respects the Uranian magnetosphere is like those of other planets: it has a bow shock located at about 23 Uranian radii ahead of it, a magnetopause at 18 Uranian radii, a fully developed magnetotail and radiation belts. Overall, the structure of the magnetosphere of Uranus is different from that of Jupiter's and more similar to that of Saturn's. Uranus' magnetotail trails behind the planet into space for millions of kilometers and is twisted by the planet's sideways rotation into a long corkscrew. 

Uranus' magnetosphere contains charged particles: protons and electrons with small amount of  ions. No heavier ions have been detected. Many of these particles probably derive from the hot atmospheric corona. The ion and electron energies can be as high as 4 and 1.2Â megaelectronvolts, respectively. The density of low energy (below 100Â electronvolts) ions in the inner magnetosphere is about 2Â cm-3. The particle population is strongly affected by the Uranian moons that sweep through the magnetosphere leaving noticeable gaps. The particle flux is high enough to cause darkening or space weathering of the moonâ€™s surfaces on an astronomically rapid timescale of 100,000Â years. This may be the cause of the uniformly dark colouration of the moons and rings. Uranus has relatively well developed aurorae, which are seen as bright arcs around both magnetic poles. However, unlike Jupiter's, Uranus' aurorae seem to be insignificant for the energy balance of the planetary thermosphere.



Uranus' atmosphere is remarkably bland in comparison to the other gas giants, even to Neptune, which it otherwise closely resembles. When Voyager 2 flew by Uranus in 1986, it observed a total of ten cloud features across the entire planet. One proposed explanation for this dearth of features is that Uranus' internal heat appears markedly lower than that of the other giant planets. The lowest temperature recorded in Uranus' tropopause is 49Â K, making Uranus the coldest planet in the Solar System, colder than Neptune.

In 1986 Voyager 2 found that the visible southern hemisphere of Uranus can be subdivided into two regions: a bright polar cap and dark equatorial bands (see figure on the right). Their boundary is located at about âˆ?5 degrees of latitude. A narrow band straddling the latitudinal range from âˆ?5 to âˆ?0 degrees is the brightest large feature on the visible surface of the planet. It is called a southern "collar". The cap and collar are thought to be a dense region of methane clouds located within the pressure range of 1.3 to 2Â bar (see above). Unfortunately Voyager 2 arrived during the height of the planet's southern summer and could not observe the northern hemisphere. However, at the beginning of the twenty-first century, when the northern polar region came into view, Hubble Space Telescope (HST) and Keck telescope observed neither a collar nor a polar cap in the northern hemisphere. So Uranus appears to be asymmetric: bright near the south pole and uniformly dark in the region north of the southern collar. 

In addition to large-scale banded structure, Voyager 2 observed ten small bright clouds, most lying several degrees to the north from the collar. In all other respects Uranus looked like a dynamically dead planet in 1986. However in the 1990s the number of the observed bright cloud features grew considerably. The majority of them were found in the northern hemisphere as it started to become visible. The common explanation of this fact is that bright clouds are easier to identify in the dark part of the planet, whereas in the southern hemisphere the bright collar masks them. Nevertheless there are differences between the clouds of each hemisphere. The northern clouds are smaller, sharper and brighter. They appear to lie at a higher altitude. The lifetime of clouds spans several orders of magnitude. Some small clouds live for hours, while at least one southern cloud has persisted since Voyager flyby. Recent observation also discovered that cloud-features on Uranus have a lot in common with those on Neptune, although the weather on Uranus is much calmer. The dark spots common on Neptune had never been observed on Uranus before 2006, when the first such feature was imaged.

The tracking of numerous cloud features allowed determination of zonal winds blowing in the upper troposphere of Uranus. At the equator winds are retrograde, which means that they blow in the reverse direction to the planetary rotation. Their speeds are from âˆ?00 to âˆ?0Â m/s. Wind speeds increase with the distance from the equator, reaching zero values near Â±20Â° latitude, where the troposphere's temperature minimum is located. Closer to the poles, the winds shift to a prograde direction, flowing with the planet's rotation. Windspeeds continue to increase reaching maxima at Â±60Â° latitude before falling to zero at the poles. Windspeeds at âˆ?0Â° latitude range from 150 to 200Â m/s. Since the collar obscures all clouds below that parallel, speeds between it and the southern pole are impossible to measure. In contrast, in the northern hemisphere maximum speeds as high as 240Â m/s are observed near +50 degrees of latitude.

For a short period from March to May 2004, a number of large clouds appeared in the Uranian atmosphere, giving it a Neptune-like appearance. Observations included record-breaking wind speeds of 229 m/s (824 km/h) and a persistent thunderstorm referred to as "Fourth of July fireworks". On August 23, 2006, researchers at the Space Science Institute (Boulder, CO) and the University of Wisconsin observed a dark spot on Uranus' surface, giving astronomers more insight into the planet's atmospheric activity. Why this sudden upsurge in activity should be occurring is not fully known, but it appears that Uranus' extreme axial tilt results in extreme seasonal variations in its weather. Determining the nature of this seasonal variation is difficult because good data on Uranus' atmosphere has existed for less than 84 years, or one full Uranian year. A number of discoveries have however been made. Photometry over the course of half a Uranian year (beginning in the 1950s) has shown regular variation in the brightness in two spectral bands, with maxima occurring at the solstices and minima occurring at the equinoxes. A similar periodic variation, with maxima at the solstices, has been noted in microwave measurements of the deep troposphere begun in the 1960s. Stratospheric temperature measurements beginning in 1970s also showed maximum values near 1986 solstice. The majority of this variability is believed to occur due to changes in the viewing geometry. 

However there are some reasons to believe that physical seasonal changes are happening in Uranus. While the planet is known to have a bright south polar region, the north pole is fairly dim, which is incompatible with the model of the seasonal change outlined above. During its previous northern solstice in 1944, Uranus displayed elevated levels of brightness, which suggests that the north pole was not always so dim. This information implies that the visible pole brightens some time before the solstice and darkens after the equinox. Detailed analysis of the visible and microwave data revealed that the periodical changes of brightness are not completely symmetrical around the solstices, which also indicates a change in the meridional albedo patterns. Finally in the 1990s, as Uranus moved away from its solstice, Hubble and ground based telescopes revealed that the south polar cap darkened noticeably (except the southern collar, which remained bright), while the northern hemisphere demonstrates increasing activity, such as cloud formations and stronger winds, bolstering expectations that it should brighten soon. 

The mechanism of physical changes is still not clear. Near the summer and winter solstices, Uranus' hemispheres lie alternately either in full glare of the Sun's rays or facing deep space. The brightening of the sunlit hemisphere is thought to result from the local thickening of the methane clouds and haze layers located in the troposphere. The bright collar at âˆ?5Â° latitude is also connected with methane clouds. Other changes in the southern polar region can be explained by changes in the lower cloud layers. The variation of the microwave emission from the planet is probably caused by a changes in the deep tropospheric circulation, because thick polar clouds and haze may inhibit convection. Now that the spring and autumn equinoxes are arriving on Uranus, the dynamics are changing and convection can occur again.

Many argue that the differences between the ice giants and the gas giants extend to their formation. The Solar System is believed to have formed from a giant rotating ball of gas and dust known as the presolar nebula. As it condensed, it formed into a disc with a slowly collapsing Sun in the middle. Much of the nebula's gas, primarily hydrogen and helium, formed the Sun, while the dust grains collected together to form the first protoplanets. As the planets grew, some of them eventually accreted enough matter for their gravity to hold onto the nebula's leftover gas. The more gas they held onto, the larger they became; the larger they became, the more gas they held onto until a critical point was reached, and their size began to increase exponentially. The ice giants, with only a few Earth masses of nebular gas, never reached that critical point. Current theories of solar system formation have difficulty accounting for the presence of Uranus and Neptune so far out from Jupiter and Saturn. They are too large to have formed from the amount of material expected at that distance. Rather, some scientists expect that both formed closer to the Sun but were scattered outward by Jupiter. However, more recent simulations, which take into account planetary migration, seem to be able to form Uranus and Neptune near their present locations.



Uranus has 27 known natural satellites. The names for these satellites are chosen from characters from the works of Shakespeare and Alexander Pope. The five main satellites are Miranda, Ariel, Umbriel, Titania and Oberon. The Uranian satellite system is the least massive among the gas giants; indeed, the combined mass of the five major satellites would be less than half that of Triton alone. The largest of the satellites, Titania, has a radius of only 788.9Â km, or less than half that of the Moon, but slightly more than Rhea, the second largest moon of Saturn, making Titania the eighth largest moon in the Solar System. The moons have relatively low albedos; ranging from 0.20 for Umbriel to 0.35 for Ariel (in green light). The moons are ice-rock conglomerates composed of roughly fifty percent ice and fifty percent rock. The ice may include ammonia and carbon dioxide. 

Among the satellites, Ariel appears to have the youngest surface with the fewest impact craters, while Umbriel's appears oldest. Miranda possesses fault canyons 20Â kilometers deep, terraced layers, and a chaotic variation in surface ages and features. Miranda's past geologic activity is believed to have been driven by tidal heating at a time when its orbit was more eccentric than currently, probably as a result of a formerly present 3:1 orbital resonance with Umbriel. Extensional processes associated with upwelling diapirs are likely the origin of the moon's 'racetrack'-like coronae. Similarly, Ariel is believed to have once been held in a 4:1 resonance with Titania.

In 1986, NASA's Voyager 2 visited Uranus. This visit is the only attempt to investigate the planet from a short distance and no other visits are currently planned. Launched in 1977, Voyager 2 made its closest approach to Uranus on January 24, 1986, coming within 81,500Â kilometers of the planet's cloud tops, before continuing its journey to Neptune. Voyager 2 studied structure and chemical composition of the atmosphere, discovered 10 new moons and studied the planet's unique weather, caused by its axial tilt of 97.77Â°; and examined its ring system. It also studied the magnetic field, its irregular structure, its tilt and its unique corkscrew magnetotail brought on by Uranus' sideways orientation. It made the first detailed investigations of its five largest moons, and studied all nine of the system's known rings, discovering two new ones.



















The Faroe Islands or Faeroe Islands or simply Faroe(s) or Faeroes (, meaning "Sheep Islands", , Old Norse: FÃ¦reyjar) are a group of islands in Northern Europe, between the Norwegian Sea and the North Atlantic Ocean, roughly equidistant between Iceland, Scotland, and Norway. They have been an autonomous province of the Kingdom of Denmark since 1948, making it a member of the RigsfÃ¦llesskab. The Faroese have, over the years, taken control of most matters except defence (though they have a native coast guard), foreign affairs and the legal system. These three areas are the responsibility of Denmark.

The Faroes have close traditional ties to Iceland, Shetland, Orkney, the Outer Hebrides and Greenland. The archipelago was politically detached from Norway in 1814. The Faroes are represented in the Nordic Council as a part of the Danish delegation.



The early history of the Faroe Islands is not well known. Irish hermits (monks) settled in the sixth century, introducing sheep and oats and the early Irish language to the islands. Saint Brendan, who lived circa 484â€?78, is said to have visited the Faroe Islands on two or three occasions (512-530 AD), naming two of the islands Sheep Island and Paradise Island of Birds.

Later (~650 AD) the Vikings replaced the early Irish and their settlers, bringing the Old Norse language to the islands, which locally evolved into the modern Faroese language spoken today.The settlers are not thought to have come directly from Norway, but rather from the Norwegian settlements in Shetland, Orkney, and around the Irish Sea, and to have been so-called Norse-Gaels.

According to FÃ¦reyinga Saga, emigrants who left Norway to escape the tyranny of Harald I of Norway settled in the islands about the end of the ninth century. Early in the eleventh century, Sigmund, whose family had flourished in the southern islands but had been almost exterminated by invaders from the northern islands, escaped to Norway and was sent back to take possession of the islands for Olaf Tryggvason, king of Norway. He introduced Christianity and, though he was subsequently murdered, Norwegian supremacy was upheld. Norwegian control of the islands continued until 1380, when Norway entered the Kalmar Union with Denmark, which gradually evolved into Danish control of the islands. The reformation reached the Faroes in 1538. When the union between Denmark and Norway was dissolved as a result of the Treaty of Kiel in 1814, Denmark retained possession of the Faroe Islands. 

The trade monopoly in the Faroe Islands was abolished in 1856 and the country has since then developed towards a modern fishing nation with its own fleet. The national awakening since 1888 was first based on a struggle for the Faroese language, and thus more culturally oriented, but after 1906 was more and more politically oriented with the foundation of the political parties of the Faroe Islands.

On April 12, 1940, the Faroes were occupied by British troops. The move followed the invasion of Denmark by Nazi Germany and had the objective of strengthening British control of the North Atlantic (see Second Battle of the Atlantic). In 1942â€?3 the British Royal Engineers built the only airport in the Faroes, VÃ¡gar Airport. Control of the islands reverted to Denmark following the war, but in 1948 a home-rule regime was implemented granting a high degree of local autonomy. The Faroes declined to join Denmark in entering the European Community (now European Union) in 1973. The islands experienced considerable economic difficulties following the collapse of the fishing industry in the early 1990s, but have since made efforts to diversify the economy. Support for independence has grown and is the objective of the government.



The government of the Faroes holds the executive power in local government affairs. The head of the government is called the LÃ¸gmaÃ°ur or prime minister in English. Any other member of the cabinet is called a landsstÃ½rismaÃ°ur.

Today, elections are held in the municipalities, on a national level for the LÃ¸gting, and inside the Kingdom of Denmark for the Folketing. For the LÃ¸gting elections there are seven electoral districts, each one comprising a sÃ½sla, while Streymoy is divided into a northern and southern part (TÃ³rshavn region).

The Treaty of Kiel in 1814 terminated the Danish-Norwegian union. Norway came under the rule of the King of Sweden, but the Faroe Islands, Iceland, and Greenland remained as possessions of Denmark. Subsequently, the LÃ¸gting was abolished (1816), and the Faroe Islands were to be governed as a regular Danish amt, with the Amtmand as its head of government. In 1851 the LÃ¸gting was resurrected, but served mainly as an advisory power until 1948.

At the end of the Second World War a portion of the population favoured independence from Denmark, and on September 14 1946 a public election was held on the question of secession. It is not considered a referendum, as the parliament was not bound to follow the decision of the vote. This was the first time that the Faroese people were asked if they favoured independence or if they wanted to continue as a part of the Danish kingdom. The outcome of the vote produced a small majority in favour of secession, but the coalition in parliament could not reach a resolution on how this election should be interpreted and implemented, and because of these irresolvable differences the coalition fell apart. A parliamentary election was held just a few months later, in which the political parties that favoured staying in the Danish kingdom increased their share of the vote and formed a coalition. Based on this increased share of the votes, they chose to reject secession. Instead, a compromise was made and the Folketing passed a home-rule law, which came into effect in 1948. The Faroe Islands' status as a Danish amt was brought to an end with the home-rule law; the Faroe Islands were given a high degree of self-governance, supported by a substantial annual subsidy from Denmark.

The islanders are about evenly split between those favouring independence and those who prefer to continue as a part of the Kingdom of Denmark. Within both camps there is, however, a wide range of opinions. Of those who favour independence, some are in favour of an immediate unilateral declaration. Others see it as something to be attained gradually and with the full consent of the Danish government and the Danish nation. In the unionist camp there are also many who foresee and welcome a gradual increase in autonomy even as strong ties to Denmark are maintained.

As explicitly asserted by both Rome treaties, the Faroe Islands are not part of the European Union. Moreover, a protocol to the treaty of accession of Denmark to the European Communities stipulates that Danish nationals residing in the Faroe Islands are not to be considered as Danish nationals within the meaning of the treaties. Hence, Danish people living in the Faroes are not citizens of the European Union. (Other EU nationals living there remain EU citizens.) The Faroes are not covered by the Schengen free movement agreement, but there are no border checks when travelling between the Faroes and any Schengen country .



Administratively, the islands are divided into 34 municipalities (kommunur) within which 120 or so cities and villages lie.

Traditionally, there are also the six ''sÃ½slur'' ("regions"; NorÃ°oyar, Eysturoy, Streymoy, VÃ¡gar, Sandoy and SuÃ°uroy). Although today sÃ½sla technically means "police district", the term is still commonly used to indicate a geographical region. In earlier times, each sÃ½sla had its own ting (assembly), the so-called vÃ¡rting ("spring ting").



The Faroe Islands are an island group consisting of eighteen islands off the coast of Northern Europe, between the Norwegian Sea and the north Atlantic Ocean, about halfway between Iceland and Norway; the closest neighbours being the Northern and Western Isles of Scotland.Its coordinates are . 

Its area is 1,399 square kilometres (540Â sq.Â mi), and has no major lakes or rivers. There are 1,117 kilometres (694Â mi) of coastline, and no land boundaries with any other country. The only island that is uninhabited is LÃ­tla DÃ­mun.

The islands are rugged and rocky with some low peaks; the coasts are mostly bordered by cliffs. The highest point is SlÃ¦ttaratindur, 882 metres (2,894Â ft) above sea level. There are areas below sea level.

The Faroe Islands are dominated by tholeiitic basalt lava which was part of the great Thulean Plateau during the Paleogene period.

 the severe economic troubles of the early 1990s, brought on by a drop in the vital fish catch and poor management of the economy, the Faroe Islands have come back in the last few years, with unemployment down to 5% in mid-1998. In 2006 unemployment declined to 3%, one of the lowest rates in Europe. Nevertheless, the almost total dependence on fishing means that the economy remains extremely vulnerable. The Faroese hope to broaden their economic base by building new fish-processing plants. As an agrarian society, other than fishing, the raising of sheep is the main industry of the islands. Petroleum found close to the Faroese area gives hope for deposits in the immediate area, which may provide a basis for sustained economic prosperity.

Since 2000, new information technology and business projects have been fostered in the Faroe Islands to attract new investment. The introduction of Burger King in TÃ³rshavn was widely publicized and a sign of the globalization of Faroese culture. It is not yet known whether these projects will succeed in broadening the islands' economic base. While having one of the lowest unemployment rates in Europe, this should not necessarily be taken as a sign of a recovering economy, as many young students move to Denmark and other countries once they are finished with high school. This leaves a largely middle-aged and elderly population that may lack the skills and knowledge to fill newly developed computing positions on the Faroes.

VÃ¡gar Airport has scheduled service to destinations from VÃ¡goy Island. The largest Faroese airline is Atlantic Airways.

Due to the rocky terrain and relatively small size of the Faroe Islands, its transportation system was not as extensive as other places of the world. This situation has changed, and today the infrastructure has been developed extensively. Some 80% of the population in the islands is connected by under-ocean tunnels, bridges, and causeways which bind the three largest islands and three other large islands to the northeast together, while the other two large islands to the south of the main area are connected to the main area with new fast ferries. There are good roads that lead to every village in the islands, except for seven of the smaller islands with only one village each.

Altogether it becomes less meaningful to perceive the Faroes as a society based on various islands and regions. The huge investments in roads, bridges and sub-sea tunnels (see also Transportation in the Faroe Islands) have tied together the islands, creating a coherent economic and cultural sphere that covers almost 90% of the entire population. From this perspective it is reasonable to perceive the Faroes as a dispersed city or even to refer to it as the Faroese Network City.

The vast majority of the population are ethnic Faroese, of Norse and Celtic descent.

Recent DNA analyses have revealed that Y chromosomes, tracing male descent, are 87% Scandinavian.The studies show that mitochondrial DNA, tracing female descent, is 84% Scottish / Irish.

Of the approximately 48,000 inhabitants of the Faroe Islands (16,921 private households (2004)), 98% are realm citizens, meaning Faroese, Danish, or Greenlandic. By birthplace one can derive the following origins of the inhabitants: born on the Faroes 91.7%, in Denmark 5.8%, and in Greenland 0.3%. The largest group of foreigners is Icelanders comprising 0.4% of the population, followed by Norwegians and Polish, each comprising 0.2%. Altogether, on the Faroe Islands there are people from 77 different nationalities.

Faroese is spoken in the entire country as a first language. It is not possible to say exactly how many people worldwide speak the Faroese language. This is for two reasons: Firstly, many ethnic Faroese live in Denmark and few who are born there return to the Faroes with their parents or as adults. Secondly, there are some established Danish families on the Faroes who speak Danish at home.

The Faroese language is one of the smallest of the Germanic languages. Faroese grammar is most similar to Icelandic and Old Norse. In contrast, spoken Faroese differs much from Icelandic and is closer to Norwegian dialects from the west coast of Norway. In the twentieth century, Faroese became the official language and since the Faroes are a part of the Danish realm Danish is taught in schools as a compulsory second language.

Faroese language policy provides for the active creation of new terms in Faroese suitable for modern life.

If the first inhabitants of the Faroe Islands were Irish monks, then they must have lived as a very small group of settlers. Later, when the Vikings colonised the Islands, there was a considerable increase in the population. However, it never exceeded 5,000 until the eighteenth century. Around 1349, about half of the islands' people died of the plague.

Only with the rise of the deep sea fishery (and thus independence from difficult agriculture) and with general progress in the health service was rapid population growth possible in the Faroes. Beginning in the eighteenth century, the population increased tenfold in 200 years.

At the beginning of the 1990s, the Faroe Islands entered a deep economic crisis with heavy, noticeable emigration; however, this trend reversed in subsequent years to a net immigration.

The Faroese population is spread across most of the country; it was not until recent decades that significant urbanization occurred. Industrialisation has been remarkably decentralised, and the country has therefore maintained quite a viable rural culture. Nevertheless, villages with poor harbour facilities have been the losers in the development from agriculture to fishing, and in the most peripheral agricultural areas, also known as the the outer islands, there are scarcely any young people left. In recent decades, the village-based social structure has nevertheless been placed under pressure; instead there has been a rise in interconnected "centres" that are better able to provide goods and services than the badly connected periphery. This means that shops and services are now relocating en masse from the villages into the centres, and in turn this also means that slowly but steadily the Faroese population concentrates in and around the centres.

In the 1990's the old national policy of developing the villages (Bygdamenning) was abandoned, and instead the government started a process of regional development (Ã˜kismenning). The term "region" referred to the large islands of the Faroes. Nevertheless the government was not able to press through the structural reform of merging the small rural municipalities in order to create sustainable, decentralized entities that could drive forward the regional development. As the regional development has been difficult on the administrative level, the government has instead made heavy investments in infrastructure, interconnecting the regions.

Altogether it becomes less meaningful to perceive the Faroes as a society based on various islands and regions. The huge investments in roads, bridges and sub-sea tunnels (see also Transportation in the Faroe Islands) have tied together the islands, creating a coherent economic and cultural sphere that covers almost 90% of the entire population. From this perspective it is reasonable to perceive the Faroes as a dispersed city or even to refer to it as the Faroese Network City.

According to FÃ¦reyinga Saga, Sigmundur Brestisson brought Christianity to the islands in 999. However, archaeology from a site in LeirvÃ­k suggests that Celtic Christianity may have arrived 150 years earlier, or more. The Faroe Islands' church Reformation was completed on 1 January 1540. According to official statistics from 2002, 84.1% of the Faroese population are members of the state church, the Faroese People's Church (FÃ³lkakirkjan), a form of Lutheranism. Faroese members of the clergy who have had historical importance include V. U. Hammershaimb (1819-1909), Frederik Petersen (1853-1917) and, perhaps most significantly, JÃ¡kup Dahl (1878-1944), who had a great influence in making sure that the Faroese language was spoken in the church instead of Danish.

In the late 1820s, the Christian Evangelical religious movement, the Plymouth Brethren, was established in England. In 1865, a member of this movement, William Gibson Sloan, travelled to the Faroes from Shetland. At the turn of the nineteenth century, the Faroese Plymouth Brethren numbered thirty. Today, approximately 10% of the Faroese population are members of the Open Brethren community (BrÃ¸Ã°rasamkoman). About 5% belong to other Christian churches, such as the Adventists, who operate a private school in TÃ³rshavn. Jehovah's Witnesses also number four congregations (approximately 80 to 100 members). The Roman Catholic congregation comprises approximately 170 members. The municipality of TÃ³rshavn operates their old Franciscan school. There are also around fifteen BahÃ¡'Ã­s who meet at four different places. Unlike Iceland, there is no organized ÃsatrÃº community, but there is a fair share of pagan lore such as ballads with pagan content, and to this day it is not officially accepted to perform Faroese ballads in consecrated buildings.

The best known church buildings in the Faroe Islands include St. Olafs Church and the Magnus Cathedral in KirkjubÃ¸ur; the Vesturkirkjan and the Maria Church, both of which are situated in TÃ³rshavn; the church of FÃ¡mjin; the octagonal church in HaldarsvÃ­k; Christianskirkjan in KlaksvÃ­k and also the two pictured here.

In 1948, Victor Danielsen (Plymouth Brethren) completed the first Bible translation. It was translated into Faroese from different modern languages. Jacob Dahl and Kristian Osvald ViderÃ¸ (FÃ³lkakirkjan) completed the second translation in 1961. The latter was translated from the original languages into Faroese.



The national holiday Ã“lavsÃ¸ka, is on the 29 July, commemorating the death of Saint Olaf. The celebrations are held in TÃ³rshavn. They commence on the evening of the 28th, and carry on until the 31 July. 

The official part of the celebration starts on the 29th, with the opening of the Faroese Parliament, a custom which dates back some 900 years. This begins with a service held in TÃ³rshavn Cathedral, all members of parliament as well as civil and church officials walk to the cathedral in a procession. All of the parish ministers take turns giving the sermon. After the service, the procession returns to the parliament for the opening ceremony.

Other celebrations are marked by different kind of sports competitions, the rowing competition (in TÃ³rshavn harbour) being the most popular, art exhibitions, pop concerts, and the famous Faroese dance. The celebrations have many facets, and only a few are mentioned here. 

Another way many people mark the occasion is to wear the national Faroese dress.

The Nordic House in the Faroe Islands (in Faroese NorÃ°urlandahÃºsiÃ°) is the most important cultural institution in the Faroes. Its aim is to support and promote Nordic and Faroese culture, locally and in the Nordic region. Erlendur Patursson (1913-1986), Faroese member of the Nordic Council, brought forward the idea of a Nordic cultural house in the Faroe Islands. A Nordic competition for architects was held in 1977, in which 158 architects participated. Winners were Ola Steen from Norway and KolbrÃºn RagnarsdÃ³ttir from Iceland. By staying true to folklore, the architects built the Nordic House to resemble an enchanting hill of elves. The house opened in TÃ³rshavn in 1983. The Nordic House is a cultural organization under the Nordic Council of Ministers. The Nordic House is run by a steering committee of eight, of which three are Faroese and five from the other Nordic countries. There is also a local advisory body of fifteen members, representing Faroese cultural organizations. The House is managed by a director appointed by the steering committee for a four-year term.

The Faroe Islands have a very active music scene. The islands have their own symphony orchestra, the classical ensemble AldubÃ¡ran and many different choirs; the most well-known being HavnarkÃ³riÃ°. The most well-known Faroese composers are Sunleif Rasmussen and the Dane Kristian Blak. 

The first Faroese opera ever was by Sunleif Rasmussen. It is entitled Ã Ã“Ã°amansgarÃ°i (The MadmanÂ´s Garden), and it opened on the October 12, 2006, at the Nordic House. The opera is based on a short story by the writer William Heinesen. 

Young Faroese musicians who have gained much popularity recently are EivÃ¸r (EivÃ¸r PÃ¡lsdÃ³ttir), Lena (Lena Andersen), Teitur (Teitur Lassen), HÃ¸gni Reistrup, HÃ¸gni Lisberg and Brandur Enni. 

Well-known bands include TÃ½r, Gestir, Boys In A Band, 200 and the former band Clickhaze. 

The festival for contemporary and classical music, SummartÃ³nar, is held each summer. Large open-air music festivals for popular music with both local and international musicians participating are G! Festival in GÃ¸ta in July and Summarfestivalurin in KlaksvÃ­k in August.

Traditional Faroese food is mainly based on meat and potatoes and uses few fresh vegetables. Mutton is the basis of many meals, and one of the most popular treats is skerpikjÃ¸t, well aged, wind-dried mutton which is quite chewy. The drying shed, known as a hjallur, is a standard feature in many Faroese homes, particularly in the small towns and villages. Other traditional foods are rÃ¦st kjÃ¸t (semi-dried mutton) and rÃ¦stur fiskur, matured fish. Another Faroese specialty is Grind og spik, pilot whale meat and blubber. (A parallel meat/fat dish made with offal is garnatÃ¡lg). Well into the last century meat and blubber from the pilot whale meant food for a long time. Fresh fish also features strongly in the traditional local diet, as do seabirds, such as Faroese puffins, and their eggs.





The climate is technically defined as Maritime Subarctic according to the (KÃ¶ppen climate classification:Cfc). The overall character of the islands' climate is determined by the strong cooling influence of the Atlantic Ocean, which here produces the North Atlantic Drift. This, together with the remoteness of any sources of warm airflows ensures that winters are mild (mean temperature 3.0 to 4.0Â°C) while summers are cool (mean temperature 9.5 to 10.5Â°C). The islands are windy, cloudy and cool throughout the year with over 260 rainy days in the year. The islands lie in the path of depressions moving north eastwards and this means that strong winds and heavy rain are possible at all times of the year. Sunny days are rare and overcast days are common.





The natural vegetation of the Faroe Islands is dominated by Arctic-alpine plants, wild flowers, grasses, moss and lichen. Most of the lowland area is grassland and some is heath, dominated by shrubby heathers, mainly Calluna vulgaris. 

The Faroese nature is characterised by the lack of trees, and resembles that of Connemara and Dingle in Ireland and the Scottish islands. 

A few small plantations consisting of plants collected from similar climates like Tierra del Fuego in South America and Alaska thrive on the islands. 





Birds

The bird fauna of the Faroe Islands is dominated by sea-birds and birds attracted to open land like heather, probably due to the lack of woodland and other suitable habitats. Many species have developed special Faroese sub-species: Eider, Starling, Wren, Guillemot, and Black Guillemot. ). 

Mammals:

Only a few species of wild land mammals are found in the Faroe Islands today, all introduced by man. 

Grey Seals (Halichoerus grypus) are very common around the Faroese shores.

Several species of whales live in the waters around the Faroe Islands. Best known are the Short-finned Pilot Whales (Globicephala melaena), but the more exotic Killer whales (Orcinus orca) sometimes visit the Faroese fjords.

A collection of Faroese marine algae resulting from a survey sponsored by NATO, the British Museum (Natural History) and the Carlsberg Foundation, is preserved in the Ulster Museum (catalogue numbers: F3195â€”F3307). It is one of ten exsiccatae sets.





</div>













African American literature is the body of literature produced in the United States by writers of African descent. The genre traces its origins to the works of such late 18th century writers as Phillis Wheatley and Olaudah Equiano, reaching early high points with slave narratives and the Harlem Renaissance, and continuing today with authors such as Toni Morrison, Maya Angelou and Walter Mosley being ranked among the top writers in the United States. Among the themes and issues explored in African American literature are the role of African Americans within the larger American society, African-American culture, racism, slavery, and equality. African American writing has also tended to incorporate within itself oral forms such as spirituals, sermons, gospel music, blues and rap.

As African Americans' place in American society has changed over the centuries, so, too, have the foci of African American literature. Before the American Civil War, African American literature primarily focused on the issue of slavery, as indicated by the subgenre of slave narratives. At the turn of the 20th century, books by authors such as W.E.B. DuBois and Booker T. Washington debated whether to confront or appease racist attitudes in the United States. During the American Civil Rights movement, authors such as Richard Wright and Gwendolyn Brooks wrote about issues of racial segregation and black nationalism. Today, African American literature has become accepted as an integral part of American literature, with books such as  by Alex Haley, The Color Purple by Alice Walker, and Beloved by Toni Morrison achieving both best-selling and award-winning status.

In broad terms, African American literature can be defined as writings by people of African descent living in the United States of America. However, just as African American history and life is extremely varied, so too is African American literature. That said, African American literature has generally focused on themes of particular interest to Black people in the United States, such as the role of African Americans within the larger American society and what it means to be an American. As Princeton University professor Albert J. Raboteau has said, all African-American studies, including African American literature, "speaks to the deeper meaning of the African-American presence in this nation. This presence has always been a test case of the nation's claims to freedom, democracy, equality, the inclusiveness of all." As such, it can be said that African American Literature explores the very issues of freedom and equality which were long denied to Black people in the United States, along with further themes such as African American culture, racism, religion, slavery, a sense of home. and more.

African American literature constitutes a vital branch of the literature of the African diaspora, with African American literature both being influenced by the great African diasporic heritage and in turn influencing African diasporic writings in many countries. In addition, African American literature exists within the larger realm of post-colonial literature, even though scholars draw a distinctive line between the two by stating that "African American literature differs from most post-colonial literature in that it is written by members of a minority community who reside within a nation of vast wealth and economic power."

African American oral culture is rich in poetry, including spirituals, African American gospel music, blues and rap. This oral poetry also appears in the African American tradition of Christian sermons, which make use of deliberate repetition, cadence and alliteration. African American literatureâ€”especially written poetry, but also proseâ€”has a strong tradition of incorporating all of these forms of oral poetry.

However, while these characteristics and themes exist on many levels of African American literature, they are not the exclusive definition of the genre and don't exist within all works within the genre. In addition, there is resistance to using Western literary theory to analyze African American literature. As Henry Louis Gates, Jr., one of the most important African American literary scholars, once said, "My desire has been to allow the black tradition to speak for itself about its nature and various functions, rather than to read it, or analyze it, in terms of literary theories borrowed whole from other traditions, appropriated from without."

Just as African American history predates the emergence of the United States as an independent country, so too does African American literature have similarly deep roots.



Lucy Terry is the author of the oldest piece of African American literature known which was "Bars Fight", 1746. This poem was not published until 1855 in Josiah Holland's "History of Western Massachusetts".Also, Briton Hammon's "The Narrative of the Uncommon Sufferings and Surprising Deliverence of Briton Hammon, A Negro Man", 1760. Poet Phillis Wheatley (1753â€?4), who published her book Poems on Various Subjects in 1773, three years before American independence. Born in Senegal, Africa, Wheatley was captured and sold into slavery at the age of seven. Brought to America, she was owned by a Boston merchant. Even though she initially spoke no English, by the time she was sixteen she had mastered the language. Her poetry was praised by many of the leading figures of the American Revolution, including George Washington, who personally thanked her for a poem she wrote in his honor. Despite this, many white people found it hard to believe that a Black woman could be so intelligent as to write poetry. As a result, Wheatley had to defend herself in court by proving she actually wrote her own poetry. Some critics cite Wheatley's successful defense as the first recognition of African American literature.

Another early African American author was Jupiter Hammon (1711â€?806?). Hammon, considered the first published Black writer in America, published his poem "An Evening Thought: Salvation by Christ with Penitential Cries" as a broadside in early 1761. In 1778 he wrote an ode to Phillis Wheatley, in which he discussed their shared humanity and common bonds. In 1786, Hammon gave his well-known Address to the Negroes of the State of New York. Hammon wrote the speech at age seventy-six after a lifetime of slavery and it contains his famous quote, "If we should ever get to Heaven, we shall find nobody to reproach us for being black, or for being slaves." Hammon's speech also promoted the idea of a gradual emancipation as a way of ending slavery. It is thought that Hammon stated this plan because he knew that slavery was so entrenched in American society that an immediate emancipation of all slaves would be difficult to achieve. Hammon apparently remained a slave until his death. His speech was later reprinted by several groups opposed to slavery.

William Wells Brown (1814â€?4) and Victor SÃ©jour (1817â€?4) produced the earliest works of fiction by African American writers. SÃ©jour was born free in New Orleans and moved to France at the age of 19. There he published his short story "Le MulÃ¢tre" ("The Mulatto") in 1837; the story represents the first known fiction by an African American, but written in French and published in a French journal, it had apparently no influence on later American literature. SÃ©jour never returned to African American themes in his subsequent works. Brown, on the other hand, was a prominent abolitionist, lecturer, novelist, playwright, and historian. Born into slavery in the Southern United States, Brown escaped to the North, where he worked for abolitionist causes and was a prolific writer. Brown wrote what is considered to be the first novel by an African American, Clotel; or, The President's Daughter (1853). The novel is based on what was at that time considered to be a rumor about Thomas Jefferson fathering a daughter with his slave, Sally Hemings.

However, because the novel was published in England, the book is not considered the first African American novel published in the United States. This honor instead goes to Harriet Wilson, whose novel Our Nig (1859) details the difficult lives of Northern free Blacks.

A subgenre of African American literature which began in the middle of the 19th century is the slave narrative. At the time, the controversy over slavery led to impassioned literature on both sides of the issue, with books like Uncle Tom's Cabin (1852) representing the abolitionist view of the evils of slavery, while the so-called Anti-Tom literature by white, southern writers like William Gilmore Simms represented the pro-slavery viewpoint.

To present the true reality of slavery, a number of former slaves such as Harriet Jacobs and Frederick Douglass wrote slave narratives, which soon became a mainstay of African American literature. Some six thousand former slaves from North America and the Caribbean wrote accounts of their lives, with about 150 of these published as separate books or pamphlets.

Slave narratives can be broadly categorized into three distinct forms: tales of religious redemption, tales to inspire the abolitionist struggle, and tales of progress. The tales written to inspire the abolitionist struggle are the most famous because they tend to have a strong autobiographical motif. Many of them are now recognized as the most literary of all 19th-century writings by African Americans, with two of the best-known being Frederick Douglass's autobiography and Incidents in the Life of a Slave Girl by Harriet Jacobs (1861).

While Frederick Douglass (c. 1818â€?5) first came to public attention as an orator and as the author of his autobiographical slave narrative, he eventually became the most prominent African American of his time and one of the most influential lecturers and authors in American history.

Born into slavery in Maryland, Douglass eventually escaped and worked for numerous abolitionist causes. He also edited a number of newspapers. Douglass' best-known work is his autobiography, Narrative of the Life of Frederick Douglass, an American Slave, which was published in 1845. At the time some critics attacked the book, not believing that a black man could have written such an eloquent work. Despite this, the book was an immediate bestseller.

Douglas later revised and expanded his autobiography, which was republished as My Bondage and My Freedom (1855). In addition to serving in a number of political posts during his life, he also wrote numerous influential articles and essays.

After the end of slavery and the American Civil War, a number of African American authors continued to write nonfiction works about the condition of African Americans in the country.

Among the most prominent of these writers is W.E.B. Du Bois (1868â€?963), one of the original founders of the NAACP. At the turn of the century, Du Bois published a highly influential collection of essays titled The Souls of Black Folk. The book's essays on race were groundbreaking and drew from DuBois's personal experiences to describe how African Americans lived in American society. The book contains Du Bois's famous quote: "The problem of the twentieth century is the problem of the color-line." Du Bois believed that African Americans should, because of their common interests, work together to battle prejudice and inequity.

Another prominent author of this time period is Booker T. Washington (1856â€?915), who in many ways represented opposite views from Du Bois. Washington was an educator and the founder of the Tuskegee Institute, a Black college in Alabama. Among his published works are Up From Slavery (1901), The Future of the American Negro (1899), Tuskegee and Its People (1905), and My Larger Education (1911). In contrast to Du Bois, who adopted a more confrontational attitude toward ending racial strife in America, Washington believed that Blacks should first lift themselves up and prove themselves the equal of whites before asking for an end to racism. While this viewpoint was popular among some Blacks (and many whites) at the time, Washington's political views would later fall out of fashion.

A third writer who gained attention during this period in the US, though not a US citizen, was the Jamaican Marcus Garvey (1887â€?940), a newspaper publisher, journalist, and crusader for Pan Africanism through his organization the UNIA. He encouraged people of African ancestry to look favorably upon their ancestral homeland. He wrote a number of essays published as editorials in the UNIA house organ the Negro World newspaper. Some of his lecture material and other writings were compiled and published as nonfiction books by his second wife Amy Jacques Garvey as the Philosophy and Opinions of Marcus Garvey Or, Africa for the Africans (1924) and More Philosophy and Opinions of Marcus Garvey (1977).

Paul Lawrence Dunbar, who often wrote in the rural, black dialect of the day, was the first African American poet to gain national prominence. His first book of poetry, Oak and Ivy, was published in 1893. Much of Dunbar's work, such as When Malindy Sings (1906), which includes photographs taken by the Hampton Institute Camera Club, and Joggin' Erlong (1906) provide revealing glimpses into the lives of rural African-Americans of the day. Though Dunbar died young, he was a prolific poet, essayist, novelist (among them The Uncalled, 1898 and The Fanatics, 1901) and short story writer.

Even though Du Bois, Washington, and Garvey were the leading African American intellectuals and authors of their time, other African American writers also rose to prominence. Among these is Charles W. Chesnutt, a well-known essayist.

The Harlem Renaissance from 1920 to 1940 brought new attention to African American literature. While the Harlem Renaissance, based in the African American community in Harlem in New York City, existed as a larger flowering of social thought and cultureâ€”with numerous Black artists, musicians, and others producing classic works in fields from jazz to theaterâ€”the renaissance is perhaps best known for the literature that came out of it.



Among the most famous writers of the renaissance is poet Langston Hughes. Hughes first received attention in the 1922 poetry collection, The Book of American Negro Poetry. This book, edited by James Weldon Johnson, featured the work of the period's most talented poets (including, among others, Claude McKay, who also published three novels, Home to Harlem, Banjo and Banana Bottom and a collection of short stories). In 1926, Hughes published a collection of poetry, The Weary Blues, and in 1930 a novel, Not Without Laughter. Perhaps, Hughes' most famous poem is "The Negro Speaks of Rivers," which he wrote as a young teen. His single, most recognized character is Jesse B. Simple, a plainspoken, pragmatic Harlemite whose comedic observations appeared in Hughes's columns for the Chicago Defender and the New York Post. Simple Speaks His Mind (1950) is, perhaps, the best-known collection of Simple stories published in book form. Until his death in 1967, Hughes published nine volumes of poetry, eight books of short stories, two novels, and a number of plays, children's books, and translations. 

Another famous writer of the renaissance is novelist Zora Neale Hurston, author of the classic novel Their Eyes Were Watching God (1937). Altogether, Hurston wrote 14 books which ranged from anthropology to short stories to novel-length fiction. Because of Hurston's gender and the fact that her work was not seen as socially or politically relevant, her writings fell into obscurity for decades. Hurston's work was rediscovered in the 1970s in a famous essay by Alice Walker, who found in Hurston a role model for all female African American writers.

While Hurston and Hughes are the two most influential writers to come out of the Harlem Renaissance, a number of other writers also became well known during this period. They include Jean Toomer, who wrote Cane, a famous collection of stories, poems, and sketches about rural and urban Black life, and Dorothy West, author of the novel The Living is Easy, which examined the life of an upper-class Black family. Another popular renaissance writer is Countee Cullen, who described everyday black life in his poems (such as a trip he made to Baltimore, which was ruined by a racial insult). Cullen's books include the poetry collections Color (1925), Copper Sun (1927), and The Ballad of the Brown Girl (1927). Frank Marshall Davis's poetry collections Black Man's Verse (1935) and I am the American Negro (1937), published by Black Cat Press, earned him critical acclaim. Author Wallace Thurman also made an impact with his novel The Blacker the Berry: A Novel of Negro Life (1929), which focused on intraracial prejudice between lighter-skinned and darker-skinned African Americans.

The Harlem Renaissance marked a turning point for African American literature. Prior to this time, books by African Americans were primarily read by other Black people. With the renaissance, though, African American literatureâ€”as well as black fine art and performance artâ€”began to be absorbed into mainstream American culture.

A large migration of African Americans began during World War I, hitting its high point during World War II. During this Great Migration, Black people left the racism and lack of opportunities in the American South and settled in northern cities like Chicago, where they found work in factories and other sectors of the economy.

This migration produced a new sense of independence in the Black community and contributed to the vibrant Black urban culture seen during the Harlem Renaissance. The migration also empowered the growing American Civil Rights movement, which made a powerful impression on Black writers during the 1940s, '50s and '60s. Just as Black activists were pushing to end segregation and racism and create a new sense of Black nationalism, so too were Black authors attempting to address these issues with their writings.

One of the first writers to do so was James Baldwin, whose work addressed issues of race and sexuality. Baldwin, who is best known for his novel Go Tell It on the Mountain, wrote deeply personal stories and essays while examining what it was like to be both Black and homosexual at a time when neither of these identities was accepted by American culture. In all, Baldwin wrote nearly 20 books, including such classics as Another Country and The Fire Next Time.

Baldwin's idol and friend was author Richard Wright, whom Baldwin called "the greatest Black writer in the world for me". Wright is best known for his novel Native Son (1940), which tells the story of Bigger Thomas, a Black man struggling for acceptance in Chicago. Baldwin was so impressed by the novel that he titled a collection of his own essays Notes of a Native Son, in reference to Wright's novel. However, their friendship fell apart due to one of the book's essays, "Everybody's Protest Novel," which criticized Native Son for lacking credible characters and psychological complexity. Among Wright's other books are the autobiographical novel Black Boy (1945), The Outsider (1953), and White Man, Listen! (1957).

The other great novelist of this period is Ralph Ellison, best known for his novel Invisible Man (1952), which won the National Book Award in 1953. Even though Ellison did not complete another novel during his lifetime, Invisible Man was so influential that it secured his place in literary history. After Ellison's death in 1994, a second novel, Juneteenth (1999), was pieced together from the 2,000-plus pages he had written over 40 years. A fuller version of the manuscript will be published as Three Days Before the Shooting (2008).Jones, Edward " The Known World", 2003Carter Stephen, "New England White" 2007Wright W.D. "Crisis of the Black Intellectual",2007



The Civil Rights time period also saw the rise of female Black poets, most notably Gwendolyn Brooks, who became the first African American to win the Pulitzer Prize when it was awarded for her 1949 book of poetry, Annie Allen. Along with Brooks, other female poets who became well known during the 1950s and '60s are Nikki Giovanni and Sonia Sanchez.

During this time, a number of playwrights also came to national attention, notably Lorraine Hansberry, whose play A Raisin in the Sun focuses on a poor Black family living in Chicago. The play won the 1959 New York Drama Critics' Circle Award. Another playwright who gained attention was Amiri Baraka, who wrote controversial off-Broadway plays. In more recent years, Baraka has become known for his poetry and music criticism.

It is also worth noting that a number of important essays and books about human rights were written by the leaders of the Civil Rights Movement. One of the leading examples of these is Martin Luther King, Jr's "Letter from Birmingham Jail".

Beginning in the 1970s, African American literature reached the mainstream as books by Black writers continually achieved best-selling and award-winning status. This was also the time when the work of African American writers began to be accepted by academia as a legitimate genre of American literature.

As part of the larger Black Arts Movement, which was inspired by the Civil Rights and Black Power Movements, African American literature began to be defined and analyzed. A number of scholars and writers are generally credited with helping to promote and define African American literature as a genre during this time period, including fiction writers Toni Morrison and Alice Walker and poet James Emanuel.

James Emanuel took a major step toward defining African American literature when he edited (with Theodore Gross) Dark Symphony: Negro Literature in America, the first collection of black writings released by a major publisher. This anthology, and Emanuel's work as an educator at the City College of New York (where he is credited with introducing the study of African-American poetry), heavily influenced the birth of the genre. Other influential African American anthologies of this time included Black Fire: An Anthology of Afro-American Writing, edited by LeRoi Jones (now known as Amiri Baraka) and Larry Neal in 1968 and The Negro Caravan, co-edited by Sterling Brown, Arthur P. Davis and Ulysses Lee in 1969.

Toni Morrison, meanwhile, helped promote Black literature and authors when she worked as an editor for Random House in the 1960s and '70s, where she edited books by such authors as Toni Cade Bambara and Gayl Jones. Morrison herself would later emerge as one of the most important African American writers of the 20th century. Her first novel, The Bluest Eye, was published in 1970. Among her most famous novels is Beloved, which won the Pulitzer Prize for Fiction in 1988. This story describes a slave who found freedom but killed her infant daughter to save her from a life of slavery. Another important novel is Song of Solomon, a tale about materialism and brotherhood. Morrison is the first African American woman to win the Nobel Prize in Literature.

In the 1970s novelist and poet Alice Walker wrote a famous essay that brought Zora Neale Hurston and her classic novel Their Eyes Were Watching God back to the attention of the literary world. In 1982, Walker won both the Pulitzer Prize and the American Book Award for her novel The Color Purple. An epistolary novel (a book written in the form of letters), The Color Purple tells the story of Celie, a young woman who is sexually abused by her stepfather and then is forced to marry a man who physically abuses her. The novel was later made into a film by Steven Spielberg.

The 1970s also saw African American books topping the bestseller lists. Among the first books to do so was  by Alex Haley. The book, a fictionalized account of Haley's family historyâ€”beginning with the kidnapping of Haley's ancestor Kunta Kinte in Gambia through his life as a slave in the United Statesâ€”won the Pulitzer Prize and became a popular television miniseries. Haley also wrote The Autobiography of Malcolm X in 1965.

Other important writers in recent years include literary fiction writers Gayl Jones, Rasheed Clark, Ishmael Reed, Jamaica Kincaid, Randall Kenan, and John Edgar Wideman. African American poets have also garnered attention. Maya Angelou read a poem at Bill Clinton's inauguration, Rita Dove won a Pulitzer Prize and served as Poet Laureate of the United States from 1993 to 1995, and Cyrus Cassells's Soul Make a Path through Shouting was nominated for a Pulitzer Prize in 1994. Cassells is a recipient of the William Carlos Williams Award. Lesser-known poets like Thylias Moss, and Natasha Trethewey also have been praised for their innovative work. Notable black playwrights include Ntozake Shange, who wrote For Colored Girls Who Have Considered Suicide When the Rainbow Is Enuf; Ed Bullins; Suzan-Lori Parks; and the prolific August Wilson, who won two Pulitzer Prizes for his plays. Most recently, Edward P. Jones won the 2004 Pulitzer Prize for Fiction for The Known World, his novel about a black slaveholder in the antebellum South. 

Young African American novelists include Edwidge Danticat, David Anthony Durham, Tayari Jones, Mat Johnson, ZZ Packer and Colson Whitehead, just to name a few. African American literature has also crossed over to genre fiction. A pioneer in this area is Chester Himes, who in the 1950s and '60s wrote a series of pulp fiction detective novels featuring "Coffin" Ed Johnson and "Gravedigger" Jones, two New York City police detectives. Himes paved the way for the later crime novels of Walter Mosley and Hugh Holton. African Americans are also represented in the genres of science fiction, fantasy and horror, with Samuel R. Delany, Octavia E. Butler, Steven Barnes, Tananarive Due, Robert Fleming, Brandon Massey, Charles R. Saunders, John Ridley, John M. Faucette, Sheree Thomas and Nalo Hopkinson being just a few of the well-known authors.

Finally, African American literature has gained added attention through the work of talk show host Oprah Winfrey, who repeatedly has leveraged her fame to promote literature through the medium of her Oprah's Book Club. At times, she has brought African American writers a far broader audience than they otherwise might have received.

While African American literature is well accepted in the United States, there are numerous views on its significance, traditions, and theories. To the genre's supporters, African American literature arose out of the experience of Blacks in the United States, especially with regards to historic racism and discrimination, and is an attempt to refute the dominant culture's literature and power. In addition, supporters see the literature existing both within and outside American literature and as helping to revitalize the country's writing. To critics, African American literature is part of a Balkanization of American literature. In addition, there are some within the African American community who do not like how their own literature sometimes showcases Black people.

Throughout American history, African Americans have been discriminated against and subject to racist attitudes. This experience inspired some Black writers, at least during the early years of African American literature, to prove they were the equals of white authors. As Henry Louis Gates, Jr, has said, "it is fair to describe the subtext of the history of black letters as this urge to refute the claim that because blacks had no written traditions they were bearers of an inferior culture."

However, by refuting the claims of the dominant culture, African American writers weren't simply "proving their worth"â€”they were also attempting to subvert the literary and power traditions of the United States. Scholars expressing this view assert that writing has traditionally been seen as "something defined by the dominant culture as a white male activity." This means that, in American society, literary acceptance has traditionally been intimately tied in with the very power dynamics which perpetrated such evils as racial discrimination. By borrowing from and incorporating the non-written oral traditions and folk life of the African diaspora, African American literature thereby broke "the mystique of connection between literary authority and patriarchal power." This view of African American literature as a tool in the struggle for Black political and cultural liberation has been stated for decades, perhaps most famously by W.E.B DuBois.

According to James Madison University English professor Joanne Gabbin, African American literature exists both inside and outside American literature. "Somehow African American literature has been relegated to a different level, outside American literature, yet it is an integral part," she says.

This view of African American literature is grounded in the experience of Black people in the United States. Even though African Americans have long claimed an American identity, during most of United States history they were not accepted as full citizens and were actively discriminated against. As a result, they were part of America while also outside it.

The same can be said for African American literature. While it exists fully within the framework of a larger American literature, it also exists as its own entity. As a result, new styles of storytelling and unique voices are created in isolation. The benefit of this is that these new styles and voices can leave their isolation and help revitalize the larger literary world (McKay, 2004). This artistic pattern has held true with many aspects of African American culture over the last century, with jazz and hip hop being just two artistic examples that developed in isolation within the Black community before reaching a larger audience and eventually revitalizing American culture.

Whether African American literature will keep to this pattern in the coming years remains to be seen. Since the genre is already popular with mainstream audiences, it is possible that its ability to develop new styles and voicesâ€”or to remain "authentic," in the words of some criticsâ€”may be a thing of the past.

Despite these views, some conservative academics and intellectuals argue that African American literature only exists as part of a balkanization of literature over the last few decades or as an extension of the culture wars into the field of literature. According to these critics, literature is splitting into distinct and separate groupings because of the rise of identity politics in the United States and other parts of the world. These critics reject bringing identity politics into literature because this would mean that "only women could write about women for women, and only Blacks about Blacks for Blacks."

People opposed to this group-based approach to writing say that it limits the ability of literature to explore the overall human condition and, more importantly, judges ethnic writers merely on the basis of their race. These critics reject this judgment and say it defies the meaning of works like Ralph Ellison's Invisible Man, in which Ellison's main character is invisible because people see him as nothing more than a Black man. Others criticize special treatment of any ethnic-based genre of literature. For example, Robert Hayden, the first African-American Poet Laureate Consultant in Poetry to the Library of Congress, once said (paraphrasing the comment by the black composer Duke Ellington about jazz and music), "There is no such thing as Black literature. There's good literature and bad. And that's all."

Proponents counter that the exploration of group and ethnic dynamics through writing actually deepens human understanding and that, previously, entire groups of people were ignored or neglected by American literature. (Jay, 1997)

The general consensus view appears to be that American literature is not breaking apart because of new genres like African American literature. Instead, American literature is simply reflecting the increasing diversity of the United States and showing more signs of diversity than ever before in its history (Andrews, 1997; McKay, 2004). This view is supported by the fact that many African American authorsâ€”and writers representing other minority groupsâ€”consistently reach the tops of the best-seller lists. If their literature only appealed to their individual ethnic groups, this would not be possible.

Some of the criticism of African American literature over the years has come from within the African American community; some argue that Black literature sometimes does not portray Black people in a positive light.

This clash of aesthetics and racial politics has its beginnings in comments made by W.E.B DuBois in the NAACP publication The Crisis. For example, in 1921 he wrote, "We want everything that is said about us to tell of the best and highest and noblest in us. We insist that our Art and Propaganda be one." He added to this in 1926 by saying, "All Art is propaganda and ever must be, despite the wailing of the purists." DuBois and the editors of The Crisis consistently stated that literature was a tool in the struggle for African American political liberation.

DuBois's belief in the propaganda value of art showed most clearly when he clashed in 1928 with African American author Claude McKay over McKay's best-selling novel Home to Harlem. To DuBois, the novel's frank depictions of sexuality and the nightlife in Harlem only appealed to the "prurient demand[s]" of white readers and publishers looking for portrayals of Black "licentiousness." DuBois also said, "Home to Harlem ... for the most part nauseates me, and after the dirtier parts of its filth I feel distinctly like taking a bath." This criticism was repeated by others in the Black community when author Wallace Thurman published his novel The Blacker the Berry in 1929. This novel, which focused on intraracial prejudice between lighter-skinned and darker-skinned Blacks, infuriated many African Americans, who did not like such a public airing of their culture's "dirty laundry."

Naturally, many African American writers did not agree with the viewpoint that all Black literature should be propaganda, and instead stated that literature should present the truth about life and people. Langston Hughes articulated this view in his essay "The Negro Artist and the Racial Mountain" (1926), when he said that Black artists intended to express themselves freely no matter what the Black public or white public thought.

A more recent occurrence of this Black-on-Black criticism arose in charges by some critics that Alice Walker's novel The Color Purple unfairly attacked Black men. In addition, African American author Charles R. Johnson, in the updated 1995 introduction to his novel Oxherding Tale, criticized Walker's novel for its negative portrayal of African-American males, adding "I leave it to readers to decide which book pushes harder at the boundaries of convention, and inhabits most confidently the space where fiction and philosophy meet." Walker later refuted these charges in her book The Same River Twice: Honoring the Difficult.











-- () 07:56, 14 March 2008 (UTC)dharm

Electromagnetic (EM) radiation, also called light even though it is not always visible, is a self-propagating wave in space with electric and magnetic components. These components oscillate at right angles to each other and to the direction of propagation, and are in phase with each other. Electromagnetic radiation is classified into types according to the frequency of the wave: these types include, in order of increasing frequency, radio waves, microwaves, terahertz radiation, infrared radiation, visible light, ultraviolet radiation, X-rays and gamma rays.

EM radiation carries energy and momentum, which may be imparted when it interacts with matter.

Electromagnetic waves were first postulated by James Clerk Maxwell and subsequently confirmed by Heinrich Hertz. Maxwell derived a wave form of the electric and magnetic equations, revealing the wave-like nature of electric and magnetic fields, and their symmetry. Because the speed of EM waves predicted by the wave equation coincided with the measured speed of light, Maxwell concluded that light itself is an EM wave.

According to Maxwell's equations, a time-varying electric field generates a magnetic field and vice versa. Therefore, as an oscillating electric field generates an oscillating magnetic field, the magnetic field in turn generates an oscillating electric field, and so on. These oscillating fields together form an electromagnetic wave.

A quantum theory of the interaction between electromagnetic radiation and matter such as electrons is described by the theory of quantum electrodynamics.

Electric and magnetic fields obey the properties of superposition, so fields due to particular particles or time-varying electric or magnetic fields contribute to the fields due to other causes. (As these fields are vector fields, all magnetic and electric field vectors add together according to vector addition.) These properties cause various phenomena including refraction and diffraction. For instance, a travelling EM wave incident on an atomic structure induces oscillation in the atoms, thereby causing them to emit their own EM waves. These emissions then alter the impinging wave through interference.

Since light is an oscillation, it is not affected by travelling through static electric or magnetic fields in a linear medium such as a vacuum. In nonlinear media such as some crystals, however, interactions can occur between light and static electric and magnetic fields - these interactions include the Faraday effect and the Kerr effect.

In refraction, a wave crossing from one medium to another of different density alters its speed and direction upon entering the new medium. The ratio of the refractive indices of the media determines the degree of refraction, and is summarized by Snell's law. Light disperses into a visible spectrum as light is shone through a prism because of refraction.

The physics of electromagnetic radiation is electrodynamics, a subfield of electromagnetism.

EM radiation exhibits both wave properties and particle properties at the same time (see wave-particle duality). The wave characteristics are more apparent when EM radiation is measured over relatively large timescales and over large distances, and the particle characteristics are more evident when measuring small distances and timescales. Both characteristics have been confirmed in a large number of experiments.

There are experiments in which the wave and particle natures of electromagnetic waves appear in the same experiment, such as the diffraction of a single photon. When a single photon is sent through two slits, it passes through both of them interfering with itself, as waves do, yet is detected by a photomultiplier or other sensitive detector only once. Similar self-interference is observed when a single photon is sent into a Michelson interferometer or other interferometers.

An important aspect of the nature of light is frequency. The frequency of a wave is its rate of oscillation and is measured in hertz, the SI unit of frequency, equal to one oscillation per second. Light usually has a spectrum of frequencies which sum together to form the resultant wave. Different frequencies undergo different angles of refraction. 

A wave consists of successive troughs and crests, and the distance between two adjacent crests or troughs is called the wavelength. Waves of the electromagnetic spectrum vary in size, from very long radio waves the size of buildings to very short gamma rays smaller than atom nuclei. Frequency is inversely proportional to wavelength, according to the equation: 

where v is the speed of the wave (c in a vacuum, or less in other media), f is the frequency and Î» is the wavelength. As waves cross boundaries between different media, their speeds change but their frequencies remain constant. 

Interference is the superposition of two or more waves resulting in a new wave pattern. If the fields have components in the same direction, they constructively interfere, while opposite directions cause destructive interference. 

The energy in electromagnetic waves is sometimes called radiant energy.

Because energy of an EM wave is quantized, in the particle model of EM radiation, a wave consists of discrete packets of energy, or quanta, called photons. The frequency of the wave is proportional to the magnitude of the particle's energy. Moreover, because photons are emitted and absorbed by charged particles, they act as transporters of energy. The energy per photon can be calculated by Planck's equation:

where E is the energy, h is Planck's constant, and f is frequency.This photon-energy expressionis a particular case of the energy levels of the more generalelectromagnetic oscillator whose average energy, which is used to obtain Planck's radiation law,can be shown to differ sharply from that predicted by the equipartition principleat low temperature, thereby establishes a failure of equipartition due to quantum effects at low temperature.

As a photon is absorbed by an atom, it excites an electron, elevating it to a higher energy level. If the energy is great enough, so that the electron jumps to a high enough energy level, it may escape the positive pull of the nucleus and be liberated from the atom in a process called photoionisation. Conversely, an electron that descends to a lower energy level in an atom emits a photon of light equal to the energy difference. Since the energy levels of electrons in atoms are discrete, each element emits and absorbs its own characteristic frequencies.

Together, these effects explain the absorption spectra of light. The dark bands in the spectrum are due to the atoms in the intervening medium absorbing different frequencies of the light. The composition of the medium through which the light travels determines the nature of the absorption spectrum. For instance, dark bands in the light emitted by a distant star are due to the atoms in the star's atmosphere. These bands correspond to the allowed energy levels in the atoms. A similar phenomenon occurs for emission. As the electrons descend to lower energy levels, a spectrum is emitted that represents the jumps between the energy levels of the electrons. This is manifested in the emission spectrum of nebulae. Today, scientists use this phenomenon to observe what elements a certain star is composed of. It is also used in the determination of the distance of a star, using the so-called red shift.

Any electric charge which accelerates, or any changing magnetic field, produces electromagnetic radiation. Electromagnetic information about the charge travels at the speed of light. Accurate treatment thus incorporates a concept known as retarded time (as opposed to advanced time, which is unphysical in light of causality), which adds to the expressions for the electrodynamic electric field and magnetic field. These extra terms are responsible for electromagnetic radiation. When any wire (or other conducting object such as an antenna) conducts alternating current, electromagnetic radiation is propagated at the same frequency as the electric current. Depending on the circumstances, it may behave as a wave or as particles. As a wave, it is characterized by a velocity (the speed of light), wavelength, and frequency. When considered as particles, they are known as photons, and each has an energy related to the frequency of the wave given by Planck's relation E = hÎ½, where E is the energy of the photon, h = 6.626 Ã— 10-34 JÂ·s is Planck's constant, and Î½ is the frequency of the wave.

One rule is always obeyed regardless of the circumstances: EM radiation in a vacuum always travels at the speed of light, relative to the observer, regardless of the observer's velocity. (This observation led to Albert Einstein's development of the theory of special relativity.)

In a medium (other than vacuum), velocity of propagation or refractive index are considered, depending on frequency and application. Both of these are ratios of the speed in a medium to speed in a vacuum.





[[Image:spectrum.png|right|frame|Legend:Î³ = Gamma raysHX = Hard X-raysSX = Soft X-RaysEUV = Extreme ultravioletNUV = Near ultravioletVisible lightNIR = Near infraredMIR = Moderate infraredFIR = Far infraredRadio waves:EHF = Extremely high frequency (Microwaves)SHF = Super high frequency (Microwaves)UHF = Ultrahigh frequencyVHF = Very high frequencyHF = High frequencyMF = Medium frequencyLF = Low frequencyVLF = Very low frequencyVF = Voice frequencyELF = Extremely low frequency]]

Generally, EM radiation is classified by wavelength into electrical energy, radio, microwave, infrared, the visible region we perceive as light, ultraviolet, X-rays and gamma rays.

The behavior of EM radiation depends on its wavelength. Higher frequencies have shorter wavelengths, and lower frequencies have longer wavelengths. When EM radiation interacts with single atoms and molecules, its behavior depends on the amount of energy per quantum it carries. Electromagnetic radiation can be divided into octaves â€?as sound waves are â€?winding up with eighty-one octaves.

Spectroscopy can detect a much wider region of the EM spectrum than the visible range of 400 nm to 700 nm. A common laboratory spectroscope can detect wavelengths from 2 nm to 2500 nm. Detailed information about the physical properties of objects, gases, or even stars can be obtained from this type of device. It is widely used in astrophysics. For example, hydrogen atoms emit radio waves of wavelength 21.12 cm.



EM radiation with a wavelength between approximately 400 nm and 700 nm is detected by the human eye and perceived as visible light. Other wavelengths, especially nearby infrared (longer than 700 nm) and ultraviolet (shorter than 400 nm) are also sometimes referred to as light, especially when the visibility to humans is not relevant. 

If radiation having a frequency in the visible region of the EM spectrum reflects off of an object, say, a bowl of fruit, and then strikes our eyes, this results in our visual perception of the scene. Our brain's visual system processes the multitude of reflected frequencies into different shades and hues, and through this not-entirely-understood psychophysical phenomenon, most people perceive a bowl of fruit.

At most wavelengths, however, the information carried by electromagnetic radiation is not directly detected by human senses. Natural sources produce EM radiation across the spectrum, and our technology can also manipulate a broad range of wavelengths. Optical fiber transmits light which, although not suitable for direct viewing, can carry data that can be translated into sound or an image. The coding used in such data is similar to that used with radio waves.



Radio waves can be made to carry information by varying a combination of the amplitude, frequency and phase of the wave within a frequency band.

When EM radiation impinges upon a conductor, it couples to the conductor, travels along it, and induces an electric current on the surface of that conductor by exciting the electrons of the conducting material. This effect (the skin effect) is used in antennas. EM radiation may also cause certain molecules to absorb energy and thus to heat up; this is exploited in microwave ovens.

Electromagnetic waves as a general phenomenon were predicted by the classical laws of electricity and magnetism, known as Maxwell's equations. If you inspect Maxwell's equations without sources (charges or currents) then you will find that, along with the possibility of nothing happening, the theory will also admit nontrivial solutions of changing electric and magnetic fields. Beginning with Maxwell's equations for free space:

One solution,

is trivial.

To see the more interesting one, we utilize vector identities, which work for any vector, as follows:

To see how we can use this take the curl of equation (2):

Evaluating the left hand side:

Evaluate the right hand side:

Equations (6) and (7) are equal, so this results in a vector-valued differential equation for the electric field, namely

Or more simply:

Which, as it turns out, is the speed of light in free space. Maxwell's equations have unified the permittivity of free space , the permeability of free space , and the speed of light itself, c0. Before this derivation it was not known that there was such a strong relationship between light and electricity and magnetism.

But these are only two equations and we started with four, so there is still more information pertaining to these waves hidden within Maxwell's equations. Let's consider a generic vector wave for the electric field.

Here  is the constant amplitude,  is any second differentiable function,  is a unit vector in the direction of propagation, and is a position vector. We observe that  is a generic solution to the wave equation. In other words 

for a generic wave traveling in the  direction.

This form will satisfy the wave equation, but will it satisfy all of Maxwell's equations, and with what corresponding magnetic field?

The first of Maxwell's equations implies that electric field is orthogonal to the direction the wave propagates.

The second of Maxwell's equations yields the magnetic field. The remaining equations will be satisfied by this choice of .

Not only are the electric and magnetic field waves traveling at the speed of light, but they have a special restricted orientation and proportional magnitudes, , which can be seen immediately from the Poynting vector. The electric field, magnetic field, and direction of wave propagation are all orthogonal, and the wave propagates in the same direction as .

From the viewpoint of an electromagnetic wave traveling forward, the electric field might be oscillating up and down, while the magnetic field oscillates right and left; but this picture can be rotated with the electric field oscillating right and left and the magnetic field oscillating down and up. This is a different solution that is traveling in the same direction. This arbitrariness in the orientation with respect to propagation direction is known as polarization.







 

Belmopan (), estimated population 12,300, is the capital of Belize. 

Belmopan is located in Belize, Central America at , at an altitude of 76 meters (250Â ft) above sea level. Belmopan was constructed just to the east of Belize River, 80 km (50 miles) inland from the former capital, the port of Belize City, after that city's near destruction by a hurricane in 1961. The government was moved to Belmopan in 1970, and its National Assembly Building is designed to resemble a Pre-Columbian Maya temple. 

After Hurricane Hattie, with winds of up to , and following the destruction of approximately 75% of the houses and business places in Belize City, the government of the day proposed to encourage and promote the building of a new capital city. This new capital would be situated on better terrain, would entail no costly reclamation of land, and would also provide for an industrial area, and in 1962, a committee chose the site now known as Belmopan, located 82 kilometres (51Â miles) west of the old capital of Belize City. In 1964, since Belize was still a colony (known as British Honduras), Premier George Cadle Price led a delegation to London in the United Kingdom to seek funds to finance the new capital. Although they were not ready to commit to funding such a large project, the British government showed interest due to the logic of locating the capital on high ground safe from tidal waves. In order to encourage financial commitment from the British Government, Premier Price and the PUP government invited Mr. Anthony Greenwood, Secretary of State for the Commonwealth and Colonies to visit Belize.

One of the highlights of this visit was the unveiling of a monument at mile 49 on the Western Highway. The monument records that Lord Greenwood dedicated the site for the new capital on October 9, 1965. Thus in a fashion there was a commitment. The name chosen for the new capital -- Belmopan -- is derived from union of two words: "Belize," the name of the longest river in the country, and "Mopan," one of the rivers in this area, which empties into the Belize River. The initial estimated cost for building this new city was forty million Belize dollars (twenty million U.S. dollars), however, only twenty million Belize dollars (ten million U.S. dollars) were available, but the momentum was not to be lost. In 1967, work began; the first phase of the new city was completed in 1970 at a cost of 24,000,000 Belize dollars (12,000,000 U.S. dollars). From 1970 to 2000 the administration of Belmopan was managed by the Reconstruction and Development Corporation, known as "Recondev." Recondev was vested with the power and authority to provide, or cause to be provided, the municipal functions necessary for the smooth running of the city's business and infrastructure. There was a reluctance initially amongst foreign governments to relocate their embassies to Belmopan, as there was some doubt as to whether this inland area would really become the functioning capital of Belize. In February 2005, the United States government broke ground and started building a new United States Embassy in Belmopan, 43 years after Belmopan was chosen as the new capital city. The Embassy was officially opened on Monday, December 11, 2006.

Belmopan is  inland from the Caribbean Sea and 76 metres (250Â ft) above sea level, located near the Belize River Valley with a view of the Mountain Pine Ridge foothills. The climate at night is cool. The city is located off of the Hummingbird Highway. Two and a half hours south of Belmopan, by road, is the Cockscomb Basin Wildlife Sanctuary.

The City of Belmopan boasts 3 Pre-Schools, 3 Primary Schools and 2 Secondary Schools as well as a modern Regional Language Centre (RLC), located on the central campus of the University of Belize, where students from neighboring Spanish speaking countries come to study English. University of Belize's campus in Belmopan comprises the following faculties: Education and Arts, Management and Social Sciences, Science and Technology, and Nursing and Allied Health. The Church/State system prevails in Belizean education especially where Pre-School, Primary and Secondary School education is concerned, and nearly all schools in Belmopan are sustained by churches.

Local missionaries and non-profit organizations also provide practical educational opportunities for Belizeans.

Belmopan proper is a mix of ethnicities including Kriols, Garifuna, Mestizo, Maya and more historically recent immigrants from Asian countries such as the People's Republic of China and Republic of China.

There are five (5) zones around Belmopan proper comprised of the following: 

Some of Belmopan's noteworthy events include presentations by the Belmopan Choral Society, the Festival of Arts for school children, and National Day Activities.

The University of Belize's Black Jaguars squad has won two national championships playing out of Belmopan. Nearby communities including Roaring Creek, Camalote, Esperanza, and Georgeville play a softball tournament in the early part of the year.

The City Council promotes Belmopan as "The Garden City." A Crime Prevention Initiative has recently been introduced by the Council in conjunction with the Belize Police Department, which introduced a Special Constable/Community Policing Programme. The Council cooperates with social organizations like the Lions Club, the Belize Scout Association, Rotary International, and other NGOs.

Belmopan currently has approximately 589 business establishments (the 1997 census revealed the presence of 373). Five international banks are located in the city, as are several local financial institutions. A modern bus terminal and market complex was constructed in 2003.

Within the zoning regulations, Belmopan has set aside approximately 200 acres (81Â ha) of land made up mostly of one acre (4,000Â mÂ²) parcels within city limits. While there is very little industrial activity at the present time, the council has embarked on a scheme to attract local and foreign investment to the city. Plans are underway to create a 100 acre (40Â ha) industrial park close to the municipal airstrip, a paved strip with no control tower or hangars.

From its inception, Belmopan was governed by a corporation answering to the government, called RECONDEV (Reconstruction and Development Corporation).

Residents of Belmopan voted in a referendum held in 1999 to switch to direct election of a city council. In 2000, Belmopan was incorporated as a city and held its first City Council election. Anthony Chanona of the People's United Party was elected mayor with a six-man slate, and reelected in 2003. Following the UDP's municipal victory of 2006, the Mayor of Belmopan is Simeon LÃ³pez.













The Relapse, or, Virtue in Danger is a Restoration comedy from 1696 written by John Vanbrugh. The play is a sequel to Colley Cibber's Love's Last Shift, or, Virtue Rewarded. 

In Cibber's Love's Last Shift, a free-living Restoration rake is brought to repentance and reform by the ruses of his wife, while in The Relapse, the rake succumbs again to temptation and has a new love affair. His virtuous wife is also subjected to a determined seduction attempt, and resists with difficulty.

Vanbrugh planned The Relapse around particular actors at Drury Lane, writing their stage habits, public reputations, and personal relationships into the text. One such actor was Colley Cibber himself, who played the luxuriant fop Lord Foppington in both Love's Last Shift and The Relapse. However, Vanbrugh's artistic plans were threatened by a cutthroat struggle between London's two theatre companies, each of which was "seducing" actors from the other. The Relapse came close to not being produced at all, but the successful performance that was eventually achieved in November 1696 vindicated Vanbrugh's intentions, and saved the company from bankruptcy as well. 

Unlike Love's Last Shift, never again performed after the 1690s, The Relapse has retained its audience appeal. In the 18th century, however, its tolerant attitude towards actual and attempted adultery gradually became unacceptable to public opinion, and the original play was for a century replaced on the stage by Sheridan's moralised version A Trip to Scarborough (1777). On the modern stage, The Relapse has been established as one of the most popular Restoration comedies, valued for Vanbrugh's light, throwaway wit and the consummate acting part of Lord Foppington, a burlesque character with a dark side.

Love's Last Shift can be seen as an early sign of Cibber's sensitivity to shifts of public opinion, which was to be useful to him in his later career as manager at Drury Lane (see Colley Cibber). In the 1690s, the economic and political power balance of the nation tilted from the aristocracy towards the middle class after the Glorious Revolution of 1689, and middle-class values of religion, morality, and gender roles became more dominant, not least in attitudes to the stage. Love's Last Shift is one of the first illustrations of a massive shift in audience taste, away from the analytic bent and sexual frankness of Restoration comedy and towards the conservative certainties and gender role backlash of exemplary or sentimental comedy. The play illustrates Cibber's opportunism at a moment in time before the change was assured: fearless of self-contradiction, he puts something into his first play to please every section of the audience, combining the old outspokenness with the new preachiness. The way Vanbrugh, in his turn, allows the reformed rake to relapse quite cheerfully, and has the only preaching in the play come from the comically corrupt parson of "Fatgoose Living", has made some early 20th-century critics refer to The Relapse as the last of the true Restoration comedies. However, Vanbrugh's play is also affected by the taste of the 1690s, and compared to a play like the courtier William Wycherley's The Country Wife of 20 years earlier, with its celebration of predatory aristocratic masculinity, The Relapse contains quite a few moments of morality and uplift. In fact it has a kind of parallel structure to Love's Last Shift: in the climactic scene of Cibber's play, Amanda's virtue reforms her husband, and in the corresponding scene of The Relapse, it reforms her admirer Worthy. Such moments have not done the play any favours with modern critics.

Love's Last Shift is the story of a last "shift" or trick that a virtuous wife, Amanda, is driven to in order to reform and retain her rakish husband Loveless. Loveless has been away for ten years, dividing his time between the brothel and the bottle, and no longer recognizes his wife when he returns to London. Acting the part of a high-class prostitute, Amanda lures Loveless into her luxurious house and treats him to the night of his dreams, confessing her true identity in the morning. Loveless is so impressed that he immediately reforms. A minor part that was a great hit with the premiÃ¨re audience is the fop Sir Novelty Fashion, written by Cibber for himself to play. Sir Novelty flirts with all the women, but is more interested in his own exquisite appearance and witticisms, and Cibber would modestly write in his autobiography 45 years later, "was thought a good portrait of the foppery then in fashion". Combining daring sex scenes with sentimental reconciliations and Sir Novelty's buffoonery, Love's Last Shift offered something for everybody, and was a great box-office hit.

Vanbrugh's The Relapse is less sentimental and more analytical than Love's Last Shift, subjecting both the reformed husband and the virtuous wife to fresh temptations, and having them react with more psychological realism. Loveless falls for the vivacious young widow Berinthia, while Amanda barely succeeds in summoning her virtue to reject her admirer Worthy. The three central characters, Amanda, Loveless, and Sir Novelty (ennobled by Vanbrugh into "Lord Foppington"), are the only ones that recur in both plays, the remainder of the Relapse characters being new. 

In the trickster subplot, young Tom tricks his elder brother Lord Foppington out of his intended bride and her large dowry. This plot takes up nearly half the play and expands the part of Sir Novelty to give more scope for the roaring success of Cibber's fop acting. Recycling Cibber's merely fashion-conscious fop, Vanbrugh lets him buy himself a title and equips him with enough aplomb and selfishness to weather all humiliations. Although Lord Foppington may be "very industrious to pass for an ass", as Amanda remarks, he is at bottom "a man who Nature has made no fool" (II.i.148). Literary historians agree in esteeming him "the greatest of all Restoration fops" (DobrÃ©e), "brutal, evil, and smart" (Hume).

In the early 1690s, London had only one officially countenanced theatre company, the "United Company", badly managed and with its takings bled off by predatory investors ("adventurers"). To counter the draining of the company's income, the manager Christopher Rich slashed the salaries and traditional perks of his skilled professional actors, antagonizing such popular performers as Thomas Betterton, the tragedienne Elizabeth Barry, and the comedienne Anne Bracegirdle. Colley Cibber wrote in his autobiography that the owners of the United Company, "who had made a monopoly of the stage, and consequently presumed they might impose what conditions they pleased upon their people, did not consider that they were all this while endeavouring to enslave a set of actors whom the publicâ€?were inclined to support." Betterton and his colleagues set forth the bad finances of the United Company and the plight of the actors in a "Petition of the Players" submitted to the Lord Chamberlain. This unusual document is signed by nine men and six women, all established professional actors, and details a disreputable jumble of secret investments and "farmed" shares, making the case that owner chicanery rather than any failure of audience interest was at the root of the company's financial problems. Barely veiled strike threats in the actors' petition were met with an answering lockout threat from Rich in a "Reply of the Patentees", but the burgeoning conflict was pre-empted by a suspension of all play-acting from December until March 1695 on account of Queen Mary's illness and death. During this interval, a cooperative actors' company took shape under the leadership of Betterton and was granted a Royal "licence to act" on March 25, to the dismay of Rich, who saw the threat too late.

The two companies that emerged from this labour/management conflict are usually known respectively as the "Patent Company" (the no-longer-united United Company) and "Betterton's Company", although Judith Milhous argues that the latter misrepresents the cooperative nature of the actors' company. In the following period of intense rivalry, the Patent Company was handicapped by a shortage of competent actors. "Seducing" actors (as the legal term was) back and forth between the companies was a key tactic in the ensuing struggle for position, and so were appeals to the Lord Chamberlain to issue injunctions against seductions from the other side, which that functionary was quite willing to do. Later Rich also resorted to hiring amateurs, and to tempting Irish actors over from Dublin. But such measures were not yet in place for the staging of The Relapse in 1696, Rich's most desperate venture.

Vanbrugh is assumed to have attempted to tailor his play to the talents of particular actors and to what audiences would expect from them, as was normal practice (Holland), but this was exceptionally difficult to accomplish in 1695â€?6. Love's Last Shift had been cast from the remnants of the Patent Companyâ€?learners" and "boys and girls"â€”after the walkout of the stars. Following the surprising success of this young cast, Vanbrugh and Rich had even greater difficulty in retaining the actors needed for The Relapse. However, in spite of the continuous emergency in which the Relapse production was mounted, most of Vanbrugh's original intentions were eventually carried out.

 To cast Love's Last Shift in January 1696, the Patent Company had to make the best use of such actors as remained after the 1694 split (see cast list right). An anonymous contemporary pamphlet describes the "despicable condition" the troupe had been reduced to: 

The disproportion was so great at parting, that it was almost impossible, in Drury Lane, to muster up a sufficient number to take in all the parts of any play; and of them so few were tolerable, that a play must of necessity be damned, that had not extraordinary favour from the audience. No fewer than sixteen (most of the old standing) went away; and with them the very beauty and vigour of the stage; they who were left being for the most part learners, boys and girls, a very unequal match for them that revolted.

The only well-regarded performers available were the Verbruggens, John and Susanna, who had been re-seduced by Rich from Betterton's company. They were of course used in Love's Last Shift, with John playing Loveless, the male lead, and his wife Susanna the flirtatious heiress Narcissa, a secondary character. The rest of the cast consisted of the new and untried (for instance Hildebrand Horden, who had just joined Rich's troupe, playing a rakish young lover), the modest and lacklustre (Jane Rogers, playing Amanda, and Mary Kent, playing Sir Novelty's mistress Flareit), and the widely disliked (the opportunist Colley Cibber, playing Sir Novelty Fashion); people who had probably never been given the option of joining Betterton. Betterton's only rival as male lead, George Powell, had most likely been left behind by the rebels with some relief (Milhous); while Powell was skilled and experienced, he was also notorious for his bad temper and alcoholism. Throughout the "seduction" tug-of-war between Rich and Betterton in 1695â€?6, Powell remained at Drury Lane, where he was in fact not used for Love's Last Shift, but would instead spectacularly demonstrate his drinking problem at the premiÃ¨re of The Relapse.

Vanbrugh planned The Relapse, too, round these limited casting resources and minor talents, which Peter Holland has argued explains the robust, farcical character of the play; Vanbrugh's second comedy, The Provoked Wife (1697), written for the better actors of the cooperative company, is a much subtler piece. The Relapse was written in six weeks and offered to the Patent Company in March, but because of the problems with contracting and retaining actors, it did not premiÃ¨re until November. It is known from Cibber's autobiography that Vanbrugh had a decisive say in the ongoing casting changes made during these seven months; it is not known whether he altered his text to accommodate them. 

To reinforce the connection with Love's Last Shift and capitalize on its unusual success, Vanbrugh designed the central roles of Loveless, Amanda, and Sir Novelty for the same actors: John Verbruggen, Jane Rogers, and Colley Cibber. Keeping Rogers as Amanda was not a problem, since she was not an actress that the companies fought over, but holding on to John Verbruggen and Colley Cibber posed challenges, to which Rich rose with energetic campaigns of bribery and re-seduction. Filling the rest of the large Relapse cast presented a varied palette of problems, which forced some unconventional emergency casting. 

John Verbruggen was one of the original rebels and had been offered a share in the actors' company, but became disgruntled when his wife Susanna, a popular comedienne, was not. For Rich, it was a stroke of luck to get Susanna and John back into his depleted and unskilled troupe. John's availability to play Loveless remained precarious, however. In September, when The Relapse had still not been staged after six months of trying (probably because Rich was still parleying with Cibber about his availability as Lord Foppington), John was still complaining about his employment situation, even getting into a physical fight over it at the theatre. This misbehaviour caused the Lord Chamberlain to declare his contract void and at the same time order him to stay with the Patent Company until January 1697, to give Rich time to find a replacement. The original Loveless was thus finally guaranteed for an autumn season run of The Relapse. Since the loyal Verbruggen couple always moved as a unit, Susanna's services were also assured. 

The Verbruggens were essential to the play, not least because Vanbrugh had customized the sprightly temptress Berinthia to Susanna's talents and reputation for witty, roguish, sexually enterprising characters, most recently Mrs Buxom in Thomas D'Urfey's Don Quixote (a success thanks to "the extraordinary well acting of Mrs Verbruggen", wrote D'Urfey). Although John was less well known, his acting skills were considerable and would flourish after January 1697 in the cooperative company, where commentators even started to compare him with the great Betterton. Verbruggen was considered a more natural, intuitive or "careless" actor, with "a negligent agreeable wildness in his action and his mien, which became him well." Anthony Aston vividly described Verbruggen as "a little in-kneed, which gave him a shambling gait, which was a carelessness, and became him." Modern critics do not find the Loveless part very lively or irresistible, but Vanbrugh was able to count on Verbruggen's shambling male magnetism and "agreeable wildness" to enrich the character. This would originally have worked even in print, since cast lists were included in the published plays: most 1690s play readers were playgoers also, and aware of the high-profile Verbruggens. Happily married in private life and playing the secret lovers Loveless and Berinthia, the Verbruggens have left traces of their charisma and erotic stage presences in Vanbrugh's dialogue. The Relapse even alludes to their real-life relationship, in meta-jokes such as Berinthia's exclamation, "Well, he is a charming man! I don't wonder his wife's so fond of him!"

Hildebrand Horden, who had played a "wild" young lover in Love's Last Shift, was the only young, handsome, potential romantic lead Rich had. He was presumably cast by Vanbrugh as Tom Fashion, Lord Foppington's clever younger brother (Holland), and it was a blow to the Patent Company when he was killed in a tavern brawl (more glamorously referred to as a "duel" in older sources) in May. At the premiÃ¨re in November, Tom Fashion was instead played as a breeches role by Mary Kent, an unusual piece of emergency casting that puts a different face on a uniquely frank homosexual scene where Tom keeps skipping nimbly out of the way of the matchmaker Coupler's lecherous groping.

Colley Cibber was a rather unsuccessful young actor at the time of the split, with a squeaky voice and without any of the physical attractiveness of the soon-to-be-dead Horden. After the success of Love's Last Shift, his status was transformed, with both companies vying for his services as actor and playwright. He made an off-season transfer to Betterton's company in the summer of 1696 and wrote part of a play for the rebels before being re-seduced by Rich by means of a fat contract (Milhous). Cibber as Lord Foppington was thus also assured, and finally the premiÃ¨re of The Relapse could be scheduled with some confidence. Cibber's performance in it was received with even greater acclaim than in his own play, Vanbrugh's Lord Foppington being a larger and, in the estimation of both contemporaries and modern critics, much funnier part than Sir Novelty Fashion. Vanbrugh's play incorporates some of the ad-libbing and affectations of Cibber's by all accounts inspired performance in Love's Last Shift. Cibber has thus imprinted not only his own playwriting but also his acting style and squeaky personality on Vanbrugh's best-known character. 

Vanbrugh's preface to the first edition preserves a single fleeting concrete detail about the premiÃ¨re performance: George Powell was drunk. He played Amanda's worldly and sophisticated admirer Worthy, the "fine gentleman of the play", and apparently brought an unintended hands-on realism to his supposedly suave seduction attempt:

One word more about the bawdy, and I have done. I own the first night this thing was acted, some indecencies had like to have happened, but it was not my fault. The fine gentleman of the play, drinking his mistress's health in Nantes brandy from six in the morning to the time he waddled upon the stage in the evening, had toasted himself up to such a pitch of vigour, I confess I once gave Amanda for gone.

The desperate straits of the United Company, and the success of The Relapse in saving it from collapse, are attested in a private letter from November 19, 1696: "The other house [Drury Lane] has no company at all, and unless a new play comes out on Saturday revives their reputation, they must break." The new play is assumed to have been The Relapse, and it turned out the success Rich needed. "This play", notes Colley Cibber in his autobiography, "from its new and easy turn of wit, had great success, and gave me, as a comedian, a second flight of reputation along with it." Charles Gildon summarizes: "This play was received with mighty applause."

The Relapse is singled out for particular censure in the Puritan clergyman Jeremy Collier's anti-theatre pamphlet Short View of the Immorality and Profaneness of the English Stage (1698), which attacks its lack of poetic justice and moral sentiment. Worthy and Berinthia, complains Collier, are allowed to enact their wiles against the Lovelesses' married virtue without being punished or losing face. The subplot is an even worse offence against religion and morality, as it positively rewards vice, allowing the trickster hero Tom to keep the girl, her dowry, and his own bad character to the end. Vanbrugh failed to take Short View seriously and published a joking reply, but Collier's censure was to colour the perception of the play for centuries. While it remained a popular stage piece through the 18th century, much praised and enjoyed for its wit, attitudes to its casual sexual morality became increasingly ambivalent as public opinion became ever more restrictive in this area, and more at odds with the permissive ethos of Restoration comedy. From 1777 Vanbrugh's original was replaced on the stage by Sheridan's A Trip to Scarborough, a close adaptation but with some "covering", as the prologue explains, drawn over Vanbrugh's "too bare" wit:

Sheridan does not allow Loveless and Berinthia to consummate their relationship, and he withdraws approval from Amanda's admirer Worthy by renaming him "Townly". Some frank quips are silently deleted, and the matchmaker Coupler with the lecherous interest in Tom becomes decorous Mrs Coupler. A small-scale but notable loss is of much of the graphic language of Hoyden's nurse, who is earthy in Vanbrugh's original, genteel in Sheridan. However, Sheridan had an appreciation of Vanbrugh's style, and retained most of the original text unaltered.

In the 19th century, A Trip to Scarborough remained the standard version, and there were also some ad hoc adaptations that sidelined the Lovelesses' drawing-room comedy in favour of the Lord Foppington/Hoyden plot with its caricatured clashes between exquisite fop and pitchfork-wielding country bumpkins. The Man of Quality (1870) was one such robust production, Miss Tomboy (1890) another. Vanbrugh's original Relapse was staged once, in 1846, at the Olympic Theatre in London. 

During the first half of the 20th century The Relapse was relatively neglected, along with other Restoration drama, and experts are uncertain about exactly when Vanbrugh's original again resurged to prominence on the stage and thereby marginalized Sheridan's version. These experts now believe the play may have been first brilliantly rehabilitated by Anthony Quayle's 1947 production at the Phoenix Theatre, starring Cyril Ritchard as Lord Foppington and brought to Broadway by Ritchard in 1950. A musical version, Virtue in Danger (1963), by Paul Dehn and "John Bernard", opened to mixed reviews. John Russell Taylor in Plays and Players praised the cast, which included Patricia Routledge as Berinthia and John Moffatt as Lord Foppington, but complained that the production was "full of the simpering, posturing and sniggering which usually stand in for style and sophistication in Restoration revivals." Vanbrugh's original play is now again a favourite of the stage. A 2001 revival by Trevor Nunn at the National Theatre was described by Sheridan Morley as "rare, loving and brilliantly cast." As so often with commentary on The Relapse, Morley focused on the role of Lord Foppington and its different interpretations: "Alex Jennings superbly inherits the role of Lord Foppington which for 20 years or so belonged to Donald Sinden, and for another 20 before that to Cyril Ritchard."

Restoration Comedy, a play by Amy Freed that draws on both The Relapse and Colley Cibber's prequel Love's Last Shift, premiered at Seattle Repertory Theatre in 2005, starring Stephen Caffrey as Loveless, Caralyn Kozlowski as Amanda, and Jonathan Freeman as Lord Foppington, and directed by Sharon Ott.















Iguanodon ( or , meaning "Iguana tooth") is a genus of ornithopod dinosaur that lived roughly halfway between the first of the swift bipedal hypsilophodontids and the ornithopods' culmination in the duck-billed dinosaurs. Most Iguanodon species lived between 140 to 120 million years ago, in the Valanginian to Barremian ages of the Early Cretaceous Period of Europe, although possible remains are known from North America, Asia, and Africa. Iguanodon's most distinctive features were its large thumb spikes, which were possibly used for defence against predators.

Discovered in 1822 and described three years later by English geologist Gideon Mantell, Iguanodon was the second dinosaur formally named, after Megalosaurus. Together with Megalosaurus and Hylaeosaurus, it was one of the three genera originally used to define Dinosauria. A large, bulky herbivore, Iguanodon is a member of Iguanodontia, along with the duck-billed hadrosaurs. The taxonomy of this genus continues to be a topic of study as new species are named or long-standing ones reassigned to other genera. 

Scientific understanding of Iguanodon has evolved over time as new information has been obtained from the fossils. The numerous specimens of this genus, including nearly complete skeletons from two well-known bonebeds, have allowed researchers to make informed hypotheses regarding many aspects of the living animal, including feeding, movement, and social behaviour. As one of the first scientifically well-known dinosaurs, Iguanodon has occupied a small but notable place in the public's perception of dinosaurs, its artistic representation changing significantly in response to new interpretations of its remains.

Iguanodon was a bulky herbivore that could shift from bipedality to quadrupedality. The best-known species, I. bernissartensis, is estimated to have weighed about 3.08 tonnes (3.5 tons) on average, and measured about 10Â metres long (32.8Â feet) as an adult, with some specimens possibly as a long as 13Â metres (42.6Â ft). Other species were not as large; the similarly robust I. dawsoni is estimated at 8Â metres long (26.2Â ft), and its more lightly-built contemporary I. fittoni at 6Â metres (19.7Â ft). This genus had a large, tall but narrow skull, with a toothless beak probably covered with keratin, and teeth like those of an iguana, but much larger and more closely packed. 

The arms were long (up to 75% the length of the legs in I. bernissartensis) and robust, with rather inflexible hands built so that the three central fingers could bear weight. The thumbs were conical spikes that stuck out away from the three main digits. In early restorations, the spike was placed on the animal's nose. Later fossils revealed the true nature of the thumb spikes, although their exact function is still debated. They could have been used for defence, or for foraging for food. The little finger was elongate and dextrous, and could have been used to manipulate objects. The legs were powerful, but not built for running, and there were three toes on each foot. The backbone and tail were supported and stiffened by ossified tendons, which were tendons that turned to bone during life (these rod-like bones are usually omitted from skeletal mounts and drawings). Overall, in body structure, it was not too dissimilar from its later relatives, the hadrosaurids.



Iguanodon gives its name to the unranked clade Iguanodontia, a very populous group of ornithopods with many species known from the Middle Jurassic to the Late Cretaceous. Aside from Iguanodon, the best-known members of the clade include Dryosaurus, Camptosaurus, Ouranosaurus, and the duck-bills, or hadrosaurs. In older sources, Iguanodontidae was shown as a distinct family. This family traditionally has been something of a wastebasket taxon, including ornithopods that were neither hypsilophodontids or hadrosaurids. In practice, animals like Callovosaurus, Camptosaurus, Craspedodon, Kangnasaurus, Mochlodon, Muttaburrasaurus, Ouranosaurus, and Probactrosaurus were usually assigned to this family. With the advent of cladistic analyses, Iguanodontidae was shown to be paraphyletic, and these animals are recognized to fall at different points in relation to hadrosaurs on a cladogram, instead of in a single distinct clade. Groups like Iguanodontoidea are still used as unranked clades in the scientific literature, though many traditional iguanodontids are now included in the superfamily Hadrosauroidea. Iguanodon lies between Camptosaurus and Ouranosaurus in cladograms, and is probably descended from a camptosaur-like animal. At one point, Jack Horner suggested, based mostly on skull features, that hadrosaurids actually formed two more distantly-related groups, with Iguanodon on the line to the flat-headed hadrosaurines, and Ouranosaurus on the line to the crested lambeosaurines, but his proposal has been rejected.

The discovery of Iguanodon has long been accompanied by a popular legend. The story goes that Gideon Mantell's wife, Mary Ann, discovered the first teeth of an Iguanodon in the strata of Tilgate Forest in Whiteman's Green, Cuckfield, Sussex, England, in 1822 while her husband was visiting a patient. However, there is no evidence that Mantell took his wife with him while seeing patients. Furthermore, he admitted in 1851 that he himself had found the teeth. Not everyone agrees that the story is false, though. Regardless of the exact circumstances, he combed the area for more fossils, and consulted the fossil experts of the time as to what sort of animal the bones might belong to. Most of the scientists, such as William Buckland and Georges Cuvier, thought that the teeth were from fish or mammals. However, Samuel Stutchbury, a naturalist from the Royal College of Surgeons, recognized that they resembled those of an iguana, albeit twenty times larger. Mantell did not describe his findings until 1825, when he presented a paper on the remains to the Royal Society of London. 

In recognition of the resemblance of the teeth to those of the iguana, Mantell named his new genus Iguanodon or "iguana-toothed", from iguana and the Greek word odontos ("tooth"). Based on isometric scaling, he estimated that the creature might have been up to 12Â metres (40Â ft) long. His initial idea for a name was Iguanasaurus ("Iguana lizard"), but his friend William Daniel Conybeare suggested that that name was more applicable to the iguana itself, so a better name would be Iguanoides ("Iguana-like") or Iguanodon. He neglected to add a species name to form a proper binomial, so one was supplied in 1829 by Friedrich Holl: I. anglicum, which was later amended to I. anglicus. 

A better specimen was discovered in a quarry in Maidstone, Kent, in 1834, which Mantell soon acquired. He was able to identify it as an Iguanodon from its distinctive teeth. The Maidstone slab allowed the first skeletal reconstructions and artistic renderings of Iguanodon. As such, he made some mistakes, the most famous of which was the placement of what he thought was a horn on the nose. The discovery of much better specimens in later years revealed that the horn was actually a modified thumb. Still encased in rock, the Maidstone skeleton is currently displayed at the Natural History Museum in London. The borough of Maidstone commemorated this find by adding an Iguanodon as a supporter to their coat of arms in 1949. This specimen has become linked with the name I. mantelli, a species named in 1832 by Christian Erich Hermann von Meyer in place of I. anglicus, but it actually comes from a different formation than the original I. mantelli/I. anglicus material.



At the same time, tension began to build between Mantell and Richard Owen, an ambitious scientist with much better funding and society connections in the turbulent worlds of Reform Act-era British politics and science. Owen, a firm creationist, opposed the early versions of evolutionary science ("transmutationism") then being debated and used what he would soon coin as dinosaurs as a weapon in this conflict. With the paper describing Dinosauria, he scaled down dinosaurs from lengths of over 61Â metres (200Â ft), determined that they were not simply giant lizards, and put forward that they were advanced and mammal-like, characteristics given to them by God; according to the understanding of the time, they could not have been "transmuted" from reptiles to mammal-like creatures. 

Shortly before his death in 1852, Mantell realized that Iguanodon was not a heavy, pachyderm-like animal, as Owen was putting forward, but his passing left him unable to participate in the creation of the Crystal Palace dinosaur sculptures, and so Owen's vision of the dinosaurs became that seen by the public for decades. With Waterhouse Hawkins, he had nearly two dozen lifesize sculptures of various prehistoric animals built out of concrete sculpted over a steel and brick framework; two Iguanodon, one standing and one resting on its belly, were included. Before the sculpture of the standing Iguanodon was completed, he held a banquet for twenty inside it.

The largest find of Iguanodon remains to date occurred in 1878 in a coal mine at Bernissart in Belgium, at a depth of 322 m (1056 ft). With the encouragement of Alphonse Briart, supervisor of mines at nearby Morlanwelz, Louis Dollo, with Louis de Pauw, oversaw excavation of the skeletons and reconstructed them. At least 38 Iguanodon individuals were uncovered, most of which were adults. Many of them went on public display beginning in 1882 and are still present for viewing; 11 are displayed as standing mounts, and 20 as they were (approximately) found. The exhibit makes an impressive display in the Royal Belgian Institute of Natural Sciences, in Brussels. A replica of one of these is on display at the Oxford University Museum of Natural History. Most of the remains were referred to a new species, I. bernissartensis, a larger and much more robust animal than the English remains had yet revealed, but one specimen was referred to the nebulous, gracile I. mantelli. The skeletons were some of the first complete dinosaur skeletons known. Found with the dinosaur skeletons were the remains of plants, fish, and other reptiles, including the crocodilian Bernissartia. 



The science of conserving fossil remains was in its infancy, and was ill-equipped to deal with what soon became known as "pyrite disease". Pyrite in the bones was changing to iron sulphate, damaging the remains by causing them to crack and crumble. When in the ground, the bones were exposed to moisture that prevented this from happening, but when removed into the drier open air, the natural chemical conversion began to occur. Not knowing the true cause, and thinking it was an actual infection, the staff at the Museum in Brussels attempted to treat the problem with a combination of alcohol, arsenic, and shellac. This combination was intended to simultaneously penetrate (alcohol), kill any biological agent (arsenic), and harden (shellac) the fossils. This treatment had the unintended effect of sealing in moisture and extending the period of damage. Modern treatments instead involve either monitoring the humidity of fossil storage, or, for fresh specimens, preparing a special coating of polyethylene glycol that is then heated in a vacuum pump, so moisture is immediately removed and pore space is infiltrated with polyethelene glycol to seal and strengthen the fossil.

Dollo's specimens allowed him to show that Owen's prehistoric pachyderms were not correct for Iguanodon. He instead modelled the skeletal mounts after the emu and wallaby, and put the spike that had been on the nose firmly on the thumb. He was not completely correct, but he also had the disadvantage of being faced with some of the first complete dinosaur remains. A problem that was later recognized was the bend he introduced into the tail. This organ was more or less straight, as shown by the skeletons he was excavating, and the presence of ossified tendons. In fact, to get the bend in the tail for a more wallaby or kangaroo-like posture, the tail would have had to be broken. With its correct, straight tail and back, the animal would have walked with its body held horizontal to the ground, arms in place to support the body if needed.

Excavations at the quarry were stopped in 1881, although it was not exhausted of fossils, as recent drilling operations have shown. During World War I, when the town was occupied by German forces, preparations were made to reopen the mine for palaeontology, and Otto Jaekel was sent from Berlin to supervise. The Allies recaptured Bernissart just as the first fossiliferous layer was about to be uncovered. Further attempts to reopen the mine were hindered by financial problems and were stopped altogether in 1921 when the mine flooded.

Research on Iguanodon decreased during the early part of the 20th century as World Wars and the Great Depression enveloped Europe. A new species that would become the subject of much study and taxonomic controversy, I. atherfieldensis, was named in 1925 by R. W. Hooley, for a specimen collected at Atherfield Point on the Isle of Wight. However, what had been a European genus was now being found worldwide, with material in Africa (teeth from Tunisia and elsewhere in the Sahara Desert), Mongolia (I. orientalis), and the United States in North America (I. ottingeri from Utah and I. lakotaensis from South Dakota).

Iguanodon was not part of the initial work of the dinosaur renaissance that began with the description of Deinonychus in 1969, but it was not neglected for long. David B. Weishampel's work on ornithopod feeding mechanisms provided a better understanding of how it fed, and David B. Norman's work on numerous aspects of the genus has made it one of the best-known dinosaurs. In addition, a further find of numerous Iguanodon skeletons, in Nehden, Nordrhein-Westphalen, Germany, has provided evidence for gregariousness in this genus, as the animals in this areally-restricted find appear to have been killed by flash floods. At least 15 individuals, from 2 to 8Â metres long (6.6 to 26.2Â ft), have been found here, although at least some of them belong to the closely related Mantellisaurus atherfieldensis (at that time believed to be another species of Iguanodon). 

Iguanodon material has also been used in the search for dinosaur DNA and other biomolecules. In research by Graham Embery et al, Iguanodon bones were processed to look for remnant proteins. In this research, identifiable remains of typical bone proteins, such as phosphoproteins and proteoglycans, were found in a rib.

Because Iguanodon is one of the first dinosaur genera to have been named, numerous species have been assigned to it. While never becoming the wastebasket taxon several other early genera of dinosaurs became (such as Megalosaurus and Pelorosaurus), Iguanodon has had a complicated history, and its taxonomy continues to undergo revisions. Remains of the best-known species have come from Belgium, England, Germany, Spain and France. Remains of similar animals possibly belonging to this genus have been found in Tunisia and Mongolia, and distinct species are present in Utah and South Dakota, USA.

I. anglicus was the original type species, but the holotype was based on a single tooth and only partial remains of the species have been recovered since. In March of 2000, the International Commission on Zoological Nomenclature changed the type species to the much better known I. bernissartensis. The original Iguanodon tooth is held at Te Papa Tongarewa, the national museum of New Zealand in Wellington, although it is not on display. The fossil arrived in New Zealand following the move of Gideon Mantell's son Walter there; after the elder Mantell's death, his fossils went to Walter.



Only a few of the many species assigned to Iguanodon are still considered to be valid and to fall within the genus Iguanodon. I. bernissartensis, described by George Albert Boulenger in 1881, is the neotype for the genus. This species is best known for the many skeletons discovered in Bernissart, but is also known from remains across Europe. It may include the dubious Mongolian I. orientalis.

Two species described by Richard Lydekker in the late 1800s are valid, but rarely discussed. I. dawsoni, described by Lydekker in 1889, is known from two partial skeletons from the Valanginian-Barremian-age Lower Cretaceous Hastings Beds of East Sussex, England. I. fittoni was also described by Lydekker, in 1888. Like I dawsoni, this species is known from the Hastings Beds of East Sussex. It may also have been found in Spain. Three partial skeletons are known. The two species are separated on the basis of vertebral and pelvic characters, size, and build. For example, I. dawsoni was more robust than I. fittoni, with large Camptosaurus-like vertebrae featuring short neural spines, whereas I. fittoni is known for its "long, narrow, and steeply inclined neural spines".

I. lakotaensis was described by David B. Weishampel and Philip R. Bjork in 1989. The only well-accepted North American species of Iguanodon, I. lakotaensis was described from a partial skull from the Barremian-age Lower Cretaceous Lakota Formation of South Dakota, USA. Its assignment has been controversial. Some researchers suggest that it was more basal than I. bernissartensis, and related to Theiophytalia, but David Norman has suggested that it was a synonym of I. bernissartensis.

Two species of Iguanodon named by Richard Owen have since been reassigned to other genera. I. hoggi (also spelled I. boggii or hoggii), named by Owen for a lower jaw from the Tithonian-Berriasian-age Upper Jurassic-Lower Cretaceous Purbeck Beds of Dorset in 1874, has been reassigned to Camptosaurus by David Norman and Paul Barrett. I. major, a vertebra from the Isle of Wight described by Owen in 1842 as a species of Streptospondylus, is a nomen dubium which is now thought to be a synonym of I. anglicus.

Other than the two species described by Owen which have been reassigned to other genera, thirteen other species have since been reclassified. Iguanodon albinus (or Albisaurus scutifer), described by Czech paleontologist Antonin Fritsch (correctly FriÄ) in 1893, is a dubious nondinosaurian reptile now known as Albisaurus albinus. I. atherfieldensis, described by R.W. Hooley in 1925, was smaller and less robust than I. bernissartensis, with longer neural spines. It was renamed Mantellisaurus atherfieldensis in 2006. I. exogyrarum (also spelled I. exogirarum or I. exogirarus) was described by Fritsch in 1878. It is a nomen dubium based on very poor material and has been reassigned, by George Olshevsky, to Ponerosteus. I. valdensis, described by Hulke in 1879 from vertebral and pelvic remains, was from the Barremian stage of the Isle of Wight. Originally named Vectisaurus, it is probably a partially-grown specimen of Mantellisaurus atherfieldensis. I. gracilis, named by Lydekker in 1888 as the type species of Sphenospondylus and assigned to Iguanodon in 1969 by Rodney Steel, may belong to Mantellisaurus atherfieldensis.

I. foxii (also spelled I. foxi) was originally described by Thomas Henry Huxley in 1869 as the type species of Hypsilophodon; Owen (1873 or 1874) reassigned it to Iguanodon, but his assignment was soon overturned.I. hollingtoniensis (also spelled I. hollingtonensis), described by Lydekker in 1889, is a synonym of I. fittoni. I. prestwichii (also spelled I. prestwichi), described by John Hulke in 1880, has been reassigned to Camptosaurus prestwichii. I. seeleyi (also spelled I. seelyi), described by Hulke two years after I. prestwichii, has been synonymized with I. bernissartensis. I. suessii, described by Emanuel Bunzel in 1871, has been reassigned to Mochlodon suessi.

Iguanodon mantelli (also spelled I. manteli or I. mantellii), described by Christian Erich Hermann von Meyer in 1832, is actually based on the same material as I. anglicus. Several skeletons, however, including the Maidstone specimen and one of the Bernissart skeletons have been assigned here over the years, and their attribution is not complete. The gracile Bernissart skeleton, for example, has been reassigned to Mantellisaurus atherfieldensis. I. orientalis, described by A. K. Rozhdestvensky in 1952, was based on poor material, but a skull with a distinctive arched snout that had been assigned to it was renamed Altirhinus kurzanovi in 1998. At the same time, I. orientalis was considered to be a nomen dubium indistinguishable from I. bernissartensis. Harry Seeley described I. phillipsi in 1869, but later reassigned it to Priodontognathus.

Five Iguanodon species are considered to be nomina dubia or undescribed. I. anglicus, described by Friedrich Holl in 1829, is the original type species of Iguanodon, but, as discussed above, was replaced by I. bernissartensis. In the past, it has been spelled as I. angelicus (Lessem and Glut, 1993) and I. anglicum (Holl, 1829 emend. Bronn, 1850). It is known from teeth from the Valanginian-Barremian-age Lower Cretaceous Tilgate Forest of East Sussex, England. I. hillii, coined by Edwin Tully Newton in 1892 for a tooth from the early Cenomanian Upper Cretaceous Lower Chalk of Hertfordshire, is an early hadrosaurid of some sort. "I. mongolensis" (Whitfield, 1992) is a nomen nudum from a photo caption in a book, of remains that would later be named Altirhinus.

I. ottingeri, described by Peter Galton and James A. Jensen in 1979, is a nomen dubium based on teeth from the possibly Aptian-age lower Cedar Mountain Formation of Utah. I. praecursor (also spelled I. precursor), described by E. Sauvage in 1876 from teeth from an unnamed Kimmeridgian (Late Jurassic) formation in Pas-de-Calais, France, is actually a sauropod, sometimes assigned to Neosodon, although the two come from different formations.

Finally, several other poorly known genera and species are included with Iguanodon without being separate species, although their assignment is less certain with the renaming of I. atherfieldensis. These include Heterosaurus neocomiensis (Cornuel, 1850), Hikanodon (Keferstein, 1834), and Therosaurus (Fitzinger, 1840), and the species "Streptospondylus" recentior (Owen, 1851), "Cetiosaurus" brachyurus, and part of "C." brevis (Owen, 1842; "C." brevis is a chimera). The nomen nudum "Proiguanodon" (van den Broeck, 1900) also belongs here, and possibly the very obscure "Streptospondylus" grandis (Owen, 1851) and meyeri (Owen, 1854).

One of the first details noted about Iguanodon was that it had the teeth of a herbivorous reptile, although there has not always been consensus on how it ate. As Mantell noted, the remains he was working with were unlike any modern reptile, especially in the toothless, scoop-shaped form of the lower jaw symphysis, which he found best compared to that of the two-toed sloth and the extinct ground sloth Mylodon. He also suggested that Iguanodon had a prehensile tongue which could be used to gather food, like a giraffe. More complete remains have shown this to be an error; for example, the hyoid bones that supported the tongue are heavily built, implying a muscular, non-prehensile tongue used for moving food around in the mouth. The giraffe-tongue idea has also been incorrectly attributed to Dollo via a broken lower jaw.

Iguanodon teeth are, as the name suggests, like those of an iguana, but larger. Unlike hadrosaurids, which had columns of replacement teeth, Iguanodon only had one replacement tooth at a time for each position. The upper jaw held up to 29 teeth per side, with none at the front of the jaw, and the lower jaw 25; the numbers differ because teeth in the lower jaw are broader than those in the upper. Because the tooth rows are deeply inset from the outside of the jaws, and because of other anatomical details, it is believed that, as with most other ornithischians, Iguanodon had some sort of cheek-like structure, muscular or non-muscular, to retain food in the mouth. 

The skull was structured in such a way that as it closed, the bones holding the teeth in the upper jaw would bow out. This would cause the lower surfaces of the upper jaw teeth to rub against the upper surface of the lower jaw's teeth, grinding anything caught in between and providing an action that is the rough equivalent of mammalian chewing. Because the teeth were always replaced, the animal could have used this mechanism throughout its life, and could eat tough plant material. Additionally, the front ends of the animal's jaws were toothless and tipped with bony nodes, both upper and lower, providing a rough margin that was likely covered and lengthened by a keratinous material to form a cropping beak for biting off twigs and shoots. Its food gathering would have been aided by its flexible little finger, which could have been used to manipulate objects, unlike the other fingers.

Exactly what Iguanodon ate with its well-developed jaws is not known. The size of the larger species, such as I. bernissartensis, would have allowed them access to food from ground level to tree foliage at 4â€?Â metres high (13â€?6.5Â ft). A diet of horsetails, cycads, and conifers was suggested by David Norman, although iguanodonts in general have been tied to the advance of angiosperm plants in the Cretaceous due to the dinosaurs' inferred low browsing habits. Angiosperm growth, according to this hypothesis, would have been encouraged by iguanodont feeding because gymnosperms would be removed, allowing more space for the weed-like early angiosperms to grow. The evidence is not conclusive, though. Whatever its exact diet, due to its size and abundance, Iguanodon is regarded as a dominant medium to large herbivore for its ecological communities. In England, this included the small predator Aristosuchus, larger predators Eotyrannus, Baryonyx, and Neovenator, low-feeding herbivores Hypsilophodon and Valdosaurus, fellow "iguanodontid" Mantellisaurus, the armoured herbivore Polacanthus, and sauropods like Pelorosaurus.

Early fossil remains were fragmentary, which led to much speculation on the posture and nature of Iguanodon. As discussed, Iguanodon was initially portrayed as a quadrupedal horn-nosed beast. However as more bones were discovered, Mantell observed that the forelimbs were much smaller than the hindlimbs. His rival Owen was of the opinion it was a stumpy creature with four pillar-like legs. The job of overseeing the first lifesize reconstruction of dinosaurs was initially offered to Mantell, who declined due to poor health, and Owen's vision subsequently formed the basis on which the sculptures took shape. Its bipedal nature was revealed with the discovery of the Bernissart skeletons. However, it was depicted in an upright posture, with the tail dragging along the ground, acting as the third leg of a tripod.

During his re-examination of Iguanodon, David Norman was able to show that this posture was unlikely, due to the presence of a long tail stiffened with ossified tendons. To get the tripodal pose, the tail would literally have to be broken. Putting the animal in a horizontal posture makes many aspects of the arms and pectoral girdle more understandable. For example, the hand is relatively immobile, with the three central fingers grouped together, bearing hoof-like phalanges, and able to hyperextend. This would have allowed them to bear weight. The wrist is also relatively immobile, and the arms and shoulder bones robust. These features all suggest that the animal spent time on all fours. 

Furthermore, it appears that Iguanodon became more quadrupedal as it got older and heavier; juvenile I. bernissartensis have shorter arms than adults (60% of hindlimb length versus 70% for adults). When walking as a quadruped, the animal's hands would have been held so that the palms faced each other, as shown by iguanodontian trackways and the anatomy of this genus' arms and hands. The three toed pes (foot) of Iguanodon was relatively long, and when walking, both the hand and the foot would have been used in a digitigrade fashion (on the fingers and toes). The maximum speed of Iguanodon has been estimated at 24Â km/h (14.9Â mph), which would have been as a biped; it would not have been able to gallop as a quadruped.

Large three-toed footprints are known in Early Cretaceous rocks of England, particularly Wealden beds on the Isle of Wight, and these trace fossils were originally difficult to interpret. Some authors associated them with dinosaurs early on. In 1846, E. Tagert went so far as to assign them to an ichnogenus he named Iguanodon, and Samuel Beckles noted in 1854 that they looked like bird tracks, but might have come from dinosaurs. The identity of the trackmakers was greatly clarified upon the discovery in 1857 of the hind leg of a young Iguanodon, with distinctly three-toed feet, showing that such dinosaurs could have made the tracks. Despite the lack of direct evidence, these tracks are often attributed to Iguanodon. A trackway in England shows what may be an Iguanodon moving on all fours, but the foot prints are poor, making a direct connection difficult. Tracks assigned to the ichnogenus Iguanodon are known from locations including places in Europe where the body fossil Iguanodon is known, to Spitsbergen, Svalbard, Norway.

The thumb spike is one of the most well-known features of Iguanodon. Although it was originally placed on the animal's nose by Mantell, the complete Bernissart specimens allowed Dollo to correctly place it on the hand, as a modified thumb. (This would not be the last time a dinosaur's modified thumb claw would be misinterpreted; Noasaurus, Baryonyx, and Megaraptor are examples since the 1980s where an enlarged thumb claw was first put on the foot, as in dromaeosaurids.) 

This thumb is typically interpreted as a close-quarters stiletto-like weapon against predators, although it could also have been used to break into seeds and fruits, or against other Iguanodon. One author has suggested that the spike was attached to a venom gland, but this has not been accepted, as the spike was not hollow, nor were there any grooves on the spike for conducting venom.

Although sometimes interpreted as the result of a single catastrophe, the Bernissart finds instead are now interpreted as recording multiple events. According to this interpretation, at least three occasions of mortality are recorded, and though numerous individuals would have died in a geologically short time span (?10â€?00 years), this does not necessarily mean these Iguanodon were herding animals.

For one thing, juvenile remains are very uncommon at this site, unlike modern cases with herd mortality. They more likely were the periodic victims of flash floods whose carcasses accumulated in a lake or marshy setting. The Nehden find, however, with its greater span of individual ages, more even mix of Mantellisaurus to Iguanodon bernissartensis, and confined geographic nature, may record mortality of herding animals migrating through rivers.

Unlike other purported herding dinosaurs (especially hadrosaurs and ceratopsids), there is no evidence that Iguanodon was sexually dimorphic, with one gender appreciably different from the other. At one time, it was suggested that the Bernissart I. "mantelli", or I. atherfieldensis (both now Mantellisaurus) represented a gender, possibly female, of the larger and more robust, possibly male, I. bernissartensis. However, this is not supported today.

Since its description in 1825, Iguanodon has been a feature of worldwide popular culture. Two lifesize reconstructions of Iguanodon built at the Crystal Palace in London in 1852 greatly contributed to the popularity of the genus. Their thumb spikes were mistaken for horns, and they were depicted as elephant-like quadrupeds, yet this was the first time an attempt was made at constructing full-size dinosaur models.

Several motion pictures have featured Iguanodon. In the Disney film Dinosaur, an Iguanodon named Aladar served as the protagonist with three other iguanodonts as other main characters; a loosely-related ride of the same name at Disney's Animal Kingdom is based around bringing an Iguanodon back to the present. Iguanodon is one of the three dinosaur genera that inspired Godzilla; the other two were Tyrannosaurus and Stegosaurus. Iguanodon has also made appearances in some of the many Land Before Time films, as well as episodes of the television series.

Aside from appearances on the silver screen, Iguanodon has also been featured on the television documentary miniseries Walking with Dinosaurs (1999) produced by the BBC, and played a starring role in Sir Arthur Conan Doyle's book, The Lost World. It also was present in Bob Bakker's Raptor Red (1995), as a Utahraptor prey item. A main belt asteroid, , has been named 9941 Iguanodon in honour of the genus.

Because it is both one of the first dinosaurs described and one of the best-known dinosaurs, Iguanodon has been well-placed as a barometer of changing public and scientific perceptions on dinosaurs. Its reconstructions have gone through three stages: the elephantine quadrupedal horn-snouted reptile of the Victorians; a bipedal but still fundamentally reptilian animal using its tail to prop itself up; and finally, its current, more agile and dynamic representation, able to shift from two legs to all fours. The second representation dominated the twentieth century, but was slowly overturned during the 1960s.













A manifold is an abstract mathematical space in which every point has a neighborhood which resembles Euclidean space, but in which the global structure may be more complicated. In discussing manifolds, the idea of dimension is important. For example, lines are one-dimensional, and planes two-dimensional.

In a one-dimensional manifold (or one-manifold), every point has a neighborhood that looks like a segment of a line. Examples of one-manifolds include a line, a circle, and two separate circles. In a two-manifold, every point has a neighborhood that looks like a disk. Examples include a plane, the surface of a sphere, and the surface of a torus.

Manifolds are important objects in mathematics and physics because they allow more complicated structures to be expressed and understood in terms of the relatively well-understood properties of simpler spaces.

Additional structures are often defined on manifolds. Examples of manifolds with additional structure include differentiable manifolds on which one can do calculus, Riemannian manifolds on which distances and angles can be defined, symplectic manifolds which serve as the phase space in classical mechanics, and the four-dimensional pseudo-Riemannian manifolds which model space-time in general relativity.

A precise mathematical definition of a manifold is given below. To fully understand the mathematics behind manifolds, it is necessary to know elementary concepts regarding sets and functions, and helpful to have a working knowledge of calculus and topology. 

The circle is the simplest example of a topological manifold after a line. Topology ignores bending, so a small piece of a circle is exactly the same as a small piece of a line. Consider, for instance, the top half of the unit circle, x2 + y2 = 1, where the ''y''-coordinate is positive (indicated by the yellow arc in Figure 1). Any point of this semicircle can be uniquely described by its x-coordinate. So, projection onto the first coordinate is a continuous, and invertible, mapping from the upper semicircle to the open interval (âˆ?,1):

Such functions along with the open regions they map are called charts. Similarly, there are charts for the bottom (red), left (blue), and right (green) parts of the circle. Together, these parts cover the whole circle and the four charts form an atlas for the circle.

The top and right charts overlap: their intersection lies in the quarter of the circle where both the x- and the y-coordinates are positive. The two charts Ï‡top and Ï‡right each map this part into the interval (0,1). Thus a function T from (0,1) to itself can be constructed, which first uses the inverse of the top chart to reach the circle and then follows the right chart back to the interval. Let a be any number in (0,1), then:

Such a function is called a transition map.



The top, bottom, left, and right charts show that the circle is a manifold, but they do not form the only possible atlas. Charts need not be geometric projections, and the number of charts is a matter of some choice. Consider the charts

and

Here s is the slope of the line through the point at coordinates (x,y) and the fixed pivot point (âˆ?,0); t is the mirror image, with pivot point (+1,0). The inverse mapping from s to (x,y) is given by

It can easily be confirmed that x2+y2Â = 1 for all values of the slope s. These two charts provide a second atlas for the circle, with

Each chart omits a single point, either (âˆ?,0) for s or (+1,0) for t, so neither chart alone is sufficient to cover the whole circle. Topology can prove that it is not possible to cover the full circle with a single chart. For example, although it is possible to construct a circle from a single line interval by overlapping and "glueing" the ends, this does not produce a chart; a portion of the circle will be mapped to both ends at once, losing invertibility.

Manifolds need not be connected (all in "one piece"); thus a pair of separate circles is also a manifold. They need not be closed; thus a line segment without its end points is a manifold. And they need not be finite; thus a parabola is a manifold. Putting these freedoms together, two other example manifolds are a hyperbola (two open, infinite pieces) and the locus of points on the cubic curve y2Â =Â x3âˆ’x (a closed loop piece and an open, infinite piece).

However, we exclude examples like two touching circles that share a point to form a figure-8; at the shared point we cannot create a satisfactory chart. Even with the bending allowed by topology, the vicinity of the shared point looks like a "+", not a line.

Viewed using calculus, the circle transition function T is simply a function between open intervals, which gives a meaning to the statement that T is differentiable. The transition map T, and all the others, are differentiable on (0, 1); therefore, with this atlas the circle is a differentiable manifold. It is also smooth and analytic because the transition functions have these properties as well.

Other circle properties allow it to meet the requirements of more specialized types of manifold. For example, the circle has a notion of distance between two points, the arc-length between the points; hence it is a Riemannian manifold.



The study of manifolds combines many important areas of mathematics: it generalizes concepts such as curves and surfaces as well as ideas from linear algebra and topology.

Before the modern concept of a manifold there were several important results.

Non-Euclidean geometry considers spaces where Euclid's parallel postulate fails. Saccheri first studied them in 1733. Lobachevsky, Bolyai, and Riemann developed them 100 years later. Their research uncovered two types of spaces whose geometric structures differ from that of classical Euclidean space; these gave rise to hyperbolic geometry and elliptic geometry. In the modern theory of manifolds, these notions correspond to Riemannian manifolds with constant negative and positive curvature, respectively.

Carl Friedrich Gauss may have been the first to consider abstract spaces as mathematical objects in their own right. His theorema egregium gives a method for computing the curvature of a surface without considering the ambient space in which the surface lies. Such a surface would, in modern terminology, be called a manifold; and in modern terms, the theorem proved that the curvature of the surface is an intrinsic property. Manifold theory has come to focus exclusively on these intrinsic properties (or invariants), while largely ignoring the extrinsic properties of the ambient space.

Another, more topological example of an intrinsic property of a manifold is its Euler characteristic. Leonhard Euler showed that for a convex polytope in the three-dimensional Euclidean space with V vertices (or corners), E edges, and F faces, 

The same formula will hold if we project the vertices and edges of the polytope onto a sphere, creating a 'map' with V vertices, E edges, and F faces, and in fact, will remain true for any spherical map, even if it does not arise from any convex polytope. Thus 2 is a topological invariant of the sphere, called its Euler characteristic. On the other hand, a torus can be sliced open by its 'parallel' and 'meridian' circles, creating a map with V=1 vertex, E=2 edges, and F=1 face. Thus the Euler characteristic of the torus is 1-2+1=0. The Euler characteristic of other surfaces is a useful topological invariant, which can be extended to higher dimensions using Betti numbers. In the mid nineteenth century, the Gaussâ€“Bonnet theorem linked the Euler characteristic to the Gaussian curvature.

Investigations of Niels Henrik Abel and Carl Gustav Jacobi on inversion of elliptic integrals in the first half of 19th century led them to consider special types of complex manifolds, now known as Jacobians. Bernhard Riemann further contributed to their theory, clarifying the geometric meaning of the process of analytic continuation of functions of complex variables, although these ideas were way ahead of their time. 

Another important source of manifolds in 19th century mathematics was analytical mechanics, as developed by Simeon Poisson, Jacobi, and William Rowan Hamilton. The possible states of a mechanical system are thought to be points of an abstract space, phase space in Lagrangian and Hamiltonian formalisms of classical mechanics. This space is, in fact, a high-dimensional manifold, whose dimension corresponds to the degrees of freedom of the system and where the points are specified by their generalized coordinates. For an unconstrained movement of free particles the manifold is equivalent to the Euclidean space, but various conservation laws constrain it to more complicated formations, e.g. Liouville tori. The theory of a rotating solid body, developed in the 18th century by Leonhard Euler and Joseph Lagrange, gives another example where the manifold is nontrivial. Geometrical and topological aspects of classical mechanics were emphasized by Henri PoincarÃ©, one of the founders of topology.

Riemann was the first one to do extensive work generalizing the idea of a surface to higher dimensions. The name manifold comes from Riemann's original German term, Mannigfaltigkeit, which William Kingdon Clifford translated as "manifoldness". In his GÃ¶ttingen inaugural lecture, Riemann described the set of all possible values of a variable with certain constraints as a Mannigfaltigkeit, because the variable can have many values. He distinguishes between stetige Mannigfaltigkeit and diskrete Mannigfaltigkeit (continuous manifoldness and discontinuous manifoldness), depending on whether the value changes continuously or not. As continuous examples, Riemann refers to not only colors and the locations of objects in space, but also the possible shapes of a spatial figure. Using induction, Riemann constructs an n-fach ausgedehnte Mannigfaltigkeit (n times extended manifoldness or n-dimensional manifoldness) as a continuous stack of (nâˆ?) dimensional manifoldnesses. Riemann's intuitive notion of a Mannigfaltigkeit evolved into what is today formalized as a manifold. Riemannian manifolds and Riemann surfaces are named after Bernhard Riemann.

Hermann Weyl gave an intrinsic definition for differentiable manifolds in his lecture course on Riemann surfaces in 1911â€?912, opening the road to the general concept of a topological space that followed shortly. During the 1930s Hassler Whitney and others clarified the foundational aspects of the subject, and thus intuitions dating back to the latter half of the 19th century became precise, and developed through differential geometry and Lie group theory.

Two-dimensional manifolds, also known as surfaces, were considered by Riemann under the guise of Riemann surfaces, and rigorously classified in the beginning of the 20th century by Poul Heegaard and Max Dehn. Henri PoincarÃ© pioneered the study of three-dimensional manifolds and raised a fundamental question about them, today known as the PoincarÃ© conjecture. After nearly a century of effort by many mathematicians, starting with PoincarÃ© himself, a consensus among experts (as of 2006) is that Grigori Perelman has proved the PoincarÃ© conjecture (see the Hamilton-Perelman solution of the PoincarÃ© conjecture). Bill Thurston's geometrization program, formulated in the 1970s, provided a far-reaching extension of the PoincarÃ© conjecture to the general three-dimensional manifolds. Four-dimensional manifolds were brought to the forefront of mathematical research in the 1980s by Michael Freedman and in a different setting, by Simon Donaldson, who was motivated by the then recent progress in theoretical physics (Yang-Mills theory), where they serve as a substitute for ordinary 'flat' space-time. Important work on higher-dimensional manifolds, including analogues of the PoincarÃ© conjecture, had been done earlier by RenÃ© Thom, John Milnor, Stephen Smale and Sergei Novikov. One of the most pervasive and flexible techniques underlying much work on the topology of manifolds is Morse theory.

Informally, a manifold is a space that is "modeled on" Euclidean space.

There are many different kinds of manifolds and generalizations.In geometry and topology, all manifolds are topological manifolds, possibly with additional structure, most often a differentiable structure. In terms of constructing manifolds via patching, a manifold has an additional structure if the transition maps between different patches satisfy axioms beyond just continuity. For instance, differentiable manifolds have homeomorphisms on overlapping neighborhoods diffeomorphic with each other, so that the manifold has a well-defined set of functions which are differentiable in each neighborhood, and so differentiable on the manifold as a whole.

Formally, a topological manifold is a second countable Hausdorff space that is locally homeomorphic to Euclidean space.

Second countable and Hausdorff are point-set conditions;second countable excludes spaces of higher cardinality such as the long line, while Hausdorff excludes spaces such as "the line with two origins" (these generalized manifolds are discussed in non-Hausdorff manifolds).

Locally homeomorphic to Euclidean space means that every point has a neighborhood homeomorphic to an open Euclidean n-ball,

Generally manifolds are taken to have a fixed dimension (the space must be locally homeomorphic to a fixed n-ball), and such a space is called an n-manifold; however, some authors admit manifolds where different points can have different dimensions. Since dimension is a local invariant, each connected component has a fixed dimension.

Scheme-theoretically, a manifold is a locally ringed space, whose structure sheaf is locally isomorphic to the sheaf of continuous (or differentiable, or complex-analytic, etc.) functions on Euclidean space. This definition is mostly used when discussing analytic manifolds in algebraic geometry.

The broadest common definition of manifold is a topological space locally homeomorphic to a topological vector space over the reals.This omits the point-set axioms (allowing higher cardinalities and non-Hausdorff manifolds) and finite dimension (allowing various manifolds from functional analysis). Usually one relaxes one or the other condition: manifolds without the point-set axioms are studied in general topology, while infinite-dimensional manifolds are studied in functional analysis.

The spherical Earth is navigated using flat maps or charts, collected in an atlas. Similarly, a differentiable manifold can be described using mathematical maps, called coordinate charts, collected in a mathematical atlas. It is not generally possible to describe a manifold with just one chart, because the global structure of the manifold is different from the simple structure of the charts. For example, no single flat map can properly represent the entire Earth. When a manifold is constructed from multiple overlapping charts, the regions where they overlap carry information essential to understanding the global structure.

A coordinate map, a coordinate chart, or simply a chart, of a manifold is an invertible map between a subset of the manifold and a simple space such that both the map and its inverse preserve the desired structure. For a topological manifold, the simple space is some Euclidean space Rn and interest focuses on the topological structure. This structure is preserved by homeomorphisms, invertible maps that are continuous in both directions.

In the case of a differentiable manifold, a set of charts called an atlas allows us to do calculus on manifolds. Polar coordinates, for example, form a chart for the plane R2 minus the positive x-axis and the origin. Another example of a chart is the map Ï‡top mentioned in the section above, a chart for the circle.

The description of most manifolds requires more than one chart (a single chart is adequate for only the simplest manifolds). A specific collection of charts which covers a manifold is called an atlas. An atlas is not unique as all manifolds can be covered multiple ways using different combinations of charts. 

The atlas containing all possible charts consistent with a given atlas is called the maximal atlas. Unlike an ordinary atlas, the maximal atlas of a given manifold is unique. Though it is useful for definitions, it is a very abstract object and not used directly (e.g. in calculations).

Charts in an atlas may overlap and a single point of a manifold may be represented in several charts. If two charts overlap, parts of them represent the same region of the manifold, just as a map of Europe and a map of Asia may both contain Moscow. Given two overlapping charts, a transition function can be defined which goes from an open ball in Rn to the manifold and then back to another (or perhaps the same) open ball in Rn. The resultant map, like the map T in the circle example above, is called a change of coordinates, a coordinate transformation, a transition function, or a transition map.

An atlas can also be used to define additional structure on the manifold. The structure is first defined on each chart separately. If all the transition maps are compatible with this structure, the structure transfers to the manifold. 

This is the standard way differentiable manifolds are defined. If the transition functions of an atlas for a topological manifold preserve the natural differential structure of Rn (that is, if they are diffeomorphisms), the differential structure transfers to the manifold and turns it into a differentiable manifold. Complex manifolds are introduced in an analogous way by requiring that the transition functions of an atlas are holomorphic functions. For symplectic manifolds, the transition functions must be symplectomorphisms.

The structure on the manifold depends on the atlas, but sometimes different atlases can be said to give rise to the same structure. Such atlases are called compatible. 

These notions are made precise in general through the use of pseudogroups.

A single manifold can be constructed in different ways, each stressing a different aspect of the manifold, thereby leading to a slightly different viewpoint. 



Perhaps the simplest way to construct a manifold is the one used in the example above of the circle. First, a subset of R2 is identified, and then an atlas covering this subset is constructed. The concept of manifold grew historically from constructions like this. Here is another example, applying this method to the construction of a sphere:

A sphere can be treated in almost the same way as the circle. In mathematics a sphere is just the surface (not the solid interior), which can be defined as a subset of R3:

The sphere is two-dimensional, so each chart will map part of the sphere to an open subset of R2. Consider the northern hemisphere, which is the part with positive z coordinate (coloured red in the picture on the right). The function Ï‡ defined by

maps the northern hemisphere to the open unit disc by projecting it on the (x, y) plane. A similar chart exists for the southern hemisphere. Together with two charts projecting on the (x, z) plane and two charts projecting on the (y, z) plane, an atlas of six charts is obtained which covers the entire sphere.

This can be easily generalized to higher-dimensional spheres.

A manifold can be constructed by gluing together pieces in a consistent manner, making them into overlapping charts. This construction is possible for any manifold and hence it is often used as a characterisation, especially for differentiable and Riemannian manifolds. It focuses on an atlas, as the patches naturally provide charts, and since there is no exterior space involved it leads to an intrinsic view of the manifold. 

The manifold is constructed by specifying an atlas, which is itself defined by transition maps. A point of the manifold is therefore an equivalence class of points which are mapped to each other by transition maps. Charts map equivalence classes to points of a single patch. There are usually strong demands on the consistency of the transition maps. For topological manifolds they are required to be homeomorphisms; if they are also diffeomorphisms, the resulting manifold is a differentiable manifold.

This can be illustrated with the transition map t = 1â„s from the second half of the circle example. Start with two copies of the line. Use the coordinate s for the first copy, and t for the second copy. Now, glue both copies together by identifying the point t on the second copy with the point 1â„s on the first copy (the point t = 0 is not identified with any point on the first copy). This gives a circle.

The first construction and this construction are very similar, but they represent rather different points of view. In the first construction, the manifold is seen as embedded in some Euclidean space. This is the extrinsic view. When a manifold is viewed in this way, it is easy to use intuition from Euclidean spaces to define additional structure. For example, in a Euclidean space it is always clear whether a vector at some point is tangential or normal to some surface through that point. 

The patchwork construction does not use any embedding, but simply views the manifold as a topological space by itself. This abstract point of view is called the intrinsic view. It can make it harder to imagine what a tangent vector might be.

The ''n''-sphere Sn is a generalisation of the idea of a circle (1-sphere) and sphere (2-sphere) to higher dimensions. An n-sphere Sn can be constructed by gluing together two copies of Rn. The transition map between them is defined as

This function is its own inverse and thus can be used in both directions. As the transition map is a smooth function, this atlas defines a smooth manifold.In the case n = 1, the example simplifies to the circle example given earlier.

It is possible to define different points of a manifold to be same. This can be visualized as gluing these points together in a single point, forming a quotient space. There is, however, no reason to expect such quotient spaces to be manifolds. Among the possible quotient spaces that are not necessarily manifolds, orbifolds and CW complexes are considered to be relatively well-behaved.

One method of identifying points (gluing them together) is through a right (or left) action of a group, which acts on the manifold. Two points are identified if one is moved onto the other by some group element. If M is the manifold and G is the group, the resulting quotient space is denoted by M / G (or G \ M).

Manifolds which can be constructed by identifying points include tori and real projective spaces (starting with a plane and a sphere, respectively).

The Cartesian product of manifolds is also a manifold. Not every manifold can be written as a product of other manifolds.

The dimension of the product manifold is the sum of the dimensions of its factors. Its topology is the product topology, and a Cartesian product of charts is a chart for the product manifold. Thus, an atlas for the product manifold can be constructed using atlases for its factors. If these atlases define a differential structure on the factors, the corresponding atlas defines a differential structure on the product manifold. The same is true for any other structure defined on the factors. If one of the factors has a boundary, the product manifold also has a boundary. Cartesian products may be used to construct tori and finite cylinders, for example, as S1Â Ã—Â S1 and S1Â Ã—Â [0,Â 1], respectively.



A manifold with boundary is a manifold with an edge. For example a sheet of paper with rounded corners is a 2-manifold with a 1-dimensional boundary. The edge of an n-manifold is an (n-1)-manifold. A disk (circle plus interior) is a 2-manifold with boundary. Its boundary is a circle, a 1-manifold. A ball (sphere plus interior) is a 3-manifold with boundary. Its boundary is a sphere, a 2-manifold. (See also Boundary (topology)).

In technical language, a manifold with boundary is a space containing both interior points and boundary points. Every interior point has a neighborhood homeomorphic to the open n-ball {(x1, x2, â€? xn) | Î£ xi2 < 1}. Every boundary point has a neighborhood homeomorphic to the "half" n-ball {(x1, x2, â€? xn) | Î£ xi2 < 1 and x1 â‰?0}. The homeomorphism must send the boundary point to a point with x1 = 0.

Two manifolds with boundaries can be glued together along a boundary. If this is done the right way, the result is also a manifold. Similarly, two boundaries of a single manifold can be glued together. 

Formally, the gluing is defined by a bijection between the two boundaries. Two points are identified when they are mapped onto each other. For a topological manifold this bijection should be a homeomorphism, otherwise the result will not be a topological manifold. Similarly for a differentiable manifold it has to be a diffeomorphism. For other manifolds other structures should be preserved.

A finite cylinder may be constructed as a manifold by starting with a strip [0, 1]Â Ã—Â [0, 1] and gluing a pair of opposite edges on the boundary by a suitable diffeomorphism. A projective plane may be obtained by gluing a sphere with a hole in it to a MÃ¶bius strip along their respective circular boundaries.





The simplest kind of manifold to define is the topological manifold, which looks locally like some "ordinary" Euclidean space Rn. Formally, a topological manifold is a topological space locally homeomorphic to a Euclidean space. This means that every point has a neighbourhood for which there exists a homeomorphism (a bijective continuous function whose inverse is also continuous) mapping that neighbourhood to Rn. These homeomorphisms are the charts of the manifold. 

It is to be noted that a topological manifold looks locally like a euclidean space in a rather weak manner: while for each individual chart it is possible to distinguish differentiable functions or measure distances and angles, merely by virtue of being a topological manifold a space does not have any particular and consistent choice of such concepts. In order to discuss such properties for a manifold, one needs to specify further structure and consider differentiable manifolds and Riemannian manifolds discussed below. In particular, a same underlying topological manifold can have several mutually incompatible classes of differentiable functions and an infinite number of ways to specify distances and angles.

Usually additional technical assumptions on the topological space are made to exclude pathological cases. It is customary to require that the space be Hausdorff and second countable.

The dimension of the manifold at a certain point is the dimension of the Euclidean space that the charts at that point map to (number n in the definition). All points in a connected manifold have the same dimension. Some authors require that all charts of a topological manifold map to Euclidean spaces of same dimension. In that case every topological manifold has a topological invariant, its dimension. Other authors allow disjoint unions of topological manifolds with differing dimensions to be called manifolds.



For most applications a special kind of topological manifold, a differentiable manifold, is used. If the local charts on a manifold are compatible in a certain sense, one can define directions, tangent spaces, and differentiable functions on that manifold. In particular it is possible to use calculus on a differentiable manifold. Each point of an n-dimensional differentiable manifold has a tangent space. This is an n-dimensional Euclidean space consisting of the tangent vectors of the curves through the point.

Two important classes of differentiable manifolds are smooth and analytic manifolds. For smooth manifolds the transition maps are smooth, that is infinitely differentiable. Analytic manifolds are smooth manifolds with the additional condition that the transition maps are analytic (they can be expressed as power series, which are essentially polynomials of infinite degree). The sphere can be given analytic structure, as can most familiar curves and surfaces.

A rectifiable set generalizes the idea of a piecewise smooth or rectifiable curve to higher dimensions; however, rectifiable sets are not in general manifolds.



To measure distances and angles on manifolds, the manifold must be Riemannian. A Riemannian manifold is a differentiable manifold in which each tangent space is equipped with an inner product ã€ˆâ‹…,â‹…ã€?in a manner which varies smoothly from point to point. Given two tangent vectors u and v, the inner product ã€ˆu,vã€?gives a real number. The dot (or scalar) product is a typical example of an inner product. This allows one to define various notions such as length, angles, areas (or volumes), curvature, gradients of functions and divergence of vector fields.

All differentiable manifolds (of constant dimension) can be given the structure of a Riemannian manifold. The Euclidean space itself carries a natural structure of Riemannian manifold (the tangent spaces are naturally identified with the Euclidean space itself and carry the standard scalar product of the space). Many familiar curves and surfaces, including for example all n-spheres, are specified as subspaces of a Euclidean space and inherit a metric from their embedding in it.



A Finsler manifold allows the definition of distance, but not of angle; it is an analytic manifold in which each tangent space is equipped with a norm, ||Â·||, in a manner which varies smoothly from point to point. This norm can be extended to a metric, defining the length of a curve; but it cannot in general be used to define an inner product.

Any Riemannian manifold is a Finsler manifold.



Lie groups, named after Sophus Lie, are differentiable manifolds that carry also the structure of a group which is such that the group operations are defined by smooth maps. 

A Euclidean vector space with the group operation of vector addition is an example of a non-compact Lie group. A simple example of a compact Lie group is the circle: the group operation is simply rotation. This group, known as U(1), can be also characterised as the group of complex numbers of modulus 1 with multiplication as the group operation.Other examples of Lie groups include special groups of matrices, which are all subgroups of the general linear group, the group of n by n matrices with non-zero determinant. If the matrix entries are real numbers, this will be an n2-dimensional disconnected manifold. The orthogonal groups, the symmetry groups of the sphere and hyperspheres, are n(n-1)/2 dimensional manifolds, where n-1 is the dimension of the sphere. Further examples can be found in the table of Lie groups.



Different notions of manifolds have different notions of classification and invariant; in this section we focus on smooth closed manifolds.

The classification of smooth closed manifolds is well-understood in principle, except in dimension 4: in low dimensions (2 and 3) it is geometric, via the uniformization theorem and the Hamilton-Perelman solution of the PoincarÃ© conjecture, and in high dimension (5 and above) it is algebraic, via surgery theory. This is a classification in principle: the general question of whether two smooth manifolds are diffeomorphic is not computable in general. Further, specific computations remain difficult, and there are many open questions.

Orientable surfaces can be visualized, and their diffeomorphism classes enumerated, by genus. Given two orientable surfaces, one can determine if they are diffeomorphic by computing their respective genera and comparing: they are diffeomorphic if and only if the genera are equal, so the genus forms a complete set of invariants.

This is much harder in higher dimensions: higher dimensional manifolds cannot be directly visualized (though visual intuition is useful in understanding them), nor can their diffeomorphism classes be enumerated, nor can one in general determine if two different descriptions of a higher-dimensional manifold refer to the same object.

However, one can determine if two manifolds are different if there is some intrinsic characteristic that differentiates them. Such criteria are commonly referred to as invariants, because, while they may be defined in terms of some presentation (such as the genus in terms of a triangulation), they are the same relative to all possible descriptions of a particular manifold: they are invariant under different descriptions.

Naively, one could hope to develop an arsenal of invariant criteria that would definitively classify all manifolds up to isomorphism. Unfortunately, it is known that for manifolds of dimension 4 and higher, no program exists that can decide whether two manifolds are diffeomorphic.

Smooth manifolds have a rich set of invariants, coming from point-set topology, classic algebraic topology, and geometric topology. The most familiar invariants, which are visible for surfaces, are orientability (a normal invariant, also detected by homology) and genus (a homological invariant).

Smooth closed manifolds have no local invariants (other than dimension), though geometric manifolds have local invariants, notably the curvature of a Riemannian manifold and the torsion of a manifold equipped with an affine connection.This distinction between no local invariants and local invariants is a common way to distinguish between geometry and topology. All invariants of a smooth closed manifold are thus global.

Algebraic topology is a source of a number of important global invariant properties. Some key criteria include the simply connected property and orientability (see below). Indeed several branches of mathematics, such as homology and homotopy theory, and the theory of characteristic classes were founded in order to study invariant properties of manifolds.

In dimensions two and higher, a simple but important invariant criterion is the question of whether a manifold admits a meaningful orientation. Consider a topological manifold with charts mapping to Rn. Given an ordered basis for Rn, a chart causes its piece of the manifold to itself acquire a sense of ordering, which in 3-dimensions can be viewed as either right-handed or left-handed. Overlapping charts are not required to agree in their sense of ordering, which gives manifolds an important freedom. For some manifolds, like the sphere, charts can be chosen so that overlapping regions agree on their "handedness"; these are orientable manifolds. For others, this is impossible. The latter possibility is easy to overlook, because any closed surface embedded (without self-intersection) in three-dimensional space is orientable.

Some illustrative examples of non-orientable manifolds include: (1) the MÃ¶bius strip, which is a manifold with boundary, (2) the Klein bottle, which must intersect itself in 3-space, and (3) the real projective plane, which arises naturally in geometry. 

Begin with an infinite circular cylinder standing vertically, a manifold without boundary. Slice across it high and low to produce two circular boundaries, and the cylindrical strip between them. This is an orientable manifold with boundary, upon which "surgery" will be performed. Slice the strip open, so that it could unroll to become a rectangle, but keep a grasp on the cut ends. Twist one end 180Â°, making the inner surface face out, and glue the ends back together seamlessly. This results in a strip with a permanent half-twist: the MÃ¶bius strip. Its boundary is no longer a pair of circles, but (topologically) a single circle; and what was once its "inside" has merged with its "outside", so that it now has only a single side.

Take two MÃ¶bius strips; each has a single loop as a boundary. Straighten out those loops into circles, and let the strips distort into cross-caps. Gluing the circles together will produce a new, closed manifold without boundary, the Klein bottle. Closing the surface does nothing to improve the lack of orientability, it merely removes the boundary. Thus, the Klein bottle is a closed surface with no distinction between inside and outside. Note that in three-dimensional space, a Klein bottle's surface must pass through itself. Building a Klein bottle which is not self-intersecting requires four or more dimensions of space.

Begin with a sphere centered on the origin. Every line through the origin pierces the sphere in two opposite points called antipodes. Although there is no way to do so physically, it is possible to mathematically merge each antipode pair into a single point. The closed surface so produced is the real projective plane, yet another non-orientable surface. It has a number of equivalent descriptions and constructions, but this route explains its name: all the points on any given line through the origin project to the same "point" on this "plane".

For two dimensional manifolds a key invariant property is the genus, or the "number of handles" present in a surface. A torus is a sphere with one handle, a double torus is a sphere with two handles, and so on. Indeed it is possible to fully characterize compact, two-dimensional manifolds on the basis of genus and orientability. In higher-dimensional manifolds genus is replaced by the notion of Euler characteristic.







! Computer and operating system|-! Unix and Unix-like|-! Software licensing|-! Computer insecurity

OpenBSD is a Unix-like computer operating system descended from Berkeley Software Distribution (BSD), a Unix derivative developed at the University of California, Berkeley. It was forked from NetBSD by project leader Theo de Raadt in late 1995. The project is widely known for the developers' insistence on open source code and quality documentation; uncompromising position on software licensing; and focus on security and code correctness. The project is coordinated from de Raadt's home in Calgary, Alberta, Canada. Its logo and mascot is Puffy, a pufferfish.

OpenBSD includes a number of security features absent or optional in other operating systems and has a tradition of developers auditing the source code for software bugs and security problems. The project maintains strict policies on licensing and prefers the open source BSD licence and its variantsâ€”in the past this has led to a comprehensive licence audit and moves to remove or replace code under licences found less acceptable.

As with most other BSD-based operating systems, the OpenBSD kernel and userland programs, such as the shell and common tools like cat and ps, are developed together in a single source repository. Third-party software is available as binary packages or may be built from source using the ports tree.

The OpenBSD project currently maintains ports for 17 different hardware platforms, including the DEC Alpha, Intel i386, Hewlett-Packard PA-RISC, AMD AMD64 and Motorola 68000 processors, Apple's PowerPC machines, Sun SPARC and SPARC64-based computers, the VAX and the Sharp Zaurus.





In December 1994, NetBSD co-founder Theo de Raadt was asked to resign his position as a senior developer and member of the NetBSD core team, and his access to the source code repository was revoked. The reason for this is not wholly clear, although there are claims that it was due to personality clashes within the NetBSD project and on its mailing lists. De Raadt has been criticized for having a sometimes abrasive personality: in his book, Free For All, Peter Wayner claims that de Raadt "began to rub some people the wrong way" before the split from NetBSD; Linus Torvalds has described him as "difficult;" and an interviewer admits to being "apprehensive" before meeting him. Many have different feelings: the same interviewer describes de Raadt's "transformation" on founding OpenBSD and his "desire to take care of his team," some find his straightforwardness refreshing, and few deny he is a talented coder and security "guru".

In October 1995, de Raadt founded OpenBSD, a new project forked from NetBSD 1.0. The initial release, OpenBSD 1.2, was made in July 1996, followed in October of the same year by OpenBSD 2.0. Since then, the project has followed a schedule of a release every six months, each of which is maintained and supported for one year. The latest release, OpenBSD 4.2, appeared on November 1, 2007.



On 25 July 2007, OpenBSD developer Bob Beck announced the formation of the OpenBSD Foundation, a Canadian not-for-profit corporation formed to "act as a single point of contact for persons and organizations requiring a legal entity to deal with when they wish to support OpenBSD."

Just how widely OpenBSD is used is hard to ascertain: the developers do not collect and publish usage statistics and there are few other sources of information. In September, 2005 the nascent BSD Certification project performed a usage survey which revealed that 32.8% of BSD users (1420 of 4330 respondents) were using OpenBSD, placing it second of the four major BSD variants, behind FreeBSD with 77.0% and ahead of NetBSD with 16.3%. The DistroWatch website, well-known in the Linux community and often used as a reference for popularity, publishes page hits for each of the Linux distributions and other operating systems it covers. As of April 14, 2007 it places OpenBSD in 55th place, with 121 hits per day. FreeBSD is in 16th place with 478 hits per day and a number of Linux distributions range between them.

When OpenBSD was created, Theo de Raadt decided that the source should be available for anyone to read at any time, so, with the assistance of Chuck Cranor, he set up a public, anonymous CVS server. This was the first of its kind in the software development world: at the time, the tradition was for only a small team of developers to have access to a project's source repository. This practice had downsides, notably that outside contributors had no way to closely follow a project's development and contributed work would often duplicate already completed efforts. This decision led to the name OpenBSD and signalled the project's insistence on open and public access to both source code and documentation.

A revealing incident regarding open documentation occurred in March 2005, when de Raadt posted a message to the openbsd-misc mailing list. He announced that after four months of discussion, Adaptec had yet to disclose the required documentation to improve the OpenBSD drivers for its AAC RAID controllers. As in similar circumstances in the past, he encouraged the OpenBSD community to become involved and express their opinion to Adaptec. Shortly after this, FreeBSD committer, former Adaptec employee and author of the FreeBSD AAC RAID support Scott Long castigated de Raadt on the OSNews website for not contacting him directly regarding the issues with Adaptec. This caused the discussion to spill over onto the freebsd-questions mailing list, where the OpenBSD project leader countered by claiming that he had received no previous offer of help from Scott Long nor been referred to him by Adaptec. The debate was amplified by disagreements between members of the two camps regarding the use of binary blob drivers and non-disclosure agreements (NDAs): OpenBSD developers do not permit the inclusion of closed source binary drivers in the source tree and are reluctant to sign NDAs. However, the policy of the FreeBSD project has been less strict and much of the Adaptec RAID management code Scott Long proposed as assistance for OpenBSD was closed source or written under an NDA. As no documentation was forthcoming before the deadline for release of OpenBSD 3.7, support for Adaptec AAC RAID controllers was removed from the standard OpenBSD kernel.

The OpenBSD policy on openness extends to hardware documentation: in the slides for a December 2006 presentation, de Raadt explained that without it "developers often make mistakes writing drivers," and pointed out that "the [oh my god, I got it to work] rush is harder to achieve, and some developers just give up." He went on to say that vendor binary drivers are unacceptable, as they cannot be trusted and there is "no way to fix [them] ... when they break," that even vendor source is only "marginally acceptable" and still difficult to fix when problems occur, and further commented "if we cannot maintain a driver after the vendor stops caring, we ... have a broken hardware [sic]."



A goal of the OpenBSD project is to "maintain the spirit of the original Berkeley Unix copyrights," which permitted a "relatively un-encumbered Unix source distribution." To this end, the Internet Systems Consortium (ISC) licence, a simplified version of the BSD licence with wording removed that is unnecessary under the Berne convention, is preferred for new code, but the MIT or BSD licences are accepted. The widely used GNU General Public License is considered overly restrictive in comparison with these: code licensed under it, and other licences the project sees as undesirable, is no longer accepted for addition to the base system. In addition, existing code under such licences is actively replaced or relicensed when possible, except in some cases, where there is no suitable replacement and creating one would be time-consuming and impractical. In September 2007, the OpenBSD team took the initial steps towards replacing the GNU Compiler Collection (GCC) by importing Anders Magnusson's BSD-licensed Portable C Compiler (PCC) into CVS. The results of the OpenBSD teams efforts to replace encumbered code have been impressive: of particular note is the development of OpenSSH, based on the original SSH suite and developed further by the OpenBSD team. It first appeared in OpenBSD 2.6 and is now the single most popular SSH implementation, available as standard or as a package on many operating systems. Also worth mentioning is the development, after licence restrictions were imposed on IPFilter, of the pf packet filter, which first appeared in OpenBSD 3.0 and is now available in DragonFly BSD, NetBSD and FreeBSD; more recently, OpenBSD releases have seen the GPL licensed tools bc, dc, diff, grep, gzip, nm, pkg-config, RCS, sendbug (part of GNATS) and size replaced with BSD licensed equivalents. OpenBSD developers are also behind OpenBGPD, OpenOSPFD, OpenNTPD and OpenCVS, BSD licensed alternatives to existing software.

In June 2001, triggered by concerns over Darren Reed's modification of IPFilter's licence wording, a systematic licence audit of the OpenBSD ports and source trees was undertaken. Code in more than a hundred files throughout the system was found to be unlicensed, ambiguously licensed or in use against the terms of the licence. To ensure that all licences were properly adhered to, an attempt was made to contact all the relevant copyright holders: some pieces of code were removed, many were replaced, and others, including the multicast routing tools, mrinfo and map-mbone, which were licensed by Xerox for research only, were relicensed so that OpenBSD could continue to use them. Also of note during this audit was the removal of all software produced by Daniel J. Bernstein from the OpenBSD ports tree. At the time, Bernstein requested that all modified versions of his code be approved by him prior to redistribution, a requirement to which OpenBSD developers were unwilling to devote time or effort. The removal led to a clash with Bernstein who felt the removal of his software to be uncalled for and cited the Netscape web browser as much less free, accusing the OpenBSD developers of hypocrisy for permitting Netscape to remain while removing his software. The OpenBSD project's stance was that Netscape, although not open source, had licence conditions that could be more easily met; they asserted that Bernstein's demand for control of derivatives would lead to a great deal of additional work and that removal was the most appropriate way to comply with his requirements.



Shortly after OpenBSD's creation, Theo de Raadt was contacted by a local security software company named Secure Networks, Inc. or SNI. They were developing a "network security auditing tool" called Ballista (later renamed to Cybercop Scanner after SNI was purchased by Network Associates) which was intended to find and attempt to exploit possible software security flaws. This coincided well with de Raadt's own interest in security, so the two agreed to cooperate, a relationship that was of particular use leading up to the release of OpenBSD 2.3 and helped to form the focal point of the project: OpenBSD developers would attempt to do what was right, proper or secure, even at the cost of ease, speed or functionality. As bugs within OpenBSD became harder to find and exploit, the security company found that it was too difficult, or not cost effective, to handle such obscure problems. After years of cooperation, the two parties decided that their goals together had been met and parted ways.

Until June 2002, the OpenBSD website featured the slogan:



In June 2002, Mark Dowd of Internet Security Systems disclosed a bug in the OpenSSH code implementing challenge-response authentication. This vulnerability in the OpenBSD default installation allowed an attacker remote access to the root account, and was extremely serious, partly due to the widespread use of OpenSSH by that time: the bug affected a considerable number of other operating systems. This problem necessitated the adjustment of the slogan on the OpenBSD website to: 



The page was updated as time passed, until on March 13 2007 when Core Security Technologies disclosed a network-related remote vulnerability, it was altered to:



This statement has been criticized because little is enabled in a default install of OpenBSD and releases have included software that was later found to have remote holes; however, the project maintains that the slogan is intended to refer to a default install and that it is correct by that measure. One of the fundamental ideas behind OpenBSD is a drive for systems to be simple, clean and secure by default. For example, OpenBSD's minimal defaults fit in with standard computer security practice of enabling as few services as possible on production machines, and the project uses open source and code auditing practices argued to be important elements of a security system.



OpenBSD includes a large number of specific features designed to improve security, including API and toolchain alterations, such as the arc4random, issetugid, strlcat, strlcpy and strtonum functions and a static bounds checker; memory protection techniques to guard against invalid accesses, such as ProPolice, StackGhost, the W^X (W xor X) page protection features, as well as alterations to malloc; and cryptography and randomization features, including network stack enhancements and the addition of the Blowfish cipher for password encryption. To reduce the risk of a vulnerability or misconfiguration allowing privilege escalation, some programs have been written or adapted to make use of privilege separation, privilege revocation and chrooting. Privilege separation is a technique, pioneered on OpenBSD and inspired by the principle of least privilege, where a program is split into two or more parts, one of which performs privileged operations and the otherâ€”almost always the bulk of the codeâ€”runs without privilege. Privilege revocation is similar and involves a program performing any necessary operations with the privileges it starts with then dropping them, and chrooting involves restricting an application to one section of the file system, prohibiting it from accessing areas that contain private or system files. Developers have applied these features to OpenBSD versions of common applications, including tcpdump and the Apache web server, which, due to licensing issues with the later Apache 2 series, is a heavily patched 1.3.29 release.

The project has a policy of continually auditing code for security problems, work developer Marc Espie has described as "never finished ... more a question of process than of a specific bug being hunted." He went on to list several typical steps once a bug is found, including examining the entire source tree for the same and similar issues, "try[ing] to find out whether the documentation ought to be amended," and investigating whether "it's possible to augment the compiler to warn against this specific problem." Along with DragonFly, OpenBSD is one of the two open source operating systems with a policy of seeking out examples of classic, K&R C code and converting it to the more modern ANSI equivalentâ€”this involves no functional change and is purely for readability and consistency reasons. A standard code style, the Kernel Normal Form, which dictates how code must look in order to be easily maintained and understood, must be applied to all code before it is considered for inclusion in the base operating system; existing code is actively updated to meet the style requirements.

OpenBSD's security enhancements, built-in cryptography and the pf firewall suit it for use in the security industry, particularly for firewalls, intrusion-detection systems and VPN gateways. It is also commonly used for servers which must resist cracking and DoS attacks, and due to including the spamd daemon, it sometimes is used in mail filtering applications.

Several proprietary systems are based on OpenBSD, including Profense from Armorlogic ApS, AccessEnforcer from Calyptix Security, GeNUGate and GeNUBox from GeNUA mbH, RTMX O/S from RTMX Inc, syswall from Syscall Network Solutions AG, HIOBMessenger from topX, and various security appliances made by .vantronix GmbH. Of these, GeNUA, RTMX, and .vantronix have contributed back to OpenBSD: GeNUA funded the development of SMP on the i386 platform, RTMX have sent patches to add further POSIX compliance to the system, and .vantronix contributed in networking and load balancing. Several open source operating systems have also been derived from OpenBSD, notably Anonym.OS and MirOS BSD, as well as the now defunct ekkoBSD, MicroBSD and Gentoo/OpenBSD. In addition, code from many of the OpenBSD system tools has been used in recent versions of Microsoft's Services for UNIX, an extension to the Windows operating system which provides some Unix-like functionality, originally based on 4.4BSD-Lite. Core force, a security product for Windows, is based on OpenBSD's pf firewall. There have also been projects which use OpenBSD as part of images for embedded systems, including OpenSoekris and flashdist; together with tools like nsh, these allow Cisco-like embedded devices to be created.



OpenBSD ships with the X window system. Following the XFree86 , it includes a recent X.Org release; an older XFree86 3.3 release is also available for legacy video cards. With these, it is possible to use OpenBSD as a desktop or workstation, making use of a desktop environment, window manager or both to give the X desktop a wide range of appearances. The OpenBSD ports tree contains many of the most popular tools for desktop use, including desktop environments GNOME, KDE, and Xfce; web browsers Konqueror, Mozilla Firefox and Opera; and multimedia programs MPlayer, VLC media player and xine. In addition, graphical software for many uses is available from both the ports tree and by compiling POSIX compliant software. Also available are compatibility layers, which allow binary code compiled for other operating systems, including Linux, FreeBSD, SunOS and HP-UX, to be run. However, despite partial support in X.Org, OpenBSD lacks accelerated 3D graphics support.

OpenBSD's performance and usability is occasionally criticised. Felix von Leitner's performance and scalability tests indicated that OpenBSD lagged behind other operating systems. In response, OpenBSD users and developers criticised von Leitner's objectivity and methodology, and asserted that although performance is given consideration, security and correct design are prioritised, with developer Nick Holland commenting: "It all boils down to what you consider important." OpenBSD is also a relatively small project, particularly when compared with FreeBSD and Linux, and developer time is sometimes seen as better spent on security enhancements than performance optimisations. Critics of usability say that OpenBSD has a lack of user-friendly configuration tools, a bare default installation, and a "spartan" and "intimidating" installer. These see much the same rebuttals as performance: a preference for simplicity, reliability and security; as one reviewer puts it, "running an ultra-secure operating system can be a bit of work."

OpenBSD is available freely in various ways: the source can be retrieved by anonymous CVS or CVSup, and binary releases and development snapshots can be downloaded either by FTP or HTTP. Prepackaged CD-ROM sets can be ordered online for a small fee, complete with an assortment of stickers and a copy of the release's theme song. These, with its artwork and other bonuses, are one of the project's few sources of income, funding hardware, bandwidth and other expenses. Until OpenBSD 4.2, only a small install ISO image was available for download, to encourage sales of the full CD-ROM set. OpenBSD 4.2 provides a complete install ISO.

In common with several other operating systems, OpenBSD uses ports and packaging systems to allow for easy installation and management of programs which are not part of the base operating system. Originally based on the FreeBSD ports tree, the system is now quite distinct. Additionally, major changes have been made since the 3.6 release, including the replacement of the package tools, the tools available to the user to manipulate packages, by more capable versions, written in Perl by Marc Espie. In contrast to FreeBSD, the OpenBSD ports system is intended as a source used to create the end product, the packages: installing a port first creates a package and then installs it using the package tools. Packages are built in bulk by the OpenBSD team and provided for download with each release. OpenBSD is also unique among the BSDs in that the ports and base operating system are developed and released together for each version: this means that the ports or packages released with, for example, 3.7 are not suitable for use with 3.6 and vice versa, a policy which lends a great deal of stability to the development process, but means that the software in ports for the latest OpenBSD release can lag somewhat from the latest version available from the author.



Around the time of the OpenBSD 2.7 release, the original mascot, a BSD daemon with a trident and halo, was replaced by Puffy, traditionally said to be a pufferfish. In fact pufferfish do not possess spikes and images of Puffy are closer to a similar species, the porcupinefish. Puffy was selected because of the Blowfish encryption algorithm used in OpenSSH and the strongly defensive image of the porcupinefish with its spikes to deter predators. He quickly became very popular, mainly because of the appealing image of the fish and his distinction from the BSD daemon, also used by FreeBSD, and the horde of daemons then used by NetBSD. Puffy made his first public appearance in OpenBSD 2.6 and, since then, has appeared in a number of guises on tee-shirts and posters. These have included Puffiana Jones, the famed hackologist and adventurer, seeking out the Lost RAID; Puffathy, a little Alberta girl, who must work with Taiwan to save the day; Sir Puffy of Ramsay, a freedom fighter who, with Little Bob of Beckley, took from the rich and gave to all; and Puff Daddy, famed rapper and political icon.

After a number of releases, OpenBSD has become notorious for its catchy songs and interesting and often comical artwork. The promotional material of early OpenBSD releases did not have a cohesive theme or design but, starting with OpenBSD 3.0, the CD-ROMs, release songs, posters and tee-shirts for each release have been produced with a single style and theme, sometimes contributed to by Ty Semaka of the Plaid Tongued Devils. At first they were done lightly and only intended to add humour but, as the concept has evolved, they have become a part of OpenBSD advocacy, with each release expanding a moral or political point important to the project, often through parody. Past themes have included: in OpenBSD 3.8, the Hackers of the Lost RAID, a parody of Indiana Jones linked to the new RAID tools featured as part of the release; The Wizard of OS, making its debut in OpenBSD 3.7, based on the work of Pink Floyd and a parody of The Wizard of Oz related to the project's recent wireless work; and OpenBSD 3.3's Puff the Barbarian, including an 80s rock-style song and parody of Conan the Barbarian, alluding to open documentation.

In addition to the slogans used on tee-shirts and posters for releases, the project occasionally produces other material: over the years, catchphrases have included "Sending script kiddies to /dev/null since 1995," "Functional, secure, free - choose 3," "Secure by default," and a few insider slogans, only available on tee-shirts made for developer gatherings, such as "World class security for much less than the price of a cruise missile" and a crusty old octopus proclaiming "Shut up and hack!"

A number of books on OpenBSD have been published, including:

 













Issyk Kul (also Ysyk KÃ¶l, Issyk-kol; , ) is an endorheic lake in the northern Tian Shan mountains in eastern Kyrgyzstan. It is the ninth largest lake in the world by volume and the second largest saline lake after the Caspian Sea. Although it is surrounded by snow-capped peaks, it never freezes; hence its name, which means "warm lake" in the Kyrgyz language. The lake is a Ramsar site of globally significant biodiversity (Ramsar Site RDB Code 2KG001) and forms part of the Issyk-Kul Biosphere Reserve. It was also the site of an ancient metropolis 2500 years ago, and archaeological excavations are ongoing.

Lake Issyk Kul has a length of 182 km, a width of up to 60 km, and covers an area of 6,336 kmÂ². This makes it the second largest mountain lake in the world behind Lake Titicaca in South America. Located at an altitude of 1,608 m, it reaches 668 m in depth.

About 118 rivers and streams flow into the lake; the largest are Djyrgalan and Tyup. It is fed by springs, including many hot springs, and snow melt-off. The lake has no current outlet, but some hydrologists hypothesize that, deep underground, lake water filters into the Chu River. The bottom of the lake contains the mineral, monohydrocalcite: one of the few known lacustrine deposits.

The lake's southern shore is dominated by the ruggedly beautiful Tian Shan mountain range. The lake water has salinity of approx. 0.6% (less than 20% that of seawater) and its level drops by approximately 5 cm per year.

Administratively, the lake and the adjacent land are within Issyk Kul Province of Kyrgyzstan.

 the Soviet era, the lake became a popular vacation resort, with numerous sanatoria, boarding houses and vacation homes along its northern shore, many concentrated in and around the town of Cholpon-Ata. These fell on hard times after the break-up of the USSR, but now hotel complexes are being refurbished and simple private bed-and-breakfast pensions are being established for a new generation of health and leisure visitors.

The city of Karakol (formerly Przhevalsk, after the Russian explorer Przhevalsky who died there) is the administrative seat of Issyk Kul ''Oblast'' (Province) of Kyrgyzstan. It is located near the eastern tip of the lake and is a good base for excursions into the surrounding area. Its small old core contains an impressive wooden mosque, built without metal nails by the Dungan people, and a wooden Orthodox church that was used as a stable during Soviet times (see state atheism).

Lake Issyk Kul was a stopover on the Silk Road, a land route for travelers from the Far East to Europe. Many historians believe that the lake was the point of origin for the Black Death that plagued Europe and Asia during the early and mid-14th century.  The lake's status as a byway for travelers allowed the plague to spread across these continents via medieval merchants who unknowingly carried infested vermin along with them. A 14th century Armenian monastery was found on the northeastern shores of the lake by retracing the steps of a medieval map used by Venetian merchants on the Silk Road.

In December 2007 a report was released by a team of Kyrgyz historians, led by Vladimir Ploskikh, vice president of the Kyrgyz Academy of Sciences, that archaeologists have discovered the remains of a 2500-year-old advanced civilization at the bottom of the Lake.The data and artifacts obtained, which are currently under study all suggest that the ancient city was a metropolis in its time.The discovery consisted of formidable walls, some stretching for 500 meters as well as traces of a large city with an area of several square kilometers.Other findings included Scythian burial mounds, eroded by waves over the centuries and numerous well preserved artifacts including bronze battleaxes, arrowheads, self-sharpening daggers, objects discarded by smiths, casting molds, and a faceted gold bar, which was a monetary unit of the time.

Articles identified as the world's oldest extant coins were also found underwater with gold wire rings used as small change and a large hexahedral goldpiece. Also found was a bronze cauldron with a level of craftsmanship that is today achieved by using an inert gas environment.

The lake contains highly endemic fish biodiversity, and some of the species, including four endemics, are highly endangered. In recent years catches of all species of fish have declined markedly, due to a combination of over-fishing, heavy predation by two of the introduced species, and the cessation of lake restocking with juvenile fish from hatcheries. At least four commercially targeted endemic fish species are sufficiently threatened to be included in the Red Book of the Kyrgyz Republic: Chebak (Leuciscus schmidti), Chebachok (Leuciscus bergi), Marinka (Schizothorax issyk-kuli), and Sheer or Naked Osman (Diptychus dybovskii). Seven other endemic species are almost certainly threatened as by-catch or are indirectly impacted by fishing activity and changes to the structure and balance of the lake's fish population.

Sevan trout, a fish endemic to Lake Sevan in Armenia, was introduced into Issyk-Kul in the 1970s. While this fish is an endangered species in its "home" lake, it has a much better chance to survive in Lake Issyk-Kul where it has ravaged the indigenous species.

In pre-Islamic legend, the king of the Ossounes had donkey's ears. He would hide them, and order each of his barbers killed to hide his secret. One barber yelled the secret into a well, but he didn't cover the well after. The well water rose and flooded the kingdom. The kingdom is today under the waters of Issyk-Kul. This is how the lake was formed, so legend says. Other legends say that four drowned cities lie at the bottom of the lake. Substantial archaeological finds indicating the presence of an advanced civilization in ancient times have been made in shallow waters of the lake.

There is a long-established Soviet, now Canadian, test site located at the lake, where submarine and torpedo technology was evaluated.







The State of Idaho () is a state in the Pacific Northwest region of the United States of America. The state's largest city and capital is Boise. Residents are called "Idahoans." Idaho was admitted to the Union on July 3, 1890, as the 43rd state. 

According to the United States Census Bureau, in 2004, Idaho had an estimated population of 1,493,262. The state's postal abbreviation is ID. Idaho is nicknamed the Gem State because of its abundance of natural resources. The state motto is Esto Perpetua (Latin for "Let it be forever").

Idaho borders six states and one Canadian province, but does not border the Pacific Ocean at any point and is not, as such, a coastal state. The states of Washington and Oregon are to the west, Nevada and Utah are to the south, and Montana and Wyoming are to the east. The province of British Columbia, to the north, also shares a small () border with Idaho. The landscape is rugged with some of the largest unspoiled natural areas in the United States. It is a Rocky Mountain state with abundant natural resources and scenic areas. The state has snow-capped mountain ranges, rapids, vast lakes and steep canyons. The waters of Snake River rush through Hells Canyon, the deepest canyon in the United States. Shoshone Falls plunges down rugged cliffs from a height greater than that of Niagara Falls. The major rivers in Idaho are the Snake River, the Clark Fork/Pend Oreille River, the Clearwater River and the Salmon River. Other significant rivers include the Coeur d'Alene/Spokane River, the Boise River and the Payette River. The Port of Lewiston, at the confluence of the Clearwater and the Snake Rivers is the farthest inland seaport in the Pacific Northwest.

Idaho's highest point is Borah Peak, , in the Lost River Range north of Mackay. Idaho's lowest point, , is in Lewiston, where the Clearwater River joins the Snake River and continues into Washington.

Southern Idaho, including the Boise metropolitan area, Idaho Falls, Pocatello, and Twin Falls are in the Mountain Time Zone. (A 1966 legislative error,  Â§ 264, which is universally ignored, places this region in the Central Time Zone.) Areas north of the Salmon River, including Coeur d'Alene, Moscow, Lewiston, and Sandpoint are in the Pacific Time Zone and revolve commercially and culturally around Spokane, Washington

Idaho has much variation in its climate. Although the state's western border is located  from the Pacific Ocean, the maritime influence is still felt in Idaho, especially in the winter when cloud cover, humidity, and precipitation are at their highest points. This influence has a moderating effect in the winter where temperatures are not as low as would otherwise be expected for a northern state with a mostly elevated altitude. The maritime influence is lowest in the southeastern part of the state where the precipitation patterns are often reversed, with wetter summers and drier winters, and seasonal temperature differences more extreme, showing a more continental climate. 

Climate in Idaho can be hot, although extended periods over  for the maximum temperature are rare. Hot summer days are tempered by the low relative humidity and cooler evenings during summer months since, for most of the state, the highest diurnal difference in temperature is often in the summer. Winters can be cold, although extended periods of bitter cold weather below zero are unusual.



Humans may have been present in the Idaho area as long as 14,500 years ago. Excavations at Wilson Butte Cave near Twin Falls in 1959 revealed evidence of human activity, including arrowheads, that rank among the oldest dated artifacts in North America. Native American tribes predominant in the area included the Nez Perce in the north and the Northern and Western Shoshone in the south.

Idaho, as part of the Oregon Country, was claimed by both the United States and United Kingdom until the United States gained undisputed jurisdiction in 1846. Between then and the creation of the Idaho Territory in 1863, parts of the present-day state were included in the Oregon, Washington, and Dakota Territories. The new territory included most of present-day Idaho, Montana, and Wyoming. The first organized communities, within the present borders of Idaho, were established in 1860.

After some tribulation as a territory, including the chaotic transfer of the territorial capital from Lewiston to Boise, disenfranchisement of the large Mormon minority and a federal attempt to split the territory between Washington Territory and the state of Nevada, Idaho achieved statehood in 1890. The economy of the state, which had been primarily supported by metal mining, shifted towards agriculture and tourism.

In recent years, Idaho has expanded its commercial base as a tourism and agricultural state to include science and technology industries. Science and technology have become the largest single economic center (over 25% of the state's total revenue) within the state and are greater than agriculture, forestry and mining combined.

The Idaho State Historical Society preserves and promotes Idahoâ€™s cultural heritage.

Idaho is the only state that was likely named as the result of a hoax (the so called "Idahoax") although this is disputed. In the early 1860s, when the United States Congress was considering organizing a new territory in the Rocky Mountains, eccentric lobbyist George M. Willing suggested the name "Idaho," which he claimed was derived from a Shoshone language term meaning "the sun comes from the mountains" or "gem of the mountains." Willing later claimed that he had made up the name himself. Congress ultimately decided to name the area Colorado Territory when it was created in February 1861.

However, the name "Idaho" did not go away. The same year Congress created Colorado Territory, a county called Idaho County was created in eastern Washington Territory. The county was named after a steamship named Idaho, which was launched on the Columbia River in 1860. It is unclear whether the steamship was named before or after Willing's claim was revealed. Regardless, a portion of Washington Territory, including Idaho County, was used to create Idaho Territory in 1863.

Despite this lack of evidence for the origin of the name, many textbooks well into the 20th century repeated as fact Willing's account that the name "Idaho" derived from the Shoshone term "ee-da-how".

Chief Joseph Seltice, of the Coeur d'Alene Tribal Nation, posits another possible origin of the name. In his history of the tribe, Saga of the Coeur d'Alene Indians, he writes:    Some sources claim that the name "Idaho" comes from an Indian word, "Ee-dah-how," meaning "Gem of the Mountains." This expression may have come from some other Tribe, and it would have a different meaning for them than it would for the Coeur d'Alenes.    As the Coeur d'Alenes understood the word "Idaho," it would be more correctly pronounced "Ah-d'Hoo." It means "greetings by surprise," indicating friendship, but surprise.    The first syllable conveys to the mind, "All are welcome, from wherever you come; but keep the friendly peace. We welcome you with out-stretched arms, and this entitles us to permanent friendship."    The last syllable is a surprise and exclamation point. The expression means that all are welcome, "though we are surprised to see so many different strangers. The first dawn of day welcomes you as the sun rises." This expression was used by many of the Coeur d'Alenes on the Bitterroot Mountains to greet all who come.    So to all who read these words: "Welcome, with open arms! We're just surprised that there are so many of you!"



 of 2005, Idaho has an estimated population of 1,429,096, which is an increase of 33,956, or 2.4%, from the prior year and an increase of 135,140, or 10.4%, since the year 2000. This includes a natural increase since the last census of 58,884 people (that is 111,131 births minus 52,247 deaths) and an increase due to net migration of 75,795 people into the state. Immigration from outside the United States resulted in a net increase of 14,522 people, and migration within the country produced a net increase of 61,273 people.

This makes Idaho the sixth fastest-growing state after Arizona, Nevada, Florida, Georgia, and Utah. From 2004 to 2005, Idaho grew the third-fastest, surpassed only by Nevada and Arizona.

Nampa, the state's second largest city, has experienced particularly strong growth in recent years. According to census estimates Nampa has grown 22.1% to nearly 65,000 residents between 2000 and 2003. As of 2007, the population in Nampa was estimated at 84,000. Growth of 5% or more over the same period has also been observed in Caldwell, Coeur d'Alene, Meridian, Post Falls and Twin Falls.

Since 1990, Idaho's population has increased by 386,000 (38%).

The Boise Metropolitan Area (officially known as the Boise City-Nampa, ID Metropolitan Statistical Area) is Idaho's largest metropolitan area. Other metropolitan areas in order of size are Coeur d'Alene, Idaho Falls, Pocatello and Lewiston.

As of 2006, six official micropolitan statistical areas are based in Idaho. Twin Falls is the largest of these.

The center of population of Idaho is located in Custer County, in the town of Stanley.The largest reported ancestries in the state are: German (18.9%), English (18.1%), Irish (10%), American (8.4%), Norwegian (3.6%), Swedish (3.5%).

In 2004, the religious affiliations of Idahoans were surveyed as: 

In 2001, the religious affiliations of the people of Idaho were surveyed as:  

As with many other Western states, the percentage of Idaho's population identifying themselves as "non-religious" is higher than the national average.

Gross state product for 2004 was US$43.6 billion. The per capita income for 2004 was US$26,881.Idaho is an important agricultural state, producing nearly one third of the potatoes grown in the United States.Important industries in Idaho are food processing, lumber and wood products, machinery, chemical products, paper products, electronics manufacturing, silver and other mining, and tourism. The world's largest factory for barrel cheese, the raw product for processed cheese is located in Gooding, Idaho. It has a capacity of 120,000 metric tons per year of barrel cheese and belongs to the Glanbia group. The Idaho National Laboratory (INL), a government lab for nuclear energy research, is also an important part of the eastern Idaho economy. Idaho also is home to three facilities of Anheuser-Busch which provide a large part of the malt for breweries located across the nation. 

Today, the largest industry in Idaho is the science and technology sector. It amounts for over 25% of the State's total revenue and 70%+ of the State's exports (in dollars). Idaho's industrial economy is growing, with high-tech products leading the way. Since the late 1970s, Boise has emerged as a center for semiconductor manufacturing. Boise is the home of Micron Technology Inc., the only U.S. manufacturer of dynamic random access memory (DRAM) chips. Hewlett-Packard has operated a large plant in Boise, in southwestern Idaho, since the 1970s, which is devoted primarily to LaserJet printers production. Dell, Inc. operates a major customer support call center in Twin Falls. AMI Semiconductor, whose worldwide headquarter locates in Pocatello, is a widely recognized innovator in state-of-the-art integrated mixed-signal semiconductor products, mixed-signal foundry services and structured digital products. Coldwater Creek, a women's clothing retailer, is headquartered in Sandpoint. 

The state personal income tax ranges from 1.6% to 7.8% in eight income brackets. Idahoans may apply for state tax credits for taxes paid to other states, as well as for donations to Idaho state educational entities and some nonprofit youth and rehabilitation facilities.

The state sales tax is 6%. Sales tax applies to the sale, rental or lease of tangible personal property and some services. Food is taxed, but prescription drugs are not. Hotel, motel, and campground accommodations are taxed at a higher rate (7% to 11%). Some jurisdictions impose local option sales tax.

Idaho has a state gambling lottery which contributed $333.5 million in payments to all Idaho public schools and Idaho higher education from 1990 - 2006.

Major highways

Idaho is among the few states in the nation without a major freeway linking the two largest metropolitan areas of Boise in the south and Coeur d'Alene in the north. US 95 links the two ends of the state, but is, like many other highways in Idaho, in bad need of repair and upgrade. In 2007, the Idaho Transportation Department stated that the state's highway infrastructure faces a $200 million per year shortfall in maintenance and upgrades. Interstate 84 is the main highway linking the Southeast and Southwest portions of the state, along with Interstate 86 and Interstate 15. 

Major airports include the Boise Airport, and the Spokane International Airport, which serves northern Idaho. Other airports with scheduled service are the Pullman-Moscow Regional Airport serving the Palouse; the Lewiston-Nez Perce County Airport, serving the Lewis-Clark Valley and north central Idaho; The Magic Valley Regional Airport in Twin Falls; the Idaho Falls Regional Airport; and the Pocatello Regional Airport. 

Idaho is served by the two transcontinental railroads. The Burlington Northern Santa Fe (BNSF) connects North Idaho with Seattle, Portland and Spokane to the west, and Minneapolis and Chicago to the east. The BNSF travels through Kootenai, Bonner and Boundary Counties. The Union Pacific Railroad crosses southern Idaho travelling between Portland and Ogden, Utah and serves Boise, Nampa, Twin Falls, and Pocatello. Amtrak's Empire Builder crosses northern Idaho, with its only stop being in Sandpoint. There has been a push recently to return Amtrak service to southern Idaho as well. 

The Port of Lewiston is the furthest inland Pacific port on the west coast. A series of dams and locks on the Snake River and Columbia River facilitate barge travel from here to Portland, where goods are loaded on ocean-going vessels. 







North



North/South

Southwest

West/East



The constitution of Idaho provides for three branches of government: the executive, legislative and judicial branches. Idaho has a bicameral legislature, elected from 35 legislative districts, each represented by one senator and two representatives. Idaho still operates under its original (1889) state constitution.

Since 1946 statewide elected constitutional officers have been elected to four-year terms. They include: Governor, Lieutenant Governor, Secretary of State, Controller (Auditor before 1994), Treasurer, Attorney General, and Superintendent of Public Instruction. 

Last contested in 1966, Inspector of Mines was an original elected constitutional office. Afterwards it was an appointed position and ultimately done away with entirely in 1974. 

Idaho's government has an alcohol monopoly.

The governor of Idaho serves a four-year term, and is elected during what is nationally referred to as midterm elections. As such, the governor is not elected in the same election year as the president of the United States. The current governor is Republican C. L. "Butch" Otter, who was elected in 2006.

Idaho's legislature is part-time. However, the session may be extended if necessary, and often is. Because of this, Idaho's legislators are considered "citizen legislators", meaning that their position as a legislator is not their main occupation.

Terms for both the Senate and House of Representatives are two years. Legislative elections occur every even numbered year.

The Idaho Legislature has been continuously controlled by the Republican Party since the late 1950s, although Democratic legislators are routinely elected from Boise, Pocatello, Blaine County and the northern Panhandle.

See also List of Idaho senators and representatives

After the Civil War, many Southern Democrats moved to Idaho Territory. As a result, the early territorial legislatures were solidly Democrat-controlled. In contrast, most of the territorial governors were appointed by Republican Presidents and were Republicans themselves. This led to sometimes bitter clashes between the two parties. In the 1880s, Republicans became more prominent in local politics.

Since statehood, the Republican Party has usually been the dominant party in Idaho, as there was a polar shift in social and political stance between the two parties, when the Democrats became more liberal and the Republicans more conservative. In the 1890s and early 1900s, the Populist Party enjoyed prominence while the Democrat Party maintained a brief dominance in the 1930s during the Great Depression. Since World War II, most statewide elected officials have been Republicans.

Idaho Congressional delegations have also been generally Republican since its statehood. Several Idaho Democrats have had electoral success in the House over the years, but the Senate delegation has been a Republican stronghold for decades. Several Idaho Republicans, including current Senators Larry Craig and Mike Crapo, have won reelection to the Senate, but only Frank Church has won reelection as a Democrat. Church was the last Idaho Democrat to win a U.S. Senate race in 1974. No Democrat has won a U.S. House race in Idaho since Larry LaRocco in 1992.

In modern times, Idaho has been a reliably Republican state in presidential politics as well. It has not supported a Democrat for president since 1964. Even in that election, Lyndon Johnson defeated Barry Goldwater by less than two percentage points. In 2004, George W. Bush carried Idaho by a margin of 38 percentage points and 68.4% of the vote, winning in 43 of 44 counties. Only Blaine County, which contains the Sun Valley ski resort, supported John Kerry, who owns a home in the area.

In the 2006 elections, Republicans led by Governor-elect C. L. "Butch" Otter won all of the state's constitutional offices and retained both of the state's seats in the United States House of Representatives. However, Democrats picked up several seats in the Idaho Legislature, notably in the Boise area. 







Population > 50,000 (urbanized area)

Population > 30,000 (urbanized area)

Population > 10,000 (urbanized area)

Smaller Towns and Cities









The Idaho State Board of Education oversees three comprehensive universities. The University of Idaho in Moscow was the first university in the state (founded in 1889). A land-grant institution, the UI is the state's flagship university. Idaho State University in Pocatello opened in 1910 as the Academy of Idaho and was granted university status in 1963. Boise State University is the most recent school to attain university status in Idaho. The school opened in 1932 as Boise Junior College and became Boise State University in 1974. Lewis-Clark State College in Lewiston is the only public, non-university 4 year college in Idaho. 

Idaho has three regional community colleges: North Idaho College in Coeur d'Alene; College of Southern Idaho in Twin Falls; and The College of Western Idaho in Nampa, which is set to open in 2009. 

Private institutions in Idaho are Brigham Young University-Idaho in Rexburg, which is affiliated with the Church of Jesus Christ of Latter-day Saints; The College of Idaho in Caldwell, which still maintains a loose affiliation with the Presbyterian Church; Northwest Nazarene University in Nampa; and New Saint Andrews College in Moscow, of reformed Christian theological background. 



Boise is the host to the largest 5 K run for women, the St. Luke's Women's Fitness Celebration.

See List of people from Idaho.













Automatic number plate recognition (ANPR; see also other names below) is a mass surveillance method that uses optical character recognition on images to read the licence plates on vehicles. As of 2006, systems can scan number plates at around one per second on cars travelling up to 100 mph (160 km/h). They can use existing closed-circuit television or road-rule enforcement cameras, or ones specifically designed for the task. They are used by various police forces and as a method of electronic toll collection on pay-per-use roads, and monitoring traffic activity such as red light adherence in an intersection.

ANPR can be used to store the images captured by the cameras as well as the text from the licence plate, with some configurable to store a photograph of the driver. Systems commonly use infrared lighting to allow the camera to take the picture at any time of day. A powerful flash is included in at least one version of the intersection-monitoring cameras, serving to both illuminate the picture and make the offender aware of his or her mistake. ANPR technology tends to be region specific, owing to plate variation from place to place.

Concerns about these systems have centered on privacy fears of government tracking citizens' movements and media reports of misidentification and high error rates. However, as they have developed, the systems have become much more accurate and reliable.

ANPR is sometimes known by various other terms:

The ANPR was invented in 1976 at the Police Scientific Development Branch in the UK. Prototype systems were working by 1979 and contracts were let to produce industrial systems, first at EMI Electronics then at Computer Recognition Systems (CRS) in Wokingham, UK. Early trial systems were deployed on the A1 road and at the Dartford Tunnel. The first arrest due to a detected stolen car was made in 1981.

The software aspect of the system runs on standard PC hardware and can be linked to other applications or databases. It first uses a series of image manipulation techniques to detect, normalise and enhance the image of the number plate, and finally optical character recognition (OCR) to extract the alphanumerics of the licence plate. ANPR/ALPR systems are generally deployed in one of two basic approaches; one allows for the entire process to be performed at the lane location in real-time, the other transmits all the images from many lanes to a remote computer location and performs the OCR process there at some later point in time. When done at the lane site, the information captured of the plate alphanumeric, date-time, lane identification, and any other information that is required is completed in somewhere around 250 milliseconds. This information, now small data packets, can easily be transmitted to some remote computer for further processing if necessary, or stored at the lane for later retrieval. In the other arrangement there are typically large numbers of PCs used in a server farm to handle high workloads, such as those found in the London congestion charge project. Often in such systems there is a requirement to forward images to the remote server and this can require larger bandwidth transmission media. 

ANPR uses optical character recognition (OCR) on images taken by cameras. When Dutch vehicle registration plates switched to a different style in 2002 one of the changes made was to the font, introducing small gaps in some letters (such as P and R) to make them more distinct and therefore more legible to such systems. Some licence plate arrangements use variations in font sizes and positioning â€?ANPR systems must be able to cope with such differences in order to be truly effective. More complicated systems can cope with international variants, though many programs are individually tailored to each country.

The cameras used can include existing road-rule enforcement or closed-circuit television cameras as well as mobile units which are usually attached to vehicles. Some systems use infrared cameras to take a clearer image of the plates.

There are six primary algorithms that the software requires for identifying a licence plate:

The complexity of each of these subsections of the program determines the accuracy of the system. During the third phase (normalisation) some systems use edge detection techniques to increase the picture difference between the letters and the plate backing. A median filter may also be used to reduce the visual "noise" on the image.

There are a number of possible difficulties that the software must be able to cope with. These include:

While some of these problems can be corrected within the software it is primarily left to the hardware side of the system to work out solutions to these difficulties. Increasing the height of the camera may avoid problems with objects (such as other vehicles) obscuring the plate, but introduces and increases other problems such as the adjusting for the increased skew of the plate.

On some cars, towbars may obscure one or two characters of the licence plate. Bikes on bike racks can also obscure the number plate, though in some countries and jurisdictions, such as Victoria, Australia, "bike plates" are supposed to be fitted.

Some small-scale systems allow for some errors in the licence plate. When used for giving specific vehicles access to a barriered area the decision may be made to have an acceptable error rate of one character. This is because the likelihood of an unauthorised car having such a similar licence plate is seen as quite small. However, this level of inaccuracy would not be acceptable in most applications of an ANPR system.

At the front end of any ANPR system is the imaging hardware which captures the image of the license plates. The initial image capture forms a critically important part of the ANPR system which, in accordance to the Garbage In, Garbage Out principle of computing, will often determine the overall performance. 

License plate capture is typically performed by specialized cameras designed specifically for the task. Factors which pose difficulty for license plate imaging cameras include speed of the vehicles being recorded, varying ambient lighting conditions, headlight glare and harsh environmental conditions. Most dedicated license plate capture cameras will incorporate infrared illumination in order to solve the problems of lighting and plate reflectivity. 

Many countries now use licence plates that are retroreflective. This returns the light back to the source and thus improves the contrast of the image. In some countries, the characters on the plate are not reflective, giving a high level of contrast with the reflective background in any lighting conditions. A camera that makes use of infrared imaging (with a normal colour filter over the lens and an infrared light-source next to it) benefits greatly from this as the infrared waves are reflected back from the plate. This is only possible on dedicated ANPR cameras, however, and so cameras used for other purposes must rely more heavily on the software capabilities. Further, when a full-colour image is required as well as use of the ANPR-retrieved details it is necessary to have one infrared-enabled camera and one normal (colour) camera working together.

 To avoid blurring it is ideal to have the shutter speed of a dedicated camera set to 1/1000th of a second. Because the car is moving, slower shutter speeds could result in an image which is too blurred to read using the OCR software, especially if the camera is much higher up than the vehicle. In slow-moving traffic, or when the camera is at a lower level and the vehicle is at an angle approaching the camera, the shutter speed does not need to be so fast. Shutter speeds of 1/500th of a second can cope with traffic moving up to 40 mph (64 km/h) and 1/250th of a second up to 5 mph (8 km/h). License plate capture cameras can now produce usable images from vehicles traveling at . 

To maximize the chances of effective license plate capture, installers should carefully consider the positioning of the camera relative to the target capture area. Exceeding threshold angles of incidence between camera lens and license plate will greatly reduce the probability of obtaining usable images due to distortion. Manufacturers have developed tools to help eliminate errors from the physical installation of license plate capture cameras

Vehicle owners have used a variety of techniques in an attempt to evade ANPR systems and road-rule enforcement cameras in general. One method increases the reflective properties of the lettering and makes it more likely that the system will be unable to locate the plate or produce a high enough level of contrast to be able to read it. This is typically done by using a plate cover or a spray, though claims regarding the effectiveness of the latter are disputed. In most jurisdictions, the covers are illegal and covered under existing laws, while in most countries there is no law to disallow the use of the sprays.

For the 407 toll route in Ontario, Canada, police have caught several advanced techniques that some motorists have attempted. One driver had a setup that allowed him to lift a wire from the driver's seat that would show a different plate as he was cruising through the camera zones . Other users have attempted to smear their licence plate with dirt or utilise covers to mask the plate.

Novelty frames around Texas licence plates were made illegal on 1 September, 2003 by Senate Bill 439 because they caused problems with ANPR devices. That law made it a Class C misdemeanor (punishable by a fine of up to US $200), or Class B (punishable by a fine of up to US $2,000 and 180 days in jail) if it can be proven that the owner did it to deliberately obscure their plates.

There are some custom car rear panels with an inset for the licence plate at an angle, which changes the alignment of characters relative to the reading grid . Since most U.S. states no longer require new plates each year, perhaps the easiest way to disable recognition is simply to allow the reflective paint on the plates to become degraded by age and therefore unreadable .

If an ANPR system cannot read the plate it can flag the image for attention, with the human operators looking to see if they are able to identify the alphanumerics. It is then possible to do lookups on a database using wildcard characters for any part of the plate obscured, and use car details (make and model, for example) to refine the search. 

In order to avoid surveillance or penalty charges, there has been an upsurge in car cloning, particularly in London . This is usually achieved by copying registration plates from another car of a similar model and age. This can be difficult to detect, especially as cloners may change the registration plates and travel behaviour to hinder investigations.

The UK has an extensive Automatic number plate recognition CCTV network. Effectively, the police and Security services track all car movements around the country and are able to track any car in close to real time. Vehicle movements are stored for 5 years in the National ANPR Data Centre to be analyzed for intelligence and to be used for evidence.

 

In 1997 a system of one hundred ANPR cameras, codenamed GLUTTON, was installed to feed into the automated British Military Intelligence Systems in Northern Ireland. Further cameras were also installed on the British mainland, including unspecified ports on the east and west coasts.

Another use for ANPR in the UK is for speed cameras which work by tracking vehicles' travel time between two fixed points, and therefore calculate the average speed. These cameras are claimed to have an advantage over traditional speed cameras in maintaining steady legal speeds over extended distances, rather than encouraging heavy braking on approach to specific camera locations and subsequent acceleration back to illegal speeds. There is no evidence that average speed cameras actually reduce accident rates long term, with many motorists arguing that average speed check systems encourage bunching. In addition with the revelation that speeding tickets can potentially be avoided by changing lanes, an additional safety hazard has been created by drivers swapping lanes between gantries just in case they have been speeding.

The longest stretch of average speed cameras in the UK is found on the A77 road in Scotland, with  being monitored between Glasgow and Ayr.

Many cities and districts have developed traffic control systems to help monitor the movement and flow of vehicles around the road network. This had typically involved looking at historical data, estimates, observations and statistics such as:

CCTV cameras can be used to help traffic control centres by giving them live data, allowing for traffic management decisions to be made in real-time. By using ANPR on this footage it is possible to monitor the travel of individual vehicles, automatically providing information about the speed and flow of various routes. These details can highlight problem areas as and when they occur and helps the centre to make informed incident management decisions.

Some counties of the United Kingdom have worked with Siemens Traffic to develop traffic monitoring systems for their own control centres and for the public. Projects such as Hampshire County Council's ROMANSE provide an interactive and real-time web site showing details about traffic in the city. The site shows information about car parks, ongoing road works, special events and footage taken from CCTV cameras. ANPR systems can be used to provide average driving times along particular routes, giving drivers the ability to choose which one to take. ROMANSE also allows travellers to see the current situation using a mobile device with an Internet connection (such as WAP, GPRS or 3G), thus allowing them to be alerted to any problems that are ahead.

The UK company Trafficmaster has used ANPR since 1998 to estimate average traffic speeds on non-motorway roads without the results being skewed by local fluctuations caused by traffic lights and similar. The company now operates a network of over 4000 ANPR cameras, but claims that only the four most central digits are identified, and no numberplate data is retained.<

Ontario's 407 ETR highway uses a combination of ANPR and radio transponders to toll vehicles entering and exiting the road. Radio antennas are located at each junction and detect the transponders, logging the unique identity of each vehicle in much the same way as the ANPR system does. Without ANPR as a second system it would not be possible to monitor all the traffic. Drivers who opt to rent a transponder for C$2.35 per month are not charged the "Video Toll Charge" of C$3.55 for using the road, with heavy vehicles (those with a gross weight of over 5,000 kg) being required to use one. Using either system, users of the highway are notified of the usage charges by post.

There are numerous other electronic toll collection networks which use this combination of Radio frequency identification and ANPR. These include:

The London congestion charge is an example of a system that charges motorists entering a payment area. Transport for London (TfL) uses ANPR systems and charges motorists a daily fee of Â£8 paid before 10pm if they enter, leave or move around within the congestion charge zone between 7 a.m. and 6:30 p.m., Monday to Friday. Fines for travelling within the zone without paying the charge are Â£50 per infraction if paid before the deadline, doubling to Â£100 per infraction thereafter.

There are currently 1,500 cameras, which use Automatic Number Plate Recognition (ANPR) technology in use. There are also a number of mobile camera units which may be deployed anywhere in the zone.

It is estimated that around 98% of vehicles moving within the zone are caught on camera. The video streams are transmitted to a data centre located in central London where the ANPR software deduces the registration plate of the vehicle. A second data centre provides a backup location for image data.

Both front and back number plates are being captured, on vehicles going both in and out â€?this gives up to four chances to capture the number plates of a vehicle entering and exiting the zone. This list is then compared with a list of cars whose owners/operators have paid to enter the zone â€?those that have not paid are fined. The registered owner of such a vehicle is looked up in a database provided by the DVLA. A government investigation has found that a significant portion of the DVLA's database is incorrect. Furthermore, it is now the car owner's responsibility to report to the DVLA if they sell their car.

Up-to-date listing of systems & suppliers exist.

In Stockholm, Sweden, ANPR is used for the congestion tax, owners of cars driving into or out of the inner city must pay a charge, depending on the time of the day.

Several companies and agencies using ANPR systems, such as Vehicle and Operator Services Agency (VOSA), Police Information Technology Organisation (PITO) and Transport for London.

The introduction of ANPR systems has led to fears of misidentification and the furthering of 1984-style surveillance. In the United States, some such as Gregg Easterbrook oppose what they call "machines that issue speeding tickets and red-light tickets" as the beginning of a slippery slope towards an automated justice system:

Similar criticisms have been raised in other countries. Easterbrook also argues that this technology is employed to maximise revenue for the state, rather than to promote safety.

Older systems had been notably unreliable. This can lead to charges being made incorrectly with the vehicle owner having to pay Â£10 in order to be issued with proof (or not) of the offense. Improvements in technology have drastically decreased error rates, but false accusations are still frequent enough to be a problem.

Other concerns include the storage of information that could be used to identify people and store details about their driving habits and daily life, contravening the Data Protection Act along with similar legislation (see personally identifiable information). The laws in the UK are strict for any system that uses CCTV footage and can identify individuals.

ANPR systems may also be used for/by:









Duke University is a private coeducational research university located in Durham, North Carolina, United States. Founded by Methodists and Quakers in the present-day town of Trinity in 1838, the school moved to Durham in 1892. In 1924, tobacco industrialist James Buchanan Duke established The Duke Endowment, prompting the institution to change its name in honor of his deceased father, Washington Duke. 

The University is organized into two undergraduate and eight graduate schools. The undergraduate student body, which includes 40 percent racial or ethnic minorities, comes from all 50 U.S. states and 117 countries. In its 2008 edition, U.S. News & World Report ranked the undergraduate division eighth in the nation, while ranking the medical, law, and business schools among the top 11 in the country. Duke's research expenditures are among the largest 20 in the U.S. and its athletic program is one of the nation's elite. Competing in the Atlantic Coast Conference, the athletic teams have captured nine national championships, including three by the men's basketball team. 

Besides academics, research, and athletics, Duke is also well known for its sizable campus and Gothic architecture, especially Duke Chapel. The forests surrounding parts of the campus belie the University's proximity to downtown Durham. Duke's 8,610Â acres (35Â kmÂ²) contain three main campuses in Durham as well as a marine lab in Beaufort. Construction projects have updated both the freshmen-populated Georgian-style East Campus and the main Gothic-style West Campus, as well as the adjacent Medical Center over the past five years. Other projects are underway on all three campuses, including a 50- to 75-year overhaul of Central Campus, the first phase of which is expected to be completed in early 2011. 





Duke University started as Brown's Schoolhouse, a private subscription school founded in 1838 in Randolph County in the present-day town of Trinity. The school was organized by the Union Institute Society, a group of Methodists and Quakers, and in 1841 North Carolina issued a charter for Union Institute Academy. The academy was renamed Normal College in 1851 and then Trinity College in 1859 because of support from the Methodist Church. In 1892, Trinity moved to Durham, largely due to generosity from Washington Duke and Julian S. Carr, powerful and respected Methodists who had grown wealthy through the tobacco industry. Washington Duke gave what was then known as Trinity College a $100,000 endowment in 1896, with the stipulation that the college "open its doors to women, placing them on an equal footing with men."

In 1924, Washington Duke's son, James B. Duke, established The Duke Endowment with a $40 million ($434 million in 2005 dollars) trust fund. The annual income of the fund was to be distributed to hospitals, orphanages, the Methodist Church, three colleges, and Trinity College. William Preston Few, the president of Trinity College, insisted that the university be named Duke University, and James B. Duke agreed that it would be a memorial to his father. Money from the endowment allowed the University to grow quickly. Duke's original campus (East Campus) was rebuilt from 1925 to 1927 with Georgian-style buildings. By 1930, the majority of the Gothic style buildings on the campus one mile (1.6 km) west were completed, and construction on West Campus culminated with the completion of Duke Chapel in 1935.

Engineering, which had been taught since 1903, became a separate school in 1939. In athletics, Duke hosted and competed in the only Rose Bowl ever played outside California in Wallace Wade Stadium in 1942. Increased activism on campus during the 1960s prompted Dr. Martin Luther King, Jr. to speak at the University on the civil rights movement's progress on November 14 1964. The former governor of North Carolina, Terry Sanford, was elected president in 1969, propelling the Fuqua School of Business's opening, the William R. Perkins library completion, and the founding of the Institute of Policy Sciences and Public Affairs. The separate Woman's College merged back with Trinity as the liberal arts college for both men and women in 1972. Beginning in the 1970s, Duke administrators began a long-term effort to strengthen Duke's reputation both nationally and internationally. Interdisciplinary work was emphasized, as was recruiting minority faculty and students. Duke University Hospital was finished in 1980 and the student union was fully constructed two years later. In 1986, the men's soccer team captured Duke's first NCAA championship, and the men's basketball team followed with championships in 1991, 1992 and 2001.



Duke University's growth and academic focus have contributed to the university's reputation as an academic and research institution. The school has regularly sent three-member teams to the William Lowell Putnam Mathematical Competition, earning the title of the best collegiate undergraduate math team in the United States and Canada in 1993, 1996 and 2000. In nine out of the past ten years, Duke's team has finished in the top three, the only school besides Harvard to do so.

Construction continued on campus, with the  Levine Science Research Center (LSRC) opening in 1994 to house interdisciplinary research, and construction has continued. These projects have updated both the freshmen-housed Georgian-style East Campus and the main Gothic-style West Campus, as well as the adjacent Medical Center in the past five years. Other projects are underway on all three campuses, including a 50- to 75-year overhaul of Central Campus, the first phase of which is expected to be completed in early 2011.

In 1998, Duke President Nan Keohane initiated a five-year $1.5 billion Campaign for Duke fundraising effort. Edmund T. Pratt, Jr. ('47) endowed the Pratt School of Engineering with a $35 million gift in 1999. The Campaign for Duke ended in 2003 with $2.36 billion raised, making it the fifth largest campaign in the history of American higher education. 

In the 2004 fiscal year, research expenditures surpassed $490 million, leading to a myriad of important breakthroughs. The first working demonstration of an invisibility cloak was unveiled by Duke researchers in October 2006. In 2005, three students were named Rhodes Scholars, a number only surpassed by one university. Overall, Duke is fifth among private universities in the number of Rhodes Scholars it has produced. Since 1990, 19 students have been honored with this scholarship. 

In 2006, three lacrosse team members were falsely accused of rape; charges against the players were later dropped, the initial prosecutor was disbarred for ethical improprieties, and the incident garnered significant media attention.



Duke's student body consists of 6,197 undergraduates and 6,627 graduate and professional students. The undergraduate student body, containing 40 percent ethnic minorities, come from all 50 U.S. states and 85 countries. For the undergraduate class of 2011, Duke received 19,206 applications, and accepted 21 percent of them. Fifty-seven percent of high school valedictorians were rejected, while 42 percent of those with combined SAT scores of 1550 or greater in the math and verbal sections (99.65 percentile) were declined offers of admission. For the class of 2010, 96 percent of admitted students ranked in the top 10 percent of their high school class. The average SAT score was 1480 (old scale) or 2210 (new scale), and the ACT average was 32. In 2007, the School of Medicine received 5,076 applicants for 100 spots (2.0% of applicants), while the average GPA and MCAT scores for accepted students were 3.88 and 36, respectively. The School of Law accepted approximately 21% of its applicants for the class of 2010, while enrolling students had a median GPA of 3.74 and median LSAT of 169.

Duke University has two schools for undergraduates: Trinity College of Arts and Sciences and Pratt School of Engineering. The University's graduate and professional schools include the Graduate School, the Pratt School of Engineering, the Nicholas School of the Environment and Earth Sciences, the School of Medicine, the School of Nursing, the Fuqua School of Business, the School of Law, and the Divinity School.

Duke students have been honored in recent years as Rhodes, Gates, Fulbright (22 students in 2005), Marshall, Goldwater, and Truman Scholars. In the past decade, Duke has had the sixth highest number of Fulbright, Rhodes, Truman, and Goldwater scholarships in the nation for private universities. The University practices need-blind admissions and meets 100 percent of admitted students' demonstrated need. More than 40 percent of students in 2007â€?8 received financial aid, with the average grant being $26,700. Roughly 60 merit-based scholarships are also offered, many of which are geared toward students in North Carolina, African-American students, and high achieving students requiring financial aid. In all, Duke spends $73.3 million annually for need-based, merit, and athletic financial aid for undergraduates. A new program beginning in the 2008-09 school year will eliminate parental contributions for families who makes less than $60,000 a year, while students from families who make less than $40,000 a year will graduate debt free. Duke's aid is expected to increase by nearly 20 percent to $86 million in 2008-09.

Duke's endowment was valued at US $8.2 billion in 2008. The University's special academic facilities include an art museum, several language labs, the Duke Forest, a lemur center, a phytotron, a free electron laser, a nuclear magnetic resonance machine, a nuclear lab, and a marine lab. Duke also is a leading participant in the National Lambda Rail Network and runs a program for gifted children known as the Talent Identification Program, or TIP.

Duke offers 36 arts and sciences majors, five engineering majors, and 46 additional majors that have been approved under Program II, which allows students to design their own interdisciplinary major. Sixteen certificate programs also are available. Students may pursue a combination of a total of up to three majors/minors/certificates. Eighty percent of undergraduates enroll in the Trinity College of Arts and Sciences, while the rest are in the Pratt School of Engineering.

Trinity's curriculum operates under the revised version of "Curriculum 2000". It ensures that students are exposed to a variety of "areas of knowledge" and "modes of inquiry." The curriculum aims to help students develop critical faculties and judgment by learning how to access, synthesize, and communicate knowledge effectively, acquiring perspective on current and historical events, conducting research and solving problems, and developing tenacity and a capacity for hard and sustained work. In addition, freshmen can elect to participate in the FOCUS Program, which allows students to engage in an interdisciplinary exploration of a specific topic in a small group setting.

Pratt's curriculum, on the other hand, is narrower in scope, but still accommodates double majors in a variety of disciplines. The school emphasizes undergraduate researchâ€”opportunities for hands-on experiences arise through internships, fellowship programs, and the structured curriculum. Furthermore, for the class of 2007, more than 27 percent of Pratt undergraduates studied abroad, small compared to the percentage for Trinity undergraduates (46 percent), but much larger than the national average for engineering students (1.5 percent).



Duke Universityâ€™s research expenditures topped $490 million in 2004. In the 2005 fiscal year, Duke University Medical Center received the fifth-largest amount of funding from the National Institute of Health, netting $349.8 million. Duke's funding increased 14.8 percent from 2004, representing the largest growth of any top-20 recipient. Throughout history, Duke researchers have made several important breakthroughs, including the biomedical engineering department's development of the world's first real-time, three-dimensional ultrasound diagnostic system and the first engineered blood vessels. In the mechanical engineering department, Adrian Bejan developed the constructal theory, which explains the shapes that arise in nature. Duke has pioneered studies involving nonlinear dynamics, chaos, and complex systems in physics. In May 2006, Duke researchers mapped the final human chromosome, which made world news as the Human Genome Project was finally complete. Reports of Duke researchers' involvement in new AIDS vaccine research surfaced in June 2006. The biology department combines two historically strong programs in botany and zoology, while the divinity school's leading theologian is Time's 2001 "America's Best Theologian", Stanley Hauerwas. The graduate program in literature boasts several internationally renowned figures, including Fredric Jameson, Michael Hardt, and Alice Kaplan, while philosophers Robert Brandon and Lakatos Award-winner Alexander Rosenberg make Duke a leading center for research in philosophy of biology.



In the 2008 U.S. News & World Report ranking of undergraduate programs at doctoral granting institutions, Duke ranked eighth. In the past decade, U.S. News has placed Duke as high as third and as low as eighth. Duke was ranked the 13th-best university in the world in 2007 by the THES - QS World University Rankings. Duke was ranked 32nd globally and 24th nationally by Shanghai Jiao Tong University in 2005 in terms of quality of scientific research and number of Nobel Prizes.The Wall Street Journal ranked Duke sixth (fifth among universities) in its "feeder" rankings in 2006, analyzing the percentage of undergraduates that enroll in what it considers the top five medical, law, and business schools. Carnegie Communications ranked Duke fifth among U.S. universities in regard to students' perceptions of quality and third for popularity in 2004. A survey by the Journal of Blacks in Higher Education in 2002 ranked Duke as the best university in the country in regard to the integration of African American students and faculty.

In U.S. News & World Report's "America's Best Graduate Schools 2008," Duke's medical school ranked 8th for research and tied for 34th for primary care, while the law school ranked 10th. Among business schools in the United States, the Fuqua School of Business was ranked 12th by U.S. News in 2007 and 9th by BusinessWeek in 2006. The graduate program for the Pratt School of Engineering was ranked 30th by U.S. News and 2nd by The Princeton Review in 2006 among national engineering schools. In the rankings of doctoral programs by U.S. News & World Report in its 2008 edition, Duke ranked 1st in literary criticism and theory, 5th in ecology and evolutionary biology, 5th in biomedical engineering, tied for 12th for doctoral programs in the sciences, tied for 21st in mathematics, tied for 25th in computer science, tied for 29th in physics, and ranked 38th in chemistry.

Political science, sociology, history, economics, and cultural anthropology departments also frequently rank in the top 20 of their respective disciplines among U.S. universities. The Philosophical Gourmet Report placed Duke's philosophy program as the 27th best in the nation in 2006, while ranking Duke as the best program in the U.S. in philosophy of biology.

Duke University owns 220 buildings on 8,611Â acres (35Â kmÂ²) of land, which includes the 7,200 acre (29Â kmÂ²) Duke Forest. The campus is divided into four main areas: West, East, and Central campuses, and the Medical Center. All the campuses are connected via a free bus service that runs frequently throughout the week. On the Atlantic coast in Beaufort, Duke owns 15Â acres as part of its Marine Lab. One of the major public attractions on the Duke Campus is the 55 acre Sarah P. Duke Gardens, established in the 1930s.Duke students often refer to the campus as "the Gothic Wonderland," a nickname referring to the Gothic revival architecture of West Campus. Much of the campus was designed by Julian Abele, one of the first prominent African American architects. The residential quadrangles are of an early and somewhat unadorned design, while the buildings in the academic quadrangles show influences of the more elaborate late French and Italian styles. Its freshman campus (East Campus) is composed of buildings in the Georgian architecture style.



The stone used for the West Campus has seven primary colors and 17 shades of color. The university supervisor of planning and construction wrote that the stone has "an older, more attractive antique effect" and a "warmer and softer coloring than the Princeton stone" that gave the university an "artistic look". James B. Duke initially suggested the use of stone from a quarry in Princeton, New Jersey, but later amended the plans to use stone from a local quarry in Hillsborough to reduce costs. Duke Chapel stands at the heart of West Campus. Constructed from 1930 to 1935, the chapel seats 1,600 people; and, at 210 feet (64Â m), is one of the tallest buildings in Durham County.

As of November 1 2005, Duke had spent $835 million dollars on 34 major construction projects initiated since February 2001. At that time, Duke initiated a five-year strategic plan, "Building on Excellence." Completed projects since 2002 include major additions to the business, law, nursing, and divinity schools, a new library, an art museum, a football training facility, two residential buildings, an engineering complex, a public policy building, an eye institute, two genetic research buildings, a student plaza, the French Family Science Center, and two new medical-research buildings.



With more than 5.5 million volumes, the Duke University Library System is one of the ten largest private university library systems in the U.S. It contains 17.7 million manuscripts, 1.2 million public documents, and tens of thousands of films and videos. Besides the main William R. Perkins Library, the university also contains the separately administered Ford (business), Divinity School, Duke Law, and Medical Center Libraries.

The William R. Perkins Library system has 11 branches on campus. In addition to Perkins Library, the system contains the Biological & Environmental Science Library, Bostock Library, the Chemistry Library, the Library Service Center, Lilly Library (which houses materials on fine arts, philosophy, film & video, and performing arts), the Music Library, Pearse Memorial Library (located at the Marine Lab), and Vesic Library (collection focuses on engineering, mathematics, and physics). The University Archives and Rare Book, Manuscript, and Special Collections are also considered part of the Perkins Library system.



Bostock Library, named for Board of Trustee member Roy J. Bostock, opened in the fall of 2005 as part of the University's strategic plan to supplement Duke's libraries. It contains 87 study carrels, 517 seats, and 96 computer stations, as well as 72,996 linear feet of shelving for overflow books from Perkins Library as well as for new collections.

Nasher Museum of Art opened in the fall of 2005, replacing the undersized Duke University Museum of Art (DUMA). The museum, designed by Rafael ViÃ±oly and named for Duke alumnus and art collector Raymond Nasher, contains over 13,000 pieces of art, including works by Andy Warhol, Henri de Toulouse-Lautrec, and Pablo Picasso.

West Campus, the heart of Duke University, houses all the sophomores, along with some juniors and seniors. In addition, most of the academic and administrative centers reside there. "Main" West Campus, with Duke Chapel at its center, contains the majority of residential quads to the south, while the main academic quad, library, and Medical Center are to the north. The campus, spanning , includes Science Drive, which consists of science and engineering buildings. Most of the campus eateries and sports facilities including the historic basketball stadium, Cameron Indoor Stadium, are on West.

East Campus, the original location of Duke University, functions as a freshman campus as well as the home of several academic departments. Since the 1995-96 academic year, all freshmenâ€”and only freshmen except for upperclassmen serving as Resident Assistantsâ€”have lived on East Campus, to build class unity. The campus encompasses 97Â acres and is 1.5Â miles (2.4Â km) away from West Campus. The Art History, History, Literature, Music, Philosophy, and Women's Studies Departments are housed on East. Programs such as dance, drama, education, film, and the University Writing Program also reside on East. East Campus, a fully self-sufficient campus, contains the freshman dormitories, a dining hall, Lilly Library, Baldwin Auditorium, a theater, Brodie Gym, tennis courts, and several academic buildings. Separated from downtown by a short walk, the area was the site of the Women's College from 1930 to 1972.

Central Campus, consisting of  between East and West campuses, houses around 850 juniors and seniors and 200 professional students in apartments. It is home to the Nasher Museum of Art, the Freeman Center for Jewish Life, the Duke Police Department, the Duke Office of Disability Management, a Ronald McDonald House, and administrative departments such as Duke Residence Life and Housing Services. Central has several recreation and social facilities such as basketball courts, tennis courts, a sand volleyball court, a swimming pool, barbecue and picnic shelter as well as barbecue grills, a general gathering building called Devil's Den, and a convenience store.

At present, there is a 20- to 50-year plan to restructure Central Campus. The idea is to develop an "academic village" as a key center for the Duke community. The first phase, costing $240 million, involves replacing the outdated apartments. Other additions in the first phase include dining, academic, recreational, and service facilities. A key goal of the Central renovations is to reintegrate the area with the rest of the Duke campus, as it is connected to the other campuses by a circuitous, inefficient bus route.



Established in 1931, the Duke Forest today consists of 7,200Â acres (29Â kmÂ²) in six divisions just west of Duke University's West Campus. Duke Forest is one of the largest continually-managed forests in the U.S. and demonstrates a variety of forest stand types and silvicultural treatments. The forest is used extensively for research and includes the Aquatic Research Facility, Forest Carbon Transfer and Storage (FACTS-I) research facility, two permanent towers suitable for micrometerological studies, and other areas designated for animal behavior and ecosystem study. More than 30Â miles (48Â km) of trails are open to the public for hiking, cycling, and horseback riding.

Located inside the Duke Forest, the Duke Lemur Center (DLC) is the world's largest sanctuary for rare and endangered prosimian primates. Founded in 1966, the Duke Lemur Center spans 85Â acres (3.44Â kmÂ²) and contains nearly 300 animals of 25 different species of lemurs, galagos and lorises.



Situated between West Campus and the apartments of Central Campus, the Sarah P. Duke Gardens, established in the early 1930s, occupy 55Â acres (2.2Â kmÂ²) divided into four major sections: the original Terraces and their surroundings, the H.L. Blomquist Garden of Native Plants (devoted to flora of the Southeastern United States), the Culberson Asiatic Arboretum (housing plants of Eastern Asia), and the Doris Duke Center Gardens. There are five miles (8Â km) of allÃ©es, walks, and pathways throughout the Doris Duke Visitorâ€™s Center and the surrounding gardens.

Directly north of West Campus, Duke University Medical Center (DUMC) combines one of the top-rated hospitals and one of the top-ranked medical schools in the U.S. Founded in 1930, the Medical Center occupies 7.5 million square feet (700,000Â mÂ²) in 91 buildings on 210Â acres (8.5Â kmÂ²).

Although located in the town of Beaufort, North Carolina, Duke University Marine Lab on Pivers Island is part of Duke's campus. The marine lab is situated on the Outer Banks of North Carolina, only  across the channel from Beaufort. Duke's interest in the area began in the early 1930s and the first buildings were erected in 1938. The resident faculty represent the disciplines of oceanography, marine biology, marine biomedicine, marine biotechnology, and coastal marine policy and management. The Marine Laboratory is a member of the National Association of Marine Laboratories (NAML).



Duke's 26 varsity sports teams, known as the Blue Devils, are members of the NCAA's Division I Atlantic Coast Conference. Duke's teams have won nine NCAA team national championshipsâ€”the women's golf team has won five (1999, 2002, 2005, 2006 and 2007), the men's basketball team has won three (1991, 1992, and 2001), and the men's soccer team has won one (1986). Historically, Duke's major rival has been the Tar Heels of the University of North Carolina at Chapel Hill, especially in basketball. The rivalry has led people to identify the two differing shades of blue in relation to their respective universityâ€”calling the lighter powder blue "Carolina blue" and the darker blue "Duke blue."

In the past ten years, Duke has finished in the top 30 every year in the NACDA Director's Cup, an overall measure of an institution's athletic success. In the past three years, Duke has finished 11th (2007), eighth (2006), and fifth (2005). Duke teams that have been ranked in the top ten nationally in the 2000s include men's and women's basketball, men's and women's tennis, men's and women's soccer, men's and women's fencing, men's and women's cross country running, men's and women's lacrosse, women's field hockey, and men's and women's golf. Eight of these teams were ranked either first or second in the country during 2004â€?. Women's golf has been particularly dominating, compiling a record of 796-45-3 (.945) in the 2000â€?005 seasons. The men's lacrosse program has been a recent powerhouse reaching the national championship game in 2005 and 2007, losing to Johns Hopkins by a single goal and accumulating season records of 17-3 both times. 

According to a 2006 evaluation conducted by the NCAA, Duke's student-athletes have the highest graduation rate of any institution in the nation. In 2005, 2006, and 2007, Duke ranked first among Division I schools in the National Collegiate Scouting Association Power Rankingsâ€”a combination of the institution's Director's Cup standing, its athletic graduation rate, and its academic rank in U.S. News & World Report.





Duke's men's basketball team, a traditional powerhouse, is the fourth most victorious program of all time. The team has captured three National Championships, while attending 14 Final Fours and nine Championship games. In addition, the Blue Devils are tied with the University of North Carolina for the most ACC Championships with 16 and have had the most National Players of the Year in the nation with 11. Seventy-one players have been selected in the NBA Draft, while 55 players have been honored as All-Americans. Duke's program is one of only two to have been to at least one Final Four and one National Championship game in each of the past five decades. The program's home facility is historic Cameron Indoor Stadium, considered one of the top venues in the nation.

The team's success has been particularly outstanding over the past 25 years under coach Mike Krzyzewski (often simply called "Coach K"). Their successes include becoming the only team to win three national championships since the NCAA Tournament field was expanded to 64 teams in 1985, ten Final Fours in the past 21 years, and eight of nine ACC tournament championships from 1999 to 2006.



The Blue Devils have won seven ACC Football Championships, have had ten players honored as ACC Player of the Year (the most in the ACC), and have had three Pro Football Hall of Famers come through the program (second in the ACC to only Miami's four). In addition, the Blue Devils have produced 11 College Football Hall of Famers which is tied for the 2nd most in the ACC. Duke has also won 17 total conference championships (7 ACC, 9 Southern Conference, and 1 Big Five Conference). That total is the highest in the ACC.

The most famous Duke football season came in 1938, when Wallace Wade coached the "Iron Dukes" that shut out all regular season opponents; only three teams in history can claim such a feat. Duke reached their first Rose Bowl appearance, where they lost 7-3 when USC scored a touchdown in the final minute of the game. Wade's Blue Devils lost another Rose Bowl to Oregon State in 1942, this one held at Duke's home stadium due to the attack on Pearl Harbor. The football program also proved successful in the 1950s and 1960s, winning six of the first ten ACC football championships from 1953 to 1962 under coach Bill Murray; the Blue Devils would not win the ACC championship again until 1989 under now legendary coach Steve Spurrier.

However, the program has been one of the least successful in Division I-A over the past ten years. Duke has not had a winning season since 1994, and has only three such seasons in the past 20 years. In the 2006 campaign, the Blue Devils failed to win any games. The recent struggles have led the program to have an overall record of 433-402-31 despite its early successes.

The graduation rate of Duke's football players is consistently among the highest among Division I-A schools. Duke's high graduation rates have earned it more American Football Coaches Association's Academic Achievement Awards than any other institution.



Duke requires its students to live on campus for the first three years of undergraduate life, except for a small percentage of second semester juniors who are exempted by a lottery system. This requirement is justified by the administration as an effort to help students connect more closely with one another and sustain a sense of belonging within the Duke community. Thus, 85 percent of undergraduates live on campus. All freshmen are housed in one of 14 dormitories on East Campus. These buildings range in occupancy size from 50 (Epworthâ€”the oldest dorm, built in 1892 as "the Inn") to 190 residents (Gilbert-Addoms). Most of these are in the Georgian style typical of the East Campus architecture, although a few newer ones differ in style. Two learning communities, the Performing Arts Community and East Campus Wellness, incorporate the residential component of East Campus with students of similar academic and social interests. 

Sophomores are required to reside on West Campus, again to build class unity. Juniors and seniors can elect to live on West Campus, space permitting. West Campus contains six quadranglesâ€”the four along "Main" West were built in 1930, while two newer ones have since been added. West Campus is home to four learning communities including West Campus Wellness and the Leadership and Civic Engagement communities. These groups are allocated "sections" of the quadrangles, thereby living close to one another, but still within the context of a larger community. Also, 25 "selective living groups" are housed within sections on West, including 15 fraternities. Nine of the ten non-fraternity selective living groups are coeducational. Central Campus provides housing for approximately 1,050 students (of which about 850 are undergraduate juniors or seniors) in 45 apartment buildings. The majority of seniors, however, choose to live off campus. Students living on campus are represented by the elected officials of Campus Council whose mission is to enhance campus life by implementing policies, provide quality programming, and ensure a safe, educational, and enjoyable experience for residents.



Fraternities and sororities enjoy a presence as 29% of men and 42% of women pledge a Greek group. While 15 of the 16 Interfraternity Council (IFC) recognized fraternity chapters live in sections within West Campus quads, the ten Panhellenic Association Sorority Chapters have no such living arrangement. Seven National Pan-Hellenic Council (historically African American) fraternities and sororities hold chapters at Duke. Fraternities not recognized by IFC typically have houses off-campus.

The nearby bars and clubs on Durham's Ninth Street and the surrounding areas are a popular outlet for Greek and "independent" students alike. Students sometimes refer to their social life as occurring within the "Duke Bubble"â€”emphasizing the isolation of the Duke campus from the surrounding community and the relatively low levels of interaction between Durham residents and Duke students. Fraternity chapters frequently host parties in their sections, which typically are more open to non-members than similar functions at other institutions due to the fact that independents live in the same building as the fraternity members.



In the mid-1990s, the administration significantly reduced the number of on-campus kegs by requiring students not only to purchase kegs directly from the university, but also to hire expensive university bartenders. According to administrators, the rule change was intended as a way to increase on-campus safety, but many students see the administration's increasingly strict policies as an attempt to undermine social life at Duke. As a result, off-campus parties have become more frequent in the past few years as they are not under the umbrella of Duke's policies. However, these off-campus parties have come under fire as they have escalated in debauchery. In 2005, one of the off-campus fraternities hosted a heavily attended baby oil wrestling party, which garnered national media attention. The widely reported lacrosse scandal broke in 2006. Many of these houses are situated in the midst of family homes, prompting neighbors to complain about excessive noise and other violations. Police have responded by breaking up parties at several houses, handing out citations, and arresting party-goers. The administration, in an attempt to increase the number of on-campus social events, reduced the price of kegs by 59 percent in August 2006. They also purchased 15 houses that Duke students typically rent off East Campus in March 2006; they plan to sell these homes to single families.

The athletics program, particularly men's basketball, is a significant component of Duke's social life. Duke's students have been recognized as some of the most creative, original, and abrasive fans in all of collegiate athletics. Students, often referred to as Cameron Crazies, show their support of the men's basketball team by "tenting" for home games against key ACC rivals, especially UNC. Because tickets to all varsity sports are free to students, they would line up for hours before the game, often spending the night on the sidewalk. The total number of participating tents is capped at 100 (each tent can have up to 12 occupants), though interest is such that it could exceed that number if space permitted. Tenting involves setting up and inhabiting a tent on the grass near Cameron Indoor Stadium, an area known as Krzyzewskiville, or K-ville for short. There are different categories of tenting based on the length of time and number of people who must be in the tent. At night, K-ville often turns into the scene of a party or occasional concert. The men's basketball coach, Mike Krzyzewski, is known to buy pizza on occasion for the inhabitants of the tent village.



Approximately 400 student clubs and organizations run on Duke's campus. These include numerous student government, special interest, and service organizations. Duke Student Government (DSG) charters and provides most of the funding for these organizations, and represents students' interests when dealing with the administration. One of the most popular activities on campus is competing in sports. Duke has 35 sports clubs and 29 intramural teams that are officially recognized.

 

According to The Princeton Review, Duke is one of 81 institutions in the country with outstanding community service programs. In February 2007, Duke announced plans for DukeEngage, a $30 million civic engagement program that will allow every undergraduate to partake in an in-depth service opportunity over the course of a summer or semester. The program's scope is "unprecedented in U.S. higher education," allotting about $6,200 to every individual who chooses to participate.

Duke's Community Service Center (CSC) oversees 31 student-run service organizations in Durham and the surrounding area. Examples include a weeklong camp for children of cancer patients (Camp Kesem) and a group that promotes awareness about sexual health, rape prevention, alcohol and drug use, and eating disorders (Healthy Devils). The Duke-Durham Neighborhood Partnership, started by the Office of Community Affairs, attempts to address major concerns of local residents and schools by utilizing university resources. Another community project, "Scholarship with a Civic Mission," is a joint program between the Hart Leadership Program and the Kenan Institute for Ethics. Other programs include: Project CHILD, a tutoring program involving 80 first-year volunteers; Project HOPE, an after-school program for at-risk students in Durham that was awarded a $2.25 million grant from the Kellogg Foundation in 2002; and Project BUILD, a freshman volunteering group that dedicates 3300 hours of service to a variety of projects such as schools, Habitat for Humanity, food banks, substance rehabilitation centers, and homeless shelters. Some courses at Duke incorporate service as part of the curriculum to augment material learned in class such as in psychology or education courses (known as service learning classes).

The Chronicle, Duke's independent undergraduate daily newspaper, has been continually published since 1905 and has a readership of about 30,000. Its editors are responsible for coining the term "Blue Devil". The newspaper won Best in Show in the tabloid division at the 2005 Associated Collegiate Press National College Media Convention. Cable 13, established in 1976, is Duke's student-run television station. It stands as a popular activity for students interested in film production and media. WXDU-FM, licensed in 1983, is the University's nationally-recognized, noncommercial FM radio station, operated by student and community volunteers.

Cultural groups on campus include: the Asian Students Association, AQUADuke (Alliance of Queer Undergraduates), Black Student Alliance, Chinese Traditional Dance, Dance Black, Diya (South Asian Association), Jewish Life at Duke, Mi Gente (Latino Student Association), International Association/International Council, Muslim Student Association, Native American Student Coalition, Newman Catholic Student Center, and Students of the Caribbean.

Duke University undergraduates have recently collaborated and worked to develop [WISER], or Women's Institute for Secondary Education and Research. This will be the first all-girls secondary boarding school in Muhuru Bay, Kenya, and will help to inspire and educate girls about leadership and gender equality.



Duke alumni are active through organizations and events such as the annual Reunion Weekend and Homecoming. There are 75 Duke clubs in the U.S. and 38 international clubs. For the 2005â€?006 fiscal year, Duke tied for third in alumni giving rate among U.S. colleges and universities. A number of Duke alumni have made significant contributions in the fields of government, law, science, academia, business, arts, journalism, and athletics, among others. 

Richard Nixon, 37th President of the United States, Elizabeth Dole, senior United States Senator from North Carolina and former President of the American Red Cross, and Ricardo Lagos, 33rd President of Chile from 2000 to 2006, are among the most notable alumni with involvement in politics. In the research realm, Duke graduates who have won the Nobel Prize in Physics include Hans Dehmelt for his development of the ion trap technique, Robert Richardson for his discovery of superfluidity in helium-3, and Charles Townes for his work on quantum electronics.



Several alumni hold top positions at large companies. The current or former Chairman, President, Vice president, or CEO of each of the following Fortune 500 companies is a Duke alumnus: BB&T (John A. Allison IV), Bear Stearns (Alan Schwartz), Boston Scientific Corporation (Peter Nicholas), Cisco Systems (John Chambers), ExxonMobil (Rex Adams), General Motors Corporation (Rick Wagoner), Medtronic (William Hawkins), Morgan Stanley (John J. Mack), Norfolk Southern (David R. Goode), Northwest Airlines (Gary L. Wilson), PepsiCo, Inc. (Karl von der Heyden), and Pfizer (Edmund T. Pratt, Jr.). Kevin Martin is Chairman of the FCC, and Rex Adams serves as the Chairman of PBS. Another alumna, Melinda Gates, is the co-founder of the $31.9 billion Bill & Melinda Gates Foundation, the nation's wealthiest charitable foundation.

John Feinstein is a notable sportswriter for The Washington Post, while Charlie Rose is a former contributor for 60 Minutes II and currently hosts his own talk show. Judy Woodruff is a senior correspondent for The NewsHour with Jim Lehrer on PBS and was formerly NBC's White House correspondent and an anchor for CNN. Jay Bilas is a basketball analyst for ESPN who co-hosts College GameDay, and also joins CBS as a game analyst for the NCAA Men's Basketball Championship. Sean McManus is president of both CBS Sports and CBS News, while Dan Abrams serves as the General Manager of MSNBC.



William C. Styron won the Pulitzer Prize for Fiction in 1968 for his novel The Confessions of Nat Turner and is also well-known for his 1979 novel Sophie's Choice. The Pulitzer Prize for Fiction was also awarded to Anne Tyler for her 1988 novel Breathing Lessons. Rik Kirkland serves as a Managing Editor for the magazine Fortune, while Clay Felker is a founding editor of New York. John Harwood is the Chief Washington Correspondent for CNBC, a Senior Contributing Writer for The Wall Street Journal, and frequent panelist on Washington Week. In the arts realm, Annabeth Gish (actress in the X-Files and The West Wing), Randall Wallace (screenwriter, producer, and director, Braveheart, Pearl Harbor, We Were Soldiers), and David Hudgins (television writer and producer, Everwood, Friday Night Lights) headline the list. Finally, several athletes have become stars at the professional level, especially in basketball's NBA. Shane Battier, Elton Brand, Carlos Boozer, Grant Hill, Chris Duhon, J.J. Redick, Luol Deng, and Corey Maggette are among the most famous.













 is a console role-playing game developed and published by Square (now Square Enix), and the tenth installment in the Final Fantasy video game series; it was released in 2001 for the Sony PlayStation 2 video game console. It was once among the top twenty best-selling console games of all time, selling 6.6 million units worldwide and was also voted by the readers of the Japanese video game magazine Famitsu to be the greatest video game of all-time. Set in the fantasy world of Spira, the game's story centers around a group of adventurers and their quest to defeat a rampaging force known as "Sin".

Final Fantasy X marks the Final Fantasy series' transition from entirely pre-rendered backdrops to fully three-dimensional areas, achieved with the PlayStation 2â€™s Emotion Engine processor. Though pre-rendered backgrounds are not entirely absent, their use has been restricted to less vibrant locations, such as building interiors. Final Fantasy X is also the first game in the series to feature a wide range of realistic facial expressions, as well as other technological developments in graphical effects achieved, such as variance in lighting and shadow from one section of a character's clothing to the next. Final Fantasy X is also the first in the series to feature voice-over actors, as well as the first to spawn a direct sequel, Final Fantasy X-2.

Final Fantasy X introduces other significant advances in the Final Fantasy series. For instance, because of the implementation of voice-overs, scenes in the game are paced according to the time taken for dialogue to be spoken, whereas previous games in the series incorporated scrolling subtitles. Final Fantasy X features changes in world design, with a focus placed on realism. The gameplay makes a significant departure from past games as well, incorporating several new elements. 

Like previous games in the series, Final Fantasy X is presented in third-person perspective, with players directly navigating the main character, Tidus, or Yuna in some cases, around the world to interact with objects and people. Unlike in previous games, however, the world and town maps have been fully integrated, with terrain outside of cities rendered to scale. When an enemy is encountered, the environment switches to a turn-based "battle area" where characters and enemies must await their turn to attack.

In line with previous titles in the series, players are given the opportunity to develop and improve their characters by defeating enemies and acquiring items, though the traditional experience point system was replaced by a new system called the "Sphere Grid". The game was initially going to feature online elements, but these were dropped during production, and online multiplayer gaming would not become part of the Final Fantasy series until Final Fantasy XI.



Final Fantasy X's gameplay differs from that of previous Final Fantasy games in its lack of an interactive top-down perspective "world map" navigation system. Previous games had featured a miniature representation of the expansive areas between towns and other distinct locations, used for long-distance traveling. In Final Fantasy X, instead, almost all game locations are essentially contiguous and never fade out to an interactive overworld map. Regional connections are mostly linear, forming a single path through the game's locations, though an airship becomes available late in the game, giving the player the ability to navigate the world of Spira in a matter of seconds.

Map director Takayoshi Nakazato has explained that with Final Fantasy X, he wanted to implement a world map concept with a more realistic approach than that of the traditional Final Fantasy game, in-line with the realism afforded by the mechanics of the game's dominant 3D backgrounds, as opposed to that offered by pre-rendered backgrounds (which he refers to as "pseudo 3D environments").

Final Fantasy X introduces the Conditional Turn-Based Battle (CTB) system in place of the series' traditional Active Time Battle (ATB) system, which was originally developed by Hiroyuki Ito and was first used in Final Fantasy IV. The system was developed by battle director Toshiro Tsuchida, who had Final Fantasy IV in mind when developing the CTB system. Whereas the ATB concept features real-time elements, the CTB system is a turn-based format that pauses the battle during each of the player's turns. Thus, the CTB design allows the player to select an action without time pressure. The CTB system also allows characters' and enemies' attributes and actions to affect the number of turns they are allowed and the order in which they occur. A graphical timeline along the upper-right side of the screen details who will be receiving turns next as well as how various actions taken (such as using the Slow spell on an enemy) will affect the subsequent order of turns. 

Character-specific special abilities (known as "Limit Breaks" in some other Final Fantasy games) reappear in Final Fantasy X under the name "Overdrives". In this new incarnation of the feature, most of the techniques are interactive, requiring fighting game-style button inputs or precise timing to increase their effectiveness. Furthermore, an "Overdrive meter" was introduced to determine when such an ability could be executed. Through the use of different "Overdrive Modes", the player is allowed to designate what circumstances (such as receiving damage, slaying an enemy, or being the only living character on the field) cause the Overdrive meter to fill.

Final Fantasy X allows the player to control only up to three characters in battle at once, but a "swapping system" allows the player to replace any of them with one of the (eventually) four others waiting on the sidelines. A player may swap one character for another at any time, unless the on-field character has been defeated. Swapping is encouraged by the fact that each character has a specialized application: Yuna has the greatest skill at healing with White Magic and can use summon spells; Rikku is adept at using and stealing items; Tidus can use time-altering magic and accurately strike agile enemies; Wakka can inflict negative status effects and accurately strike flying enemies; Auron can pierce enemies' defenses and has the greatest physical strength; Kimahri can use enemy skills; and Lulu has elemental Black Magic spells suited for use against elementally aligned enemies.

Final Fantasy X introduces an overhaul of the summoning system employed in previous installments of the series. Whereas in previous games a summoned creature would arrive, perform a single action, and then depart, Final Fantasy X's summons (called "aeons") arrive and entirely replace the battle party, fighting in their place until either the enemy has been slain, the aeon itself has been defeated, or the aeon is dismissed by the player. Aeons have their own stats, commands, special attacks, spells, and Overdrives, and in addition to providing powerful attacks, they can be employed as "meat shields" while fighting difficult bosses, as the enemy must first kill any summoned aeon before it can damage the party directly. The player acquires a minimum of five aeons over the course of the game, and three additional aeons can be unlocked by completing various sidequests.

Originally, Final Fantasy X was going to feature wandering enemies visible on the field map, seamless transitions into battles, and the option for players to move around the landscape during enemy encounters. Battle art director Shintaro Takai has explained that it was his intention that battles in Final Fantasy X come across as a natural part of the story and not an independent element. However, due to hardware and system limitations, these ideas were not used until Final Fantasy XI and Final Fantasy XII. Instead, a compromise was made, whereby some transitions from the field screen to battle arenas were made relatively seamless with the implementation of a motion blur effect. The desire for seamless transitions also led to the implementation of the new summoning system seen in the game.

Final Fantasy X's leveling system, the Sphere Grid, was unique in the computer role-playing game genre at the time of its release; however aspects of the approach were adopted by some later titles, for example the "Mantra Grid" in the game . Instead of characters gaining pre-determined statistic bonuses for their attributes after a certain number of battles, each character gains a "sphere level" after collecting enough Ability Points (AP). Sphere levels, in turn, allow players to move around the Sphere Grid, a predetermined grid of several hundred interconnected nodes consisting of various stat and ability bonuses. Items called "spheres" (obtained from defeated enemies, treasure chests, and event prizes) are applied to these nodes, unlocking its function for the selected character. In this way, the playable characters' development resembles a board game.

Producer Yoshinori Kitase has explained that the purpose behind the Sphere Grid is to give players an interactive means of increasing their characters' attributes, such that they will be able to observe the development of those attributes firsthand. The Sphere Grid system also allows players to fully customize characters in contrast to their intended battle roles, such as turning the magician Yuna into a physical powerhouse and the swordsman Auron into a healer. The International and PAL versions of the game include an optional "Expert" version of the Sphere Grid; in these versions, all of the characters start in the middle of the grid and may follow whichever path the player chooses. As a tradeoff, however, the Expert grid has noticeably fewer nodes in total, thus decreasing the total statistic upgrades available during the game.

The world of Final Fantasy X is known as "Spira". It consists of one large landmass divided into three subcontinents, surrounded by small tropical islands. It features diverse climates, ranging from the tropical Besaid and Kilika islands to the temperate Mi'ihen region to the frigid Macalania and Mount Gagazet.

Although it is predominantly populated by humans, Spira features a variety of races. Among them are the Al Bhed, a technologically advanced but disenfranchised sub-group of humans with distinctive spiral-green eyes and unique language. The Guado are somewhat less human in appearance, with elongated fingers and other subtle differences. They also have a natural propensity for magic and conjuring monsters. Still less human in appearance are the large, lion-like, one-horned Ronso, and the frog-like Hypello.

Spira's wildlife population introduces several new concepts into the series. Although most creatures are drawn from real animals, such as cats, dogs, birds and butterflies, a few fictional species appear, such as the gigantic, amphibious shoopuf and the emu-like chocobo. Both are used primarily for transportation purposes. Most other unusual creatures encountered in Final Fantasy X are fiends.

Spira is very different from the mainly European-style worlds found in previous Final Fantasy games, being much more closely modeled on Southeast Asia, most notably with respect to vegetation, topography, architecture, and names. Nomura has identified the South Pacific, Thailand and Japan as major influences on the cultural and geographic design of Spira, particularly concerning the geographic locations of Besaid and Kilika. He has also said that Spira deviates from the worlds of past Final Fantasy games most notably in the level of detail incorporated, something he has expressed to have made a conscious effort to maintain during the design process. Though a southeast Asian theme is dominant, like other games in the franchise, Final Fantasy X borrows elements from many other cultures, featuring references to demonology, Hindu, Norse, Arabic and other mythologies. Psychology is also represented, with Carl Jung referenced by the aeon Anima.

The most distinctive, basic features of Final Fantasy X's mythology are pyreflies, luminescent "bundles of life energy" that emerge from the newly-dead and wander the land. If left to their own devices, they usually cluster together and form into fiends, dangerous monsters that take a variety of forms and return to balls of pyreflies when defeated. The vast majority of enemies in Final Fantasy X are fiends. In rare cases, pyreflies maintain enough cohesion and sentience to become unsent, beings that appear human but are actually the lingering remnant of a dead individual with a purpose left unfinished.

One of the tasks of a summoner in Final Fantasy X is to help guide stray pyreflies to their final resting place, a mystical domain guarded by the Guado and known as the Farplane. They do this through a ritual dance known as "the sending". The other chief function of summoners is to summon aeons, fierce magical creatures created when people sacrifice their lives to encase their souls within statues, becoming fayth. Fayth grant summoners the ability to summon their respective aeons, which are described as "dreams of the fayth". Summoners are charged with the duty of defeating Sin, a gigantic monster that has plagued Spira for a thousand years, capable of wiping out entire towns and armies with ease.

Spira's human population is deeply religious and centered around the temples of Yevon, a millennium-old religious organization that has gained enormous influence. The Yevonite temples consider Sin a divine punishment set upon people for their pride in the use of machines (or machina, as they are called in the game), and forbid the use of advanced technology. However, it is eventually revealed that the highest priests, known as the maesters, have become increasingly corrupt and unfaithful to their own doctrine, making use of machina to increase their power.

Summoners go on pilgrimages to gather aeons and increase their powers. They are accompanied by guardians, trained fighters whose duty is to protect the summoners and assist them along the way. The end of the pilgrimage is in the sacred ruined city of Zanarkand, where summoners acquire the Final Aeon, the only known power that can destroy Sin. It is revealed late in Final Fantasy X that the fayth for the Final Aeon is actually created from one who is personally close to a summoner, requiring a guardian of each summoner who defeats Sin to sacrifice his or her life. Additionally, using the Final Aeon against Sin costs the summoner's life as well. However, even this measure is only temporary: after a small period of peace, known as "the Calm", Sin returns, thus requiring the process to start anew.

Final Fantasy X begins late in the story, with the main character, Tidus, waiting with his allies outside the ruined city of Zanarkand. From this in medias res beginning, Tidus proceeds to narrate the series of events leading up to his present situation, and this extended flashback sequence spans most of the game's storyline. It begins in an unruined Zanarkand, a high-tech metropolis and Tidus' home city, where he is a renowned star of the fictional underwater sport blitzball. When Zanarkand is suddenly attacked by Sin during a blitzball game, Tidus â€?along with his long-time mentor, Auron â€?is sucked into the creature and awakens to find himself alone in the ruins of a deserted temple.

Tidus is then rescued by Al Bhed divers in the area, and one of them, Rikku, tells him that Zanarkand had been destroyed one thousand years earlier. He has little time to dwell on the significance of this news before Sin attacks once again, separating Tidus from the others. He eventually washes up on the tropical island of Besaid, where he meets Wakka, the captain of the Besaid Aurochs, a blitzball team. Impressed by Tidus' blitzball skills, Wakka asks Tidus to join the Aurochs in an upcoming tournament in Luca, suggesting that he may meet someone he knows there.

Tidus is introduced to Yuna, a young summoner who is following in the footsteps of her deceased father, the High Summoner Braska, who temporarily vanquished Sin ten years earlier. Braska's guardians were Auron and Jecht, Tidus's missing father, who had been assumed dead at sea ten years earlier. Tidus also meets Lulu and Kimahri, who, along with Wakka, are to serve as Yuna's guardians, journeying with her on her pilgrimage to the ruins of Zanarkand. There, she plans to acquire the power to summon the "Final Aeon" and use it to defeat Sin.

The party travels by boat to Kilika Island â€?where Sin is encountered yet again, decimating most of the town and its villagers â€?and then to Luca. After the blitzball tournament, the party encounters Auron, who joins them. Not long after, following another encounter with Sin where a crusader fleet is decimated, they are joined by Rikku, who is revealed to be Yuna's cousin.

The party also encounters Seymour Guado, the leader of the Guado and a maester of Yevon, while in Luca. Although he initially presents himself as an ally, it later becomes apparent that he is an enemy who wishes to become Sin, so that he may use its power to kill everyone in Spira in what he believes to be an act of compassion. For this reason, he attempts to force Yuna to marry him, so that he may become her Final Aeon, and thus transform into Sin. Although Seymour is killed after his first battle with Yuna's guardians, he becomes an Unsent and attacks the party again many times.

Around the same time, Tidus becomes increasingly fond of Yuna, but discovers, to his horror, that if she completes her pilgrimage and uses the Final Aeon against Sin, she will die. Aware of her fate, Yuna intends to give her life to provide the people of Spira with the Calm, the brief period of peace that follows Sin's destruction. Complicating matters further, Auron reveals to Tidus that his father Jecht is alive, but no longer human, having now himself become the unwilling embodiment of Sin. Tidus resolves to find a way to free his estranged father and permanently destroy Sin without sacrificing Yuna's life.

As the player approaches Zanarkand, Tidus learns that he, Jecht, and the Zanarkand they hail from are all "dreams", summoned entities akin to aeons. Their city, Dream Zanarkand, was created one thousand years earlier, when a conflict known as the "Machina War" led to Yevon, Zanarkand's ruler and a powerful summoner, taking desperate measures to preserve its memory. He had his city's surviving people become fayth so that he could use their memories of Zanarkand to create a new city in its image, far removed from the warfare on the Spiran mainland. Sin was also created at this time, given form by Yevon himself to serve as "armor" protecting himself and the fayth. While continuously summoning Dream Zanarkand, Yevon lost his humanity and became known as "Yu Yevon" (or "the Curse of Yevon"), a being existing solely to maintain Dream Zanarkand's existence. Over the next one thousand years, Sin would persistently attack the people of Spira to keep them from gaining the technology to learn of Dream Zanarkand's existence.

Once the player completes Yuna's pilgrimage to Zanarkand â€?ending Tidus' extended flashback sequence recounting most of the game's events â€?she and her companions learn from the unsent spirit of Lady Yunalesca â€?Yevon's daughter and the first summoner to have defeated Sin â€?that the Final Aeon is created from the spirit of one close to a summoner, and that when Sin is defeated, Yu Yevon's spirit then possesses it, transforming it into a new Sin. Additionally, it is revealed that Auron himself is an Unsent, having been killed by Yunalesca ten years earlier when he confronted her in rage after the deaths of Braska and Jecht. After the party defeats Yunalesca, Yuna and her guardians decide to seek a new way to defeat Sin: one that will permanently destroy him and will not require any sacrifices. Without having acquired the use of a Final Aeon, the party attacks Sin directly using a forbidden machina airship salvaged by the Al Bhed, and enters Sin's shredded body.

Inside Sin, the party battles Seymour, Jecht's imprisoned spirit, and Yu Yevon, defeating each and sending their spirits to the Farplane. Thus, they are able to end Sin's cycle of rebirth forever. Auron dissipates and goes to the Farplane as well, having fulfilled his promise to Jecht and Braska to guard their children. Lastly, the spirits of all the fayth of Spira are freed from their imprisonment, dispersing the aeons, Dream Zanarkand, and Tidus in the process. In a speech to the citizens of Spira, she resolves to help rebuild the world now that it is free of Sin. However, she asks that they never forget the people who have been lost along the way.

Development for Final Fantasy X began in 1999, costing approximately 4 billion Japanese yen (approximately $32.3 million) with a crew of more than one hundred people, most of whom worked on previous games in the series.

As with most other games in the Final Fantasy franchise, the characters and story of Final Fantasy X are distinct from those of its predecessors. Executive producer Hironobu Sakaguchi states that this is to maintain the novelty of each title and to show off his team's true potential. Although he had certain reservations about the transition from 2D to 3D backgrounds, the voice acting, and the transition to real-time story-telling, Sakaguchi believes Final Fantasy's success can be attributed to constantly challenging the development team to try new things. For his part, scenario writer Kazushige Nojima has said that with this installment of the series, he was particularly concerned with establishing a connection in the relationship between the player and main character. Thus, he penned the story such thatâ€”since both Tidus and the player find themselves in a new worldâ€”the player's progress through the world and growing knowledge about it is reflected in Tidus' own developing understanding and narration.

Final Fantasy X also features innovations in the rendering of facial expressions on characters, achieved through motion capture and skeletal animation technology. This technology allowed animators to create realistic lip movements, which were then programmed to match the speech of the game's voice actors.

Nojima also revealed that the inclusion of voice-overs had a substantial impact on the writing of the game's story. He has explained that the presence of voice actors allowed him to maintain a more simple method of storytelling, as the range of emotions that could be expressed through them was greater than that provided by text alone. Nojima has further revealed that the presence of voice actors led him to make various changes to the story and characters themselves, so as to solidify the voice actors' personalities with the characters they were portraying.

In some respects, however, the inclusion of voice-overs led to additional difficulties. With the game's cutscenes already programmed around the Japanese voice work, Final Fantasy X's English localization team faced not only the difficulty of establishing English-oriented dialogue, but also the added obstacle of incorporating this modified wording with the previously established rhythm and timing of the characters' lip movements. In his words, lead localization specialist Alexander O. Smith described the process of "fitting natural-sounding English speech into [...] the high-polygon scenes and CG movies" as "something akin to writing four or five movies worth of dialogue entirely in haiku form [and] of course the actors had to act, and act well, within those restraints". To this end, each voice actor was briefed on their character's motivations and feelings for every scene, and also shown various scenes from the game itself.

Final Fantasy X marks the first time Nobuo Uematsu has had any assistance in composing the score for a Final Fantasy game. His fellow composers for Final Fantasy X were Masashi Hamauzu and Junya Nakano.

The game includes three songs with vocalized elements, one of which is the J-pop ballad "Suteki Da Ne". It is sung by Japanese folk singer Ritsuki Nakano (also known as "RIKKI"), whom the music team contacted while searching for a singer whose music reflected an Okinawan atmosphere. "Suteki Da Ne" is sung in its original Japanese form in both the Japanese and English versions of Final Fantasy X. The song's title translates to "Isn't it Beautiful?" in English, and its lyrics were written by Nojima, while Uematsu composed the instrumentals. Like the ballads from Final Fantasy VIII and IX, "Suteki Da Ne" has an in-game version together with an orchestrated version used as part of the ending theme. The other songs featuring lyrics are the heavy metal opening theme, "Otherworld", sung in English by singer Bill Muir, and the "Hymn of the Fayth", a recurring piece sung using Japanese syllabary.

Final Fantasy X's reception was largely positive, with high sales figures and critical acclaim from the gaming industry. The game sold 90% of its initial 2,140,000-unit shipment â€?1,926,000 units â€?within just the first four days of release in Japan, having already sold between 1.4 million and 1.5 million copies in pre-orders. These figures exceeded the performances of Final Fantasy IX and Final Fantasy VII in a comparable period, and Final Fantasy X became the first PlayStation 2 game to reach sales totals of 2 million and 4 million copies. Once among the top twenty best-selling console games of all time, as of March 2006, the game's consistent sales have earned it the position of the second best-selling Final Fantasy game. As of July 2006, the game has been rated the US market's 11th best selling game of the 21st century, and was nominated for the 6th Annual Interactive Achievement Awards for animation and console role-playing game of the year in 2003. At the 7th anniversary of the PS2 in the United States (October 2007), the game was listed as the eighth best selling game for the PS2. In 2007, Final Fantasy X was named 3rd best PlayStation 2 game of all time in IGN's feature reflecting on the PlayStation 2's long lifespan.

Both Japanese and western critics have generally given Final Fantasy X high scores, with the game attaining a 92/100 ("universal acclaim") according to Metacritic. A leading Japanese video game magazine, Famitsu, awarded the game a near-perfect 39/40 score, while readers of the same magazine voted it the best game of all time in early 2006. Another leading Japanese gaming magazine, The Play Station, gave the game a score of 29/30.

As part of their reviews, Famitsu and The Play Station expressed particularly favorable responses toward the game's storyline and graphics, as did the UK-based magazine Edge. However, the magazine only gave the game a 6/10, describing it as "Sequential software that labels itself next-gen" without providing a next generation gaming experience, instead repeating "the mistakes ... made on the last version". In this regard, Edge cites the game's battle and character-leveling systems, describing the former as only "fractionally more complex" than was the case in previous installments of the series, and the latter as "[no] more flexible than the straight leveling from previous games". Edge also dealt harsh criticism to the game's English script and voice-overs, regarding the dialogue, "both textual and verbal", as "nauseating". The magazine went on to say that it "renders the pathos comedic, the comedy dead, and ... butchers the whole game". Multimedia website IGN offered extensive praise for the voice actors and the innovations in gameplay, particularly with regard to the revised battle and summon magic system, the option to change party members during battle, and what they felt were more efficient character development and inventory management systems. Offering additional praise for the game's graphics, which they suggested "improves on its predecessors in every area possible", they commented that the game as a whole was "the best-looking game of the series [and] arguably the best-playing as well" at the time of release. GameSpot admired the game's storyline, calling it surprisingly complex, its ending satisfying, and its avoidance of RPG clichÃ©s commendable. GamePro magazine agreed, saying that despite an "anticlimactic final battle", the story remained engaging every step of the way.

Due to its commercial and popular success, Square Enix released a direct sequel to Final Fantasy X in 2003, entitled "Final Fantasy X-2". This sequelâ€”the first direct sequel developed in the Final Fantasy seriesâ€”is set two years after the conclusion of the original story, establishing new conflicts and dilemmas and resolving loose ends left by the original game. Also as a result of the game's reception, Kitase and Nojima decided to establish a plot-related connection between Final Fantasy X and Final Fantasy VII, another popular Final Fantasy title.

The advancements in portraying realistic emotions achieved with Final Fantasy X through voice-overs and detailed facial expressions have since become a staple of the series, with its sequel and other subsequent titles â€?such as Final Fantasy XII and  â€?also featuring this development. Additionally, traversing real-time 3D environments instead of an overworld map has also become a standard of the series, as demonstrated in both Final Fantasy XI and Final Fantasy XII.

The Japanese version of the game included an additional disc titled "The Other Side of Final Fantasy", which included interviews, storyboards, and trailers for Blue Wing Blitz, Kingdom Hearts, , as well as the first footage of Final Fantasy XI.

An international version of the game was released in Japan as "Final Fantasy X International" and in PAL territories under its original title. It features content not available in the original NTSC releases, including battles with dark versions of the game's aeons and an airship fight with the superboss Penance. The Japanese release of Final Fantasy X International also includes a twelve minute video clip bridging the story of Final Fantasy X with that of its sequel, Final Fantasy X-2.

Additionally, the European release includes a bonus DVD as "Beyond Final Fantasy", a disc including interviews with the game's developers, as well as two of the game's English voice actors, James Arnold Taylor (Tidus) and Hedy Burress (Yuna). Also included are various trailers for Final Fantasy X and Kingdom Hearts, a gallery of concept and promotional art for the game, and a music video of "Suteki Da Ne" performed by Rikki.

In addition to a sequel, Square Enix produced numerous action figures, several versions of the  and various books, including The Art of Final Fantasy X and three Ultimania guides, a series of artbooks/strategy guides published by Square Enix in Japan. They feature original artwork from Final Fantasy X, offer gameplay walkthroughs, expand upon many aspects of the game's storyline and feature several interviews with the game's designers. There are three books in the series: Final Fantasy X Scenario Ultimania, Final Fantasy X Battle Ultimania and Final Fantasy X Ultimania Î©. A similar three-book series was produced for Final Fantasy X-2.

In 2005, a compilation featuring Final Fantasy X and Final Fantasy X-2 was released in Japan as Final Fantasy X/X-2 Ultimate Box.













The humpback whale (Megaptera novaeangliae) is a baleen whale. One of the larger rorqual species, adults range in length from 12â€?6Â metres (40â€?0Â ft) and weigh approximately 36,000Â kilograms (79,000Â lb). The humpback has a distinctive body shape, with unusually long pectoral fins and a knobbly head. It is an acrobatic animal, often breaching and slapping the water. Males produce a complex whale song, which lasts for 10 to 20 minutes and is repeated for hours at a time. The purpose of the song is not yet clear, although it appears to have a role in mating.

Found in oceans and seas around the world, humpback whales typically migrate up to 25,000 kilometres each year. Humpbacks feed only in summer, in polar waters, and migrate to tropical or sub-tropical waters to breed and give birth in the winter. During the winter, humpbacks fast and live off their fat reserves. The species' diet consists mostly of krill and small fish. Humpbacks have a diverse repertoire of feeding methods, including the spectacular bubble net feeding technique.

Like other large whales, the humpback was and is a target for the whaling industry. Due to over-hunting, its population fell by an estimated 90% before a whaling moratorium was introduced in 1966. Stocks of the species have since partially recovered; however, entanglement in fishing gear, collisions with ships, and noise pollution also remain concerns. There are at least 70,000 humpback whales worldwide. Once hunted to the brink of extinction, humpbacks are now sought out by whale-watchers, particularly off parts of Australia and the United States.

A phylogenetic tree of animals related to the humpback whaleHumpback whales are rorquals (family Balaenopteridae), a family that includes the blue whale, the fin whale, the Bryde's whale, the Sei whale and the Minke whale. The rorquals are believed to have diverged from the other families of the suborder Mysticeti as long ago as the middle Miocene. However, it is not known when the members of these families diverged from each other.

Though clearly related to the giant whales of the genus Balaenoptera, the humpback has been the sole member of its genus since Gray's work in 1846. More recently though, DNA sequencing analysis has indicated both the humpback and the Gray whale are close relatives of the Blue Whale, the world's largest animal. If further research confirms these relationships, it will be necessary to reclassify the rorquals.

The humpback whale was first identified as "baleine de la Nouvelle Angleterre" by Mathurin Jacques Brisson in his Regnum Animale of 1756. In 1781, Georg Heinrich Borowski described the species, converting Brisson's name to its Latin equivalent, Balaena novaeangliae. Early in the 19th century LacÃ©pÃ¨de shifted the humpback from the Balaenidae family, renaming it Balaenoptera jubartes. In 1846, John Edward Gray created the genus Megaptera, classifying the humpback as Megaptera longpinna, but in 1932, Remington Kellogg reverted the species names to use Borowski's novaeangliae. The common name is derived from their humping motion while swimming. The generic name Megaptera from the Greek mega-/Î¼ÎµÎ³Î±- "giant" and ptera/Ï€Ï„ÎµÏÎ± "wing", refers to their large front flippers. The specific name means "New Englander" and was probably given by Brisson due the regular sightings of humpbacks off the coast of New England.

Humpback whales can easily be identified by their stocky bodies with obvious humps and black dorsal colouring. The head and lower jaw are covered with knobs called tubercles, which are actually hair follicles and are characteristic of the species. The tail flukes, which are lifted high in some dive sequences, have wavy trailing edges. There are four global populations, all being studied. North Pacific, Atlantic, and southern ocean humpbacks have distinct populations which make an annual migration. One population in the Indian Ocean does not migrate. The Indian Ocean has a northern coastline, while the Atlantic and Pacific oceans do not, thereby preventing the humpbacks from migrating to the pole.

The long black and white tail fin, which can be up to a third of body length, and the pectoral fins have unique patterns, which enable individual whales to be recognised. Several suggestions have been made to explain the evolution of the humpback's pectoral fins, which are proportionally the longest fins of any cetacean. The two most enduring hypotheses are the higher maneuverability afforded by long fins, or that the increased surface area is useful for temperature control when migrating between warm and cold climates. Humpbacks also have 'rete mirable' a heat exchanging system, which works similarly to the same structured system in certain species of sharks and other fish.

Humpbacks have 270 to 400 darkly coloured baleen plates on each side of the mouth. The plates measure from a mere 18 inches in the front to approximately 3 feet long in the back, behind the hinge. Ventral grooves run from the lower jaw to the umbilicus about halfway along the bottom of the whale. These grooves are less numerous (usually 16â€?0) and consequently more prominent than in other rorquals. The stubby dorsal fin is visible soon after the blow when the whale surfaces, but has disappeared by the time the flukes emerge. Humpbacks have a distinctive 3Â m (10Â ft) heart shaped to bushy blow, or exhalation of water through the blowholes. Early whalers also noted blows from humpback adults to be 10 - 20 feet high. Whaling records show they understood each specie has its own distinct shape and height of blows.

Newborn calves are roughly the length of their mother's head. A  mother would have a  newborn weighing in at . They are nursed by their mothers for approximately six months, then are sustained through a mixture of nursing and independent feeding for possibly six months more. Humpback milk is 50% fat and pink in color. Some calves have been observed alone after arrival in Alaskan waters. Females reach sexual maturity at the age of five with full adult size being achieved a little later. According to new research, males reach sexual maturity at approximately 7 years of age. Fully grown the males average 15â€?6Â m (49â€?2Â ft), the females being slightly larger at 16â€?7Â m (52â€?6Â ft), with a weight of 40,000Â kg (or 44Â tons); the largest recorded specimen was 19Â m (62Â ft) long and had pectoral fins measuring 6Â m (20Â ft) each. The largest humpback on record, according to whaling records, was killed in the Caribbean. She was  long, weighing nearly 90 tons.

Females have a hemispherical lobe about 15Â centimetres (6Â in) in diameter in their genital region. This allows males and females to be distinguished if the underside of the whale can be seen, even though the male's penis usually remains unseen in the genital slit. Male whales have distinctive scars on heads and bodies, some resulting from battles over females. 

Females typically breed every two or three years. The gestation period is 11.5 months, yet some individuals can breed in two consecutive years. Humpback whales were thought to live 50&ndash60 years, but new studies using the changes in amino acids behind eye lenses proved another baleen whale, the Bowhead, to be 211 years old. This was an animal taken by the Inuit off Alaska. More studies on ages are currently being done.

The varying patterns on the humpback's tail flukes are sufficient to identify an individual. Unique visual identification is not possible in most cetacean species (exceptions include Orcas and Right Whales), so the humpback has become one of the most-studied species. A study using data from 1973 to 1998 on whales in the North Atlantic gave researchers detailed information on gestation times, growth rates, and calving periods, as well as allowing more accurate population predictions by simulating the mark-release-recapture technique. A photographic catalogue of all known whales in the North Atlantic was developed over this period and is currently maintained by Wheelock College. Similar photographic identification projects have subsequently begun in the North Pacific by SPLASH (Structure of Populations, Levels of Abundance and Status of Humpbacks), and around the world. Another organization (Cascadia Research) headed by well known researcher John Calambokidis, along with Dr. Robin Baird, have joined with others from NOAA, hoping to soon have an online catalog of more than 3500 fluke identification pictures that the public can access, and possibly contribute to.

The humpback social structure is loose-knit. Usually, individuals live alone or in small transient groups that assemble and break up over the course of a few hours. Groups may stay together a little longer in summer in order to forage and feed cooperatively. Longer-term relationships between pairs or small groups, lasting months or even years, have been observed, but are rare. Recent studies extrapolate feeding bonds observed with many females in Alaskan waters over the last 10 years. It is possible some females may have these bonds for a lifetime. More studies need to be done on this. The range of the humpback overlaps considerably with many other whale and dolphin species â€?whilst it may be seen near other species (for instance, the Minke Whale), it rarely interacts socially with them. Humpback calves have been observed in Hawaiian waters playing with bottlenose dolphin calves.

Courtship rituals take place during the winter months, when the whales migrate toward the equator from their summer feeding grounds closer to the poles. Competition for a mate is usually fierce, and female whales as well as mother-calf dyads are frequently trailed by unrelated male whales dubbed escorts by researcher Louis Herman. Groups of two to twenty males typically gather around a single female and exhibit a variety of behaviours in order to establish dominance in what is known as a competitive group. The displays may last several hours, the group size may ebb and flow as unsuccessful males retreat and others arrive to try their luck. Techniques used include breaching, spy-hopping, lob-tailing, tail-slapping, flipper-slapping, charging and parrying. "Super pods" have been observed numbering more than 40 males, all vying for the same female. (M. Ferrari et. al) 

Whale song is assumed to have an important role in mate selection; however, scientists remain unsure whether the song is used between males in order to establish identity and dominance, between a male and a female as a mating call, or a mixture of the two. All these vocal and physical techniques have also been observed while not in the presence of potential mates. This indicates that they are probably important as a more general communication tool. Recent studies showed singing males attract other males. Scientists are extrapolating possibilities the singing may be a way to keep the migrating populations connected. (Ferrari, Nicklin, Darling, et. al.) It has also been noted that the singing begins when the competition ends. Studies on this are ongoing. (www.whaletrust.com)



The species feeds only in summer and lives off fat reserves during winter. Humpback whales will only feed rarely and opportunistically while in their wintering waters. It is an energetic feeder, taking krill and small schooling fish, such as herring (Clupea harengus), salmon, capelin (Mallotus villosus) and sand lance (Ammodytes americanus) as well as Mackerel (Scomber scombrus), pollock (Pollachius virens) and haddock (Melanogrammus aeglefinus) in the North Atlantic. Krill and Copepods have been recorded from Australian and Antarctic waters. It hunts fish by direct attack or by stunning them by hitting the water with its pectoral fins or flukes.The humpback has the most diverse repertoire of feeding methods of all baleen whales. Its most inventive technique is known as bubble net feeding: a group of whales blows bubbles while swimming in circles to create a ring of bubbles. The ring encircles the fish, which are confined in an ever-tighter area as the whales swim in a smaller and smaller circles. The whales then suddenly swim upward through the bubble net, mouths agape, swallowing thousands of fish in one gulp. This technique can involve a ring of bubbles up to 30Â m (100Â ft) in diameter and the cooperation of a dozen animals. Some of the whales take the task of blowing the bubbles through their blowholes, some dive deeper to drive fish toward the surface, and others herd fish into the net by vocalizing. Humpbacks have been observed bubblenet feeding alone as well.	Humpback whales are preyed upon by Orcas. The result of these attacks is generally nothing more serious than some scarring of the skin, but it is likely that young calves are sometimes killed.

Both male and female humpback whales can produce sounds, however only the males produce the long, loud, complex "songs" for which the species is famous. Each song consists of several sounds in a low register that vary in amplitude and frequency, and typically lasts from 10 to 20 minutes. Songs may be repeated continuously for several hours; humpback whales have been observed to sing continuously for more than 24 hours at a time. As cetaceans have no vocal cords, whales generate their song by forcing air through their massive nasal cavities.

Whales within an area sing the same song, for example all of the humpback whales of the North Atlantic sing the same song, and those of the North Pacific sing a different song. Each population's song changes slowly over a period of years â€”never returning to the same sequence of notes. 

Scientists are still unsure of the purpose of whale song. Only male humpbacks sing, so it was initially assumed that the purpose of the songs was to attract females. However, many of the whales observed to approach singing whales have been other males, with the meeting resulting in a conflict. Thus, one interpretation is that the whale songs serve as a threat to other males. Some scientists have hypothesized that the song may serve an echolocative function. During the feeding season, humpback whales make altogether different vocalizations, which they use to herd fish into their bubble nets.

The humpback whale is found in all the major oceans, in a wide band running from the Antarctic ice edge to 65Â° N latitude, though is not found in the eastern Mediterranean, the Baltic Sea or the Arctic Ocean. There are at least 70,000 humpback whales worldwide, with 10,000-25,000 in the North Pacific, about 12,000 in the North Atlantic, and over 50,000 in the Southern Hemisphere, down from a pre-whaling population of 125,000. 

The humpback is a migratory species, spending its summers in cooler, high-latitude waters, but mating and calving in tropical and sub-tropical waters.An exception to this rule is a population in the Arabian Sea, which remains in these tropical waters year-round. Annual migrations of up to 25,000Â kilometres (16,000Â statute miles) are typical, making it one of the farthest-travelling of any mammalian species.

A 2007 study identified seven individual whales wintering off the Pacific coast of Costa Rica as those which had made a trip from the Antarctic of around 8,300Â km. Identified by their unique tail patterns, these animals have made the longest documented migration by a mammal.

In Australia, two main migratory populations have been identified, off the west and east coast respectively. These two populations are distinct with only a few females in each generation crossing between the two groups.

One of the first attempts to hunt the humpback whale was made by John Smith in 1614 off the coast of Maine. Opportunistic killing of the species is likely to have occurred long before, and it continued with increasing pace in the following centuries. By the 18th century, the commercial value of humpback whales had been recognized, and they became a common target for whalers for many years.

By the 19th century, many nations (the United States in particular), were hunting the animal heavily in the Atlantic Ocean, and to a lesser extent in the Indian and Pacific Oceans. It was, however, the introduction of the explosive harpoon in the late 19th century that allowed whalers to accelerate their take. This, along with hunting beginning in the Antarctic Ocean in 1904, led to a sharp decline in all whale populations.

It is estimated that during the 20th century at least 200,000 humpbacks were taken, reducing the global population by over 90%, with the population in the North Atlantic estimated to have dropped to as low as 700 individuals.

To prevent extinction, the International Whaling Commission introduced a ban on commercial humpback whaling in 1966. That ban is still in force. By that time humpback whales were so scarce that commercial hunting was no longer worthwhile.

At this time, 250,000 were recorded killed. However, the true toll is likely to be higher. It is now known that the Soviet Union was deliberately under-recording its kills; the total Soviet humpback kill was reported at 2,820 whereas the true number is now believed to be over 48,000.

As of 2004, hunting of humpback whales is restricted to a few animals each year off the Caribbean island Bequia in the nation of St. Vincent and the Grenadines. The take is not believed to threaten the local population.

Japan had planned to kill 50 humpback whales in the 2007/08 season under its JARPA II research program in the Antarctic Ocean, starting in November 2007. The announcement sparked global protests. 

In New Zealand, protests came from Maori and Pacific community leaders. Whales hold a significant place in the tradition and culture of many Pacific countries, according to Melino Maka, chairman of the Tongan Advisory Council. "We have a spiritual connection with our whales in our waters." he said.

Protests occurred in 20 centres around Australia as well as Tonga. Many whales known to locals and tourism operators in Australian waters were born after whaling finished, so around humans they're benign. It is feared that the whales may become fearful of humans and that it will damage tourism. Whale watching is worth an estimated $260 million in Australia. The Australian government has been vocal in its opposition to whaling, but has been criticized for not taking legal action against it. In the lead up to the Federal election, the Australian shadow environment minister, Peter Garrett, announced a policy whereby Australian navy ships would intercept and board whaling vessels.

After a visit to Tokyo by the chairman of the IWC, asking the Japanese for their co-operation in sorting out the differences between pro- and anti-whaling nations on the Commission, the Japanese whaling fleet agreed that no humpback whales would be caught for the two years it would take for the IWC to reach a formal agreement.

Internationally this species is considered vulnerable. Most monitored stocks of humpback whales have rebounded well since the end of the commercial whaling era, such as the North Atlantic where stocks are now believed to be approaching pre-hunting levels. However, the species is considered endangered in some countries where local populations have recovered slowly, including the United States.

Today, individuals are vulnerable to collisions with ships, entanglement in fishing gear, and noise pollution. Like other cetaceans, humpbacks are sensitive to noise and can even be injured by it. In the 19th century, two humpback whales were found dead near sites of repeated oceanic sub-bottom blasting, with traumatic injuries and fractures in the ears.

The ingestion of saxitoxin, a Paralytic shellfish poisoning (PSP) from contaminated mackerel has been implicated in humpback whale deaths.

Some countries are creating action plans to protect the humpback; for example, in the United Kingdom, the humpback whale has been designated as a priority species under the national Biodiversity Action Plan, generating a set of actions to conserve the species. The sanctuary provided by National Parks such as Glacier Bay National Park and Preserve and Cape Hatteras National Seashore, among others, have also become a major factor in sustaining the populations of the species in those areas.

Although much was known about the humpback whale due to information obtained through whaling, the migratory patterns and social interactions of the species were not well known until two separate studies by R. Chittleborough and W. H. Dawbin in the 1960s. Roger Payne and Scott McVay made further studies of the species in 1971. Their analysis of whale song led to worldwide media interest in the species, and left an impression in the public mind that whales were a highly intelligent cetacean species, a contributing factor to the anti-whaling stance of many countries.

Humpback whales are generally curious about objects in their environment. They will often approach and circle boats. This has become an attraction of whale-watching tourism in many locations around the world since the 1990s. 

Whale-watching locations include the Atlantic coast off the SamanÃ¡ Province of the Dominican Republic, the Pacific coast off Oregon, Washington, Vancouver, Hawaii and Alaska, the Bay of Biscay to the west of France, Sydney,Byron Bay north of Sydney, Hervey Bay north of Brisbane, North and East of Cape Town, the coasts of New England, Nova Scotia and Newfoundland, New Zealand, the Tongan islands, the northern St. Lawrence River and the Snaefellsnes peninsula in the west of Iceland. The species is popular because it breaches regularly and spectacularly, and displays a range of other social behaviours.

As with other cetacean species, however, a mother whale will generally be extremely protective of her infant, and will seek to place herself between any boat and the calf before moving quickly away from the vessel. Whale-watching tour operators are asked to avoid stressing the mother.

A presumably albino humpback whale that travels up and down the east coast of Australia has become famous in the local media, on account of its extremely rare all-white appearance. The whale, first sighted in 1991 and believed to be 3-5 years old at that time, is called Migaloo (a word for "white fellow" from one of the languages of the Indigenous Australians). Speculation about the whale's gender was resolved in October 2004 when researchers from Southern Cross University collected sloughed skin samples from Migaloo as he migrated past Lennox Head, and subsequent genetic analysis of the samples proved he is a male. Because of the intense interest, environmentalists feared that the whale was becoming distressed by the number of boats following it each day. In response, the Queensland and New South Wales governments introduce legislation each year to order the maintenance of a 500Â m (1,600Â ft) exclusion zone around the whale. Recent close up pictures have shown Migaloo to have skin cancer and/or skin cysts as a result of his lack of protection from the sun.

One of the most notable humpback whales is Humphrey the whale, who was rescued twice in California by The Marine Mammal Center and other concerned groups. The first rescue was in 1985, when he swam into San Francisco Bay and then up the Sacramento River towards Rio Vista. Five years later, Humphrey returned and became stuck on a mudflat in San Francisco Bay immediately north of Sierra Point below the view of onlookers from the upper floors of the Dakin Building. He was pulled off the mudflat with a large cargo net and the help of a Coast Guard boat. Both times he was successfully guided back to the Pacific Ocean using a "sound net" in which people in a flotilla of boats made unpleasant noises behind the whale by banging on steel pipes, a Japanese fishing technique known as "oikami." At the same time, the attractive sounds of humpback whales preparing to feed were broadcast from a boat headed towards the open ocean. Since leaving the San Francisco Bay in 1990 Humphrey has been seen only once, at the Farallon Islands in 1991.

A humpback whale mother and calf captivated the San Francisco Bay Area in May 2007. This pair appeared to have gotten lost on their Northern migration, swam into the bay and up the Sacramento River as far as the Port of Sacramento. First spotted on 13 May, the whales inspired intense news coverage and were named Delta and Dawn. Whale fans became worried as the whales, both injured with what were possibly cuts caused by boat propellers, continued their stay in the brackish waters, despite efforts to get them to return to the sea. Unexpectedly, on 20 May they headed back towards the bay, but they tarried near the Rio Vista bridge for 10 days. Finally, on Memorial Day weekend, they left Rio Vista, California; passing Tuesday night, 29 May, through the Golden Gate Bridge out to the Pacific Ocean.

Mister Splashy Pants is a humpback in the south Pacific Ocean. It's being tracked with a satellite tag by Greenpeace as a part of its Great Whale Trail Expedition. The whale's name was chosen in an online poll that garnered attention from several websites, including Boing Boing and Reddit. The name "Mister Splashy Pants" received over 78 % of the votes.

























The World Wide Web (commonly shortened to the Web) is a system of interlinked hypertext documents accessed via the Internet. With a Web browser, a user views Web pages that may contain text, images, videos, and other multimedia and navigates between them using hyperlinks. The World Wide Web was created in 1989 by Sir Tim Berners-Lee, working at CERN in Geneva, Switzerland. Since then, Berners-Lee has played an active role in guiding the development of Web standards (such as the markup languages in which Web pages are composed), and in recent years has advocated his vision of a Semantic Web. Robert Cailliau, also at CERN, was an early evangelist for the project.

Viewing a Web page on the World Wide Web normally begins either by typing the URL of the page into a Web browser, or by following a hyperlink to that page or resource. The Web browser then initiates a series of communication messages, behind the scenes, in order to fetch and display it.

First, the server-name portion of the URL is resolved into an IP address using the global, distributed Internet database known as the domain name system, or DNS. This IP address is necessary to contact and send data packets to the Web server.

The browser then requests the resource by sending an HTTP request to the Web server at that particular address. In the case of a typical Web page, the HTML text of the page is requested first and parsed immediately by the Web browser, which will then make additional requests for images and any other files that form a part of the page. Statistics measuring a website's popularity are usually based on the number of 'page views' or associated server 'hits', or file requests, which take place.

Having received the required files from the Web server, the browser then renders the page onto the screen as specified by its HTML, CSS, and other Web languages. Any images and other resources are incorporated to produce the on-screen Web page that the user sees.

Most Web pages will themselves contain hyperlinks to other related pages and perhaps to downloads, source documents, definitions and other Web resources. Such a collection of useful, related resources, interconnected via hypertext links, is what was dubbed a "web" of information. Making it available on the Internet created what Tim Berners-Lee first called the WorldWideWeb (a term written in CamelCase, subsequently discarded) in 1990.



The underlying ideas of the Web can be traced as far back as 1980, when, at CERN in Switzerland, Tim Berners-Lee built ENQUIRE (referring to Enquire Within Upon Everything, a book he recalled from his youth). While it was rather different from the system in use today, it contained many of the same core ideas (and even some of the ideas of Berners-Lee's next project after the World Wide Web, the Semantic Web). 

In March 1989, Tim Berners-Lee wrote a proposal, which referenced ENQUIRE and described a more elaborate information management system. With help from Robert Cailliau, he published a more formal proposal for the World Wide Web on November 12, 1990. The role model was provided by EBT's (Electronic Book Technology, a spin-off from theInstitute for Research in Information and Scholarship at Brown University) Dynatext SGML reader that CERN had licensed. The Dynatext system was considered, however technically advanced (a key player in the extension of SGML ISO 8879:1986 to Hypermedia within HyTime), too expensive and with an inappropriate licensing policy for general HEP (High Energy Physics) community use: a fee for each document and each time a document was charged.

A NeXTcube was used by Berners-Lee as the world's first Web server and also to write the first Web browser, WorldWideWeb, in 1990. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the first Web browser (which was a Web editor as well), the first Web server, and the first Web pages which described the project itself.

On August 6, 1991, he posted a short summary of the World Wide Web project on the alt.hypertext newsgroup. This date also marked the debut of the Web as a publicly available service on the Internet.

First server outside of Europe was created at SLAC in December 1991 .

The crucial underlying concept of hypertext originated with older projects from the 1960s, such as the Hypertext Editing System (HES) at Brown University--- among others Ted Nelson and Andries van Dam--- Ted Nelson's Project Xanadu and Douglas Engelbart's oN-Line System (NLS). Both Nelson and Engelbart were in turn inspired by Vannevar Bush's microfilm-based "memex," which was described in the 1945 essay "As We May Think". 

Berners-Lee's breakthrough was to marry hypertext to the Internet. In his book Weaving The Web, he explains that he had repeatedly suggested that a marriage between the two technologies was possible to members of both technical communities, but when no one took up his invitation, he finally tackled the project himself. In the process, he developed a system of globally unique identifiers for resources on the Web and elsewhere: the Uniform Resource Identifier.

The World Wide Web had a number of differences from other hypertext systems that were then available. The Web required only unidirectional links rather than bidirectional ones. This made it possible for someone to link to another resource without action by the owner of that resource. It also significantly reduced the difficulty of implementing Web servers and browsers (in comparison to earlier systems), but in turn presented the chronic problem of link rot. Unlike predecessors such as HyperCard, the World Wide Web was non-proprietary, making it possible to develop servers and clients independently and to add extensions without licensing restrictions.

On April 30, 1993, CERN announced that the World Wide Web would be free to anyone, with no fees due. Coming two months after the announcement that the Gopher protocol was no longer free to use, this produced a rapid shift away from Gopher and towards the Web. An early popular Web browser was ViolaWWW, which was based upon HyperCard. 

Scholars generally agree, however, that the turning point for the World Wide Web began with the introduction of the Mosaic Web browser in 1993, a graphical browser developed by a team at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign (NCSA-UIUC), led by Marc Andreessen. Funding for Mosaic came from the High-Performance Computing and Communications Initiative, a funding program initiated by then-Senator Al Gore's High Performance Computing and Communication Act of 1991, also known as the Gore Bill. (See Al Gore's contributions to the Internet and technology for more information.) Prior to the release of Mosaic, graphics were not commonly mixed with text in Web pages, and its popularity was less than older protocols in use over the Internet, such as Gopher and Wide Area Information Servers (WAIS). Mosaic's graphical user interface allowed the Web to become, by far, the most popular Internet protocol.

The concept of a home-based global information system goes back at least as far as Isaac Asimov's short story "Anniversary" (Amazing Stories, March 1959), in which the characters look up information on a home computer called a "Multivac outlet" -- which was connected by a "planetwide network of circuits" to a mile-long "super-computer" somewhere in the bowels of the Earth. One character is thinking of installing a Multivac, Jr. model for his kids. 

The story was set in the far distant future when commercial space travel was commonplace, and yet the machine "prints the answer on a slip of tape" that comes out a slot -- there is no video display -- and the owner of the home computer says that he doesn't spend the kind of money to get a Multivac outlet that talks.

Many formal standards and other technical specifications define the operation of different aspects of the World Wide Web, the Internet, and computer information exchange. Many of the documents are the work of the World Wide Web Consortium (W3C), headed by Berners-Lee, but some are produced by the Internet Engineering Task Force (IETF) and other organizations.

Usually, when Web standards are discussed, the following publications are seen as foundational:

Additional publications provide definitions of other essential technologies for the World Wide Web, including, but not limited to, the following:

A significant advance in Web technology was Sun Microsystems' Java platform. It enables Web pages to embed small programs (called applets) directly into the view. These applets run on the end-user's computer, providing a richer user interface than simple Web pages. Java client-side applets never gained the popularity that Sun had hoped for a variety of reasons, including lack of integration with other content (applets were confined to small boxes within the rendered page) and the fact that many computers at the time were supplied to end users without a suitably installed Java Virtual Machine, and so required a download by the user before applets would appear. Adobe Flash now performs many of the functions that were originally envisioned for Java applets, including the playing of video content, animation, and some rich UI features. Java itself has become more widely used as a platform and language for server-side and other programming. 

JavaScript, on the other hand, is a scripting language that was initially developed for use within Web pages. The standardized version is ECMAScript. While its name is similar to Java, JavaScript was developed by Netscape and it has almost nothing to do with Java, although, like Java, its syntax is derived from the C programming language. In conjunction with a Web page's Document Object Model, JavaScript has become a much more powerful technology than its creators originally envisioned. The manipulation of a page's Document Object Model after the page is delivered to the client has been called Dynamic HTML (DHTML), to emphasize a shift away from static HTML displays. 

In simple cases, all the optional information and actions available on a JavaScript-enhanced Web page will have been downloaded when the page was first delivered. Ajax ("Asynchronous JavaScript And XML") is a JavaScript-based technology that provides a method whereby parts within a Web page may be updated, using new information obtained over the network at a later time in response to user actions. This allows the page to be more responsive, interactive and interesting, without the user having to wait for whole-page reloads. Ajax is seen as an important aspect of what is being called Web 2.0. Examples of Ajax techniques currently in use can be seen in Gmail, Google Maps, and other dynamic Web applications.

Web page production is available to individuals outside the mass media. In order to publish a Web page, one does not have to go through a publisher or other media institution, and potential readers could be found in all corners of the globe.

Many different kinds of information are available on the Web, and for those who wish to know other societies, cultures, and peoples, it has become easier. 

The increased opportunity to publish materials is observable in the countless personal and social networking pages, as well as sites by families, small shops, etc., facilitated by the emergence of free Web hosting services.

According to a 2001 study, there were more than 550 billion documents on the Web, mostly in the "invisible Web", or deep Web. A 2002 survey of 2,024 million Web pages determined that by far the most Web content was in English: 56.4%; next were pages in German (7.7%), French (5.6%), and Japanese (4.9%). A more recent study, which used Web searches in 75 different languages to sample the Web, determined that there were over 11.5 billion Web pages in the publicly indexable Web as of the end of January 2005.

Over 100.1 million websites operated as of March 2008. Of these 74% were commercial or other sites operating in the .com generic top-level domain. Among services paid for by advertising, Yahoo! could collect the most data about commercial Web users, about 2,500 bits of information per month about each typical user of its site and its affilated advertising network sites. Yahoo! was followed by MySpace with about half that potential and then by AOL-TimeWarner, Google, Facebook, Microsoft, and eBay. About 26% of websites operated outside .com addresses.

Frustration over congestion issues in the Internet infrastructure and the high latency that results in slow browsing has led to an alternative, pejorative name for the World Wide Web: the World Wide Wait. Speeding up the Internet is an ongoing discussion over the use of peering and QoS technologies. Other solutions to reduce the World Wide Wait can be found on W3C.

Standard guidelines for ideal Web response times are (Nielsen 1999, page 42):

These numbers are useful for planning server capacity.

If a user revisits a Web page after only a short interval, the page data may not need to be re-obtained from the source Web server. Almost all Web browsers cache recently-obtained data, usually on the local hard drive. HTTP requests sent by a browser will usually only ask for data that has changed since the last download. If the locally-cached data is still current, it will be reused.

Caching helps reduce the amount of Web traffic on the Internet. The decision about expiration is made independently for each downloaded file, whether image, stylesheet, JavaScript, HTML, or whatever other content the site may provide. Thus even on sites with highly dynamic content, many of the basic resources only need to be refreshed occasionally. Web site designers find it worthwhile to collate resources such as CSS data and JavaScript into a few site-wide files so that they can be cached efficiently. This helps reduce page download times and lowers demands on the Web server.

There are other components of the Internet that can cache Web content. Corporate and academic firewalls often cache Web resources requested by one user for the benefit of all. (See also Caching proxy server.) Some search engines, such as Google or Yahoo!, also store cached content from websites.

Apart from the facilities built into Web servers that can determine when files have been updated and so need to be re-sent, designers of dynamically-generated Web pages can control the HTTP headers sent back to requesting users, so that transient or sensitive pages are not cached. Internet banking and news sites frequently use this facility.

Data requested with an HTTP 'GET' is likely to be cached if other conditions are met; data obtained in response to a 'POST' is assumed to depend on the data that was POSTed and so is not cached.

Over time, many Web resources pointed to by hyperlinks disappear, relocate, or are replaced with different content. This phenomenon is referred to in some circles as "link rot" and the hyperlinks affected by it are often called "dead links".

The ephemeral nature of the Web has prompted many efforts to archive Web sites. The Internet Archive is one of the most well-known efforts; it has been active since 1996.

The major academic event covering the Web is the World Wide Web series of conferences, promoted by IW3C2.



There are numerous matters of security related to the Web. There are security risks for Web servers, their computer networks, as well as the end-users. 

See also:



Many countries are regulating the web accessibility as requirement for the web sites.

The letters "www" are commonly found at the beginning of Web addresses because of the long-standing practice of naming Internet hosts (servers) according to the services they provide. So for example, the host name for a Web server is often "www"; for an FTP server, "ftp"; and for a USENET news server, "news" or "nntp" (after the news protocol NNTP). These host names appear as DNS subdomain names, as in "www.example.com".

This use of such prefixes is not required by any technical standard; indeed, the first Web server was at "nxoc01.cern.ch", and even today many Web sites exist without a "www" prefix. The "www" prefix has no meaning in the way the main Web site is shown. The "www" prefix is simply one choice for a Web site's subdomain name.

Some Web browsers will automatically try adding "www." to the beginning, and possibly ".com" to the end, of typed URLs if no host is found without them. Internet Explorer, Mozilla Firefox, Safari, and Opera will also prefix "http://www." and append ".com" to the address bar contents if the Control and Enter keys are pressed simultaneously. For example, entering "example" in the address bar and then pressing either just Enter or Control+Enter will usually resolve to "http://www.example.com", depending on the exact browser version and its settings.

In English, "www" (pronounced "double you double you double you") is the longest possible three-letter acronym to pronounce, requiring nine syllables. Often shorter versions such as "triple w" (pronounced triple double you) and "www" pronounced "wuh wuh wuh" are used as replacements. The latter has a much less appealing sound.

In New Zealand, the pronunciation is sometimes shortened to "dub dub dub". 

In Chinese, the World Wide Web is commonly translated to wÃ n wÃ©i wÇŽng (), which satisfies "www" and literally means "ten-thousand dimensional net".



[[windows 3.0]]









Music is an art form consisting of sound and silence. Elements of music are pitch (which governs melody and harmony), rhythm (and its associated concepts tempo, meter, and articulation), dynamics, structure, and the sonic qualities of timbre and texture.

The creation, performance, significance, and even the definition of music vary according to culture and social context. Music ranges from strictly organized compositions (and their recreation in performance), through improvisational music to aleatoric forms. Music can be divided into genres and sub-genres, although the dividing lines and relationships between music genres are often subtle, sometimes open to individual interpretation, and occasionally controversial. Within "the arts", music can be classified as a performing art, a fine art, or an auditory art form. 

The history of music predates the written word. The development of music among humans must have taken place against the backdrop of natural sounds such as birdsong and the sounds other animals use to communicate. Prehistoric music is the name given to all music produced in preliterate cultures.

A range of paleolithic sites have yielded bones in which lateral holes have been pierced: these are usually identified as flutes, blown at one end like the Japanese shakuhachi. The earliest written records of musical expression are to be found in the Sama Veda of India and in 4,000 year old cuneiform from Ur. Instruments, such as the seven-holed flute and various types of stringed instruments have been recovered from the Indus valley civilization archaeological sites. India has one of the oldest musical traditions in the worldâ€”references to Indian classical music (marga) can be found in the ancient scriptures of the Hindu tradition, the Vedas. The traditional art or court music of China has a history stretching for more than three thousand years. Music was an important part of cultural and social life in Ancient Greece: mixed-gender choruses performed for entertainment, celebration and spiritual ceremonies; musicians and singers had a prominent role in ancient Greek theater 

In the 9th century, al-Farabi wrote a notable book on music titled Kitab al-Musiqi al-Kabir ("Great Book of Music"). He played and invented a variety of musical instruments and devised the Arab tone system of pitch organisation, which is still used in Arabic music.

While musical life in Europe was undoubtedly rich in the early Medieval era, as attested by artistic depictions of instruments, writings about music, and other records, the only European repertory which has survived from before about 800 is the monophonic liturgical plainsong of the Roman Catholic Church, the central tradition of which was called Gregorian chant. Several schools of liturgical polyphony flourished beginning in the 12th century. Alongside these traditions of sacred music, a vibrant tradition of secular song developed, exemplified by the music of the troubadours, trouvÃ¨res and MinnesÃ¤nger. 

Much of the surviving music of 14th century Europe is secular. By the middle of the 15th century, composers and singers used a smooth polyphony for sacred musical compositions such as the mass, the motet, and the laude, and secular forms such as the chanson and the madrigal. The introduction of commercial printing had an immense influence on the dissemination of musical styles.

The first operas, written around 1600 and the rise of contrapuntal music define the end of the Renaissance and the beginning of the Baroque era that lasted until roughly 1750, the year of the death of Johann Sebastian Bach.German Baroque composers wrote for small ensembles including strings, brass, and woodwinds, as well as Choirs, pipe organ, harpsichord, and clavichord. During the Baroque period, several major music forms were defined that lasted into later periods when they were expanded and evolved further, including the Fugue, the Invention, the Sonata, and the Concerto.

The music of the Classical period is characterized by homophonic texture, often featuring prominent melody with accompaniment. These new melodies tended to be almost voice-like and singable. The now popular instrumental music was dominated by further evolution of musical forms initially defined in the Baroque period: the sonata, and the concerto, with the addition of the new form, the symphony. Joseph Haydn and Wolfgang Amadeus Mozart, well known even today, are among the central figures of the Classical period.

Ludwig van Beethoven and Franz Schubert were transitional composers, leading into the Romantic period, with their expansion of existing genres, forms, and functions of music. In the Romantic period, the emotional and expressive qualities of music came to take precedence over the orientation towards technique and tradition. The late 19th century saw a dramatic expansion in the size of the orchestra, and in the role of concerts as part of urban society. Later Romantic composers created complex and often much longer musical works, merging and expanding traditional forms that had previously been used separately. For example, counterpoint, combined with harmonic structures to create more extended chords with increased use of dissonance and to create dramatic tension and resolution.

In the 20th century there was a vast increase in music listening as the radio gained popularity worldwide and new media and technologies were developed to record, capture, reproduce and distribute music. The focus of art music was characterized by exploration. Claude Debussy has become well-known and respected for his orientation towards colors and depictions in his compositional style. Igor Stravinsky, Arnold Schoenberg, and John Cage were all influential composers in 20th century art music. Jazz evolved and became a significant genre of music over the course of the 20th century, and during the second half of that century, rock music and rap music did the same.



Performance is the physical expression of music. Often, a musical work is performed once its structure and instrumentation are satisfactory to its creators; however, as it gets performed, it can evolve and change.

A performance can either be rehearsed or improvised. Improvisation is a musical idea created on the spot (such as a guitar solo or a drum solo), with no prior premeditation, while rehearsal is vigorous repetition of an idea until it has achieved cohesion. Musicians will generally add improvisation to a well-rehearsed idea to create a unique performance.Many cultures include strong traditions of solo and performance, such as in Indian classical music, and in the Western Art music tradition. Other cultures, such as in Bali, include strong traditions of group performance. All cultures include a mixture of both, and performance may range from improvised solo playing for one's enjoyment to highly planned and organised performance rituals such as the modern classical concert, religious processions, music festivals or music competitions. 

Chamber music, which is music for a small ensemble with only a few of each type of instrument, is often seen as more intimate than symphonic works. A performer may be referred to as a musician.

Many types of music, such as traditional blues and folk music were originally preserved in the memory of performers, and the songs were handed down orally, or aurally (by ear). When the composer of music is no longer known, this music is often classified as "traditional". Different musical traditions have different attitudes towards how and where to make changes to the original source material, from quite strict, to those which demand improvisation or modification to the music. A culture's history may also be passed by ear through song.

The detail included explicitly in the music notation varies between genres and historical periods. In general, art music notation from the 17th through the 19th century required performers to have a great deal of contextual knowledge about performing styles. 

For example, in the 17th and 18th century, music notated for solo performers typically indicated a simple, unornamented melody. However, it was expected that performers would know how to add stylistically-appropriate ornaments such as trills and turns. In the 19th century, art music for solo performers may give a general instruction such as to perform the music expressively, without describing in detail how the performer should do this. It was expected that the performer would know how to use tempo changes, accentuation, and pauses (among other devices) to obtain this "expressive" performance style. In the 20th century, art music notation often became more explicit and used a range of markings and annotations to indicate to performers how they should play or sing the piece. 

In popular music and jazz, music notation almost always indicates only the basic framework of the melody, harmony, or performance approach; musicians and singers are expected to know the performance conventions and styles associated with specific genres and pieces. For example, the "lead sheet" for a jazz tune may only indicate the melody and the chord changes. The performers in the jazz ensemble are expected to know how to "flesh out" this basic structure by adding ornaments, improvised music, and chordal accompaniment.

Music is composed and performed for many purposes, ranging from aesthetic pleasure, religious or ceremonial purposes, or as an entertainment product for the marketplace. Amateur musicians compose and perform music for their own pleasure, and they do not derive their income from music. Professional musicians are employed by a range of institutions and organisations, including armed forces, churches and synagogues, symphony orchestras, broadcasting or film production companies, and music schools. Professional musicians sometimes work as freelancers, seeking contracts and engagements in a variety of settings.

There are often many links between amateur and professional musicians. Beginning amateur musicians take lessons with professional musicians. In community settings, advanced amateur musicians perform with professional musicians in a variety of ensembles and orchestras. In some cases, amateur musicians attain a professional level of competence, and they are able to perform in professional performance settings.

A distinction is often made between music performed for the benefit of a live audience and music that is performed for the purpose of being recorded and distributed through the music retail system or the broadcasting system. However, there are also many cases where a live performance in front of an audience is recorded and distributed (or broadcast).

"Composition" is often classed as the creation and recording of music via a medium by which others can interpret it (i.e. paper or sound). Many cultures use at least part of the concept of preconceiving musical material, or composition, as held in western classical music. Even when music is notated precisely, there are still many decisions that a performer has to make. The process of a performer deciding how to perform music that has been previously composed and notated is termed interpretation. 

Different performers' interpretations of the same music can vary widely. Composers and song writers who present their own music are interpreting, just as much as those who perform the music of others or folk music. The standard body of choices and techniques present at a given time and a given place is referred to as performance practice, where as interpretation is generally used to mean either individual choices of a performer, or an aspect of music which is not clear, and therefore has a "standard" interpretation.

In some musical genres, such as jazz and blues, even more freedom is given to the performer to engage in improvisation on a basic melodic, harmonic, or rhythmic framework. The greatest latitude is given to the performer in a style of performing called free improvisation, which is material that is spontaneously "thought of" (imagined) while being performed, not preconceived. According to the analysis of Georgiana Costescu, improvised music usually follows stylistic or genre conventions and even "fully composed" includes some freely chosen material. Composition does not always mean the use of notation, or the known sole authorship of one individual. 

Music can also be determined by describing a "process" which may create musical sounds; examples of this range from wind chimes, through computer programs which select sounds. Music which contains elements selected by chance is called Aleatoric music, and is associated with such composers as John Cage, Morton Feldman, and Witold LutosÅ‚awski.

Musical composition is a term that describes the composition of a piece of music. Methods of composition vary widely from one composer to another, however in analysing music all forms â€?spontaneous, trained, or untrained â€?are built from elements comprising a musical piece. Music can be composed for repeated performance or it can be improvised: composed on the spot. The music can be performed entirely from memory, from a written system of musical notation, or some combination of both. Study of composition has traditionally been dominated by examination of methods and practice of Western classical music, but the definition of composition is broad enough to include spontaneously improvised works like those of free jazz performers and African drummers. 

What is important in understanding the composition of a piece is singling out its elements. An understanding of music's formal elements can be helpful in deciphering exactly how a piece is constructed. A universal element of music is how sounds occur in time, which is referred to as the rhythm of a piece of music. 

When a piece appears to have a changing time-feel, it is considered to be in rubato time, an Italian expression that indicates that the tempo of the piece changes to suit the expressive intent of the performer. Even random placement of random sounds, which occurs in musical montage, occurs within some kind of time, and thus employs time as a musical element.

Notation is the written expression of music notes and rhythms on paper using symbols. When music is written down, the pitches and rhythm of the music is notated, along with instructions on how to perform the music. The study of how to read notation involves music theory, harmony, the study of performance practice, and in some cases an understanding of historical performance methods. Written notation varies with style and period of music. In Western Art music, the most common types of written notation are scores, which include all the music parts of an ensemble piece, and parts, which are the music notation for the individual performers or singers. In popular music, jazz, and blues, the standard musical notation is the lead sheet, which notates the melody, chords, lyrics (if it is a vocal piece), and structure of the music. Scores and parts are also used in popular music and jazz, particularly in large ensembles such as jazz "big bands." 

In popular music, guitarists and electric bass players often read music notated in tablature, which indicates the location of the notes to be played on the instrument using a diagram of the guitar or bass fingerboard. Tabulature was also used in the Baroque era to notate music for the lute, a stringed, fretted instrument. 

Notated music is produced as sheet music. To perform music from notation requires an understanding of both the musical style and the performance practice that is associated with a piece of music or genre.

Improvisation is the creation of spontaneous music. Improvisation is often considered an act of instantaneous composition by composers, where compositional techniques are employed with or without preparation.

Music theory encompasses the nature and mechanics of music. It often involves identifying patterns that govern composers' techniques. In a more detailed sense, music theory (in the western system) also distills and analyzes the elements of music â€?rhythm, harmony (harmonic function), melody, structure, and texture. People who study these properties are known as music theorists.





The field of music cognition involves the study of many aspects of music including how it is processed by listeners. Rather than accepting the standard practices of analyzing, composing, and performing music as a given, much research in music cognition seeks instead to uncover the mental processes that underlie these practices. Also, research in the field seeks to uncover commonalities between the musical traditions of disparate cultures and possible cognitive "constraints" that limit these musical systems. Questions regarding musical innateness, and emotional responses to music are also major areas of research in the field.

Deaf people can experience music by feeling the vibrations in their body, a process which can be enhanced if the individual holds a resonant, hollow object. A well-known deaf musician is the composer Ludwig van Beethoven, who composed many famous works even after he had completely lost his hearing. Recent examples of deaf musicians include Evelyn Glennie, a highly acclaimed percussionist who has been deaf since age twelve, and Chris Buck, a virtuoso violinist who has lost his hearing. This is relevant because it indicates that music is a deeper cognitive process than unexamined phrases such as, "pleasing to the ear" would suggest. Much research in music cognition seeks to uncover these complex mental processes involved in listening to music, which may seem intuitively simple, yet are vastly intricate and complex.

Music is experienced by individuals in a range of social settings ranging from being alone to attending a large concert. Musical performances take different forms in different cultures and socioeconomic milieus. In Europe and North America, there is often a divide between what types of music are viewed as a "high culture" and "low culture." "High culture" types of music typically include Western art music such as Baroque, Classical, Romantic, and modern-era symphonies, concertos, and solo works, and are typically heard in formal concerts in concert halls and churches, with the audience sitting quietly in seats. 

Other types of music such as jazz, blues, soul, and country are often performed in bars, nightclubs, and theatres, where the audience may be able to drink, dance, and express themselves by cheering. Until the later 20th century, the division between "high" and "low" musical forms was widely accepted as a valid distinction that separated out better quality, more advanced "art music" from the popular styles of music heard in bars and dance halls. 

However, in the 1980s and 1990s, musicologists studying this perceived divide between "high" and "low" musical genres argued that this distinction is not based on the musical value or quality of the different types of music. Rather, they argued that this distinction was based largely on the socioeconomic standing or social class of the performers or audience of the different types of music. For example, whereas the audience for Classical symphony concerts typically have above-average incomes, the audience for a Rap concert in an inner-city area may have below-average incomes. Even though the performers, audience, or venue where non-"art" music is performed may have a lower socioeconomic status, the music that is performed, such as blues, rap, punk, funk, or ska may be very complex and sophisticated.

When composers introduce styles of music which break with convention, there can be a strong resistance from academic music experts and popular culture. Late-period Beethoven string quartets, Stravinsky ballet scores, serialism, bebop-era jazz, hip hop, punk rock, and electronica have all been considered non-music by some critics when they were first introduced. 

Such themes are examined in the sociology of music. The sociological study of music, sometimes called sociomusicology, is often pursued in departments of sociology, media studies, or music, and is closely related to the field of ethnomusicology.

The music that composers make can be heard through several media; the most traditional way is to hear it live, in the presence, or as one of the musicians. Live music can also be broadcast over the radio, television or the internet. Some musical styles focus on producing a sound for a performance, while others focus on producing a recording which mixes together sounds which were never played "live". Recording, even of styles which are essentially live, often uses the ability to edit and splice to produce recordings which are considered better than the actual performance.

As talking pictures emerged in the early 20th century, with their prerecorded musical tracks, an increasing number of moviehouse orchestra musicians found themselves out of work. During the 1920s live musical performances by orchestras, pianists, and theater organists were common at first-run theaters With the coming of the talking motion pictures, those featured performances were largely eliminated. The AFM took out newspaper advertisements protesting the replacement of live musicians with mechanical playing devices. One 1929 ad that appeared in the Pittsburgh Press features an image of a can labeled "Canned Music / Big Noise Brand / Guaranteed to Produce No Intellectual or Emotional Reaction Whatever" 

Since legislation introduced to help protect performers, composers, publishers and producers, including the Audio Home Recording Act of 1992 in the United States, and the 1979 revised Berne Convention for the Protection of Literary and Artistic Works in the United Kingdom, recordings and live performances have also become more accessible through computers, devices and internet in a form that is commonly known as music-on-demand.

In many cultures, there is less distinction between performing and listening to music, since virtually everyone is involved in some sort of musical activity, often communal. In industrialised countries, listening to music through a recorded form, such as sound recording or watching a music video, became more common than experiencing live performance, roughly in the middle of the 20th century. 

Sometimes, live performances incorporate prerecorded sounds. For example, a DJ uses disc records for scratching, and some 20th-century works have a solo for an instrument or voice that is performed along with music that is prerecorded onto a tape. Computers and many keyboards can be programmed to produce and play MIDI music. Audiences can also become performers by participating in Karaoke, an activity of Japanese origin which centres around a device that plays voice-eliminated versions of well-known songs. Most karaoke machines also have video screens that show lyrics to songs being performed; performers can follow the lyrics as they sing over the instrumental tracks.

The advent of the Internet has greatly transformed the experience of music, most notably through the greatly increased ease with which people can access music content and the greatly increased choice of accessible music. According to Anderson, in his book, The Long Tail: Why the future of business is selling less of more, he proposes that while the previous supply and demand economic model was based on scarcity, the new Internet model is based on abundance. In contrast to real life, space on a server costs next to nothing, so a company can afford to make their whole inventory available. Since almost everything can be put online, costumers now basically have infinite choice. This breaks the old model of supply and demand because now there is no reason not to make available products that very few people are interested in. And thus, there is now a trend of consumers' increasing consciousness of choice resulting in a closer association between choice of listening and identity as well as the creation of thousands of niches.

Another effect the Internet has had on music comes from online communities like Youtube and Myspace. Myspace has made social networking with other musicians much easier and greatly facilitates distribution of one's music. Youtube is another forum that has a large community of both amateur and professional musicians participating in posting videos and commenting. Professional musicians are also using Youtube as free promotional publishing. 

Viewed differently, Youtube users are no longer content to just consume content, like downloading and listening to mp3s, but are now actively creating their own content. According to Tapscott and Williams, there has been a shift from a traditional consumer role to a â€œprosumerâ€?role, a consumer who creates value and well as consumes it. Manifestations of this in music are the production of mashes, remixes, and music videos by fans. 

The music industry refers to the business industry connected with the creation and sale of music. It consists of record companies, labels and publishers that distribute recorded music products internationally and that often control the rights to those products. Some music labels are "independent," while others are subsidiaries of larger corporate entities or international media groups.

The incorporation of music training from preschool to post secondary education is common in North America and Europe. Involvement in music is thought to teach basic skills such as concentration, counting, listening, and cooperation while also promoting understanding of language, improving the ability to recall information, and creating an environment more conducive to learning in other areas.  In elementary schools, children often learn to play instruments such as the recorder, sing in small choirs, and learn about the history of Western art music. In secondary schools students may have the opportunity to perform some type of musical ensembles, such as choirs, marching bands, concert bands, jazz bands, or orchestras, and in some school systems, music classes may be available. Some students also take private music lessons with a teacher. Amateur musicians typically take lessons to learn musical rudiments and beginner- to intermediate-level musical techniques.

At the university level, students in most arts and humanities programs can receive credit for taking music courses, which typically take the form of an overview course on the history of music, or a music appreciation course that focuses on listening to music and learning about different musical styles. In addition, most North American and European universities have some type of musical ensembles that non-music students are able to participate in, such as choirs, marching bands, or orchestras. The study of Western art music is increasingly common outside of North America and Europe, such as STSI in Bali, or the Classical music programs that are available in Asian countries such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of non-Western cultures, such as the music of Africa or Bali (e.g. Gamelan music).

Musicology is the study of the subject of music. The earliest definitions defined three sub-disciplines: systematic musicology, historical musicology, and comparative musicology. In contemporary scholarship, one is more likely to encounter a division of the discipline into music theory, music history, and ethnomusicology. Research in musicology has often been enriched by cross-disciplinary work, for example in the field of psychoacoustics. The study of music of non-western cultures, and the cultural study of music, is called ethnomusicology. 

Graduates of undergraduate music programs can go on to further study in music graduate programs. Graduate degrees include the Master of Music, the Master of Arts, the PhD (e.g., in musicology or music theory), and more recently, the Doctor of Musical Arts, or DMA. The Master of Music degree, which takes one to two years to complete, is typically awarded to students studying the performance of an instrument, education, voice or composition. The Master of Arts degree, which takes one to two years to complete and often requires a thesis, is typically awarded to students studying musicology, music history, or music theory. Undergraduate university degrees in music, including the Bachelor of Music, the Bachelor of Music Education, and the Bachelor of Arts (with a major in music) typically take three to five years to complete. These degrees provide students with a grounding in music theory and music history, and many students also study an instrument or learn singing technique as part of their program. 

The PhD, which is required for students who want to work as university professors in musicology, music history, or music theory, takes three to five years of study after the Master's degree, during which time the student will complete advanced courses and undertake research for a dissertation. The Doctor of Musical Arts (DMA) is a relatively new degree that was created to provide a credential for professional performers or composers that want to work as university professors in musical performance or composition. The DMA takes three to five years after a Master's degree, and includes advanced courses, projects, and performances. In Medieval times, the study of music was one of the Quadrivium of the seven Liberal Arts and considered vital to higher learning. Within the quantitative Quadrivium, music, or more accurately harmonics, was the study of rational proportions. 

Zoomusicology is the study of the music of non-human animals, or the musical aspects of sounds produced by non-human animals. As George Herzog (1941) asked, "do animals have music?" FranÃ§ois-Bernard MÃ¢che's Musique, mythe, nature, ou les Dauphins d'Arion (1983), a study of "ornitho-musicology" using a technique of Ruwet's Language, musique, poÃ©sie (1972) paradigmatic segmentation analysis, shows that bird songs are organised according to a repetition-transformation principle. Jean-Jacques Nattiez (1990), argues that "in the last analysis, it is a human being who decides what is and is not musical, even when the sound is not of human origin. If we acknowledge that sound is not organised and conceptualised (that is, made to form music) merely by its producer, but by the mind that perceives it, then music is uniquely human."

Music theory is the study of music, generally in a highly technical manner outside of other disciplines. More broadly it refers to any study of music, usually related in some form with compositional concerns, and may include mathematics, physics, and anthropology. What is most commonly taught in beginning music theory classes are guidelines to write in the style of the common practice period, or tonal music. Theory, even that which studies music of the common practice period, may take many other forms. Musical set theory is the application of mathematical set theory to music, first applied to atonal music. Speculative music theory, contrasted with analytic music theory, is devoted to the analysis and synthesis of music materials, for example tuning systems, generally as preparation for composition.

In the West, much of the history of music that is taught deals with the Western civilization's art music. The history of music in other cultures ("world music" or the field of "ethnomusicology") is also taught in Western universities. This includes the documented classical traditions of Asian countries outside the influence of Western Europe, as well as the folk or indigenous music of various other cultures. 

Popular styles of music varied widely from culture to culture, and from period to period. Different cultures emphasised different instruments, or techniques, or uses for music. Music has been used not only for entertainment, for ceremonies, and for practical and artistic communication, but also for propaganda in totalitarian countries. 

There is a host of music classifications, many of which are caught up in the argument over the definition of music. Among the largest of these is the division between classical music (or "art" music), and popular music (or commercial music - including rock and roll, country music, and pop music). Some genres don't fit neatly into one of these "big two" classifications, (such as folk music, world music, or jazz music). 

As world cultures have come into greater contact, their indigenous musical styles have often merged into new styles. For example, the United States bluegrass style contains elements from Anglo-Irish, Scottish, Irish, German and African instrumental and vocal traditions, which were able to fuse in the United States' multi-ethnic society. Genres of music are determined as much by tradition and presentation as by the actual music. Some works, like Gershwin's Rhapsody in Blue, are claimed by both jazz and classical music. Many current music festivals celebrate a particular musical genre.

Indian music, for example, is one of the oldest and longest living types of music, and is still widely heard and performed in South Asia, as well as internationally (especially since the 1960s). Indian music has mainly 3 forms of Classical music, Hindustani, Carnatic, and Dhrupad styles. It has also a large repertoire of styles, which involve only Percussion music such as the Tala-vadya performances famous in South India.

Robert Burton wrote in the 17th century in his work, The Anatomy of Melancholy, that music and dance were critical in treating mental illness, especially melancholia. He said that "But to leave all declamatory speeches in praise of divine music, Iwill confine myself to my proper subject: besides that excellent power it hath to expel many other diseases, it is a sovereign remedy against despair and melancholy, and will drive away the devil himself." Burton noted that "...Canus, aRhodian fiddler, in Philostratus, when Apollonius was inquisitive toknow what he could do with his pipe, told him, "That he would make amelancholy man merry, and him that was merry much merrier than before, alover more enamoured, a religious man more devout."

In November 2006, Dr. Michael J. Crawford and his colleagues also found that music therapy helped schizophrenic patients. In the Ottoman Empire, mental illnesses were treated with music. 













The Iowa-class was a planned class of six battleships ordered by the United States Navy in 1939 and 1940 to escort the Fast Carrier Task Forces that would operate in the Pacific Theatre of World War II. Four were completed in the early to mid-1940s; two more were laid down, canceled prior to completion, and ultimately scrapped. They comprised the final class of U.S. battleships to be built. 

Built with no regard for cost, the Iowa class was arguably the ultimate in the evolution of the capital ship. The ships topped the Discovery Channel's list of the ten "most fearsome vessels in the history of naval warfare." Yet even as these leviathans entered service, they were being eclipsed by aircraft carriers as the most important naval vessels.

The Iowa-class battleships served in every major U.S. war of the mid and latter half of the 20th century. In World War II, they defended aircraft carriers and shelled Japanese positions before being placed in reserve at the end of the war. Recalled for action during the Korean War, the battleships provided artillery support for UN forces fighting against North Korea. In 1968, New Jersey was recalled for action in the Vietnam War and shelled Communist targets near the Vietnamese Demilitarized Zone. All four were reactivated and armed with missiles during the 1980s as part of the 600-ship Navy. In 1991, Missouri and Wisconsin fired missiles and  guns at Iraqi targets during the Gulf War. All four battleships were decommissioned in the early 1990s as the Cold War drew to a close, and were initially removed from the Naval Vessel Register; however, at the insistence of the United States Congress, two were reinstated to the Naval Vessel Register for maintenance in the mothball fleet in 1995. These last two battleships were removed from the Naval Vessel Register in 2006. 

The Iowa-class battleships were shaped by the Battle of Jutland, by naval treaties signed by various countries during the 1920s and 1930s, and by the need to keep up with aircraft carriers and protect them from aerial attack. 

The sinking of three lightly armored Royal Navy battlecruisers at Jutland in 1916 led the world's builders of capital ships to improve their naval armor.

The Washington Naval Treaty was proposed by U.S. Secretary of State Charles Evans Hughes and forged during a November 1922 conference attended by Great Britain, France, Italy, and Japan. The attending nations agreed to abandon ongoing construction of battleships and battlecruisers, to limit ships to 35,000Â tons, to cap armament at  cannons, and to limit replacement tonnage. The London Naval Treaty further restricted battleship construction and banned new battleships through 1937. These treaties stopped U.S. construction of battleships and battlecruisers.

At the Second London Naval Conference in 1935, the Empire of Japan denounced the naval treaty and withdrew its delegates. The other conferees agreed that if Japan did not sign the treaty by April 1937, other nations would be free to build guns up to , the maximum size under the Washington Naval Treaty. Tonnage limits would also be relaxed.

That same year (1935), an empirical formula for predicting a ship's maximum speed was developed, based on scale-model studies in flumes of various hull forms and propellers. The formula used the length-to-speed ratio originally developed for 12-meter yachts: Speed = âˆ?.408 * waterline length, and was later redefined as Capital Ship Speed = 1.19 * âˆšLength at Waterline. It quickly became apparent that propeller cavitation caused a drop in efficiency at speeds over 30 knots (55.56 km/h). Propeller design therefore took on new importance.

In 1936, on the heels of the Empire of Japan's withdrawal from the Second London Naval Conference, President Franklin Roosevelt issued an executive order creating the Battleship Design Advisory Board (BDAB) and charged the new group with developing new battleship designs, in particular for a 45,000-ton battleship. The board, composed of renowned U.S. naval architects and headed by Captain Allan Chantry, played with various designs for the upcoming U.S. fleet of North Carolina and South Dakota-class battleships to help lessen the gap between the U.S. based battleships and those being commissioned in Germany and Japan at the time.

The United States began building the ''North Carolina'' and ''South Dakota''-class battleships in the late 1930s. Designed mostly within treaty limitations, these new battleships could steam at , fast for a battleship but not fast enough to keep pace with the aircraft carriers being planned.

The Iowa class, like the South Dakota class and North Carolina class, began in response to the need for fast escorts for the s. Plans for fast battleships that displaced 45,000Â tons had been under development since 1935, beginning with a study of the idea of creating an extended South Dakota class that would take full advantage of the escalator clause of the Second London Naval Treaty. When the Second Vinson Act was passed by the United States Congress in 1938, the U.S. Navy moved quickly to develop a 45,000-ton battleship that would pass through the 110 ft (34 m) wide Panama Canal. Drawing on the earlier speed equations and a newly developed empirical theorem that related waterline length to maximum beam, the Navy drafted plans for a battleship class with a maximum beam of 108 ft (32.9 m), which when multiplied by 7.96 produced a waterline length of 860 ft (262 m), permitting a maximum speed of . The Navy also called for the class to have a lengthened forecastle and amid-ship, which would increase speed, and a bulbous bow.

As with the preceding battleship classes, the Iowa-class battleships were "Panamax" ships â€”built within the size limits required to transition the Panama Canal. The main reason for this was logistical: the largest U.S. shipyards were located on the East Coast of the United States, while the United States had territorial interests in both oceans. Requiring the battleships to fit within the Panama Canal shaved days off the transition time from the Atlantic Ocean to the Pacific Ocean by allowing ships to move through the canal instead of sail all the way around South America.

Originally, the ships were to mount the Mark 2  / 50-caliber gun, which had been intended to arm the battleships and battlecruisers canceled in 1922. But due to a miscommunication between the Bureau of Ordnance and the Bureau of Construction and Repair that left the ship's barbettes too small, the Mark 2 guns were replaced in the design by the new, lighter Mark 7 16-inch 50-caliber gun. The Mark 7 was heavier and had a greater range than the 16"/45 Caliber Mark 6 guns used on the preceding ''North Carolina'' and ''South Dakota'' classes. The Mark 7 was originally intended to fire the same  shell as the 16-in/45-caliber gun, but as the design was being completed a new "super-heavy"  shell was developed for both guns. However, the Iowa's armor was only designed to resist 2,240 lb (1,016 kg) shells, as adding armor would have pushed the ship's weight over the 45,000-ton limit.

At the time the Iowa-class had been cleared for construction, the United States Congress had allocated only enough money to construct the first two ships (Iowa and New Jersey). Congress had not expected the Iowa-class to be so costly; with a price tag of $125 million per ship, the Iowas were 60% more expensive than the previously authorized battleship classes. Moreover, some policymakers were not sold on the U.S. need for more battleships, and proposed turning the ships into aircraft carriers by retaining the hull design but switching their decks to carry and handle aircraft. The proposal to build the Iowas as aircraft carriers was countered by Admiral Ernest King, the Chief of Naval Operations, and Congress' position on the funding for the Iowa-class changed after the Fall of France, when Roosevelt demanded that Congress fund a two ocean navy to meet the threats posed in the Atlantic and Pacific Oceans. Concern over the German invasion prompted Congress to respond by allotting enough money to complete the last four Iowa-class battleships (Missouri, Wisconsin, Illinois, and Kentucky).

Under the direction of Secretary of the Navy Charles Edison, the design was finalized and a contract was signed with the shipyards in July 1939 for the construction of BB-61, BB-62, BB-63, and BB-64 (all Iowa-class battleships) along with BB-65 and BB-66, the first two ships of the ''Montana''-class of battleships. By 1942, however, the United States Navy shifted its building focus from battleships to aircraft carriers after the successes of carrier combat in both the Battle of Coral Sea, and to a greater extent, the Battle of Midway.As a result, the construction of the U.S. fleet of s had been given the highest priority for completion in the U.S. shipyards by the U.S. Navy. The Essex-class carriers required escorts that could steam with the carriers at a comparable speed, which prompted the U.S. Navy to reorder BB-65 and BB-66 as Iowa-class battleships, enabling both battleships to steam at a comparable speed with the Essex-class and provide the carriers with the maximum amount of anti-aircraft protection.

When brought into service during the final years of World War II, the Iowa-class battleships were assigned to operate in the Pacific, primarily to provide anti-aircraft screening for U.S. aircraft carriers and perform shore bombardment. Among the Iowas, only USS New Jersey engaged a surface ship during World War II. At the end of the war, Iowa, New Jersey and Wisconsin were decommissioned and placed in the mothball fleet; construction on two uncompleted ships (Illinois and Kentucky) was halted.

The Iowas were recalled in 1950 with the outbreak of the Korean War, then returned to mothballs after hostilities ceased, in 1955. 

In 1968, due in large part to congressional pressure, New Jersey was recommissioned and sent to assist U.S. troops during the Vietnam War. She did one tour on the firing line, then was decommissioned the following year. 

In the 1980s, the battleships were recommissioned. President Ronald Reagan had vowed to rebuild the U.S. military and create a 600-ship Navy. With the  heavy cruisers worn out, the relatively low mileage Iowas were brought back to fill the offshore bombardment role. The ships also provided a counter to the new Soviet Orlan-class large missile cruisers, better known in the West as the ''Kirov''-class battlecruisers. Each Iowa battleship was modernized to carry electronic warfare suites, CIWS self-defense systems, and missiles. They became the centerpieces of their own battleship battle groups (BBBGs). Their missions in the 1980s and early 1990s included the U.S. intervention in the Lebanese Civil War following the 1983 Beirut barracks bombing and the 1991 Gulf War, first as part of Operation ''Desert Shield'' and then as part of Operation ''Desert Storm''. Decommissioned for the last time in the early 1990s, the Iowas were split into two groups: those retained in the United States Navy reserve fleets (better known as the "mothball fleet") and those donated for use as museum ships. 

In 1996, the National Defense Authorization Act led Iowa and Missouri to be struck from the Naval Vessel Register. Missouri was donated to the Missouri Memorial Association of Pearl Harbor, Hawaii, for use as a museum ship. Iowa was set to be donated with Missouri, but was reinstated to the Naval Vessel Register after the Strom Thurmond National Defense Authorization Act of 1999 allowed New Jersey to be donated as a museum ship. The last two Iowa-class battleships were removed from the mothball fleet in 2006, and are currently awaiting transfer for use as museum ships.

The Iowa-class ships were built to steam at the same speed as the U.S. fleet of s. Their main battery and secondary battery guns were designed to take on the ships of the Imperial Japanese Navy, and to shell beachheads in advance of U.S. Army and Marine Corps amphibious assaults. They carried a fearsome array of anti-aircraft guns to defend themselves and their carriers. 

 was ordered 1 July 1939, laid down 27 June 1940, launched 27 August 1942, and commissioned 22 February 1943. Iowa conducted a shakedown cruise in the Chesapeake Bay before sailing to Naval Station Argentia, Newfoundland to counter the German battleship ''Tirpitz''. Transferred to the Pacific Fleet in 1944, Iowa made her combat debut in the campaign for the Marshall Islands. The ship escorted U.S. aircraft carriers conducting air raids in the Marianas campaign, then was present at the Battle of Leyte Gulf. During the Korean War, Iowa bombarded enemy targets at Songjin, Hungnam, and Kojo, North Korea. Iowa returned to the U.S. for operational and training exercises before being decommissioned. Reactivated in the early 1980s, Iowa made several operation cruises in European waters. On 19 April 1989, an explosion of undetermined origin ripped through her No. 2 turret, killing 47 sailors. The turret remained inoperable when Iowa was decommissioned for the last time in 1990. In 1999, Iowa was placed in the mothball fleet as a replacement for sister ship New Jersey. Stricken from the Naval Vessel Register on 17 March 2006, Iowa is currently berthed at Suisun Bay in San Francisco, California, pending a decision on requests to turn the ship into a museum ship.

 was ordered 1 July 1939, laid down 16 September 1940, launched 7 December 1942, and commissioned 23 May 1943. New Jersey completed fitting out and trained her initial crew in the Western Atlantic and Caribbean before transferring to the Pacific Theatre in advance of the planned assault on the Marshall Islands, where she screened the U.S. fleet of aircraft carriers from enemy air raids. At the Battle of Leyte Gulf, the ship protected carriers with her anti-aircraft guns. New Jersey then bombarded Iwo Jima and Okinawa. During the Korean War, the ship pounded targets at Wonsan, Yangyang, and Kansong. Following the ceasefire, New Jersey conducted training and operation cruises until she was decommissioned. Recalled for action in 1968, New Jersey reported for duty near the Vietnam DMZ, and remained there until 1969, whereupon she was again decommissioned. Reactivated under the 600-ship Navy program, New Jersey was sent to Lebanon to protect U.S. interests and U.S. Marines, firing her main guns at Druze and Syrian positions in the Bekaa valley east of Beirut. Decommissioned for the last time 8 February 1991, New Jersey was briefly retained on the Naval Vessel Register before being donated to the Home Port Alliance of Camden, New Jersey, for use as a museum ship.

 was ordered 12 June 1940, laid down 6 January 1941, launched 29 January 1944, and commissioned 11 June 1944. Missouri conducted her trials off New York and shakedown and battle practice in the Chesapeake Bay before transferring to the Pacific Fleet, where she screened U.S. aircraft carriers involved in offensive operations against the Japanese before reporting to Okinawa to shell the island in advance of the planned landings. Following the bombardment of Okinawa Missouri turned her attention to HonshÅ« and HokkaidÅ, shelling the islands and screening U.S. carriers involved in combat operations against the Japanese positions. She garnered international attention in September 1945 when representatives of the Empire of Japan boarded the battleship to sign the documents of unconditional surrender to the Allied powers. After World War II Missouri turned her attention to conducting training and operational cruises before being dispatched to Korea at the outbreak of the Korean War. Missouri served two tours of duty in Korea before being decommissioned in 1956. Reactivated 1984 as part of the 600-ship Navy plan, Missouri was sent on operational cruises until being assigned to Operation ''Earnest Will'' in 1988. In 1991, Missouri participated in the Gulf War by firing Tomahawk Missiles at Iraqi target and shelling known Iraqi positions along the coast. Decommissioned for the last time in 1992, Missouri was donated to the USS Missouri Memorial Association (MMA) of Pearl Harbor, Hawaii, for use as a museum ship in 1999.

 was ordered 12 June 1940, laid down 25 January 1942, launched 7 December 1943, and commissioned 16 April 1944. After trials and initial training in the Chesapeake Bay, Wisconsin transferred to the Pacific Fleet in 1944 and assigned to protect the U.S. fleet of aircraft carriers involved in operations in the Philippines until summoned to Iwo Jima to bombard the island in advance of the Marine landings. After the landings on Iwo Jima she turned her attention to Okinawa, bombarding the island in advance of the allied amphibious assault. In mid-1945 Wisconsin turned her attention to pounding the Japanese home islands, a job she retained until the surrender of Japan. Reactivated in 1950 for the Korean War, Wisconsin served two tours of duty assisting South Korean and UN forces by providing call fire support and shelling targets of opportunity. Decommissioned in 1958, Wisconsin was placed in the reserve fleet at the Philadelphia Naval Yard until reactivated in 1986 as part of the 600-ship Navy plan. In 1991, Wisconsin participated in the Gulf War by firing Tomahawk Missiles at Iraqi targets and shelling Iraqi troop formations along the coast. Decommissioned for the last time 30 September 1991 Wisconsin was placed in the reserve fleet until struck from the Naval Vessel Register 17 March 2006. She is currently berthed in Norfolk, Virginia, pending a formal transfer of the battleship for use as a museum ship.

 was ordered 9 September 1940 and laid down 15 January 1945. Construction was canceled 11 August 1945 when Illinois was 22% complete. She was sold for scrap in September 1958. Illinois' design called for an all-welded hull, lighter and stronger than the riveted/welded hull of the four completed Iowa-class ships. A proposal to redesign the hull with a Montana-class type torpedo protection system was rejected.

 was ordered 9 September 1940 and laid down on 6 December 1944. Construction was suspended 17 February 1947 when Kentucky was 72% complete. She was informally launched 20 January 1950 to clear a dry-dock for repairs to Missouri, which had run aground. In 1956, Kentuckyâ€™s bow was removed and shipped in one piece across Hampton Roads, where it was grafted on the battleship Wisconsin, which had collided with the destroyer . Later, Kentuckyâ€™s engines were salvaged and installed on the fast combat support ships  and . Nothing came of several proposals to complete Kentucky as a guided missile ship. Ultimately, Kentucky was sold to Boston Metals Co. for scrap on 31 October 1958. Like Illinois, Kentucky's hull was of all-welded construction, lighter and stronger than the other Iowas, and a proposal to redesign the hull with a Montana-class torpedo protection system was rejected.

The Iowa-class battleships were among the most heavily armed ships the United States ever put to sea. The main battery of  guns could hit targets nearly 24Â miles (39Â km) away with a variety of artillery shells, from standard armor piercing rounds to tactical nuclear charges called "Katies" (from "kt" for kiloton). The secondary battery of  guns could hit targets nearly  away with solid projectiles or proximity fused shells, and were equally adept in an anti-aircraft role and for damaging smaller ships. When commissioned these battleships carried a fearsome array of 20Â mm and 40Â mm anti-aircraft guns, which were gradually replaced with Tomahawk and Harpoon missiles, Phalanx anti-aircraft/anti-missile gatling gun systems, and electronic warfare suites. By the time the last Iowa-class battleship was decommissioned in 1992 the Iowas had set a new record for battleship weaponry: No other battleship class in history has had so many weapons at its disposal for use against an opponent.



The primary armament of an Iowa-class battleship is nine  / 50-caliber Mark 7 naval guns, which are housed in three 3-gun turrets: two forward and one aft in a configuration known as "2-A-1". The guns are  long (50 times their  bore, or 50 calibers, from breechface to muzzle). About  protrudes from the gun house. Each gun weighs about 239,000Â pounds (108,000Â kg) without the breech, or  with the breech. They fire projectiles weighing from 1,900 to 2,700Â pounds (850 to 1,200Â kg) at a maximum speed of 2,690Â ft/s (820Â m/s) up to 24Â nautical miles (39Â km). At maximum range the projectile spends almost 1Â½Â minutes in flight. The maximum firing rate for each gun on an Iowa-class battleship is two rounds per minute. When firing two broadside per minute a single Iowa-class battleship can put  of ordinace on a target, a figure that can only be matched (and in some cases beaten) by a single B-52 Stratofortress of the United States Air Force, which can carry up to  of bombs, missiles, and mines, or any combination there of.

Each gun rests within an armored turret, but only the top of the turret protrudes above the main deck. The turret extends either four decks (Turrets 1 and 3) or five decks (Turret 2) down. The lower spaces contain rooms for handling the projectiles and storing the powder bags used to fire them. Each turret required a crew of 94 men to operate. The turrets are not actually attached to the ship, but sit on rollers, which means that if the ship were to capsize the turrets would fall out. Each turret costs US $1.4 million, but this number does not take into account the cost of the guns themselves.

The turrets are "three-gun", not "triple", because each barrel can be elevated independently; they can also be fired independently. The ship could fire any combination of its guns, including a broadside of all nine. Contrary to myth, the ships do not move noticeably sideways when a broadside is fired. The guns can be elevated from âˆ?Â° to +45Â°, moving at up to 12Â° per second. The turrets can be rotated about 300Â° at about four degrees per second and can even be fired back beyond the beam, which is sometimes called "over the shoulder." The guns are never fired directly forward because of the shape of the bow and risk that firing the guns forward would damage the ship; in addition to this concern, a satellite up-link antenna was mounted at the bow of each battleship when reactivated in the 1980s.

The secondary battery of the ship consists of  Mark 12 guns in 10 twin mounts, five each to port and starboard, and four Mark 37 Gun Fire Control Systems. These guns were introduced on destroyers in 1934, but by World War II had been installed on nearly every major U.S. warship. The secondary battery was intended to fight off aircraft. Its effectiveness soon declined as Japanese airplanes became faster, then rose again toward the end of the war because of an upgrade to the Mark 37 Fire Control System and the proximity-fuzed 5-inch shells. During the 1980s modernization, four twin mounts were removed to make room for missiles, the two farthest aft and the two at mid-ship on each side. In the Gulf War, the secondary battery was largely relegated to shore bombardment and littoral defense.

Since they were designed to escort the U.S. fleet of fast attack aircraft carriers the Iowa-class battleships were all outfitted with a fearsome array of anti-aircraft guns to protect U.S. aircraft carriers from Japanese fighters and dive bombers.

The Oerlikon 20Â mm anti-aircraft gun was one of the most heavily produced anti-aircraft guns of World War II; The US alone manufactured a total of 124,735 of these guns. When activated in 1941 these guns replaced the 0.50"/90 (12.7Â mm) M2 Browning MG on a one-for-one basis. The Oerlikon 20Â mm AA gun remained the primary anti-aircraft weapon of the United States Navy until the introduction of the 40Â mm Bofors AA gun in 1943.

Arguably the best light anti-aircraft weapon of World War II, the 40Â mm Bofors AA gun was used on almost every major warship in the US and UK fleet during World War II from about 1943 to 1945. Although a descendant of German and Swedish designs, the Bofors mounts used by the United States Navy during World War II had been heavily "Americanized" to bring the guns up to the standards placed on them by the US Navy. This resulted in a guns system set to English standards (now known as the Standard System) with interchangeable ammunition, which simplified the logistics situation for World War II. When coupled with hydraulic couple drives to reduce salt contamination and the Mark 51 director for improved accuracy the Bofors 40Â mm gun became a fearsome adversary, accounting for roughly half of all Japanese aircraft shot down between 1 October 1944 and 1 February 1945.

During their modernization in the 1980s each Iowa-class battleship was equipped with four of the United States Navy's Phalanx CIWS mounts, two which sat just behind the bridge and two which were fixed to a platform installed between the ship's funnels. Iowa, New Jersey, and Missouri were equipped with the Block 0 version of the Phalanx, while Wisconsin received the first operational Block 1 version in 1988. Phalanx CIWS mounts were used by Missouri and Wisconsin during the 1991 Gulf War; Wisconsin alone fired 5,200 20Â mm Phalanx CIWS rounds.

During the modernization in the 1980s, three weapons were added to the Iowa-class battleships. The first was the CIWS anti-aircraft/anti-missile system discussed above. The other two were missiles for use against both land and sea targets. At one point, the NATO Sea Sparrow was to be installed on the reactivated battleships; however, it was determined that the system could not withstand the overpressure effects when firing the main battery.

The BGM-109 Tomahawk Land Attack Missile (TLAM) entered U.S. service in 1983. A long-range, all-weather, subsonic cruise missile, the Tomahawk could hit targets  away, more than 40 times farther than the  guns'  range.

For protection against enemy ships, the Iowa class carried Boeing RGM-84 Harpoon anti-ship missiles in four MkÂ 141 shock-hardened quad-cell canister launchers located alongside the aft stack, two launchers per side. At firing, the Harpoon weighs , including a booster of about . The cruising speed is Mach 0.87 and the range is 64Â nautical miles (119.5 km) in Range and Bearing Launch mode and 85Â nm (157.4 km) in Bearing Only Launch mode.

Aside from its firepower, a battleship's other defining feature is its armor. Battleships are usually armored to withstand an attack from guns the size of its own, but the exact design and placement of the armorâ€”factors inextricably linked with the ship's stability and performanceâ€”is a complex science honed over decades. The Iowas benefitted from advances in steel technology that allowed mills to forge the steel at higher temperatures and heat treatment, which produced a much higher-quality, stronger and more elastic armor. The metal was a nickel-steel compound, classified as a stainless steel, that can bend easily and resists corrosion. Most of the armor was manufactured at Bethlehem Steelâ€™s main mill in Bethlehem, Pennsylvania, and Luken Steelâ€™s Coatsville mill just outside Philadelphia, Pennsylvania. The exception was the turret plating, which was forged at a plant built especially for the Iowas: the Charleston Ordnance Works in Charleston, West Virginia.

The Iowa-class battleships' armor can be divided into the part above the waterline, which is designed to protect the ship against gunfire and aerial bombing, and that below the waterline, intended to protect the vessel from mines, near-miss bombs, and torpedoes. 

Overall, Iowa-class armor is essentially the same as on the earlier ''South Dakota''-class battleships. Both have an internal main belt, a change from the previous two s that was reluctantly adopted because it was difficult to install and repair. An external belt that could ward off   shells would have required a belt incline of 19Â° and a beam too wide for the Panama Canal.

The underwater armor includes side protection and a triple bottom, both multi-layered systems designed to absorb the energy from an underwater explosion equivalent to  of TNTâ€”the Navy's best guess in the 1930s about Japanese weapons. However, unbeknownst to U.S. Naval Intelligence, the Japanese  "Long Lance" torpedo carried a  warhead. 

The Iowa-class torpedo defense is virtually the same as the South Dakota's. Each side of the ship is protected by one tank mounted outside the hull and loaded with fuel oil or other liquid ballast, and an empty inboard tank, all running from the third deck to the bottom of the ship. The liquid tanks are to deform and absorb the shock from the explosion and contain most of the shards from the damaged structure. The inner void is expected to contain any leakage into the interior ship spaces. The armor belt is designed to stop fragments that penetrate the second torpedo bulkhead; however, tests in 1943 showed structural defects in the system.

The Iowa class used several types of aircraft for reconnaissance and for gunnery spotting. The early aircraft were floatplanes launched from catapults on the ship's Fantail. They landed on the water, taxied to the stern of the ship, and were lifted by a crane back to the catapult.

Initially, the Iowas carried the Vought OS2U Kingfisher, a lightly armed two-man aircraft designed in 1937. The ships typically carried three Kingfishers: two on the catapults and a spare on a trailer nearby. 

The Kingfisher's high operating ceiling made it well-suited for its primary mission: to observe the fall of shot from the battleship's guns and radio corrections back to the ship. The floatplanes also performed search and rescue for naval aviators who were shot down or forced to ditch in the ocean.

In June 1942, the U.S. Navy Bureau of Aeronautics requested industry proposals for a new seaplane to replace the Kingfisher and Curtiss SO3C Seamew. The new aircraft was required to be able to use landing gear as well as floats.

Curtiss submitted a design on August 1, and received a contract for two prototypes and five service-test aircraft on August 25. The first flight of a prototype XSC-1 took place 16 February 1944 at the Columbus, Ohio Curtiss plant. The first production aircraft were delivered in October 1944, and by the beginning of 1945 the single-seat Curtiss SC Seahawk floatplane began replacing the Kingfisher.

Around 1949, helicopters replaced floatplanes on the Iowa class. They operated from atop of Turret 2 until the catapults were removed, allowing helicopter operations to shift to the fantail. The aft guns are forbidden to fire when a helicopter is on the aft deck.

Helicopters added a logistics role to gunnery spotting and search-and-rescue; they ferried troops and supplies between ships and to and from land bases. Like the seaplanes before them, the helicopters had no hangar facilities, but the Iowas did have support facilities for five types of helicopters: the UH-1 Iroquois, SH-2 Seasprites, CH-46 Sea Knight, CH-53 Sea Stallion and the LAMPS III SH-60B Seahawk.

In 1975, Tadiran Electronic Industries introduced the Mastiff unmanned aerial vehicle (UAV) to the Israeli Defense Forces (IDF). The drone, designed for use in a reconnaissance role, proved its worth during the 1982 Lebanon War, when IDF personnel used Mastiff drones to aid in the location and destruction of Syrian SAM sites.

The Mastiff first came to the attention of the United States Navy after a botched American airstrike against Syrian air defenses in Lebanon in December 1983. During the operation, U.S. aircraft carriers operating off the coast of Lebanon sent out 28 aircraft to bomb targets in the Bekaa Valley in retaliation for anti-aircraft fire directed at F-14 reconnaissance flights operating from the carriers. The main target of the bombing run was a Syrian radar station, but before the aircraft could reach the site, two American planes were shot down by Syrian guns.

In an analysis of the incident, U.S. Navy Secretary John Lehman determined that the targeted SAM sites had been within the range of the battleship New Jersey and her  guns, but there had been no way for the battleship to accurately target the sites without an aerial observer to direct the ship's rounds to the target. In part because of the Israeli success with the drones, the U.S. Navy made a covert request for a Mastiff drone system. Israel responded by lending a drone to the U.S. in 1984. The success of the Mastiff system in tests ultimately led the Navy to develop its own UAV system, resulting in the creation of the RQ-2 Pioneer UAV.



In 1985, the United States Navy, impressed with Israeli successes with Mastiff unmanned aerial vehicle in the early 1980s, began looking for UAVs to provide imagery for gun spotters aboard the recently reactivated battleships. The result of this effort was the remote-controlled RQ-2 Pioneer, which made its first deployment in December 1986 aboard the Iowa.

Launched from the fantail using a rocket-assist booster that was discarded shortly after takeoff, a Pioneer used an aft-mounted, push-propeller engine to achieve speeds of up to  with a mission endurance of about four hours. The Pioneer carried a video camera in a pod under the belly of the aircraft, which transmitted live video back to the ship so that the operators can observe enemy actions or fall of shot during naval gunnery. Because it was difficult to land the Pioneer without damaging itself or the ship, a large net was strung up for recovery as for a volleyball game, and the aircraft is flown into it. 

Each battleship could carry as many as eight Pioneers, sometimes referred to as remote piloted vehicles (RPVs).

Pioneer garnered international attention for its use during the 1991 Gulf War, when it saw extensive use from the Missouri and Wisconsin. The latter became the first ship to have enemy forces surrender to one of its remotely controlled observation drones.

The Iowa-class battleships are the fastest battleships ever launched, capable of sustained speeds of 33Â knots (61Â km/h) or better. The engineering plant consists of four General Electric double-expansion steam turbine engines, each driving a single shaft that turns one screw. The two outboard screws on the Iowa class have four blades and are just over  in diameter. The two inboard screws have five blades and are about  in diameter. 

Eight Babcock and Wilcox M-Type boilers operate at 600Â psi (4136.85Â kPa) with a maximum superheater outlet temperature of 875Â Â°F (468Â Â°C)

The double-expansion engines consist of a high-pressure (HP) turbine and a low-pressure (LP) turbine. The steam is first passed through the HP turbine which turns at up to 2,100Â rpm. The steam, largely depleted at this point, is then passed through a large conduit to the LP turbine. By the time it reaches the LP turbine, it has no more than 50Â psi (300Â kPa) of pressure left. The LP turbine increases efficiency and power by extracting the last little bit of energy from the steam. 

After leaving the LP turbine, the exhaust steam passes into a condenser and is then returned as feed water to the boilers. Water lost in the process is replaced by three evaporators, which can make a total of 60,000Â US gallons per day (3Â liters per second) of fresh water. After the boilers have had their fill, the remaining fresh water is fed to the ship's potable water systems for drinking, showers, hand washing, cooking, etc. All of the urinals and all but one of the toilets on the Iowa class flush with saltwater in order to conserve fresh water.

The turbines, especially the HP turbine, can turn at 2,000Â rpm; their shafts drive through reduction gearing that turns the propeller shafts at speeds up to 225Â rpm, depending upon the desired speed of the ship.

Electricity drives many systems aboard ship, including rotating the turrets and elevating the guns. Each of the four engine rooms has a pair of Ship's Service Turbine Generators (SSTGs) manufactured by Westinghouse. Each SSTG generates 1.25Â MW for a total of 10Â MW of electricity. The SSTGs are powered by steam from the same boilers that feed the engines. For backup, the ship also has a pair of 250-kW diesel generators.

To allow battle-damaged electrical circuits to be repaired or bypassed, the lower decks of the ship have a Casualty Power System whose large 3-wire cables and wall outlets called "biscuits" can be used to reroute power.

Since the first commercial radar system was installed aboard the battleship , battleships have used radar for aerial reconnaissance, surface surveillance, and as part of the fire control system for the battleship's guns. Since their modernization in the 1980s, the four Iowa class battleships have also used electronic countermeasures systems for defense against enemy missiles and aircraft.

Each of the four Iowa-class battleships are equipped with the AN/SPS-49 Radar Set, an L-band, long-range, two-dimensional, air-search radar system that provides automatic detection and reporting of targets within its surveillance volume. The AN/SPS-49 performs accurate centroiding of target range, azimuth, amplitude, ECM level background, and radial velocity with an associated confidence factor to produce contact data for command and control systems. Additionally, the contact range and bearing information is provided for display on standard plan position indicator consoles.

The AN/SPS-49 uses a line-of-sight, horizon-stabilized antenna to provide acquisition of low-altitude targets in all sea states, and also utilizes an upspot feature to provide coverage for high diving threats in the high diver mode. External control of AN/SPS-49 modes and operation by the command and control system, and processing to identify and flag contacts as special alerts are provided for self-defense support. 

The AN/SPS-49 has several operational features to allow optimum radar performance: an automatic target detection capability with pulse doppler processing and clutter maps, ensuring reliable detection in normal and severe types of clutter; an electronic counter-countermeasures capability for jamming environments; a moving target indicator capability to distinguish moving targets from stationary targets and to improve target detection during the presence of clutter and chaff; the Medium PRF Upgrade (MPU) to increase detection capabilities and reduce false contacts; and a Coherent Sidelobe Cancellation (CSLC) feature. 

The Iowa-class battleships are also equipped with the Radar Set AN/SPS-67, a short-range, two-dimensional, surface-search/navigation radar system that provides highly accurate surface and limited low-flyer detection and tracking capabilities. The AN/SPS-67 is a solid-state replacement for the AN/SPS-10 radar, using a more reliable antenna and incorporating standard electronic module technology for simpler repair and maintenance. The AN/SPS-67 provides excellent performance in rain and sea clutter, and is useful in harbor navigation, since the AN/SPS-67 is capable of detecting buoys and small obstructions without difficulty. 

The AN/SPS-67(V)1 radar is a two-dimensional (azimuth and range) pulsed radar set primarily designed for surface operations with a secondary capability of anti-ship-missile and low flier detection. The radar set operates in the 5450 to 5825Â MHz range, using a coaxial magnetron as the transmitter output tube. The transmitter/receiver is capable of operation in a long (1.0Â msec), medium (0.25Â msec), or short (0.10Â msec) pulse mode to enhance radar performance for specific operational or tactical situations. Pulse repetition frequencies (PRF) of 750,1200, and 2400Â pulses/second are used for the long, medium, and short pulse modes, respectively.

In 1967 Egypt sank the Israeli destroyer ''Eilat'' using a Soviet SS-N-2 STYX missile, prompting the Chief of Naval Operations (CNO) to consider creating a family of inexpensive Electronic Warfare suites to replace and/or complement existing and planned ship surveillance sensors in the early 1970s, a feeling increased when an analysis of the existing AN/WLR-1 and AN/ULQ-6 systems installed on most ships determined that neither system could counter an Anti-Ship Cruise Missile (ASCM) in time to prevent a hit. In addition, hard kill weapons were not effective because there was little early warning of an attack due to the characteristics of ASCMs. The resulting EW suite was the AN/SLQ-32(V), which debuted in 1979 and was capable of early warning of threat weapon system emitters and emitters associated with targeting platforms, threat information to own ship hard-kill weapons, automatic dispensing of chaff decoys, and Electronic Attack (EA) to alter specific and generic ASCM trajectories. This system, specifically the SLQ-32(V)3 variant, was fitted to the Iowa class battleships in 1980s for defense against enemy anti-ship missiles.

To counter the threat posed by enemy submarines the Iowa class were also outfitted with the AN/SLQ-25 Nixie, a towed torpedo decoy used on US and allied warships. It consists of a towed decoy device, and a shipboard signal generator. The decoy emits signals to draw a torpedo away from its intended target. The Nixie attempts to defeat a torpedo's passive sonar by emitting simulated ship noise, such as propeller and engine noise, which is more attractive than the ship to the torpedo's sensors. Active sonar is decoyed by amplifying and returning "pings" from the torpedo, presenting a larger false target to the torpedo.

During their modernization in the 1980s each of the Iowa-class battleships were outfitted with the Mark 36 Super Rapid Bloom Offboard Chaff (SRBOC) system, enabling the Iowas to carry and fire chaff rockets which, when launched from their tubes, release missile decoys or lures. The decoys/lures are intended to act as an anti-missile shield by providing false targets for an enemy missile to attack. During the 1991 Gulf War, chaff was blamed for a friendly fire incident between the ''Oliver Hazard Perry''-class frigate  and the battleship : during an Iraqi missile attack Missouri fired chaff into the air to confuse the incoming missile; however a Phalanx CIWS mount on Jarrett accidentally engaged the chaff fire by Missouri. Rounds from the Phalanx mount on Jarret struck Missouri, causing one minor injury to a crewman on the battleship; fortunately, no serious injuries or damage resulted from the attack.



After World War II, the United States maintained the four Iowa-class battleships in the United States Navy reserve fleets, better known as the "mothball fleet", and on several occasions reactivated these battleships for naval gunfire support. The U.S. Navy has held onto its battleships long after the expense and the arrival of aircraft and precision guided munitions led other nations to scrap their big-gun fleets. The United States Congress is largely responsible for this. The lawmakers argue that the battleships' large-caliber guns have a militarily useful destructive power lacking in the smaller, cheaper, and faster guns mounted by U.S. cruisers and destroyers.

The Navy, which sees the battleships as too costly, is working to persuade Congress to allow it to remove Iowa and Wisconsin from the Naval Vessel Register by developing extended-range guided munitions and a new ship to fulfill Marine Corps requirements for naval surface fire support (NSFS). 

The Navy plan calls for the extension of the range of the  guns on the Flight I  guided missile destroyers ( to ) with Extended Range Guided Munitions (ERGMs) that would enable the ships to fire precision guided projectiles about  inland. This program was initiated in 1996 with a preliminary cost of US $78.6 million; since then, the cost of the program has increased 400%. The results of the program have been similarly disappointing, with the original expected operational capability date pushed from 2001 to 2011. These weapons are not intended or expected to satisfy the full range of the Marine Corps NSFS requirements. The result of the latter effort to design and build a replacement ship for the two battleships is the ''Zumwalt''-class destroyer program, also known either as the DD(X) or DDG-1000 (in reference to Zumwaltâ€™s hull number). The DD(X) is to mount a pair of Advanced Gun System (AGS) turrets capable of firing specially designed Long Range Land Attack Projectiles (LRLAPs) some  inland. The Navy currently expects sufficient numbers of DD(X) destroyers to be ready to help fill the NSFS gap by 2018 at the earliest.



On 17 March 2006 the Secretary of the Navy exercised his authority to strike Iowa and Wisconsin from the Naval Vessel Register, which has cleared the way for both ships to be donated for use as museums. The United States Navy and the United States Marine Corps have both certified that battleships will not be needed in any future war, and have thus turned their attention to development and construction of the next generation  guided missile destroyers. 

This move has drawn fire from a variety of sources familiar with the subject; among them are dissenting members of the United States Marine Corps, who feel that battleships are still a viable solution to naval gunfire support, members of the United States Congress who remain "deeply concerned" over the loss of naval surface gunfire support that the battleships provided, and number of independent groups such as the United States' Naval Fire Support Association (USNFSA) whose ranks frequently include former members of the armed service and fans of the battleships. Although the arguments presented from each group differ, they all agree that the United States Navy has not in good faith considered the potential of reactivated battleships for use in the field, a position that is supported by a 1999 Government Accountability Office report regarding the United States Navy's gunfire support program. 

In response, the Navy has pointed to the cost of reactivating the two Iowa class battleships to their decommissioned capability. The Navy estimates costs in excess of $500 million, but this does not include an additional $110 million needed to replenish gunpowder for the  guns because a recent survey found the powder to be unsafe. In terms of schedule, the Navy's program management office estimates that reactivation would take 20 to 40 months, given the loss of corporate memory and the shipyard industrial base.



Reactivating the battleships would require a wide range of battleship modernization improvements, according to the Navy's program management office. At a minimum, these modernization improvements include command and control, communications, computers, and intelligence equipment; environmental protection (including ozone-depleting substances); a plastic-waste processor; pulper/shredder and wastewater alterations; firefighting/fire safety and women-at-sea alterations; a modernized sensor suite (air and surface search radar); and new combat and self-defense systems. The Navy's program management office also identified other issues that would strongly discourage the Navy from reactivating and modernizing the battleships. For example, personnel needed to operate the battleships would be extensive, and the skills needed may not be available or easily reconstituted. Other issues include the age and unreliability of the battleships' propulsion systems and the fact that the Navy no longer maintains the capability to manufacture their  gun system components and ordnance.

Although the Navy firmly believes in the capabilities of the DD(X) destroyer program, members of the United States Congress remain skeptical about the efficiency of the new destroyers when compared to the battleships. Partially as a consequence the US House of Representatives have asked that the battleships be kept in a state of readiness should they ever be needed again. Congress has asked that the following measures be implemented to ensure that, if need be, Iowa and Wisconsin can be returned to active duty:

These four conditions closely mirror the original three conditions that the Nation Defense Authorization Act of 1996 laid out for the maintenance of Iowa and Wisconsin while they were in the Mothball Fleet. It is unlikely that these conditions will impede the current plan to turn Iowa and Wisconsin into museum ships.

The Iowa class-battleships have been featured prominently in American culture. Iowa was the centerpiece for the book A Glimpse of Hell: The Explosion on the U. S. S. Iowa & Its Cover-Up  which dealt with the events surrounding the 1989 explosion of her #2 turret. In 2001 the book was turned into a movie by the same name staring James Caan and directed by Mikael Salomon. Missouri was featured in the 1977 movie ''MacArthur'', starring Gregory Peck, the 1983 television mini-series The Winds of War, starring Ralph Bellamy and Robert Mitchum, and Cher's music video "If I Could Turn Back Time". The 1992 movie Under Siege, staring Steven Seagal, was also set aboard Missouri, although the movie was actually filmed aboard the battleship . Wisconsin was featured prominently in the news during the 1991 Gulf War, when she became the first ship to receive the surrender of enemy troops on the ground when her Pioneer drone recorded Iraqi soldiers waving white flags after being shelled by Missouri. 





 







Sodium chloride, also known as  common salt, table salt, or halite, is a chemical compound with the formula NaCl. Sodium chloride is the salt most responsible for the salinity of the ocean and of the extracellular fluid of many multicellular organisms. As the major ingredient in edible salt, it is commonly used as a condiment and food preservative. In one gram of sodium chloride, there are approximately 0.3933 grams of sodium, and 0.6067 grams of chlorine.

Salt is currently mass produced by evaporation of seawater or brine from other sources, such as brine wells and salt lakes, and by mining rock salt, called halite. In 2002, world production was estimated at 210 million metric tonnes, the top five producers being the United States (40.3 million tonnes), China (32.9), Germany (17.7), India (14.5), and Canada (12.3).

As well as the familiar uses of salt in cooking, salt is used in many applications, from manufacturing pulp and paper to setting dyes in textiles and fabric, to producing soaps and detergents. In cold countries, large quantities of rock salt are used to help clear highways of ice during winter, although "Road Salt" loses its melting ability at temperatures below -15Â°C to -20Â°C (5Â°F to -4Â°F). Sodium chloride is sometimes used as a cheap and safe desiccant due to its hygroscopic properties, making salting an effective method of food preservation historically. Even though more effective desiccants are available, few are safe for humans to ingest. 

Salt is also the raw material used to produce chlorine which itself is required for the production of many modern materials including PVC and pesticides. Industrially, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. Along with chlorine, this chloralkali process yields hydrogen gas and sodium hydroxide, according to the chemical equation

Sodium metal is produced commercially through the electrolysis of liquid sodium chloride. This is done in a Down's cell in which sodium chloride is mixed with calcium chloride to lower the melting point below 700 Â°C. As calcium is more electropositive than sodium, no calcium will be formed at the cathode. This method is less expensive than the previous method of electrolyzing sodium hydroxide.

Sodium chloride is used in other chemical processes for the large-scale production of compounds containing sodium or chlorine. In the Solvay process, sodium chloride is used for producing sodium carbonate and calcium chloride. In the Mannheim process and in the Hargreaves process, it is used for the production of sodium sulfate and hydrochloric acid.

Salt is commonly used as a flavour enhancer and preservative for food and has been identified as one of the basic tastes. Excess salt consumption is commonly linked to high blood pressure (hypertension) . Consuming salt in excess can also dehydrate the human body.

Many microorganisms cannot live in an overly salty environment: water is drawn out of their cells by osmosis. For this reason salt is used to preserve some foods, such as smoked bacon or fish and can also be used to detach leeches that have attached themselves to feed. It has also been used to disinfect wounds. In medieval times salt would be rubbed into household surfaces as a cleansing agent.

In humans, a high-salt intake was demonstrated to attenuate Nitric Oxide production. Nitric oxide (NO) contributes to vessel homeostasis by inhibiting vascular smooth muscle contraction and growth, platelet aggregation, and leukocyte adhesion to the endothelium 

Sodium chloride forms crystals with cubic symmetry. In these, the larger chloride ions, shown to the right as green spheres, are arranged in a cubic close-packing, while the smaller sodium ions, shown to the right as blue spheres, fill the octahedral gaps between them.

Each ion is surrounded by six ions of the other kind. This same basic structure is found in many other minerals, and is known as the halite structure. This arrangement is known as cubic close packed (ccp). It can be represented as two interpenetrating face-centered cubic (fcc) lattices, or one fcc lattice with a two atom basis. It is most commonly known as the rocksalt crystal structure.

It is held together with an ionic bond and electrostatic forces.

While salt was once a scarce commodity in history, industrialized production has now made salt plentiful. About 51% of world output is now used by cold countries to de-ice roads in winter, both in grit bins and spread by winter service vehicles. This works because salt and water form an eutectic mixture. Adding salt to water will lower the freezing temperature of the water, depending on the concentration. The salinity of water is measured as grams salt per kilogram (1000g) water, and the freezing temperatures are as follows. 				

Much of the road salt used in Europe comes from mines in Carrickfergus.

Table salt sold for consumption today is not pure sodium chloride. In 1911 magnesium carbonate was first added to salt to make it flow more freely. In 1924 trace amounts of iodine in form of sodium iodide, potassium iodide or potassium iodate were first added, to reduce the incidence of simple goiter.

Salt for de-icing in the UK typically contains sodium hexacyanoferrate (II) at less than 100ppm as an anti-caking agent. In recent years this additive has also been used in table salt.

Chemicals used in de-icing salts are mostly found to be sodium chloride (NaCl) or calcium chloride (CaCl2). Both are similar and are effective in de-icing roads. When these chemicals are produced, they are mined/made, crushed to fine granules, then treated with an anti-caking agent. Addingsalt lowers the freezing point of the water, which allows the liquid to bestable at lower temperatures and allows the ice to melt. Alternative de-icing chemicals have also been used. Chemicals such as calcium magnesium acetate and potassium formate are being produced. These chemicals have few of the negative chemical effects on the environment commonly associated with NaCl and CaCl2.













The Republic of the Congo (; Kongo: Repubilika ya Kongo; Lingala: Republiki ya KongÃ³), also known as Congo-Brazzaville or the Congo, is a country in Central Africa. It is bordered by Gabon, Cameroon, the Central African Republic, the Democratic Republic of the Congo, the Angolan exclave province of Cabinda, and the Gulf of Guinea.

The republic is a former French colony. Upon independence in 1960, the former French region of Middle Congo became the Republic of the Congo. After a quarter century of Marxism, Congo completed its transition into a multi-party democracy in 1992. However, a brief civil war in 1997 ended in the restoration of former Marxist President Denis Sassou Nguesso to power.

The earliest inhabitants of the area were Pygmy peoples. They were largely replaced and absorbed by Bantu tribes during Bantu expansions. The Bakongo are Bantu groups that also occupied parts of present-day Angola, Gabon, and Democratic Republic of the Congo, forming the basis for ethnic affinities and rivalries among those states. Several Bantu kingdomsâ€”notably those of the Kongo, the Loango, and the Tekeâ€”built trade links leading into the Congo River basin. The first European contacts came in the late 15th century, and commercial relationships were quickly established with the kingdomsâ€”trading for slaves captured in the interior. The coastal area was a major source for the transatlantic slave trade, and when that commerce ended in the early 19th century, the power of the Bantu kingdoms eroded. 

Following independence as the Congo Republic on August 15 1960, Fulbert Youlou ruled as the country's first president until labor elements and rival political parties instigated a three-day uprising that ousted him. The Congolese military took charge of the country briefly and installed a civilian provisional government headed by Alphonse Massamba-DÃ©bat.

Under the 1963 constitution, Massamba-DÃ©bat was elected President for a five-year term but it was ended abruptly with an August 1968 coup d'Ã©tat. Capt. Marien Ngouabi, who had participated in the coup, assumed the presidency on December 31, 1968. One year later, President Ngouabi proclaimed Congo to be Africa's first "people's republic" and announced the decision of the National Revolutionary Movement to change its name to the Congolese Labour Party (PCT). On March 16, 1977, President Ngouabi was assassinated. An 11-member Military Committee of the Party (CMP) was named to head an interim government with Col. (later Gen.) Joachim Yhombi-Opango to serve as President of the Republic.

After decades of turbulent politics bolstered by Marxist-Leninist rhetoric, and with the collapse of the Soviet Union, Congo completed a transition to multi-party democracy with elections in August 1992. Denis Sassou Nguesso conceded defeat and Congo's new president, Prof. Pascal Lissouba, was inaugurated on August 31, 1992.

However, Congo's democratic progress was derailed in 1997. As presidential elections scheduled for July 1997 approached, tensions between the Lissouba and Sassou camps mounted. On June 5, President Lissouba's government forces surrounded Sassou's compound in Brazzaville and Sassou ordered members of his private militia (known as "Cobras") to resist. Thus began a four-month conflict that destroyed or damaged much of Brazzaville and caused tens of thousands of civilian deaths. In early October, Angolan troops invaded Congo on the side of Sassou and, in mid-October, the Lissouba government fell. Soon thereafter, Sassou declared himself President. The Congo Civil War continued for another year and a half until a peace deal was struck between the various factions in December 1999. 

Sham elections in 2002 saw Sassou win with almost 10% of the vote cast. His two main rivals Lissouba and Bernard Kolelas were prevented from competing and the only remaining credible rival, Andre Milongo, advised his supporters to boycott the elections and then withdrew from the race. A new constitution, agreed upon by referendum in January 2002, granted the president new powers and also extended his term to seven years as well as introducing a new bicameral assembly. International observers took issue with the organization of the presidential election as well as the constitutional referendum, both of which were reminiscent in their organization of Congo's era of the single-party state.

The most important of the many political parties are the Democratic and Patriotic Forces or FDP [Denis Sassou Nguesso, president], an alliance consisting of:

Other significant parties include:

The Republic of the Congo is divided into 10 ''rÃ©gions'' (regions) and one commune, the capital Brazzaville. These are:The regions are subdivided into forty-six districts.

Congo is located in the central-western part of sub-Saharan Africa, along the Equator. To the south and east of it is the Democratic Republic of Congo. It is also bounded by Gabon to the west, Cameroon and the Central African Republic to the north, and Cabinda (Angola) to the southwest. It has a short Atlantic coast. 

The capital, Brazzaville, is located on the Congo River, in the south of the country, immediately across from Kinshasa, the capital of the Democratic Republic of the Congo.

The southwest of the country is a coastal plain for which the primary drainage is the Kouilou-Niari River; the interior of the country consists of a central plateau between two basins to the south and north.

The economy is a mixture of village agriculture and handicrafts, an industrial sector based largely on petroleum, support services, and a government characterized by budget problems and overstaffing. Petroleum extraction has supplanted forestry as the mainstay of the economy, providing a major share of government revenues and exports. In the early 1980s, rapidly rising oil revenues enabled the government to finance large-scale development projects with GDP growth averaging 5% annually, one of the highest rates in Africa. The government has mortgaged a substantial portion of its petroleum earnings, contributing to a shortage of revenues. The January 12, 1994 devaluation of Franc Zone currencies by 50% resulted in inflation of 61% in 1994, but inflation has subsided since. Economic reform efforts continued with the support of international organizations, notably the World Bank and the IMF. The reform program came to a halt in June 1997 when civil war erupted. When Sassou Nguesso returned to power at the war ended in October 1997, he publicly expressed interest in moving forward on economic reforms and privatization and in renewing cooperation with international financial institutions. However, economic progress was badly hurt by slumping oil prices and the resumption of armed conflict in December 1998, which worsened the republic's budget deficit. The current administration presides over an uneasy internal peace and faces difficult economic problems of stimulating recovery and reducing poverty, despite record-high oil prices since 2003. Natural gas and diamonds are also recent major Congolese exports, although Congo was excluded from the Kimberley Process in 2004 amid allegations that most of its diamond exports were in fact being smuggled out of the neighboring Democratic Republic of Congo.

The Republic of the Congo's sparse population is concentrated in the southwestern portion of the country, leaving the vast areas of tropical jungle in the north virtually uninhabited. Thus, Congo is one of the most urbanized countries in Africa, with 85% of its total population living in a few urban areas, namely in Brazzaville, Pointe-Noire, or one of the small cities or villages lining the  railway which connects the two cities. In rural areas, industrial and commercial activity has declined rapidly in recent years, leaving rural economies dependent on the government for support and subsistence. Before the 1997 war, about 15,000 Europeans and other non-Africans lived in Congo, most of whom were French. Presently, only about 9,500 remain.

Since the country is located on the Equator, the climate is consistent year-round, with the average day temperature being a humid  () and nights generally between  ().





















Soup is a food that is made by combining ingredients such as meat, vegetables or legumes in stock or hot water, until the flavor is extracted, forming a broth.

Traditionally, soups are classified into two broad groups: clear soups and thick soups. The established French classifications of clear soups are bouillon and consommÃ©. Thick soups are classified depending upon the type of thickening agent used: purÃ©es are vegetable soups thickened with starch; bisques are made from purÃ©ed shellfish thickened with cream; cream soups are thickened with bÃ©chamel sauce; and veloutÃ©s are thickened with eggs, butter and cream. Other ingredients commonly used to thicken soups and broths include rice, flour, and .

One of the first types of soups can be dated to about 6000 B.C. Boiling was not a common cooking technique until the invention of waterproof containers (which probably came in the form of pouches made of clay or animal skin) about 9,000 years ago.

The word soup originates from "sop", a dish originally consisting of a soup or thick stew which was soaked up with pieces of bread. The modern meaning of sop has been limited to just the bread intended to be dipped.

The word restaurant was first used in France in the 16th century, to describe a highly concentrated, inexpensive soup, sold by street vendors called restaurer, that was advertised as an antidote to physical exhaustion. In 1765, a Parisian entrepreneur opened a shop specializing in restaurers. This prompted the use of the modern word restaurant to describe the shops. 

In America, the first  cookbook was published by William Parks in Williamsburg, Virginia, in 1742, based on Eliza Smith's The Compleat Housewife; or Accomplished Gentlewoman's Companion and it included several recipes for soups and bisques. A 1772 cookbook, The Frugal Housewife, contained an entire chapter on the topic. English cooking dominated early colonial cooking; but as new immigrants arrived from other countries, other national soups gained popularity. In particular, German immigrants living in Pennsylvania were famous for their potato soups. In 1794, Jean Baptiste Gilbert Payplat dis Julien, a refugee from the French Revolution, opened an eating establishment in Boston called Restorator, and became known as "The Prince of Soups." The first American cooking pamphlet dedicated to soup recipes was written in 1882 by Emma Ewing: Soups and Soup Making. 

Portable soup was devised in the 18th century by boiling seasoned meat until a thick,   was left that could be dried and stored for months at a time. The Japanese miso is an example of a concentrated soup paste.

Commercial soup became popular with the invention of canning in the 19th century, and today a great variety of canned and dried soups are on the market. Dr. John T. Dorrance, a chemist with the Campbell Soup Company invented condensed soup in 1897. Today, Campbell's Tomato, Cream of Mushroom and Chicken Noodle soups are three of the most popular soups in America. Americans consume approximately 2.5 billion bowls of these three soups alone each year. Canned Italian-style soups, such as minestrone are also popular. Oriental-style soup mixes containing ramen noodles are marketed as an inexpensive instant lunch, requiring only hot water for preparation. Vegetable, chicken base, potato, pasta and cheese soups are also available in dry mix form, ready to be served by adding hot water.

Nutritional developments



Fruit soups are served hot or cold depending on the recipe. Many recipes are for cold soups served when fruit is in season during hot weather. Some like Norwegian 'fruktsuppe' may be served hot and rely on dried fruit such as raisins and prunes and so could be made in any season. Fruit soups may include milk, sweet or savoury dumplings, spices, or alcoholic beverages like brandy or champagne.

Cold fruit soups are most common in Scandinavian, Baltic and Eastern European cuisines while hot fruit soups with meat appear in Middle Eastern, Central Asian and Chinese cuisines. Fruit soups are uncommon or absent in the cuisines of the Americas, Africa and Western Europe. They are also not seen in Japan, Southeast Asia or Oceania.

A feature of East Asian soups not normally found in Western cuisine is the use of tofu in soups. Many traditional East Asian soups are typically broths, clear soups, or starch thickened soups. Many soups are eaten and drunk as much for their flavour as well as for their health benefits.





In the English language, the word "soup" has developed several phrasal uses.













Napoleon I (born Napoleone di Buonaparte, later NapolÃ©on Bonaparte) (15 August 1769 â€?5 May 1821) was a French military and political leader who had significant impact on modern European history. He was a general during the French Revolution, the ruler of France as Premier Consul of the French Republic, Empereur des FranÃ§ais, King of Italy, Mediator of the Swiss Confederation and Protector of the Confederation of the Rhine. 

Born in Corsica and trained in mainland France as an artillery officer, he first rose to prominence as a general of the French Revolution, leading several successful campaigns against the First Coalition and the Second Coalition arrayed against France. In late 1799, Napoleon staged a coup d'Ã©tat and installed himself as First Consul; five years later he became the Emperor of the French. In the first decade of the nineteenth century, he turned the armies of France against almost every major European power, dominating continental Europe through a lengthy streak of military victoriesâ€”epitomized through battles such as Austerlitz and Friedlandâ€”and through the formation of extensive alliance systems. He appointed close friends and several members of his family as monarchs and important government figures of French-dominated states. 

The disastrous French invasion of Russia in 1812 marked a turning point in Napoleon's fortunes. The campaign wrecked the Grande ArmÃ©e, which never regained its previous strength. In October 1813, the Sixth Coalition defeated his forces at Leipzig and then invaded France. The coalition forced Napoleon to abdicate in April 1814, exiling him to the island of Elba. Less than a year later, he returned to France and regained control of the government in the Hundred Days (les Cent Jours) prior to his final defeat at Waterloo on 18 June, 1815. Napoleon spent the remaining six years of his life under British supervision on the island of St. Helena.Napoleon developed relatively few military innovations, although his placement of artillery into batteries and the elevation of the army corps as the standard all-arms unit have become accepted doctrines in virtually all large modern armies. He drew his best tactics from a variety of sources and scored several major victories with a modernized and reformed French army. His campaigns are studied at military academies all over the world and he is widely regarded as one of history's greatest commanders. Aside from his military achievements, Napoleon is also remembered for the establishment of the Napoleonic Code (Code NapolÃ©on), which laid the bureaucratic foundations for the modern French state.



At birth Napoleon was named Napoleone di Buonaparte (in Corsican, Nabolione or Nabulione) after his deceased elder brother Napoleone, who died in 1765. Napoleon grew up in the town of Ajaccio on Corsica, France, on 15 August 1769, one year after the island was transferred to France by the Republic of Genoa. However, neither Napoleone nor his family used the nobiliary particle di. He later adopted the more French-sounding NapolÃ©on Bonaparte. Napoleon was ethnically Corsican of ancient Italian heritage. He wrote to Pasquale di Paoli (leader of a Corsican revolt against the French) in 1789: "I was born when my country was dying. Thirty thousand Frenchmen disgorged upon our shores, drowning the throne of liberty in a sea of blood; such was the hateful spectacle that offended my infant eyes." Napoleon's heritage earned him popularity among Italians during his Italian campaigns. 

The family, formerly known as Buonaparte, were minor Italian nobility coming from Tuscan stock of Lombard origin set in Lunigiana. The family moved to Florence and later broke into two branches; the original one, Buonaparte-Sarzana, were compelled to leave Florence, coming to Corsica in the 16th century when the island was a possession of the Republic of Genoa.

His father Carlo Buonaparte was born 1746 and in the Republic of Genoa; an attorney, he was named Corsica's representative to the court of Louis XVI in 1778, where he remained for a number of years. The dominant influence of Napoleon's childhood was his mother, Maria Letizia Ramolino. Her firm discipline helped restrain the rambunctious Napoleon, nicknamed Rabullione (the "meddler" or "disrupter").

Napoleon had an elder brother, Joseph, and younger siblings Lucien, Elisa, Louis, Pauline, Caroline and JÃ©rÃ´me. 

Napoleon's noble, moderately affluent background and family connections afforded him greater opportunities to study than were available to a typical Corsican of the time. On 15 May 1779, at age nine, Napoleon was admitted to a French military school at Brienne-le-ChÃ¢teau, a small town near Troyes. He had to learn French before entering the school, but he spoke with a marked Italian accent throughout his life and never learned to spell properly.. During his schooling years Napoleon was often teased by other students for his Corsican accent. However he ignored this criticism and buried himself in study.  Upon graduation from Brienne in 1784, Bonaparte was admitted to the elite Ã‰cole Royale Militaire in Paris, where he completed the two-year course of study in only one year. An examiner judged him as "very applied [to the study of] abstract sciences, little curious as to the others; [having] a thorough knowledge of mathematics and geography ..." Although he had initially sought a naval assignment, he studied artillery at the Ã‰cole Militaire.

Upon graduation in September 1785, he was commissioned a second lieutenant in La FÃ¨re artillery regiment and took up his new duties in January 1786 at the age of 16.Napoleon served on garrison duty in Valence and Auxonne until after the outbreak of the Revolution in 1789 (although he took nearly two years of leave in Corsica and Paris during this period). He spent most of the next several years on Corsica, where a complex three-way struggle was playing out between royalists, revolutionaries, and Corsican nationalists. Bonaparte supported the Jacobin faction and gained the rank of lieutenant-colonel of a regiment of volunteers. After coming into conflict with the increasingly conservative nationalist leader, Pasquale Paoli, Bonaparte and his family fled to the French mainland in June 1793.

Through the help of fellow Corsican Saliceti, Napoleon was appointed artillery commander in the French forces besieging Toulon, which had risen in revolt against the republican government and was occupied by British troops. He placed guns at Point l'Eguillete, threatening the British ships in the harbour, forcing them to evacuate. An assault, during which Bonaparte was wounded in the thigh, led to the recapture of the city and his promotion to brigadier-general. His actions brought him to the attention of the Committee of Public Safety, and he became a close associate of Augustin Robespierre, younger brother of the Revolutionary leader Maximilien Robespierre. Following the fall of the elder Robespierre he was briefly imprisoned in the Chateau d'Antibes on 6 August 1794, but was released within two weeks.

In 1795, Bonaparte was serving in Paris when royalists and counter-revolutionaries organized an armed protest against the National Convention on 3 October. Bonaparte was given command of the improvised forces defending the Convention in the Tuileries Palace. He seized artillery pieces with the aid of a young cavalry officer, Joachim Murat, who later became his brother-in-law. He used the artillery the following day to repel the attackers. He later boasted that he had cleared the streets with "a whiff of grapeshot," as a result of which many had died and those who had survived had fled. This triumph earned him sudden fame, wealth, and the patronage of the new Directory, particularly that of its leader, Barras. Within weeks he was romantically attached to Barras's former mistress, Josephine de Beauharnais, whom he married on 9 March, 1796. (He had been engaged for two years (1794-96) to DÃ©sirÃ©e Clary, later Queen of Sweden and Norway, but the engagement was broken off by the future emperor, in the face of her parents' opposition and their concern over his lack of fortune.)



Days after his marriage, Bonaparte took command of the French "Army of Italy" on 27 March 1796, leading it on a successful . At the Lodi, he gained the nickname of "the Little Corporal," literally le petit caporal. This term reflected his camaraderie with his soldiers, many of whom he knew by name, and emphasized how rarely general officers fought wars alongside their own men. He drove the Austrians out of Lombardy and defeated the army of the Papal States. Because Pope Pius VI had protested the execution of Louis XVI, France retaliated by annexing two small papal territories. Bonaparte ignored the Directory's order to march on Rome and dethrone the Pope. It was not until the next year that General Berthier captured Rome and took Pius VI prisoner on 20 February. The Pope died of illness while in captivity. In early 1797, Bonaparte  and forced that power to sue for peace. The resulting Treaty of Campo Formio gave France control of most of northern Italy, along with the Low Countries and Rhineland, but a secret clause promised Venice to Austria. Bonaparte then marched on Venice and forced its surrender, ending more than 1,000 years of independence. Later in 1797, Bonaparte organized many of the French-dominated territories in Italy into the Cisalpine Republic.

His remarkable series of military triumphs were a result of his ability to apply his encyclopedic knowledge of conventional military thought to real-world situations, as demonstrated by his creative use of artillery tactics, using it as a mobile force to support his infantry. As he described it: "I have fought sixty battles and I have learned nothing which I did not know at the beginning." Contemporary paintings of his headquarters during the Italian campaign depict his use of the Chappe semaphore line, first implemented in 1792. He was also a master of both intelligence and deception and had an uncanny sense of when to strike. He often won battles by concentrating his forces on an unsuspecting enemy, by using spies to gather information about opposing forces, and by concealing his own troop deployments. In this campaign, often considered his greatest, Napoleon's army captured 160,000 prisoners, 2,000 cannons, and 170 standards. A year of campaigning had witnessed major breaks with the traditional norms of 18th century warfare and marked a new era in military history.

While campaigning in Italy, General Bonaparte became increasingly influential in French politics. He published two newspapers, ostensibly for the troops in his army, but widely circulated within France as well. In May 1797 he founded a third newspaper, published in Paris, Le Journal de Bonaparte et des hommes vertueux. Elections in mid-1797 gave the royalist party increased power, alarming Barras and his allies on the Directory. The royalists, in turn, began attacking Bonaparte for looting Italy and overstepping his authority in dealings with the Austrians. Bonaparte sent General Augereau to Paris to lead a coup d'etat and purge the royalists on 4 September (18 Fructidor). This left Barras and his Republican allies in firm control again, but dependent on Bonaparte to maintain it. Bonaparte himself proceeded to the peace negotiations with Austria, then returned to Paris in December as the conquering hero and the dominant force in government, far more popular than any of the Directors. Napoleon then asked to be added to the Directory which they dismissed as out of hand; this greatly offended Napoleon.



In March 1798, Bonaparte proposed a military expedition to seize Egypt, then a province of the Ottoman Empire, seeking to protect French trade interests and undermine Britain's access to India. The Directory, although troubled by the scope and cost of the enterprise, readily agreed to the plan in order to remove the popular general from the center of power.

In May 1798, Bonaparte was elected a member of the French Academy of Sciences. His Egyptian expedition included a group of 167 scientists: mathematicians, naturalists, chemists and geodesers among them. One of their discoveries was the Rosetta Stone. This deployment of intellectual resources is considered by some an indication of Bonaparte's devotion to the principles of the Enlightenment, and by others as a masterstroke of propaganda, obfuscating the true imperialist motives of the invasion. In a largely unsuccessful effort to gain the support of the Egyptian populace, Bonaparte also issued proclamations casting himself as a liberator of the people from Ottoman oppression, and praising the precepts of Islam.

Bonaparte's expedition seized Malta from the Knights of Saint John on 9 June and then landed successfully at Alexandria on 1 July, temporarily eluding pursuit by the British Royal Navy.

After landing on the coast of Egypt, he fought The Battle of Chobrakit and Battle of the Pyramids against the Mamelukes, an old power in the Middle East, approximately four miles (6 km) from the pyramids. Bonaparte's forces were greatly outnumbered by the Mamelukes cavalry, 20,000 to 60,000, but Bonaparte formed hollow squares, keeping cannons and supplies safely on the inside. In all, 300 French and approximately 6,000 Egyptians were killed.

While the battle on land was a resounding French victory, the British Royal Navy managed to compensate at sea. The ships that had landed Bonaparte and his army sailed back to France, while a fleet of ships of the line remained to support the army along the coast. On 1 August the British fleet under Horatio Nelson fought the French in the Battle of the Nile, capturing or destroying all but two French vessels. With Bonaparte land-bound, his goal of strengthening the French position in the Mediterranean Sea was frustrated, but his army succeeded in consolidating power in Egypt, although it faced repeated uprisings.

In early 1799, he led the army into the Ottoman province of Damascus (Syria and northern Israel) and defeated numerically superior Ottoman forces in several battles, but his army was weakened by diseaseâ€”mostly bubonic plagueâ€”and poor supplies. Napoleon led 13,000 French soldiers to the conquest of the coastal towns of El Arish, Gaza, Jaffa, and Haifa.

The storming of Jaffa was particularly brutal. Although the French took control of the city within a few hours after the attack began, the French soldiers bayoneted approximately 2,000 Turkish soldiers who were trying to surrender. The soldiers' ferocity then turned to the inhabitants of the town. Men, women, and children were robbed and murdered for three days, and the massacre ended with even more bloodshed, as Napoleon ordered 3,000 more Turkish prisoners executed.

After his army was weakened by the plague, Napoleon was unable to reduce the fortress of Acre, and returned to Egypt in May. In order to speed up the retreat, Bonaparte took the controversial step of killing prisoners and plague-stricken men along the way. His supporters have argued that this decision was necessary given the continuing harassment of stragglers by Ottoman forces. Back in Egypt, on 25 July, Bonaparte defeated an Ottoman amphibious invasion at Abukir.

With the Egyptian campaign stagnating, and political instability developing back home, Bonaparte left Egypt for France in August, 1799, leaving his army under General KlÃ©ber.

While in Egypt, Bonaparte stayed informed on European affairs by relying on the irregular delivery of newspapers and dispatches. On 23 August 1799, he set sail for France, taking advantage of the temporary departure of British ships blockading French coastal ports. Although he was later accused of abandoning his troops, the Directory ordered his departure, as France had suffered a series of  to Second Coalition forces, and a possible invasion of French territory loomed.

By the time he returned to Paris in October, a series of French victories meant an improvement in the previously precarious military situation. The Republic was bankrupt, however, and the corrupt and inefficient Directory was more unpopular than ever with the French public.

Bonaparte was approached by one of the Directors, Emmanuel Joseph SieyÃ¨s, seeking his support for a coup d'Ã©tat to overthrow the constitutional government. The plot included Bonaparte's brother Lucien (then serving as speaker of the Council of Five Hundred), Roger Ducos, another Director, and Talleyrand. On 9 November (18 Brumaire) and the following day, troops led by Bonaparte seized control of and dispersed the legislative councils, leaving a legislative rump to name Bonaparte, SieyÃ¨s, and Ducos as provisional Consuls to administer the government. Although SieyÃ¨s expected to dominate the new regime, he was outmaneuvered by Bonaparte, who drafted the Constitution of the Year VIII and secured his own election as First Consul. This made Bonaparte the most powerful person in France, powers that were increased by the Constitution of the Year X, which declared him First Consul for life.



Bonaparte instituted several lasting reforms, including centralized administration of the dÃ©partements, higher education, a tax system, a central bank, law codes, and road and sewer systems. He negotiated the Concordat of 1801 with the Catholic Church, seeking to reconcile the mostly Catholic population with his regime. It was presented alongside the Organic Articles, which regulated public worship in France. His set of civil laws, the Napoleonic Code or Civil Code, has importance to this day in many countries. The Code was prepared by committees of legal experts under the supervision of Jean Jacques RÃ©gis de CambacÃ©rÃ¨s, who held the office Second Consul from 1799 to 1804; Bonaparte participated actively in the sessions of the Council of State that revised the drafts. Other codes were commissioned by Bonaparte to codify criminal and commerce law. In 1808, a Code of Criminal Instruction was published, which enacted precise rules of judicial procedure. Although by contemporary standards the code excessively favours the prosecution, when enacted it sought to protect personal freedoms and to remedy the prosecutorial abuses commonplace in European courts.





In 1800, Bonaparte returned to Italy, which the Austrians had reconquered during his absence in Egypt. He and his troops crossed the Alps in spring (although he actually rode a mule, not the white charger on which David famously depicted him). While the campaign began badly, Napoleon's forces eventually routed the Austrians in June at the Battle of Marengo, leading to an armistice. Napoleon's brother Joseph, who was leading the peace negotiations in LunÃ©ville, reported that due to British backing for Austria, Austria would not recognize France's newly gained territory. As negotiations became more and more fractious, Bonaparte gave orders to his general Moreau to strike Austria once more. Moreau led France to victory at Hohenlinden. As a result the Treaty of LunÃ©ville was signed in February 1801, under which the French gains of the Treaty of Campo Formio were reaffirmed and increased.

Later this year, Bonaparte became President of the French Academy of Sciences and appointed Jean Baptiste Joseph Delambre its Permanent Secretary.



The British signed the Treaty of Amiens in March 1802, which set terms for peace, including the withdrawal of British troops from several colonial territories recently occupied. The peace between France and Britain was uneasy and short-lived. The monarchies of Europe were reluctant to recognize a republic, fearing that the ideas of the revolution might be exported to them. In Britain, the brother of Louis XVI was welcomed as a state guest although officially Britain recognized France as a republic. Britain failed to evacuate Malta, as promised, and protested against France's annexation of Piedmont, and Napoleon's Act of Mediation in Switzerland (although neither of these areas was covered by the Treaty of Amiens).

In 1803 Bonaparte faced a major setback and eventual defeat when an army he sent to reconquer Haiti (Saint Domingue) and establish a base was destroyed by a combination of yellow fever and fierce resistance led by Haitian GeneralsToussaint L'Ouverture and Jean-Jacques Dessalines. Facing imminent war with Britain and bankruptcy, he recognized that French possessions on the mainland of North America would now be indefensible and sold them to the United Statesâ€”the Louisiana Purchaseâ€”for less than three cents per acre ($7.40/kmÂ²). The dispute over Malta ended with Britain declaring war on France in 1803 to support French royalists.

In January 1804, Bonaparte's police uncovered an assassination plot against him, ostensibly sponsored by the Bourbons. In retaliation, Bonaparte ordered the arrest of the Duc d'Enghien, in a violation of the sovereignty of Baden. After a hurried secret trial, the Duke was executed on 21 March. Bonaparte then used this incident to justify the re-creation of a hereditary monarchy in France, with himself as Emperor, on the theory that a Bourbon restoration would be impossible once the Bonapartist succession was entrenched in the constitution.

Napoleon crowned himself Emperor on 2 December 1804 at Notre Dame de Paris. Claims that he seized the crown out of the hands of Pope Pius VII during the ceremony in order to avoid subjecting himself to the authority of the pontiff are apocryphal; in fact, the coronation procedure had been agreed upon in advance. After the Imperial regalia had been blessed by the Pope, Napoleon crowned himself before crowning his wife JosÃ©phine Empress (the moment depicted in David's famous painting, illustrated above). Then at Milan's cathedral on 26 May 1805, Napoleon was crowned King of Italy with the Iron Crown of Lombardy.

In 1805 Britain convinced Austria and Russia to join a Third Coalition against Napoleon. Napoleon knew the French fleet could not defeat the Royal Navy and therefore tried to lure the British fleet away from the English Channel in hopes that a Spanish and French fleet could take control of the Channel long enough for French armies to cross to England. However, with Austria and Russia preparing an invasion of France and its allies, he had to change his plans and turn his attention to the continent. The newly formed Grande Armee secretly marched to Germany. On 20 October 1805, it surprised the Austrians at Ulm. The next day, however, with the Battle of Trafalgar (21 October 1805), the Royal Navy gained lasting control of the seas. A few weeks later, Napoleon defeated Austria and Russia at Austerlitz (a decisive victory for which he remained more proud than any other) on 2 December, the first anniversary of his coronation. Again Austria had to sue for peace.

The Fourth Coalition was assembled the following year, and Napoleon defeated Prussia at the Battle of Jena-Auerstedt (14 October 1806). He marched on against advancing Russian armies through Poland, and was involved at the bloody stalemate of the Battle of Eylau on 6 February 1807. After a decisive victory at Friedland, he signed a treaty at Tilsit in East Prussia with Tsar Alexander I of Russia, dividing Europe between the two powers. He placed puppet rulers on the thrones of German states, including his brother Jerome as king of the new state of Westphalia. In the French-controlled part of Poland, he established the Duchy of Warsaw, with King Frederick Augustus I of Saxony as ruler. Between 1809 and 1813, Napoleon also served as Regent of the Grand Duchy of Berg for his brother Louis Bonaparte.

In addition to military endeavours against Britain, Napoleon also waged economic war, attempting to enforce a Europe-wide commercial boycott of Britain called the "Continental System". Although this action hurt the British economy, it also damaged the French economy and was not a decisive blow against the enemy.

Portugal did not comply with the Continental System and in 1807 Napoleon invaded Portugal with the support of Spain. Under the pretext of reinforcing the Franco-Spanish army occupying Portugal, Napoleon invaded Spain as well, replacing Charles IV with his brother Joseph, placing brother-in-law Joachim Murat in Joseph's stead at Naples. This led to unexpected resistance from the Spanish army and civilians. Following a French retreat from much of the country, Napoleon himself took command and defeated the Spanish army, retook Madrid and then outmaneuvered a British army sent to support the Spanish, driving it to the coast. But before the Spanish population had been fully subdued, Austria again threatened war and Napoleon returned to France. The costly and often brutal Peninsular War continued, and Napoleon left several hundred thousand of his finest troops to battle Spanish guerrillas as well as British and Portuguese forces commanded by the Duke of Wellington. French control over the Iberian Peninsula deteriorated in 1812, and collapsed the following year when Joseph abdicated his throne. The last French troops were driven from the peninsula in 1814.



In 1809, Austria abruptly broke its alliance with France and Napoleon was forced to assume command of forces on the Danube and German fronts. After achieving early successes, the French faced difficulties crossing the Danube and then suffered a defeat at Aspern-Essling (21â€?2 May 1809) near Vienna. The Austrians failed to capitalise on the situation and allowed Napoleon's forces to regroup. The Austrians were defeated once again at Wagram (6 July), and a new peace was signed between Austria and France. In the following year the Austrian Archduchess Marie Louise married Napoleon, following his divorce of Josephine.

The other member of the coalition was Britain. Along with efforts in the Iberian Peninsula, the British planned to open another front in mainland Europe. However, by the time the British landed at Walcheren, Austria had already sued for peace. The expedition was a disaster and was characterized by little fighting but many casualties thanks to the popularly dubbed "Walcheren Fever".

Although the Congress of Erfurt had sought to preserve the Russo-French alliance, by 1811 tensions were again increasing between the two nations. Although Alexander and Napoleon had a friendly personal relationship since their first meeting in 1807, Alexander had been under strong pressure from the Russian aristocracy to break off the alliance with France. In order to keep other countries from revolting against France, Napoleon decided to make an example of Russia. 

The first sign that the alliance was deteriorating was the easing of the application of the Continental System in Russia, angering Napoleon. By 1812, advisors to Alexander suggested the possibility of an invasion of the French Empire and the recapture of Poland.

Russia deployed large numbers of troops to the Polish borders, eventually placing there more than 300,000 of its total army strength of 410,000. After receiving initial reports of Russia's war preparations, Napoleon began expanding his Grande ArmÃ©e to more than 450,000-600,000 men (in addition to more than 300,000 men already deployed in Iberia). Napoleon ignored repeated advice against an invasion of the vast Russian heartland, and prepared for an offensive campaign.

On 22 June 1812, Napoleon's invasion of Russia commenced. In an attempt to gain increased support from Polish nationalists and patriots, Napoleon termed the war the "Second Polish War" (the first Polish war being the liberation of Poland from Russia, Prussia and Austria). Polish patriots wanted the Russian part of partitioned Poland to be incorporated into the Grand Duchy of Warsaw and a new Kingdom of Poland created, although this was rejected by Napoleon, who feared it would bring Prussia and Austria into the war against France. Napoleon also rejected requests to free the Russian serfs, fearing this might provoke a conservative reaction in his rear.

The Russians under Mikhail Bogdanovich Barclay de Tolly avoided a decisive engagement which Napoleon longed for, preferring to retreat ever deeper into the heart of Russia. A brief attempt at resistance was offered at Smolensk (16â€?7 August), but the Russians were defeated in a series of battles in the area and Napoleon resumed the advance. The Russians then repeatedly avoided battle with the Grande ArmÃ©e, although in a few cases only because Napoleon uncharacteristically hesitated to attack when the opportunity arose. Thanks to the Russian army's scorched earth tactics, the Grande ArmÃ©e had more and more trouble foraging food for its men and horses. Along with hunger, the French also suffered from the harsh Russian winter. 

Barclay was criticized for his tentative strategy of continual retreat and was replaced by Kutuzov. However, Kutuzov continued Barclay's strategy. Kutuzov eventually offered battle outside Moscow on 7 September. Losses were nearly even for both armies, with slightly more casualties on the Russian side, after what may have been the bloodiest day of battle in history: the Battle of Borodino (see article for comparisons to the first day of the Battle of the Somme). Although Napoleon was far from defeated, the Russian army had accepted, and withstood, the major battle the French hoped would be decisive. After the battle, the Russian army withdrew and retreated past Moscow.

Napoleon then entered Moscow, assuming that the fall of Moscow would end the war and that Alexander I would negotiate peace. However, on orders of the city's military governor and commander-in-chief, Fyodor Rostopchin, rather than capitulating, Moscow was ordered burned. Within the month, fearing loss of control back in France, Napoleon left Moscow.

The French suffered greatly in the course of a ruinous retreat; the Army had begun as over 650,000 frontline troops, but in the end fewer than 40,000 crossed the Berezina River (November 1812) to escape. The strategy employed by Barclay and Kutuzov had worn down the invaders and maintained the Tsar's domination over the Russian people. In total, French losses in the campaign were 570,000 against about 400,000 Russian casualties and several hundred thousand civilian deaths.

One American study concluded that the winter only had a major effect once Napoleon was in full retreat:

"However, in regard to the claims of "General Winter," the main body of Napoleon's Grande ArmÃ©e diminished by half during the first eight weeks of his invasion before the major battle of the campaign. This decrease was partly due to garrisoning supply centres, but disease, desertions, and casualties sustained in various minor actions caused thousands of losses. At Borodino on 7 September 1812 â€?the only major engagement fought in Russia â€?Napoleon could muster no more than 135,000 troops, and he lost at least 30,000 of them to gain a narrow and Pyrrhic victory almost  deep in hostile territory. The sequels were his uncontested and self-defeating occupation of Moscow and his humiliating retreat, which began on 19 October, before the first severe frosts later that month and the first snow on 5 November."

There was a lull in fighting over the winter of 1812â€?3 whilst both the Russians and the French recovered from their massive losses. A small Russian army harassed the French in Poland and eventually 30,000 French troops there withdrew to the German states to rejoin the expanding force there â€?numbering 130,000 with the reinforcements from Poland. This force continued to expand, with Napoleon aiming for a force of 400,000 French troops supported by a quarter of a million German troops.

Heartened by Napoleon's losses in Russia, Prussia rejoined the Coalition that now included Russia, the United Kingdom, Spain, and Portugal. Napoleon assumed command in Germany and inflicted a series of defeats on the Allies culminating in the Battle of Dresden on 26â€?7 August 1813 causing almost 100,000 casualties to the Coalition forces (the French sustaining only around 30,000).

Despite these initial successes, the numbers continued to mount against Napoleon as Sweden and Austria joined the Coalition. Eventually the French army was pinned down by a force twice its size at the Battle of Nations (16â€?9 October) at Leipzig. Some of the German states switched sides in the midst of the battle to fight against France. This was by far the largest battle of the Napoleonic Wars and cost both sides a total of more than 120,000 casualties.

After this Napoleon withdrew in an orderly fashion back into France, his army was now reduced to less than 100,000 against more than half a million Allied troops. The French were now surrounded (with British armies pressing from the south in addition to the Coalition forces moving in from the German states) and vastly outnumbered.

On April 6, 1813, Napoleon abdicated in favor of his son, and when the allies refused to accept this, he made his abdication unconditional on April 11. Paris was occupied on 31 March 1814. Napoleon proposed that they march on Paris. His soldiers and regimental officers were eager to fight on. But his marshals mutinied. On April 4, Napoleon's marshals, led by Ney, confronted him. They said that they refused to march. Napoleon asserted that the army would follow him. Ney replied that the army would follow its generals. Napoleon abdicated in favor of his son. The Allies were not satisfied with this and demanded unconditional surrender. Napoleon abdicated again, unconditionally, on 11 April, however they allowed him to retain his title of Emperor. In the Treaty of Fontainebleau the victors exiled him to Elba, a small island in the Mediterranean 20 km off the coast of Italy.

In his exile, he ran Elba as a little country; he created a tiny navy and army, opened some mines, and helped farmers improve their land. However he became restless.

In France, the royalists had taken over and restored Louis XVIII to power. Meanwhile Napoleon, separated from his wife and son (who had come under Austrian control), cut off from the allowance guaranteed to him by the Treaty of Fontainebleau, and aware of rumours that he was about to be banished to a remote island in the Atlantic, escaped from Elba on 26 February 1815 and returned to the French mainland on 1 March 1815. Louis XVIII sent the 5th Regiment of the Line, led by Marshal Ney who had formerly served under Napoleon in Russia, to meet him at Grenoble on 7 March 1815. Napoleon approached the regiment alone, dismounted his horse and, when he was within earshot of Ney's forces, shouted, "Soldiers of the Fifth, you recognize me. If any man would shoot his emperor, he may do so now." Following a brief silence, the soldiers shouted, "Vive L'Empereur!" With that, they marched with Napoleon to Paris. He arrived on 20 March, quickly raising a regular army of 140,000 and a volunteer force of around 200,000, and governed for a period now called the Hundred Days.

For much of his public life, Napoleon was troubled by ill health, including haemorrhoids, which made sitting on a horse for long periods of time difficult and painful. This condition would have disastrous results when he fought at Waterloo; during the battle, his inability to sit on his horse for other than very short periods of time interfered with his ability to survey, and thus exercise command of, his troops in combat.Napoleon was finally defeated by the Duke of Wellington and Gebhard Leberecht von BlÃ¼cher at Waterloo in present-day Belgium on 18 June 1815.

Off the port of Rochefort, after unsuccessfully attempting to escape to the United States, NapolÃ©on made his formal surrender while on board HMS ''Bellerophon'' on 15 July 1815.



Napoleon was imprisoned and then exiled by the British to the island of Saint Helena (2,800 km off the Bight of Guinea in the South Atlantic Ocean) from 15 October 1815. Before Napoleon moved to Longwood House in November 1815, he lived in a pavilion on the estate The Briars belonging to William Balcombe (1779-1829), and became friendly with the family, especially the younger daughter Lucia Elizabeth (Betsy) who later wrote Recollections of the Emperor Napoleon (London, 1844). This relationship ended in March 1818 when Balcombe was accused of acting as an intermediary between Napoleon and Paris.

Whilst there, with a small cadre of followers, he dictated his memoirs, and criticized his captors. There were several plots to rescue Napoleon from captivity, including one from Brazil and another from Texas, where some four hundred exiled soldiers from the Grand Army dreamed of a resurrection of the Napoleonic Empire in America. There was even a plan to rescue him using a submarine.

The question of the British treatment of Napoleon is a matter of some dispute. Certainly his accommodation was poorly built, and the location was damp, windswept and generally considered unhealthy. The paranoia, tactlessness and often petty-minded behaviour of Hudson Lowe also exacerbated what was bound to be a difficult situation. At the same time Napoleon and his entourage never accepted the legality or justice of his captivity, and the slights they received tended to be magnified. In the early years of the captivity Napoleon received many visitors, to the anger and consternation of the French minister Richelieu, who said, "this devil of a man exercises an astonishing seduction on all those who approach him." From 1818 however, as the restrictions placed on him were increased, he lived the life of a recluse.

In Britain, Napoleon came to be transformed in the public mind from a monster to a hero, no doubt a direct expression of discontent at the reactionary post-war government of Lord Liverpool. In 1818 The Times, which Napoleon received in exile, in reporting a false rumour of his escape, said that this had been greeted by spontaneous illuminations in London. There was some sympathy for him also in the political opposition in Parliament. Lord Holland, the nephew of Charles James Fox, the former Whig leader, sent more than 1,000 books and pamphlets to Longwood, as well as jam and other comforts. Holland also accused the government of attempting to kill the Emperor by a process of slow assassination. Napoleon knew of this, and based his hopes for release on the possibility of Holland becoming Prime Minister, which was Richelieu's greatest fear. 

Napoleon also enjoyed the support of Admiral Lord Cochrane, one of the greatest sailors of the age, closely involved in Chile and Brazil's struggle for independence. It was his expressed aim to make him Emperor of a unified South American state, a scheme that was frustrated by Napoleon's death in 1821. For Lord Byron, amongst others, Napoleon was the very epitome of the Romantic hero, the persecuted, lonely and flawed genius. At quite the other extreme, the news that Napoleon had taken up gardening at Longwood appealed to more domestic British sensibilities, which had the effect of humanising him still further. 

The nature of Napoleon's personal religious faith has come to be a frequent topic of debate. Not long after Napoleonâ€™s death, in a lecture before Oxford University, Henry Parry Liddon asserted that Napoleon, while in exile on St. Helena, compared himself unfavorably to Jesus Christ. According to Liddon's sources, Napoleon pointed out to Count Montholon that while he and others such as "Alexander, Caesar [and] Charlemagne" founded vast empires, their achievements relied on force, while Jesus "founded his empire on love." After further discourse about the wonders of Christ and his legacy, Napoleon then reputedly said, "This it is which proves to me quite convincingly the Divinity of Jesus Christ."

An earlier quotation attributed to Napoleon suggests there had been a time he may have also been an admirer of Islam:I hope the time is not far off when I shall be able to unite all the wise and educated men of all the countries and establish a uniform regime based on the principles of Qur'an which alone are true and which alone can lead men to happiness.

However, Napoleon's private secretary during his conquest of Egypt, Louis Antoine Fauvelet de Bourrienne, wrote in his memoirs that Napoleon had no serious interest in Islam or any other religion beyond their political value: Bonaparte's principle was, as he himself has often told me, to look upon religions as the work of men, but to respect them everywhere as a powerful engine of government. However, I will not go so far as to say that he would not have changed his religion had the conquest of the East been the price of that change. All that he said about Mahomet, Islamism, and the Koran to the great men of the country he laughed at himself... I confess that Bonaparte frequently conversed with the chiefs of the Mussulman religion on the subject of his conversion; but only for the sake of amusement.... If Bonaparte spoke as a Mussulman, it was merely in his character of a military and political chief in a Mussulman country. To do so was essential to his success, to the safety of his army, and, consequently, to his glory. In every country he would have drawn up proclamations and delivered addresses on the same principle. In India he would have been for Ali, at Thibet for the Dalai-lama, and in China for Confucius.

Napoleon died reconciled to the Catholic Church, having confessed his sins and received Extreme Unction and Viaticum at the hands of Father Ange Vignali on May 5, 1821.

Napoleon had asked in his will to be buried on the banks of the Seine, but was buried on Saint Helena, in the "valley of the willows". He was buried in an unmarked tomb, because Sir Hudson Lowe refused to allow the simple inscription Napoleon to be placed on it, insisting that the word Bonaparte must also be there. In 1840 his remains were taken to France in the frigate ''Belle-Poule'' and were to be entombed in a porphyry sarcophagus at Les Invalides, Paris.Egyptian porphyry (used for the tombs of Roman emperors) was unavailable, so red quartzite was obtained from Russian Finland, eliciting protests from those who still remembered the Russians as enemies. Hundreds of millions have visited his tomb since that date. A replica of his simple Saint Helena tomb is also to be found at Les Invalides.

The cause of Napoleon's death has been disputed on a number of occasions. Francesco Antommarchi, the physician chosen by Napoleon's family and the leader of the post mortem examination, gave stomach cancer as a reason for Napoleon's death on his death certificate. In the later half of the twentieth century, a different theory arose conjecturing that Napoleon was the victim of arsenic poisoning.

In 1955 the diaries of Louis Marchand, Napoleon's valet, appeared in print. His description of Napoleon in the months before his death led many, most notably Sten Forshufvud and Ben Weider, to conclude that he had been indirectly killed by arsenic poisoning. Arsenic was sometimes used as a poison because at that time it was undetectable when administered over a long period. As Napoleon's body was found to be remarkably well preserved when it was moved in 1840, it gives support to the arsenic theory, as arsenic is a strong preservative. In 2001, Pascal Kintz, of the Strasbourg Forensic Institute in France, added credence to this claim with a study of arsenic levels found in a lock of Napoleon's hair preserved after his death: they were seven to 38 times higher than normal.

Cutting up hairs into short segments and analysing each segment individually provides a histogram of arsenic concentration in the body. This analysis on hair from NapolÃ©on suggests that large but non-lethal doses were absorbed at random intervals. The arsenic severely weakened NapolÃ©on and remained in his system.

The medical regimen imposed on Napoleon by his doctors included treatment with antimony potassium tartrate, also called tartar emetic, regular enemas, and a 600-milligram dose of mercurous chloride, also called calomel, to purge his intestines in the days immediately prior to his death. A group of researchers from the San Francisco Medical Examiner's Department speculate that this treatment may have led to Napoleon's death by causing a serious potassium deficiency. Forshufvud and Weider noted that this was coupled with high levels of orgeat that Napoleon was drinking, to attempt to quench abnormally high thirst of which he was complaining, at the time; the bitter almonds used to flavor orgeat contained cyanide compounds which, Forshufvud and Weider maintained, the frequent doses of tartar emetic were preventing Napoleon's stomach from expelling by vomiting. They remarked that the thirst of which Napoleon complained was also a possible symptom of slow chronic arsenic poisoning, and added that the dosage of calomel given to Napoleon was essentially a massive overdose. They said that it caused almost immediate corrosion and bleeding of his stomach, killing him within two days and leaving behind extensive tissue damage. Not having looked for aftereffects of arsenic poisoning, they noted, the doctors who performed the autopsy (except for Antommarchi, the only pathologist present) could easily have mistaken this tissue damage for aftereffects of cancer.



More recent analysis on behalf of the magazine Science et Vie showed that similar concentrations of arsenic can be found in Napoleon's hair in samples taken from 1805, 1814 and 1821. The lead investigator, Ivan Ricordel (head of toxicology for the Paris Police), stated that if arsenic had been the cause, NapolÃ©on would have died years earlier. The group suggested that the most likely source in this case was a hair tonic. However, the group did not address the irregular arsenic absorption patterns revealed by the analysis commissioned by Forshufvud.

It has also been discovered that the form of wallpaper used in NapolÃ©on's house contained a high level of arsenic which, when made in a compound with copper, was used by British textile makers to make the greens present in the wallpaper. It has been said that the adhesive, which in the cooler environment of Britain was innocuous, grew mold and turned the copper-arsenic compound into a deadly gas in the warm and humid climate of St. Helena. But, as above, this analysis also fails to explain the irregular arsenic absorption patterns revealed in the analysis that Forshufvud had commissioned.

Prior to the discovery of antibiotics, arsenic was also a widely used treatment for syphilis. This has led to speculation that Napoleon might have suffered from that disease.

In a 2008 study researchers analyzed samples of Napoleon's hair throughout his life, and also samples from his son and wife as well as other contemporaries. Through neutron activation analysis they found that all the samples had high levels of arsenic, approximately 100 times higher than the current average. According to the researchers, Napoleon's body was already heavily contaminated with arsenic as a boy, concluding that the high arsenic concentration in Napoleon's hair was not due to poisoning. During his life he was constantly exposed to arsenic from materials such as commonly-used glues and dyes of the era.

In May 2005, a team of Swiss physicians claimed that the reason for Napoleon's death was stomach cancer, which was also the cause of his father's death. From a multitude of forensic reports they derive that Napoleon at his death weighed approx. 76 kg (168 lb) while a year earlier he weighed approx. 91 kg (200 lb), confirming the autopsy result reported by Antommarchi. 

In October 2005, a document was unearthed in Scotland that presented an account of the autopsy, which again seems to confirm Antommarchi's conclusion. More recent analysis of the etiology and pathogenesis of Napoleon's illness also suggests that Napoleon's illness was a sporadic gastric carcinoma of advanced stage. The original post-mortem examination carried out by Francesco Antommarchi concluded Napoleon died of stomach cancer without knowing Napoleonâ€™s father had died of stomach cancer. An extensive 2007 study found no evidence of arsenic poisoning in the organs, such as hemÂ­orÂ­rhagÂ­ing in the linÂ­ing inÂ­side the heart, and also concluded that stomach cancer was the cause of death.

But none of these claims adequately convinced those who believed that Napoleon had died as an indirect result of arsenic poisoning, since cancers are "wasting" diseases that emaciate their victims as they progress whereas Napoleon was reported to have grown progressively fatter almost to the end; obesity has been observed in slow chronic arsenic poisoning victims.



Napoleon was married twice:

Acknowledged two illegitimate children, both of whom had issue:

May have had further illegitimate offspring:



Napoleon is credited with introducing the concept of the modern professional conscript army to Europe, an innovation which other states eventually followed. He did not introduce many new concepts into the French military system, borrowing mostly from previous theorists and the implementations of preceding French governments, but he did expand or develop much of what was already in place. Corps replaced divisions as the largest army units, artillery was integrated into reserve batteries, the staff system became more fluid, and cavalry once again became a crucial formation in French military doctrine.

Napoleon's biggest influence in the military sphere was in the conduct of warfare. Weapons and technology remained largely static through the Revolutionary and Napoleonic eras, but 18th century operational strategy underwent massive restructuring. Sieges became infrequent to the point of near-irrelevance, and a new emphasis towards the destruction, not just outmaneuvering, of enemy armies emerged. Invasions of enemy territory occurred over broader fronts, thus introducing a plethora of strategic opportunities that made wars costlier and, just as importantly, more decisive (this strategy has since become known as Napoleonic warfare, though he himself did not give it this name). Defeat for a European power now meant much more than losing isolated enclaves; near-Carthaginian peaces intertwined whole national efforts â€?sociopolitical, economic, and militaristic â€?into gargantuan collisions that severely upset international conventions. It can be argued that Napoleon's initial success sowed the seeds for his downfall. Not used to such catastrophic defeats in the rigid power system of 18th century Europe, many nations found life under the French yoke intolerable, sparking revolts, wars, and general instability that plagued the continent until 1815.

In France, Napoleon is seen by some as having ended lawlessness and disorder, and the wars he fought as having served to export the Revolution to the rest of Europe. The movements of national unification and the rise of the nation state, notably in Italy and Germany, may have been precipitated by the Napoleonic rule of those areas.

The Napoleonic Code was adopted throughout much of Europe and remained in force after Napoleon's defeat. Napoleon himself once said: "My true glory is not to have won 40 battles... Waterloo will erase the memory of so many victories. ... But what nothing will destroy, what will live forever, is my Civil Code." Professor Dieter Langewiesche of the University of TÃ¼bingen described the code as a "revolutionary project" which spurred the development of bourgeois society in Germany by expanding the right to own property and breaking the back of feudalism. Langewiesche also credits Napoleon with reorganizing what had been the Holy Roman Empire, made up of more than 1,000 entities, into a more streamlined network of 40 states, providing the basis for the German Confederation and the future unification of Germany under the German Empire in 1871.Critics of Napoleon argue that his true legacy was a loss of status for France and many needless deaths:

After all, the military record is unquestionedâ€?7 years of wars, perhaps six million Europeans dead, France bankrupt, her overseas colonies lost. And it was all such a great waste, for when the self-proclaimed tÃªte d'armÃ©e was done, France's "losses were permanent" and she "began to slip from her position as the leading power in Europe to second-class statusâ€”that was Bonaparte's true legacy."

Napoleon is sometimes alleged to have been in many ways the direct inspiration for later autocrats: he never flinched when facing the prospect of war and death for thousands, friend or foe, and turned his search of undisputed rule into a continuous cycle of conflict throughout Europe, ignoring treaties and conventions alike. Even if other European powers continually offered Napoleon terms that would have restored France's borders to situations only dreamt by the Bourbon kings, he always refused compromise, and only accepted surrender. 

Living at the end of the Enlightenment, Napoleon also became notorious for his effort to suppress the slave revolt in Haiti and his 1801 decision to re-establish slavery in France after it was banned following the revolution.Nevertheless, many in the international community still admire the many accomplishments of the emperor as evidenced by the International Napoleonic Congress held in Dinard, France in July 2005 that included participation by members of the French and American military, French politicians, scholars from as far away as Israel and Russia, and a parade recreating the Grand Army. Napoleon was in many ways close to historical figures like Alexander or Caesar, and it is one of the reasons for the vivacity and strength of his legacy.

Napoleon was hated by his many enemies, but respected by them at the same time. Wellington, when asked who he thought was the greatest general of the day, answered: "In this age, in past ages, in any age, Napoleon." By his opponents within France, mostly monarchist loyalists as well as republicans, he was considered an usurper and tyrant. He is one of the most reviled figures in European history, noted for his attempt to conquer large parts of Europe, which has later led some historians to compare him to Adolf Hitler.

In military school at Brienne-le-ChÃ¢teau, Bonaparte first met the Champagne maker Jean-Remy MoÃ«t. The friendship of these two men would have lasting impact on the history of the Champagne region and on the beverage itself.

Many historians have recently argued that, contrary to popular belief, Napoleon was not short as often depicted in popular culture. Although historians disagree on Napoleonâ€™s precise height, it has been suggested that he was actually slightly taller than the average early 19th century Frenchman. Some historians claim the French emperor's height was recorded as 5 ft 2 in French units, corresponding to 1.68 meters or 5 ft. 6 in Imperial units. A French inch was 2.71 centimetres while an Imperial inch is 2.54 centimetres. The metric system was introduced during the French First Republic, but was not in widespread use until after Napoleon's death. Other historians reject this claim, pointing out it is unlikely that Napoleon was measured with a French yardstick after his death. Napoleon was under British control on St. Helena, and was almost certainly measured with a British yardstick, which would suggest that the measurement of 5 ft 2 in is accurate. A counter-claim can be made that Francesco Antommarchi, Napoleon's personal physician, despised the English, considered their touch "polluting,"  and would never have used their yardstick to measure his emperor. It's also unlikely that the only time Napoleon's height had been established conclusively was at his autopsy, yet there are no known sources citing that Bonaparte was under five (French) feet in height, which he would have been necessarily if the autopsy measurement had been taken in Imperial units.

Napoleon's nickname of le petit caporal may add to the confusion, as non-Francophones may mistakenly interpret petit by its literal meaning of "small"; in fact, it is an affectionate term reflecting on his camaraderie with ordinary soldiers (for example, petit ami means "boyfriend" in French, petite amie means "girlfriend," and mon petit chou ["my little cabbage"] is a term of affection). He also surrounded himself with the soldiers of his elite guard, who were usually six feet or taller.

Stanley Kubrick worked all his life on a film project about Napoleon. He never made it and put all his research efforts into the Academy award-winning film Barry Lyndon.

At birth, the child's name was Napoleone di Buonaparte. When he was a child his family declined using the nobiliary particle di. Though the Buonaparte family belonged to minor nobility, they were financially poor and did not regard themselves as aristocrats. But because it was necessary to belong to a proven noble family to enroll at the military academy at Brienne, in school the child was known as Napoleone de Buonaparte. He gradually adopted a French version of his first name, NapolÃ©on. In 1795, after becoming a general, he dropped the "u" from his last name, making it Bonaparte.





|-|-|-|-|-|-|-|-|-|-













Writing is the representation of language in a textual medium through the use of signs or symbols. It is distinguished from illustration, such as cave drawing and painting, and the recording of language via a non-textual medium such as magnetic tape audio. 

Writing began as a consequence of the burgeoning needs of accounting. Around the 4th millennium BC, the complexity of trade and administration outgrew the power of memory, and writing became a more dependable method of recording and presenting transactions in a permanent form (Robinson, 2003, p. 36). 

Writing, more particularly, refers to two things: writing as a noun, the thing that is written; and writing as a verb, which designates the activity of writing. It refers to the inscription of characters on a medium, thereby forming words, and larger units of language, known as texts. It also refers to the creation of meaning and the information thereby generated. In that regard, linguistics (and related sciences) distinguishes between the written language and the spoken language. The significance of the medium by which meaning and information is conveyed is indicated by the distinction made in the arts and sciences. For example, while public speaking and poetry reading are both types of speech, the former is governed by the rules of rhetoric and the latter by poetics.

A person who composes a message or story in the form of text is generally known as a writer or an author. However, more specific designations exist which are dictated by the particular nature of the text such as that of poet, essayist, novelist, playwright, journalist, and more. A person who transcribes, translates or produces text to deliver a message authored by another person is known as a scribe, typist or typesetter. A person who produces text with emphasis on the aesthetics of glyphs is known as a calligrapher or graphic designer. 

Writing is also a distinctly human activity. It has been said that a monkey, randomly typing away on a typewriter (in the days when typewriters replaced the pen or plume as the preferred instrument of writing) could re-create Shakespeare-- but only if it lived long enough (this is known as the infinite monkey theorem). Such writing has been speculatively designated as coincidental. It is also speculated that extra-terrestrial beings exist who may possess knowledge of writing. The fact is that the only known writing is human writing.

Wells argues that writing has the ability to "put agreements, laws, commandments on record. It made the growth of states larger than the old city states possible. The command of the priest or king and his seal could go far beyond his sight and voice and could survive his death" (Wells in Robinson, 2003, p. 35). 

The major writing systems â€?methods of inscription â€?broadly fall into four categories: logographic, syllabic, alphabetic, and featural.Another category, ideographic (symbols for ideas), has never been developed sufficiently to represent language. A sixth category, pictographic, is insufficient to represent language on its own, but often forms the core of logographies.

A logogram is a written character which represents a word or morpheme. The vast number of logograms needed to write language, and the many years required to learn them, are the major disadvantage of the logographic systems over alphabetic systems. However, the efficiency of reading logographic writing once it is learned is a major advantage.No writing system is wholly logographic: all have phonetic components as well as logograms ("logosyllabic" components in the case of Chinese characters, cuneiform, and Mayan, where a glyph may stand for a morpheme, a syllable, or both; "logoconsonantal" in the case of hieroglyphs), and many have an ideographic component (Chinese "radicals", hieroglyphic "determiners"). For example, in Mayan, the glyph for "fin", pronounced "ka'", was also used to represent the syllable "ka" whenever the pronunciation of a logogram needed to be indicated, or when there was no logogram. In Chinese, about 90% of characters are compounds of a semantic (meaning) element called a radical with an existing character to indicate the pronunciation, called a phonetic. However, such phonetic elements complement the logographic elements, rather than vice versa.

The main logographic system in use today is Chinese characters, used with some modification for various languages of China, Japanese, and, to a lesser extent, Korean in South Korea. Another is the classical Yi script.

A syllabary is a set of written symbols that represent (or approximate) syllables. A glyph in a syllabary typically represents a consonant followed by a vowel, or just a vowel alone, though in some scripts more complex syllables (such as consonant-vowel-consonant, or consonant-consonant-vowel) may have dedicated glyphs. Phonetically related syllables are not so indicated in the script. For instance, the syllable "ka" may look nothing like the syllable "ki", nor will syllables with the same vowels be similar.

Syllabaries are best suited to languages with relatively simple syllable structure, such as Japanese. Other languages that use syllabic writing include the Linear B script for Mycenaean Greek; Cherokee; Ndjuka, an English-based creole language of Surinam; and the Vai script of Liberia. Most logographic systems have a strong syllabic component. Ethiopic, though technically an alphabet, has fused consonants and vowels together to the point that it's learned as if it were a syllabary.



An alphabet is a small set of symbols, each of which roughly represents or historically represented a phoneme of the language. In a perfectly phonological alphabet, the phonemes and letters would correspond perfectly in two directions: a writer could predict the spelling of a word given its pronunciation, and a speaker could predict the pronunciation of a word given its spelling. As languages often evolve independently of their writing systems, and writing systems have been borrowed for languages they were not designed for, the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.

In most of the alphabets of the Mid-East, only consonants are indicated, or vowels may be indicated with optional diacritics. Such systems are called abjads. In most of the alphabets of India and Southeast Asia, vowels are indicated through diacritics or modification of the shape of the consonant. These are called abugidas. Some abugidas, such as Ethiopic and Cree, are learned by children as syllabaries, and so are often called "syllabics". However, unlike true syllabaries, there is not an independent glyph for each syllable.

Sometimes the term "alphabet" is restricted to systems with separate letters for consonants and vowels, such as the Latin alphabet. Because of this use, Greek is often considered to be the first alphabet. 

A featural script notates the building blocks of the phonemes that make up a language. For instance, all sounds pronounced with the lips ("labial" sounds) may have some element in common. In the Latin alphabet, this is accidentally the case with the letters "b" and "p"; however, labial "m" is completely dissimilar, and the similar-looking "q" is not labial. In Korean hangul, however, all four labial consonants are based on the same basic element. However, in practice, Korean is learned by children as an ordinary alphabet, and the featural elements tend to pass unnoticed.

Another featural script is SignWriting, the most popular writing system for many sign languages, where the shapes and movements of the hands and face are represented iconically. Featural scripts are also common in fictional or invented systems, such as Tolkien's Tengwar.

Historians draw a distinction between prehistory and history, with history defined by the advent of writing. The cave paintings and petroglyphs of prehistoric peoples can be considered precursors of writing, but are not considered writing because they did not represent language directly.

Writing systems always develop and change based on the needs of the people who use them. Sometimes the shape, orientation and meaning of individual signs also changes over time. By tracing the development of a script it is possible to learn about the needs of the people who used the script as well as how it changed over time.

The many tools and writing materials used throughout history include stone tablets, clay tablets, wax tablets, vellum, parchment, paper, copperplate, styluses, quills, ink brushes, pencils, pens, and many styles of lithography. It is speculated that the Incas might have employed knotted threads known as quipu (or khipu) as a writing system. 

For more information see writing implements.

By definition, history begins with written records; evidence of human culture without writing is the realm of prehistory. 

The evolution of writing was a process involving economic necessity in the ancient near east. Archaeologist Denise Schmandt-Besserat determined the link between previously uncategorized clay "tokens" and the first known writing, cuneiform. The clay tokens were used to represent commodities, and perhaps even units of time spent in labor, and their number and type became more complex as civilization advanced. A degree of complexity was reached when over a hundred different kinds of tokens had to be accounted for, and tokens were wrapped and fired in clay, with markings to indicate the kind of tokens inside. These markings soon replaced the tokens themselves, and the clay envelopes were demonstrably the prototype for clay writing tablets.

The original Mesopotamian writing system was derived from this method of keeping accounts, and by the end of the 4th millennium BC, this had evolved into using a triangular-shaped stylus pressed into soft clay for recording numbers. This was gradually augmented with pictographic writing using a sharp stylus to indicate what was being counted. Round-stylus and sharp-stylus writing was gradually replaced by writing using a wedge-shaped stylus (hence the term cuneiform), at first only for logograms, but evolved to include phonetic elements by the 29th century BC. Around the 26th century BC, cuneiform began to represent syllables of spoken Sumerian. Also in that period, cuneiform writing became a general purpose writing system for logograms, syllables, and numbers, and this script was adapted to another Mesopotamian language, Akkadian, and from there to others such as Hurrian, and Hittite. Scripts similar in appearance to this writing system include those for Ugaritic and Old Persian.

In China historians have found out a lot about the early Chinese dynasties from the written documents left behind. From the Shang Dynasty most of this writing has survived on bones or bronze implements. Markings on turtle shells have been carbon-dated to around 1500 BC. Historians have found that the type of media used had an effect on what the writing was documenting and how it was used.

There have recently been discoveries of tortoise-shell carvings dating back to c. 6000 BC, but whether or not the carvings are of sufficient complexity to qualify as writing is under debate. If it is deemed to be a written language, writing in China will predate Mesopotamian cuneiform, long acknowledged as the first appearance of writing, by some 2000 years.

The earliest known hieroglyphic inscriptions are the Narmer Palette, dating to c.3200 BC, and several recent discoveries that may be slightly older, though the glyphs were based on a much older artistic tradition. The hieroglyphic script was logographic with phonetic adjuncts that included an effective alphabet.

Writing was very important in maintaining the Egyptian empire, and literacy was concentrated among an educated elite of scribes. Only people from certain backgrounds were allowed to train to become scribes, in the service of temple, pharaonic, and military authorities. The hieroglyph system was always difficult to learn, but in later centuries was purposely made even more so, as this preserved the scribes' status.

The world's oldest known alphabet was developed in central Egypt around 2000 BC from a hieroglyphic prototype, and over the next 500 years spread to Canaan and eventually to the rest of the world.



The Indus Valley script is a mysterious aspect of ancient Indian culture as it has not yet been deciphered. All known inscriptions are short.

The Phoenician writing system was adapted from the Proto-Caananite script in around the 11th century BC, which in turn borrowed ideas from Egyptian hieroglyphics. This writing system was an abjad â€?that is, a writing system in which only consonants are represented. This script was adapted by the Greeks, who adapted certain consonantal signs to represent their vowels. The Cumae alphabet, a variant of the early Greek alphabet gave rise to the Etruscan alphabet, and its own descendants, such as the Latin alphabet and Runes. Other descendants from the Greek alphabet include the Cyrillic alphabet, used to write Russian, among others. The Phoenician system was also adapted into the Aramaic script, from which the Hebrew script and also that of Arabic are descended.

The Tifinagh script (Berber languages) is descended from the Libyco-Berber script which is assumed to be of Phoenician origin.

Of several pre-Colombian scripts in Mesoamerica, the one that appears to have been best developed, and the only one to be deciphered, is the Maya script. The earliest inscriptions which are identifiably Maya date to the 3rd century BC, and writing was in continuous use until shortly after the arrival of the Spanish conquistadores in the 16th century AD. Maya writing used logograms complemented by a set of syllabic glyphs, somewhat similar in function to modern Japanese writing.







Writers sometimes search out others to evaluate or criticize their work. To this end, many writers join writing circles, often found at local libraries or bookstores. With the evolution of the Internet, writing circles have started to go online.













Liverpool is governed by one of five councils within the metropolitan county of Merseyside, and is one of England's core cities and its third most populous â€?447,500 in 2006, with 816,000 in the Liverpool Urban Area, the conurbation around the city that includes other towns (such as St. Helens and Haydock) on the Liverpool side of the Mersey but not those on the Wirral Peninsula. The term Greater Merseyside is sometimes used to describe a broader area, which also includes the borough of Halton.

Built across a ridge of hills rising up to a height of around 230Â feet (70Â metres) above sea-level at Everton Hill. The city's urban area runs directly into Bootle, Crosby and Maghull in south Sefton to the north, and Kirkby, Huyton, Prescot and Halewood in Knowsley to the east. It faces Wallasey and Birkenhead across the River Mersey to the west.

Inhabitants of Liverpool are referred to as Liverpudlians but are also known as "Scousers", in reference to the local meal known as 'scouse', a form of stew. The word 'scouse' is probably a contraction of 'lobscouse'. If that is the case, then the Lancashire stew known as 'lobby' may well also have the same roots. The word scouse has also become synonymous with the Liverpool accent and dialect.In 2007 the city celebrated its 800th anniversary, and in 2008 it holds the European Capital of Culture title together with Stavanger, Norway.

 King John's letters patent of 1207 announced the foundation of the borough of Liverpool, but by the middle of the 16th century the population was still only around 500. In the 17th century there was slow progress in trade and population growth. Battles for the town were waged during the English Civil War, including an eighteen-day siege in 1644. In 1699 Liverpool was made a parish by Act of Parliament, that same year its first slave ship, Liverpool Merchant, set sail for Africa. As trade from the West Indies surpassed that of Ireland and Europe, and as the River Dee silted up, Liverpool began to grow. The first wet dock in Britain was built in Liverpool in 1715. Substantial profits from the slave trade helped the town to prosper and rapidly grow. By the close of the century Liverpool controlled over 41% of Europe's and 80% of Britain's slave commerce.

By the start of the 19th century, 40% of the world's trade was passing through Liverpool and the construction of major buildings reflected this wealth. In 1830, Liverpool and Manchester became the first cities to have an intercity rail link, through the Liverpool and Manchester Railway. The population continued to rise rapidly, especially during the 1840s when Irish migrants began arriving by the hundreds of thousands as a result of the Great Famine. By 1851, approximately 25% of the city's population was Irish-born. During the first part of the 20th century, Liverpool was drawing immigrants from across Europe.

The Housing Act 1919 resulted in mass council housing building across Liverpool during the 1920s and 1930s. Thousands of families were rehoused from decrepit inner-city slums to well-equipped new homes on suburban housing estates which offered a far higher standard of living. A great deal of private houses were also built during this era. The process continued after the Second World War, with many more new housing estates being built in suburban areas, while some of the older inner city areas where also redeveloped for new homes.

The population of Liverpool peaked in the 1931 census, which reported 855,688 inhabitants. This had declined to 610,114 by 1961, and decreased further to 439,476 in the 2001 census.

During World War II there were 80 air-raids on Merseyside, killing 2500 people and causing damage to almost half the homes in the metropolitan area. Since 1952 Liverpool has been twinned with Cologne, Germany, a city which also shared the horrific experience of excessive aerial bombing. Significant rebuilding followed the war, including massive housing estates and the Seaforth Dock, the largest dock project in Britain.

In early August 1947 there was an anti-semitic pogrom in Liverpool which lasted several days, leading to damage to hundreds of Jewish owned properties. After a few days The Times reported that attacks on Jews were now taking place in daylight as well as at night. The Mayor of Liverpool appealed for calm, claiming that "not only property owned by Jews is being damaged..." (The Times page 4, August 5th 1947).

In the 1960s Liverpool became a centre of youth culture. The "Merseybeat" sound which became synonymous with The Beatles and fellow Liverpudlian rock bands of the era catapulted the city to the front of the popular music scene.

From the mid-1970s onwards Liverpool's docks and traditional manufacturing industries went into sharp decline. The advent of containerization meant that the city's docks became largely obsolete. In the early 1980s unemployment rates in Liverpool were among the highest in the UK. In recent years, Liverpool's economy has recovered and has experienced growth rates higher than the national average since the mid-nineties.

Previously a part of administrative Lancashire, created in 1888, and more recently as a county borough in itself, Liverpool became in 1974 a metropolitan district within the newly created metropolitan county of Merseyside, but still remains part of the ancient County Palatine of Lancashire for cultural and historic purposes.

At the end of the 20th century Liverpool was concentrating on regeneration, a process which still continues today, with the city winning the accolade of European Capital of Culture for 2008.

Capitalising on the popularity of the 1960s pop group The Beatles and other groups of the Merseybeat era, tourism has also become a significant factor in Liverpool's economy.

In 2004, property developer Grosvenor started the Paradise Project, a Â£920Â m development centered on Paradise Street, which will involve the most significant changes to Liverpool's city centre since the post-war reconstruction. Now known as Liverpool 1, parts are nearing completion.

2007 is the anniversary of the foundation of the city (1207), for which a number of events were planned.

Liverpool is internationally known as a cultural centre, with a particularly rich history in popular music (most notably The Beatles), performing and visual arts. In 2003, Liverpool was named European Capital of Culture for 2008. A series of cultural events during 2004-9 is planned, peaking in 2008.

During the late 1960s the city became well-known for the Liverpool poets, of whom Roger McGough and the late Adrian Henri are among the best known. The anthology The Mersey Sound, by Henri, McGough and Brian Patten, has sold over 500,000 copies since first being published in 1967.

In recent years The Dead Good Poets Society and particularly poets like David Bateman and Jim Bennett, both of whom at various times have been called the best in Liverpool, have ensured that the reputation of Liverpool based performance poetry has been maintained.

Liverpool has a strong history of performing arts which is reflected in the number of theatres in the city, including the Empire, Everyman, Neptune, Royal Court and Unity Theatres, and the Liverpool Playhouse. The Everyman and Playhouse run their own theatre company as does the Unity Theatre.

A flourishing orchestra, the Royal Liverpool Philharmonic Orchestra, performs in its own home, the Philharmonic Hall. The city also features a youth orchestra.

Since the 1960s, Liverpool has been famous for its thriving music scene.

[[Image:Superlambbanana.JPG|thumb|right|SuperLambBanana, a well-knownsculpture in the Albert Dock area, recently relocated to Tithebarn Street]]

 Liverpool has long had a reputation in the visual arts. Painter George Stubbs was born in the city in 1724. Pre-Raphaelites are among the important paintings in the Walker Art Gallery. Sudley House contains another major collection of pre 20th century art. Liverpool has more galleries and national museums than any other city in the United Kingdom apart from London. The Tate Liverpool gallery houses the modern art collection of the Tate in the north of England, and was until the opening of Tate Modern the largest exhibition space dedicated to modern art in the United Kingdom. The FACT centre hosts touring multimedia exhibitions.

The Liverpool Biennial is a festival of arts held (as the name implies) every two years. The festival generally runs from mid-September to late November and comprises three main sections; the International, The Independents and New Contemporaries although fringe events are timed to coincide. It was during this event in 2004 that Yoko Ono's work "My mother is beautiful" caused widespread public protest by exhibiting photographs of a naked woman's pubic area on the main shopping street. Despite protests the work remained in place.

The 2006 Biennial ran until mid-November: exhibitions could be found dotted around Liverpool City centre and included such features as the lions in front of St George's Hall being caged, St Luke's Church being filled with upturned boats, and an artist (Birgit R. Deubner) walking across downtown Liverpool wearing wings made from lead feathers (the "Icarus Project").

Liverpool is associated with a variety of sports, most notably football, but also a number of others.

Liverpool has two Premier League football clubs: Everton F.C. at Goodison Park and Liverpool F.C. at Anfield. Liverpool are statistically the most successful English football club of all-time, having won the league title 18 times, FA Cup seven times, Football League Cup seven times, European Cup/UEFA Champions League five times and UEFA Cup three times. Everton have also enjoyed spells of dominance, having won the league title nine times, FA Cup five times, and the European Cup Winners' Cup once. Their most recent success was the FA Cup in 1995. South Liverpool F.C. were once another successful professional side, however they have experienced a turbulent history and are currently in the Liverpool County Premier League. 

Liverpool is the only city to have staged top division football every season since the formation of the Football League in 1888, and both Liverpool and Everton have played in the top division every season since 1962.

Professional basketball is played in the city with the addition of Everton Tigers into the elite British Basketball League in 2007. The club is associated with Everton Football Club, and is part of the Toxteth Tigers youth development programme, which reaches over 1,500 young people every year. The Tigers will commence play in Britain's top league for the 2007-08 season, though their home venue has yet to be confirmed. Their closest professional rivals are the Chester Jets, based 18Â miles away in Chester.

County cricket is occasionally played in Liverpool, with Lancashire County Cricket Club typically playing one match every year at Liverpool Cricket Club, Aigburth.

Aintree Racecourse to the north of Liverpool in the adjacent borough of Sefton is home to the famous steeplechase, the Grand National, One of the most famous events in the international horse racing calendar, it is held in early April each year. In addition to horse-racing, Aintree has also hosted motor racing, including the British Grand Prix in the 1950s and 1960s.

Speedway racing was formerly staged at Stanley Stadium in Prescot Road from the 1920s until the late 1930s. It then reopened in 1949, with the Liverpool Chads taking part in the National League, until the track closed mid-season in 1953. A brief open season in 1959 was followed by the final season in 1960 when the Liverpool Pirates participated in the Provincial League. Peter Craven, the World Champion in 1955 and 1962, started out at Stanley Stadium before moving on to Belle Vue in Manchester.

A speedway track also operated in the mid-1930s at Seaforth Stadium.

Liverpool Harriers, who meet at Wavertree Athletics Centre, are one of five athletic clubs. Liverpool has a long history of boxing that has produced John Conteh, Alan Rudkin and Paul Hodkinson and hosts high level amateur boxing events. Park Road Gymnastics Centre provides training to a high level. The City of Liverpool Swimming Club has been National Speedo League Champions 8 out of the last 11 years. Liverpool Tennis Development Programme based at Wavertree Tennis Centre is one of the largest in the UK. Liverpool is also home to the Red Triangle Karate Club, which provided many of the 1990 squad that won the World Shotokan Championships in Sunderland. Luminaries include Sensei Keinosuke Enoeda, Sensei Frank Brennan, Sensei Omry Weiss, Sensei Dekel Kerer, Sensei Andy Sherry and Sensei Terry O'Neill, who is also famous for various acting roles.

Rugby league is played at amateur and student level within the city; the last professional team bearing the city's name was Liverpool City, which folded in the 1960s. Liverpool St Helens F.C. is one of the oldest rugby union teams in the UK.

Liverpool is one of three cities which still host the traditional sport of British Baseball and it hosts the annual England-Wales international match every two years, alternating with Cardiff and Newport. Liverpool Trojans are the oldest existing baseball club in the UK.

The Royal Liverpool Golf Club, situated in the nearby town of Hoylake on the Wirral Peninsula, has hosted The Open Championship on a number of occasions, most recently in 2006. It has also hosted the Walker Cup.

Liverpool now has its own Men's Lacrosse Club. Playing at Liverpool Cricket Club in Aigburth, South Liverpool, the team is mainly built around the University squad, but with University old boys and locals joining the team. Currently in Division 5.

Parkour/freerunning is a popular sport in liverpool. Two well-known traceurs from the city are Daniel Ilabaca and Ryan Doyle.

||-||-||-||-||}

Liverpool contains over 2,500 listed buildings (of which 26 are Grade I listed and 85 are Grade II* listed). It has been the beneficiary of high-minded public spirit since the late 18th century, largely with Dissenter impetus, resulting in more public sculpture than in any UK city aside from Westminster, more listed buildings than any city apart from London and, surprisingly, more Georgian houses than the city of Bath. Well-known architects are represented in Liverpool, including Peter Ellis, Harvey Lonsdale Elmes, John Foster, Sir Giles Gilbert Scott, Sir Edwin Lutyens, Sir Frederick Gibberd, and Norman Shaw.

In 2004 Liverpool's waterfront was declared as a UNESCO World Heritage site, reflecting the city's importance in the development of the world's trading system and dock technology.

The docks are central to Liverpool's history, with the best-known being Albert Dock: the first enclosed, non-combustible dock warehouse system in the world and is built in cast iron, brick and stone. It was designed by Jesse Hartley. Restored in the 1980s, the Albert Dock is the largest collection of Grade I listed buildings in Britain. Part of the old dock complex is now the home to the Merseyside Maritime Museum (an Anchor Point of ERIH, The European Route of Industrial Heritage), the International Slavery Museum, Museum of Liverpool Life and the Tate Liverpool. Other relics of the dock system include the Stanley Dock Tobacco Warehouse, which at the time of its construction in 1901, was the world's largest building in terms of area, and is still the world's largest brick-work building.

The Pier Head is the most famous image of Liverpool, the location of the Three Graces (a fairly recent phrase), three of Liverpool's most recognisable buildings. The first is the Royal Liver Building, built in the early 1900s and surmounted by two bronze domes with a Liver Bird (the symbol of Liverpool) on each. The second is the Cunard Building, the headquarters of the former Cunard shipping company. The third is the Port of Liverpool Building, the home of the former Mersey Docks and Harbour Board which regulated the city's docks. Kings Dock immediately to the South of the Albert dock is the site of the Liverpool Echo Arena and BT Convention Centre which officially opened on the 12 January 2008.

In front of these buildings at the waters edge are the memorials to the men of the merchant navy who sailed out of the port during both World Wars. Memorials to the British mariners, Norwegian, Dutch and to the thousands of Chinese seamen who manned Britain's ships cluster together here. Perhaps most interesting is the Chinese memorial to the men forcibly deported from the city after World War Two and to the families they left behind.

The thousands of migrants and sailors passing through Liverpool resulted in a religious diversity that is still apparent today. This is reflected in the equally diverse collection of religious buildings, and two Christian cathedrals.

The parish church of Liverpool is the Anglican Our Lady and St Nicholas, colloquially known as "the sailors church", which has existed near the waterfront since 1257. It regularly plays host to Catholic masses. Other notable churches include the Greek Orthodox Church of St Nicholas (built in the Byzantine style), and the Gustav Adolfus Kyrka (the Swedish Seamen's Church, reminiscent of Nordic styles).

Liverpool's wealth as a port city enabled the construction of two enormous cathedrals, both dating from the 20th century. The Anglican Cathedral, designed by Sir Giles Gilbert Scott, has one of the longest naves, largest organs and heaviest and highest peals of bells in the world. The Roman Catholic Metropolitan Cathedral, colloquially known as "Paddy's Wigwam", on Mount Pleasant next to Liverpool Science Park was initially planned to be even larger. Of Sir Edwin Lutyens' original design, only the crypt was completed. The cathedral was eventually built to a simpler design by Sir Frederick Gibberd; while this is on a smaller scale than Lutyens' original design, it still manages to incorporate the largest panel of stained glass in the world. Appropriately enough, the road running between the two cathedrals is called Hope Street.

Liverpool contains synagogues, of which the Grade II* listed Moorish-revival Princes Road Synagogue is perhaps most notable. Liverpool has a thriving Jewish community with a further 2 Synagogues, one in the Greenbank Park area of L17 and a second in the Childwall district of the city where a significant Jewish community reside. Liverpool has had a Jewish community since the mid-18th century. The current Jewish population of Liverpool is around 3000.

Liverpool also has an increasing Hindu community, with a Mandir on Edge Lane; the Radha Krishna Hindu Temple from the Hindu Cultural Organisation based there. The current Hindu population in Liverpool is about 1147. In comparison, Manchester has nearly 3000.Liverpool also has the Guru Nanak Sikh Gurudwara in L15.

The city had one of the earliest mosques in Britain, founded in 1887 by William Abdullah Quilliam, a lawyer who had converted to Islam. This mosque, however, no longer exists. Plans have been ongoing to re-convert the building where the mosque once stood into a museum. Currently there are two mosques in Liverpool: the Al-Rahma mosque in the Toxteth area of the city and a mosque recently opened in the Mossley Hill district of the city.

The area around William Brown Street has been labeled the city's 'Cultural Quarter', owing to the presence of the William Brown Library, Walker Art Gallery and World Museum Liverpool, just three of Liverpool's neo-classical buildings. Nearby is St George's Hall, perhaps the most impressive of these neo-classical buildings. It was built to serve a variety of civic functions, including both as a concert hall and as the city's law courts. Its doors, inscribed "S.P.Q.L." (Latin senatus populusque Liverpudliensis - "the senate and people of Liverpool"), as well as its grand architecture proclaim the municipal pride and ambition of the city in the mid-nineteenth century. Also in this area are Wellington's Column and the Steble Fountain.

Liverpool's Town Hall dates from 1754 and has an interior considered beautiful. The city's stock exchange and financial district are set immediately behind this splendid building, and show how closely government and commerce were tied in the city's development.

The term ''Red Brick University'', applied to British universities dating from a similar period, was inspired by the University of Liverpool's Victoria Building, noted for its clock tower.

Some of Liverpool's landmarks are better known for their oddness rather than for their role. Williamson's tunnels are architecturally unique as being the largest underground folly in the world. The Philharmonic Dining Rooms are noteworthy for their ornate Victorian toilets, which have become a tourist attraction in their own right.

On Renshaw Street there is the new alternative shopping centre Grand Central Hall - which has not only fine external architecture but also has much to offer inside, such as the metalwork and ceiling decoration of the Ground floor and the fantastic domed ceiling of Roscoe Hall. Also in Roscoe Hall is the organ (although recent shop additions to the hall have obscured the view somewhat) which is a listed item itself.

The town of Liverpool is home to two professional football clubs, Everton and Liverpool. Liverpool is the only English city to have staged top division football every single season since the formation of the Football League in 1888, and both of the city's clubs play in high-capacity stadiums.

Liverpool have played at Anfield since 1892, when the club was formed to occupy the stadium following Everton's departure following a dispute with their landlord. Liverpool are still playing there 116 years later, although the ground has been completely rebuilt since the 1970s and only the Main Stand survives from before 1992. The Spion Kop (rebuilt as an all-seater stand in 1994/1995) was the most famous part of the ground, gaining cult status across the world due to the songs and celebrations of the many fans who packed onto its terraces. Anfield can now hold more than 45,000 spectators in comfort, and is a distinctive landmark in an area filled with smaller and older buildings.

Everton moved to Goodison Park in 1892 after a dispute with their landlord caused them to pull out of Anfield. The ground is situated at the far side of Stanley Park to Anfield. Goodison Park was the first major football stadium built in England. Molineux (Wolves' ground) had been opened three years earlier but was still relatively undeveloped. St. James's Park, Newcastle, opened in 1892, was little more than a field. Only Scotland had more advanced grounds. Rangers opened Ibrox in 1887, while Celtic Park was officially inaugurated at the same time as Goodison Park. Everton performed a miraculous transformation at Mere Green, spending up to Â£3000 on laying out the ground and erecting stands on three sides. For Â£552 Mr. Barton prepared the land at 4Â½d a square yard. Kelly Brothers of Walton built two uncovered stands each for 4,000 people, and a covered stand seating 3,000, at a total cost of Â£1,460. Outside, hoardings cost a further Â£150, gates and sheds cost Â£132 10s and 12 turnstiles added another Â£7 15s to the bill.The ground was immediately renamed Goodison Park and proudly opened on 24 August 1892, by Lord Kinnaird and Frederick Wall of the FA. But instead of a match the 12,000 crowd saw a short athletics meeting followed by a selection of music and a fireworks display. Everton's first game there was on 2 September 1892 when they beat Bolton 4-2. It now has the capacity for more than 40,000 spectators all-seated, but the last expansion took place in 1994 when a new goal-end stand gave the stadium an all-seater capacity. The Main Stand dates back to the 1970s, while the other two stands are refurbished pre-Second World War structures.

There are currently plans for both stadiums to be pulled down and for the teams to relocate. Liverpool have been considering a move to a new stadium in Stanley Park since 2000; seven years on work has started and the 60,000-seat stadium is expected to be ready by 2010.

Everton have been considering relocation since 1996, and in 2003 were forced to scrap plans for a 55,000-seat stadium at King's Dock due to financial reasons. The latest plan has been to move beyond Liverpool's council boundary to Kirkby, but this has proved controversial with some fans, as well as members of the local community. At one point there were plans for Everton to ground-share with Liverpool at the proposed new stadium in Stanley Park, but these were abandoned.

In Liverpool primary and secondary education is available in various forms supported by the state including secular, Church of England, Jewish, and Roman Catholic. Islamic education is available at primary level, but there is currently no secondary provision.One of Liverpool's important early schools was The Liverpool Blue Coat School; founded in 1708 as a charitable school.

The Liverpool Institute High School for Boys opened in 1825 closed in 1985, the building after extensive renovation and rebuilding for the Liverpool Institute of Performing Arts is no longer a secondary school. Prior to closure it had been an English grammar school for boys ages 11 to 18 with an excellent academic reputation built up over more than a century. Its list of scholarships and places at Oxford University and Cambridge University runs to some 300 names â€?in addition to distinctions gained at Liverpool University and at many other prominent British universities like . The school was a true measure of Liverpool's intellectual capital and its old boys could and can be found in later life in many fields of professional distinction including: law, the Church, armed forces, politics, academia, government and colonial administration as well as in trade & commerce. The Liverpool Institute High School for Girls also closed in 1985.

The Liverpool Blue Coat School is the top-performing school in the city all down to  attaining 100% 5 or more A*-C grades at GCSE resulting in the 30th best GCSE results in the country and an average point score per student of 1087.4 in A/AS levels. Other notable schools include Liverpool College founded in 1840 Merchant Taylors' School founded in 1620. Another of Liverpool's notable senior schools is Auckland College situated in Aigburth Rd. Historic grammar schools, such as the Liverpool Institute High School & Liverpool Collegiate, closed in the 1980s are still remembered as centres of academic excellence. Bellerive Catholic College is the city's top performing non selective school, based upon GCSE results in 2007.

Liverpool has three universities: the University of Liverpool, Liverpool John Moores University and Liverpool Hope University. Edge Hill University, originally founded as a teacher-training college in the Edge Hill district of Liverpool, is now located in Ormskirk in South-West Lancashire.

The University of Liverpool was established in 1881 as University College Liverpool. In 1884, became part of the federal Victoria University. Following a Royal Charter and Act of Parliament in 1903, it became an independent university, the University of Liverpool, with the right to confer its own degrees.

Liverpool Hope University, founded in 1844, is situated on both sides of Taggart Avenue in Childwall and a second Campus in the City Centre (The Cornerstone). Hope is quickly making a name for itself within the Liberal Arts, the University has also enjoyed successes in terms of high graduate employability, campus development, and a substantial increase in student applications from outside of the City.

The Liverpool School of Tropical Medicine, founded to address some of the problems created by trade, continues today as a in the University of Liverpool and is one of only two institutions internationally that house the de facto standard anti-venom repository.

Liverpool John Moores University was previously a Taylor, and gained status in 1992. It is named in honour of Sir John Moores, one of the founders of the Littlewoods football pools and retail group, who was a major benefactor. The institution was previously owned and run by Liverpool City Council.

The city has one further education college, Liverpool Community College. The college will recruit around 21,000 students in the academic year 2006/07.

There are two Jewish schools in Liverpool, both belonging to the King David Foundation. King David School, Liverpool is the High School, located on Childwall Road, close to Childwall Comprehensive and Childwall Fiveways. The King David Primary School is situated at Beauclair Drive. There is also a King David Kindergarten, featured in the community centre of Harold House. These schools are all run by the King David Foundation based in Harold House in Childwall; conveniently next door to the Childwall Synagogue

There are three tunnels under the River Mersey: one railway tunnel, the Mersey Railway Tunnel; and two road tunnels, Queensway Tunnel and Kingsway Tunnel.

The Mersey Ferry continues to provide an important link between Liverpool and the Wirral, as well as a tourist attraction. Made famous by the song Ferry Cross the Mersey by Gerry and the Pacemakers, the song is now played on the ferryboats themselves every time they prepare to dock at Liverpool after a tourist cruise.

The Mersey is crossed upstream from Liverpool at Runcorn and Widnes, by the Silver Jubilee Bridge (usually known simply as the "Runcorn Bridge") and the Runcorn Railway Bridge.

Built between 1770 and 1816 the Leeds and Liverpool Canal links Liverpool and the Mersey to Leeds and the River Aire. Its terminus had been at Old Hall Street, Pall Mall, Chisenhale Street, but that section now ends at Eldonian Village. A flight of locks just north of there takes the canal down to Stanley Dock, famous for the Tobacco Warehouse, and on to the main dock system.

A new link across the front of the Pier Head buildings will link the northern docks to the Albert Dock is presently under construction, with the plan being to open it during Liverpool's Capital of Culture Year of 2008.

Opened in the 1930s, Liverpool Airport, is situated near Speke in the south of the city. It was renamed Liverpool John Lennon Airport in 2001, in honour of the late Beatle John Lennon. The airport's logo consists of a sketch that John Lennon had drawn of himself, and the words "Above us only sky", lyrics from his song Imagine. The sensitivity surrounding the airport's name change meant that the logo had to be designed in secret before it could be unveiled by John Lennon's widow Yoko Ono. The airport was the starting point for Beatles tours in the sixties, and images of the boys boarding planes there were seen throughout the world. In 2006 the airport handled nearly 5 million passengers and now serves 64 destinations, including the key European cities of Amsterdam, Barcelona, Berlin, Dublin, Geneva, Madrid, Paris, and Rome. New routes to New York and Toronto in summer 2007 were withdrawn towards the end of the year, as was the route to London (City) Airport.

In 2002, 716,000 passengers used the Port of Liverpool, with the Isle of Man and Ireland being the two most important passenger routes, goods trade which was very low in the past decades, is growing up now.

Liverpool is served by the Merseyrail urban rail network. The sections in the city centre are mostly underground. It has three lines: the Northern Line, which runs to Southport, Ormskirk, Kirkby and Hunts Cross; the Wirral Line, which runs through the Mersey Railway Tunnel and has branches to New Brighton, West Kirby, Chester and Ellesmere Port; and the City Line, only from Lime Street, for St Helens,Wigan, preston, Warrington and Manchester.

The city's main railway station for longer-distance services is Lime Street station, one of the most famous train stations in Britain . Trains operate to destinations including London in 2 hours 1/2 with Pendolino trains, Birmingham, Manchester, Preston, Leeds, Scarborough, Sheffield, Nottingham and Norwich. Full timetable details can be found at National Rail website

The London line was one of the first electrified in Britain with wire (with Manchester and Glasgow).

Liverpool had been home to the first electrically powered overhead railway in the world. Known as the Liverpool Overhead Railway or (Dockers Umbrella) it opened on February 4, 1893 with an eventual total of 14 stations. The line suffered extensive damages during the second world war and was eventually closed down on December 30, 1956 with considerable protest. The tunnel portal in Dingle is one of the only surviving signs of the railway's existence as the iron bridges were removed for scrap.

Long distance coach services arrive at and depart from the Norton Street Coach Station. Local buses serve the whole of the city and its surrounding areas. The two principal termini for local buses are Queen Square Bus Station (located near Lime Street railway station) for services north and east of the city, and Paradise Street Interchange (located near the Albert Dock) for services to the south and east. Cross-river services to the Wirral use roadside terminus points in Castle Street and Sir Thomas Street.

Historically, Liverpool had an extensive tram network; however, this was dismantled in the 1950s. Other railway lines, such as the Canada Dock Branch from Edge Hill to Kirkdale, no longer see passenger services, or have been removed completely, such as the North Liverpool Extension Line.

In 2001, a plan to build new a light rail system, Merseytram was developed. After central government insisted on additional guarantees prior to the release of previously committed funds, it was cancelled in November 2005. However, it is to be included in the transport plan from 2006-2011, as it is deemed to be an important part of Liverpool's development.

Many famous names have been associated with Liverpool; see .

Liverpool has also played a large part in UK (and sometimes world) Pop Music culture since the 1960s. For a list of some noteworthy groups from the area, consult the list of famous bands from Liverpool. The most popular group from Liverpool is The Beatles.

The Wall of Fame is located opposite the famous Cavern Club, near the original one where bricks are engraved with the name of bands and musicians who have played at the Cavern Club.

Liverpool has also been home to numerous football stars. Robbie Fowler, Steven Gerrard, Jamie Carragher, Phil Thompson, Mick Quinn, Peter Reid, Wayne Rooney, Tommy Smith and Steve McManaman are just some of the many footballers to have been born in the city.

Ian Broudie who fronted 1990's band The Lightning Seeds is also from Liverpool.

Television and film personalities born in Liverpool include: stage and film actor Rex Harrison, renowned comedian Ken Dodd, Singer/TV personality Cilla Black, BAFTA and Golden Globe nominee Cathy Tyson (for cult movie Mona Lisa (film) and Band of Gold (TV series)), 2 times BAFTA award-nominee Lesley Sharp, actor, (Shaun of the Dead) Peter Serafinowicz, anarchic comedian/author Alexei Sayle (star of The Young Ones (TV series)), Margi Clarke (star of cult movie Letter to Brezhnev), John Gregson (star of Treasure Island (1950 film), The Treasure of Monte Cristo and Gideon's Way), Olivier award-winning and 2 times BAFTA nominee Alison Steadman, 3 times BAFTA award-nominee Leonard Rossiter (Star of , Oliver! (film) and TV show Rising Damp), Actor Craig Charles (star of TV show Red Dwarf, Robot Wars and Coronation Street), 2 times BAFTA nominee Tom Bell (actor) (starring in Prime Suspect and The Krays (film)), the McGann brothers (Paul, Joe, Stephen and Mark), David Yip (star of Indiana Jones and the Temple of Doom and James Bond movie A View to a Kill) and 2 times Golden Globe nominee Tom Baker and Elisabeth Sladen (both of Doctor Who fame) Also Kim Cattrall of Sex and the City was born in Wavertree, a Liverpool suburb.

Famous writers such as, Academy Award and BAFTA nominee playwright Willy Russell (author of Blood Brothers (musical), Shirley Valentine, Our Day Out and Educating Rita), Brian Jacques (author of the Redwall and Castaways of the Flying Dutchman), award-winning horror author/director/artist Clive Barker ( mostly credited for Rawhead Rex (film), Candyman (film) and Hellraiser fame) and BAFTA award-winning scriptwriter Jimmy McGovern (author of Cracker (UK TV series), Hillsborough (a dramatised reconstruction of the events of the 1989 Hillsborough disaster) and The Street (TV series)) are from Liverpool.

William Gladstone, Prime Minister of the United Kingdom on four separate occasions, was born in Liverpool and lived there till the age of 11.



The ITV region which covers Liverpool is ITV Granada. In 2006, the Television company opened a new newsroom in the Royal Liver Building. Granada's regional news broadcasts were produced at the Albert Dock News Centre during the 1980s and 1990s. The BBC also opened a new newsroom on Hanover Street in 2006. But with both broadcasters based in Manchester, the arrangement is sometimes controversial, with Manchester's perceived influence over the region's media.

ITV's daily magazine programme ''This Morning'' was famously broadcast from studios at Albert Dock until 1996, when production was moved to London. Granada's short-lived shopping channel "Shop!" was also produced in Liverpool until it was axed in 2002.

Liverpool is the home of the TV production company Lime Pictures, formerly Mersey Television, which produced the now-defunct soap opera, Brookside, and currently produces Hollyoaks for Channel 4 and ''Grange Hill'' for the BBC. Lime Pictures is owned by All3Media. These programmes are regularly filmed in and around the Childwall area.

The city fares better with regards to other media. The city has two daily newspapers: the morning Daily Post and the evening Echo, both published by the same company, the Trinity Mirror group. The Daily Post, especially, serves a wider area, including north Wales. The UK's first online only weekly newspaper called Southport Reporter (Southport & Mersey Reporter), is also one of the many other news outlets that covers the city. Radio stations include BBC Radio Merseyside, Juice FM, KCR 106.7 FM and Radio City 96.7 as well as Magic 1548. The last two are both based in St. John's Beacon which, along with the two cathedrals, dominates the city's skyline. The independent media organisation Indymedia also covers Liverpool, while 'Nerve' magazine publishes articles and reviews of cultural events.

Liverpool has also featured in films; see List of films set in Liverpool for some of them.

Liverpool will be the host city for the 2008 MTV Europe Music Awards.

This is a chart of trend of regional gross value added of Liverpool at current basic prices published (pp.240-253) by the Office for National Statistics with figures in millions of pounds sterling.

The economy of Liverpool is beginning to recover from its long, post-World War II decline. Between 1995 and 2001 GVA per head grew at 6.3% annum. This compared with 5.8% for inner London and 5.7% for Bristol. The rate of job growth was 9.2% compared with a national average of 4.9% for the same period, 1998-2002. However, Liverpool is still comparatively poor; a 2001 report by CACI showed that Liverpool still had four of the ten poorest postcode districts in the country, and almost 30% of people aged 65 or over are without central heating.

Like the rest of the United Kingdom the city has seen a large growth in the service sector, both public and private. Government offices include parts of the National Health Service, Revenue and Customs and Home Office agencies such as the Criminal Records Bureau and the Identity and Passport Service, formerly the UK Passport Agency. Private sector service industries have invested in Liverpool too with major call centres opening of late. The activities of the port have left the site with a communications infrastructure that had for a long time exceeded requirements.

Growth in the areas of New Media has been helped by the existence of a relatively large computer game development community. Sony based one of only a handful of European PlayStation research and development centres in Wavertree, after buying out noted software publisher Psygnosis. Indeed, according to a 2006 issue of industry magazine 'Edge' (issue 162), the first professional quality PlayStation software developer's kits were largely programmed by Sony's Liverpool 'studio' â€?the console has since become one of the World's most successful consumer products ever.

Tourism is a major factor in the economy and will be of increasing importance in the run up to the Liverpool's year as European Capital of Culture. This has led to a great increase in the provision of high quality services such as hotels, restaurants and clubs. The buildings of Liverpool not only attract tourists but also film makers, who regularly use Liverpool to double for cities around the world and making it the second most filmed city in the UK.

Car-manufacturing also takes place in the city at the Halewood plant where the Jaguar X-Type and Land Rover Freelander models are assembled.

The owner of Liverpool's port and airport, Peel Holdings, announced on March 6 2007 that is had plans to redevelop the city's northern dock area with a scheme entitled Liverpool Waters, which may see the creation of 17,000 jobs and Â£5.5bn invested in the vicinity over a 50 year period.

Liverpool's main shopping area is Church Street, lying between Bold Street to the East and Lord Street to the West.

Like many cities, Liverpool participates in international town twinning schemes. It has five twin towns:

Furthermore the city has "friendship links" with other cities, which are less formal than twinning arrangements. These are:

In addition, there are links with New York, USA (which has been granted the Freedom of the City of Liverpool); Riga, Latvia; and Stavanger, Norway.



Districts of Liverpool include:

Other areas not within the City of Liverpool borough but form part of the city include: Aintree, Blundellsands, Bootle, Bowring Park, Crosby, Ford, Hale, Halewood, Huyton, Kirkby, Knowsley Village, Litherland, Lydiate, Maghull, Melling, Netherton, Page Moss, Prescot, Rainhill, Roby, Seaforth, Sefton Village, Stockbridge Village, Tarbock, Thornton, Waterloo, Whiston.

Liverpool City Council as of May 2007 is controlled by the Liberal Democrats with 51 seats to Labour's 35. The Green Party also hold one seat. Liverpool has been under Lib Dem control for over 9 years.City council wards of Liverpool include:

Liverpool has five parliamentary constituencies: Liverpool Garston, Liverpool Riverside, Liverpool Walton, Liverpool Wavertree and Liverpool West Derby. At the 2005 general election, these were held by the Labour Party, and are represented by Maria Eagle, Louise Ellman, Jane Kennedy, Peter Kilfoyle and Robert Wareing respectively. Liberal Democrat candidates finished second in every Liverpool seat.











The Caspian Sea is the largest enclosed body of water on Earth by area, variously classed as the world's largest lake or a full-fledged sea. It has a surface area of 371,000 square kilometers (143,244Â sqÂ mi) and a volume of 78,200 cubic kilometers (18,761Â cuÂ mi).

The worldâ€™s first offshore wells and machine-drilled wells were made in Bibi-Heybat Bay, near Baku, Azerbaijan. In 1873 exploration and development of oil began in some of the largest fields known to exist in the world at that time on the Absheron peninsula near the villages of Balakhanli, Sabunchi, Ramana and Bibi Heybat. Total recoverable reserves at that time number more than 500 million tons. By 1900 Baku had more than 3,000 oil wells, 2,000 of which were producing at industrial levels. By the end of the 19th Baku's fame as the "Black Gold Capital" was spreading throughout of the world, causing many skilled workers and specialists to flock to the city

By the turn of the 20th century, Baku was the global center for the international oil industry. In 1920, when the Bolsheviks captured Azerbaijan, all private property - including oil wells and factories - was confiscated. After that, the republic's entire oil industry was directed towards the purposes of the Soviet Union. By 1941 Azerbaijan was producing a record 23.5 million tons of oil, and the Baku region supplied nearly 72% of all oil extracted in the entire USSR.

In 1994 the "Contract of the Century" was signed, signaling the start of major international development of the Baku oil fields. The Baku-Tbilisi-Ceyhan pipeline, a major pipeline allowing Azerbaijan oil to flow straight to the Turkish Mediterranean port of Ceyhan, opened in 2006.

The oil in the Caspian basin is estimated to be worth over US $12 trillion. The sudden collapse of the USSR and subsequent opening of the region has led to an intense investment and development scramble by international oil companies. In 1998 Dick Cheney commented that "I can't think of a time when we've had a region emerge as suddenly to become as strategically significant as the Caspian."

A key problem to further development in the region is the status of the Caspian Sea and the establishment of the water boundaries among the five littoral states (see below). The current disputes along Azerbaijan's maritime borders with Turkmenistan and Iran could potentially effect future development plans.

Much controversy currently exists over the proposed Trans-Caspian oil and gas pipelines. These projects would allow western markets easier access to Kazakh oil, and potentially Uzbek and Turkmen gas as well. The United States has given its support for the pipelines. Russia officially opposes the project on environmental grounds. Analysts note that the pipelines would bypass Russia completely, thereby denying the country valuable transit fees, as well as destroying its current monopoly on westward-bound hydrocarbon exports from the region.  Recently both Kazakhstan and Turkmenistan have expressed their support for the Trans-Caspian Pipeline.

Although the Caspian Sea is endorheic, its main tributary, Volga, is connected by important shipping canals with the Don River (and thus the Black Sea) and with the Baltic Sea, with branch canals to Northern Dvina and to the White Sea.

Another Caspian tributary, the Kuma River, is connected by an irrigation canal with the Don basin as well.

The Main Turkmen Canal, construction of which was started in 1950, would run from Nukus on the Amu-Darya to Krasnovodsk on the Caspian Sea. It would be used not only for irrigation, but also for shipping, connecting the Amu-Darya and the Aral Sea with the Caspian. The project was abandoned soon after the death of Joseph Stalin, in favor of the Qaraqum Canal, which runs on a more southerly route and does not reach the Caspian.

Since the 1930s through the 1980s, the projects for a Pechora-Kama Canal were widely discussed, and some construction experiments using nuclear explosions were conducted in 1971. For this project, shipping was a secondary consideration; the main goal was to redirect some of the water of the Pechora River (which flows into the Arctic Ocean) via the Kama into the Volga. The goals were both irrigation and stabilizing the water level in the Caspian, which was thought to be falling dangerously fast at the time.

In June 2007, in order to boost his oil-rich country's access to markets, Kazakhstan's President Nursultan Nazarbaev proposed a 700km link between the Caspian and Black seas. It is hoped that the "Eurasia Canal" would transform the landlocked Kazakhstan and other Central Asian countries into maritime states, enabling them to significantly increase trade volume. While the canal would traverse Russian territory, it would benefit Kazakhstan through its Caspian Sea ports.The most likely route for the canal, the officials at the Committee on Water Resources at Kazakhstan's Agriculture Ministry say, would follow the Kuma-Manych Depression, where currently a chain of rivers and lakes is already connected by an irrigation canal (Kuma-Manych Canal). Upgrading the Volga-Don Canal would be another option.

Negotiations related to the demarcation of the Caspian Sea have been going on for nearly a decade now among the littoral states bordering the Caspian - Azerbaijan, Russia, Kazakhstan, Turkmenistan and Iran.The status of the Caspian Sea is the key problem. There are three major issues regulated by the Caspian Sea status: access to mineral resources (oil and natural gas), access for fishing and access to international waters (through Russia's Volga river and the canals connecting it to the Black Sea and Baltic Sea). Access to the Volga River is particularly important for the landlocked states of Azerbaijan, Kazakhstan and Turkmenistan. This issue is of course sensitive to Russia, because this potential traffic will move through its territory (albeit onto the inland waterways). If a body of water is labeled as Sea then there would be some precedents and international treaties obliging the granting of access permits to foreign vessels. If a body of water is labeled merely as lake then there are no such obligations. Environmental issues are also somewhat connected to the status and borders issue. 

It should be mentioned that Russia got the bulk of the former Soviet Caspian military fleet (and also currently has the most powerful military presence in the Caspian Sea). Some assets were assigned to Azerbaijan. Kazakhstan and especially Turkmenistan got a very small share because they lack major port cities.

Kazakhstan, Azerbaijan and Turkmenistan announced that they do not consider themselves parties to this treaty.

Russia, Kazakhstan and Azerbaijan have agreed to a solution about their sectors. There are no problems between Kazakhstan and Turkmenistan, but the latter is not actively participating, so there is no agreement either. Azerbaijan is at odds with Iran over some oil fields that the both states claim. There have been occasions where Iranian patrol boats have opened fire at vessels sent by Azerbaijan for exploration into the disputed region. There are similar tensions between Azerbaijan and Turkmenistan (the latter claims that the former has pumped more oil than agreed from a field, recognized by both parties as shared). Less acute are the issues between Turkmenistan and Iran. Regardless, the southern part of the sea remains disputed.

After Russia adopted the median line sectoral division and the three treaties already signed between some littoral states this is looking like the realistic method for regulating the Caspian borders. The Russian sector is fully defined. The Kazakhstan sector is not fully defined, but is not disputed either. Azerbaijan's, Turkmenistan's and Iran's sectors are not fully defined. It is not clear if the issue of Volga-access to vessels from Azerbaijan and Kazakhstan is covered by their agreements with Russia and also what the conditions are for Volga-access for vessels from Turkmenistan and Iran.

The Caspian littoral States meeting in 2007 signed an agreement that stops any ship not flying the national flag of a littoral state on Caspian waters.

The Caspian has characteristics common to both seas and lakes. It is often listed as the world's largest lake, though it is not a freshwater lake.

The Volga River (about 80% of the inflow) and the Ural River discharge into the Caspian Sea, but it is endorheic, i.e. there is no natural outflow (other than by evaporation). Thus the Caspian ecosystem is a closed basin, with its own sea level history that is independent of the eustatic level of the world's oceans. The Caspian became landlocked about 5.5 million years ago. The level of the Caspian has fallen and risen, often rapidly, many times over the centuries. Some Russian historians claim that a medieval rising of the Caspian caused the coastal towns of Khazaria, such as Atil, to flood. In 2004, the water level was -28 metres, or 28 metres (92 feet) below sea level.

Over the centuries, Caspian Sea levels have changed in synchronicity with the estimated discharge of the Volga, which in turn depends on rainfall levels in its vast catchment basin. Precipitation is related to variations in the amount of North Atlantic depressions that reach the interior, and they in turn are affected by cycles of the North Atlantic Oscillation. Thus levels in the Caspian sea relate to atmospheric conditions in the North Atlantic thousands of miles to the north and west. These factors make the Caspian Sea a valuable place to study the causes and effects of global climate change.

The last short-term sea-level cycle started with a sea-level fall of 3 m from 1929 to 1977, followed by a rise of 3 m from 1977 until 1995. Since then smaller oscillations have taken place.

Several scheduled ferry services operate on the Caspian Sea, including:









The Kuiper belt (, to rhyme with "viper"), sometimes called the Edgeworth-Kuiper belt, is a region of the Solar System beyond the planets extending from the orbit of Neptune (at 30 AU) to approximately 55 AU from the Sun. It is similar to the asteroid belt, although it is far larger; 20 times as wide and 20â€?00 times as massive. Like the asteroid belt, it consists mainly of small bodies (remnants from the Solar System's formation) and at least one dwarf planet â€?Pluto. But while the asteroid belt is composed primarily of rock and metal, the Kuiper belt objects are composed largely of frozen volatiles (dubbed "ices"), such as methane, ammonia and water. 

Since the first was discovered in 1992, the number of known Kuiper belt objects (KBOs) has increased to over a thousand, and more than 70,000 KBOs over 100 km in diameter are believed to reside there. The Kuiper belt is believed to be the main repository for periodic comets, those with orbits lasting less than 200 years. The centaurs, comet-like bodies that orbit among the gas giants, are also believed to originate there, as are the scattered disc objects such as Erisâ€”KBO-like bodies with extremely large orbits that take them as far as 100 AU from the Sun. Neptune's moon Triton is believed to be a captured KBO. Pluto, a dwarf planet, is the largest known member of the Kuiper belt. Originally considered a planet, it has many physical properties in common with the objects of the Kuiper belt, and has been known since the early 1990s to share its orbit with a number of similarly sized KBOs, now called Plutinos.

The Kuiper belt should not be confused with the hypothesized Oort cloud, which is a thousand times more distant. The objects within the Kuiper belt, together with the members of the scattered disc and any potential Hills cloud or Oort cloud objects, are collectively referred to as trans-Neptunian objects (TNOs).

Since the discovery of Pluto, many have speculated that it might not be alone. The region now called the Kuiper belt had been hypothesized in various forms for decades. It was only in 1992 that the first direct evidence for its existence was found. The number and variety of prior speculations on the nature of the Kuiper belt have led to continued uncertainty as to who deserves credit for first proposing it.

The first astronomer to suggest the existence of a trans-Neptunian population was Frederick C. Leonard. In 1930, soon after Pluto's discovery, he pondered whether it was "not likely that in Pluto there has come to light the first of a series of ultra-Neptunian bodies, the remaining members of which still await discovery but which are destined eventually to be detected". 

In 1943, in the Journal of the British Astronomical Association, Kenneth Edgeworth hypothesised that, in the region beyond Neptune, the material within the primordial solar nebula was too widely spaced to condense into planets, and so rather condensed into a myriad of smaller bodies. From this he concluded that â€œthe outer region of the solar system, beyond the orbits of the planets, is occupied by a very large number of comparatively small bodies" and that, from time to time, one of their number "wanders from its own sphere and appears as an occasional visitor to the inner solar system,â€?becoming what we call a comet. 

In 1951, in an article for the journal Astrophysics, Gerard Kuiper speculated on a similar disc having formed early in the Solar System's evolution, however, he did not believe that such a belt still existed today. Kuiper was operating on the assumption common in his time, that Pluto was the size of the Earth, and had therefore scattered these bodies out toward the Oort cloud or out of the Solar System. By Kuiper's formulation, there would not be a Kuiper belt where we now see it. 

The hypothesis took many other forms in the following decades: in 1962, physicist Al G.W. Cameron postulated the existence of â€œa tremendous mass of small material on the outskirts of the solar system,â€?while in 1964, Fred Whipple, who popularised the famous "dirty snowball" hypothesis for cometary structure, thought that a "comet belt" might be massive enough to cause the purported discrepancies in the orbit of Uranus that had sparked the search for Planet X, or at the very least, to affect the orbits of known comets. Observation, however, ruled out this hypothesis. 

In 1977, Charles Kowal discovered 2060 Chiron, an icy planetoid with an orbit between Saturn and Uranus. He used a blink comparator; the same device that had allowed Clyde Tombaugh to discover Pluto nearly 50 years before. In 1992, another object 5145 Pholus, was discovered in a similar orbit. Today, an entire population of comet-like bodies, the centaurs, is known to exist in the region between Jupiter and Neptune. The centaurs' orbits are unstable over periods longer than roughly 100 million years, a relatively short span when compared to the age of the Solar System. From the time of Chiron's discovery, astronomers speculated that they therefore must be frequently replenished by some outer reservoir. 

Further evidence for the belt's existence later emerged from the study of comets. That comets have finite lifespans has been known for some time. As they approach the Sun, its heat causes their volatile surfaces to sublimate into space, eating them gradually away. In order to still be visible over the age of the Solar System, they must be frequently replenished. One such area of replenishment is the Oort cloud; the spherical swarm of comets extending beyond 50,000 AU from the Sun first hypothesised by astronomer Jan Oort in 1950. It is believed to be the point of origin for long period comets, those, like Hale-Bopp, with orbits lasting thousands of years. 

There is however another comet population, known as short period or periodic comets; those with orbits lasting less than 200 years. By the 1970s, the rate at which short-period comets were being discovered was becoming increasingly inconsistent with them having emerged solely from the Oort cloud. For an Oort cloud object to become a short-period comet, it would first have to be captured by the giant planets. In 1980, in the monthly notice of the Royal Astronomical Society, Julio Fernandez stated that for every short period comet to be sent into the inner solar system from the Oort cloud, 600 would have to be ejected into interstellar space. He speculated that a comet belt from between 35 and 50 AU would be required to account for the observed number of comets. Following up on Fernandez's work, in 1988 the Canadian team of Martin Duncan, Tom Quinn and Scott Tremaine ran a number of computer simulations to determine if all observed comets could have arrived from the Oort cloud. They found that the Oort cloud could not account for short-period comets, particularly as short-period comets are clustered near the plane of the Solar System, whereas Oort cloud comets tend to arrive from any point in the sky. With a belt as Fernandez described it added to the formulations, the simulations matched observations. Reportedly because the words "Kuiper" and "comet belt" appeared in the opening sentence of Fernandez's paper, Tremaine named this region the "Kuiper belt."

In 1987, astronomer David Jewitt, then at MIT, became increasingly puzzled by "the apparent emptiness of the outer Solar System." He encouraged then-graduate student Jane Luu to aid him in his endeavour to locate another object beyond Pluto's orbit, because, as he told her, "If we don't, nobody will." Using telescopes at the Kitt Peak National Observatory in Arizona and the Cerro Tololo Inter-American Observatory in Chile, Jewitt and Luu conducted their search in much the same way as Clyde Tombaugh and Charles Kowal had, with a blink comparator. Initially, examination of each pair of plates took about eight hours, but the process was sped up with the arrival of electronic Charge-coupled devices or CCDs, which, though their field of view was narrower, were not only more efficient at collecting light (they retained 90 percent of the light that hit them, rather than the ten percent achieved by photographs) but allowed the blinking process to be done virtually, on a computer screen. Today, CCDs form the basis for most astronomical detectors. In 1988, Jewitt moved to the Institute of Astronomy at the University of Hawaii. He was later joined by Jane Luu to work at the University of Hawaiiâ€™s 2.24 m telescope at Mauna Kea. Eventually, the field of view for CCDs had increased to 1024 by 1024 pixels, which allowed searches to be conducted far more rapidly. Finally, after five years of searching, on August 30, 1992, Jewitt and Luu announced the "Discovery of the candidate Kuiper belt object" ; Six months later, they discovered a second object in the region, 1993 FW.

Astronomers sometimes use alternative name Edgeworth-Kuiper belt to credit Edgeworth, and KBOs are occasionally referred to as EKOs. However, Brian Marsden claims neither deserve true credit; "Neither Edgeworth or Kuiper wrote about anything remotely like what we are now seeing, but Fred Whipple did." Conversely, David Jewitt comments that, "If anything . . . Fernandez most nearly deserves the credit for predicting the Kuiper Belt." The term trans-Neptunian object (TNO) is recommended for objects in the belt by several scientific groups because the term is less controversial than all others â€?it is not a synonym though, as TNOs include all objects orbiting the Sun at the outer edge of the solar system, not just those in the Kuiper belt.

The precise origins of the Kuiper belt and its complex structure are still unclear, and astronomers are awaiting the completion of the Pan-STARRS survey telescope, which should reveal many currently unknown KBOs, to determine more about this. 

The Kuiper belt is believed to consist of planetesimals; fragments from the original protoplanetary disc around the Sun that failed to fully coalesce into planets and instead formed into smaller bodies, the largest less than 3000 km in diameter. 

Modern computer simulations show the Kuiper belt to have been strongly influenced by Jupiter and Neptune, and also suggest that neither Uranus nor Neptune could have formed in situ beyond Saturn, as too little primordial matter existed at that range to produce objects of such high mass. Instead, these planets are believed to have formed closer to Jupiter, but to have been flung outwards during the course of the Solar System's early evolution. Work in 1984 by Fernandez and Ip suggests that exchange of angular momentum with the scattered objects can cause the planets to drift. Eventually, the orbits shifted to the point where Jupiter and Saturn existed in an exact 2:1 resonance; Jupiter orbited the Sun twice for every one Saturn orbit. The gravitational pull from such a resonance ultimately disrupted the orbits of Uranus and Neptune, causing them to switch places and for Neptune to travel outward into the proto-Kuiper belt, sending it into temporary chaos. As Neptune traveled outward, it excited and scattered many TNOs into higher and more eccentric orbits.

However, the present models still fail to account for many of the characteristics of the distribution and, quoting one of the scientific articles, the problems "continue to challenge analytical techniques and the fastest numerical modeling hardware and software".

At its fullest extent, including its outlying regions, the Kuiper belt stretches from roughly 30 to 55 AU. However, the main body of the belt is generally accepted to extend from the 2:3 resonance (see below) at 39.5 AU to the 1:2 resonance at roughly 48 AU. The Kuiper belt is quite thick, with the main concentration extending as much as ten degrees outside the ecliptic plane and a more diffuse distribution of objects extending several times farther. Overall it more resembles a torus or doughnut than a belt. Its mean position is inclined to the ecliptic by 1.86 degrees.

The presence of Neptune has a profound effect on the Kuiper belt's structure due to orbital resonances. Over a timescale comparable to the age of the Solar System, Neptune's gravity destabilises the orbits of any objects which happen to lie in certain regions, and either sends them into the inner Solar System or out into the Scattered disc or interstellar space. This causes the Kuiper belt to possess pronounced gaps in its current layout, similar to the Kirkwood gaps in the Asteroid belt. In the region between 40 and 42 AU, for instance, no objects can retain a stable orbit over such times, and any observed in that region must have migrated there relatively recently. 

Between ~42 ~48 AU, however, the gravitational influence of Neptune is negligible, and objects can exist with their orbits pretty much unmolested. This region is known as the classical Kuiper belt, and its members comprise roughly two thirds of KBOs observed to date. Because the first modern KBO discovered, 1992 QB1, is considered the prototype of this group, classical KBOs are often referred to as cubewanos ("Q-B-1-os").

The classical Kuiper belt appears to be a composite of two separate populations. The first, known as "dynamically cold" population, has orbits much like the planets; nearly circular, with an orbital eccentricity of less than 0.1, and with relatively low inclinations up to about 10Â° (they lie close to the plane of the Solar System rather than at an angle). The second, the "dynamically hot" population, has orbits much more inclined to the ecliptic, by up to 30Â°. The two populations have been named this way not because of any major difference in temperature, but from analogy to particles in a gas, which increase their relative velocity as they become heated up. The two populations not only possess different orbits, but different compositions; the cold population is markedly redder than the hot, suggesting it formed in a different region. The hot population is believed to have formed near Jupiter, and to have been ejected out by movements among the gas giants. The cold population, on the other hand, is believed to have formed more or less in its current position although it may also have been later swept outwards by Neptune during its migration.

When an object's orbital period is an exact ratio of Neptune's (a situation called a mean motion resonance), then it can become locked in a synchronised motion with Neptune and avoid being perturbed away if their relative alignments are appropriate. If, for instance, an object is in just the right kind of orbit so that it orbits the Sun two times for every three Neptune orbits, then whenever it returns to its original position, Neptune will always be half an orbit away from it, since it will have completed 1Â½ orbits in the same time. This is known as the 2:3 (or 3:2) resonance, and it corresponds to a characteristic semi-major axis of ~39.4AU. This 2:3 resonance is populated by about 200 known objects, including Pluto together with its moons. In recognition of this, the other members of this family are known as Plutinos. Many Plutinos, including Pluto, often have orbits which cross that of Neptune, though their resonance means they can never collide. Many others, such as 90482 Orcus and 28978 Ixion, are over half of Pluto's size. Plutinos have high orbital eccentricities, suggesting that they are not native to their current positions but were instead thrown haphazardly into their orbits by the migrating Neptune. The 1:2 resonance (whose objects complete half an orbit for each of Neptune's) corresponds to semi-major axes of ~47.7AU, and is sparsely populated. Its residents are sometimes referred to as twotinos. Minor resonances also exist at 3:4, 3:5, 4:7 and 2:5. Neptune possesses a number of trojan objects, which occupy its L4 and L5 points; gravitationally stable regions leading and trailing it in its orbit. Neptune trojans are often described as being in a 1:1 resonance with Neptune. Neptune trojans are remarkably stable in their orbits and are unlikely to have been captured by Neptune, but rather to have formed alongside it. 

Additionally, there is a relative absence of objects with semi-major axes below 39 AU which cannot apparently be explained by the present resonances. The currently accepted hypothesis for the cause of this is that as Neptune migrated outward, unstable orbital resonances moved gradually through this region, and thus any objects within it were swept up, or gravitationally ejected from it.

 The 1:2 resonance appears to be an edge beyond which few objects are known. It is not clear whether it is actually the outer edge of the Classical belt or just the beginning of a broad gap. Objects have been detected at the 2:5 resonance at roughly 55 AU, well outside the classical belt; however, predictions of a large number of bodies in classical orbits between these resonances have not been verified through observation.

Earlier models of the Kuiper belt had suggested that the number of large objects would increase by a factor of two beyond 50 AU; so this sudden drastic falloff, known as the "Kuiper cliff", was completely unexpected, and its cause, to date, is unknown. Bernstein and Trilling et al. have found evidence that the rapid decline in objects of 100Â km or more in radius beyond 50 AU is real, and not due to observational bias. Possible explanations include that material at that distance is too scarce or too scattered to accrete into large objects, or that subsequent processes removed or destroyed those which did form. Patryk Lykawka of Kobe University has claimed that the gravitational attraction of an unseen large planetary object, perhaps the size of Earth or Mars, might be responsible.



Studies of the Kuiper belt since its discovery have generally indicated that its members are primarily composed of ices; a mixture of light hydrocarbons (such as methane), ammonia, and water ice, a composition they share with comets. The temperature of the belt is only about 50K, so many compounds that would remain gaseous closer to the Sun are solid. 

Due to their small size and extreme distance from Earth, the chemical makeup of KBOs is very difficult to determine. The principal method by which astronomers determine the composition of a celestial object is spectroscopy. When an object's light is broken into its component colours, an image akin to a rainbow is formed. This image is called a spectrum. Different substances absorb light at different wavelengths, and when the spectrum for a specific object is unravelled, dark lines (called absorption lines) appear where the substances within it have absorbed that particular wavelength of light. Every element or compound has its own unique spectroscopic signature, and by reading an object's full spectral "fingerprint", astronomers can determine what it is made of.

Initially, such detailed analysis of KBOs was impossible, and so astronomers were only able to determine the most basic facts about their makeup, primarily their colour. These first data showed a broad range of colours among KBOs, ranging from neutral grey to deep red. This suggested that their surfaces were composed of a wide range of compounds, from dirty ices to hydrocarbons. This diversity was startling, as astronomers had expected KBOs to be uniformly dark, having lost most of their volatile ices to the effects of cosmic rays. Various solutions were suggested for this discrepancy, including resurfacing by impacts or outgassing. However, Jewitt and Luu's spectral analysis of the known Kuiper belt objects in 2001 found that the variation in colour was too extreme to be easily explained by random impacts.

Although to date most KBOs still appear spectrally featureless due to their faintness, there have been a number of successes in determining their composition. In 1996, Robert H. Brown et al obtained spectroscopic data on the KBO 1993 SC, revealing its surface composition to be markedly similar to that of Pluto, as well as Neptune's moon Triton, possessing large amounts of methane ice. 

Water ice has been detected in several KBOs, including 1996 TO66, 2000 EB173 and 2000 WR106. In 2004, Mike Brown et al determined the existence of crystalline water ice and ammonia hydrate on one of the largest known KBOs, 50000 Quaoar. Both of these substances would have been destroyed over the age of the solar system, suggesting that Quaoar had been recently resurfaced, either by internal tectonic activity or by meteorite impacts.

Despite its vast extent, the collective mass of the Kuiper belt is relatively low; estimated at roughly a tenth the mass of the Earth. Conversely, models of the Solar System's formation predict a collective mass for the Kuiper belt of 30 Earth masses. This missing >99% of the mass can hardly be dismissed, as it is required for the accretion of any KBOs larger than 100Â km in diameter. At the current low density, these objects simply should not exist. Moreover, the eccentricity and inclination of current orbits makes the encounters quite "violent," resulting in destruction rather than accretion. It appears that either the current residents of the Kuiper belt have been created closer to the Sun or some mechanism dispersed the original mass. Neptuneâ€™s influence is too weak to explain such a massive "vacuuming". While the question remains open, the conjectures vary from a passing star scenario to grinding of smaller objects, via collisions, into dust small enough to be affected by solar radiation.

Bright objects are rare compared with the dominant dim population, as expected from accretion models of origin, given that only some objects of a given size would have grown further. This relationship N(D), the population expressed as a function of the diameter, referred to as brightness slope, has been confirmed by observations. The slope is inversely proportional to some power of the diameter D. 

Less formally, there are for instance 8 (=2Â³) times more objects in 100â€?00Â km range than objects in 200â€?00Â km range. In other words, for every object with the diameter of 1000Â km there should be around 1000 (=10Â³) objects with diameter of 100Â km.

The law is expressed in this differential form rather than as a cumulative cubic relationship, because only the middle part of the slope can be measured; the law must break at smaller sizes, beyond the current measure.

Of course, only the magnitude is actually known, the size is inferred assuming albedo (not a safe assumption for larger objects)



Image:EightTNOs.png|thumb|300 px|The relative sizes of the largest trans-Neptunian objects as compared to Earth.

rect 646 1714 2142 1994 The Earth

circle 226 412 16 Dysnomiacircle 350 626 197 (136199) Eris

circle 1252 684 86 Charoncircle 1038 632 188 (134340) Pluto

circle 1786 614 142 (136472) 2005 FY9

circle 2438 616 155 (136108) 2003 EL61

circle 342 1305 137 (90377) Sedna

circle 1088 1305 114 (90482) Orcus

circle 1784 1305 97 (50000) Quaoar

circle 2420 1305 58 (20000) Varuna

desc none

Since the year 2000, a number of KBOs with diameters of between 500 and 1200 km (about half that of Pluto) have been discovered. 50000 Quaoar, a classical KBO discovered in 2002, is over 1200 km across.  (nicknamed "Easterbunny") and  (nicknamed "Santa"), both announced on 29 July 2005, are larger still. Other objects, such as 28978 Ixion (discovered in 2001) and 20000 Varuna (discovered in 2000) measure roughly 500 km across. 

The discovery of these large KBOs in similar orbits to Pluto led many to conclude that, bar its relative size, Pluto was not particularly different from other members of the Kuiper belt. Not only did these objects approach Pluto in size, but many also possessed satellites, and were of similar composition (methane and carbon monoxide have been found both on Pluto and on the largest KBOs). Ceres was considered a planet before the discovery of its fellow asteroids, and, based on this precedent, many astronomers concluded that Pluto should also be reclassified.

The issue was brought to a head by the discovery of Eris, an object in the scattered disc far beyond the Kuiper belt, that is now known to be 27 percent more massive than Pluto. In response, the International Astronomical Union (IAU), was forced to define a planet for the first time, and in so doing included in their definition that a planet must have "cleared the neighbourhood around its orbit." As Pluto shared its orbit with so many KBOs, it was deemed not to have cleared its orbit, and was thus reclassified from a planet to a member of the Kuiper belt. 

Though Pluto is the largest KBO, a number of objects outside the Kuiper belt which may have begun their lives as KBOs are larger. Eris is the most obvious example, but Neptune's moon Triton, which, as explained above, is probably a captured KBO, is also larger than Pluto.

As of 2007, only three objects in the Solar System, Ceres, Pluto and Eris, are considered dwarf planets. However, a number of other Kuiper belt objects are also large enough to be spherical and could be classified as dwarf planets in the future.

Of the four largest TNOs, three (Eris, Pluto, and 2003 EL61) possess satellites, and two have more than one. A higher percentage of the largest KBOs possess satellites than the smaller objects in the Kuiper belt, suggesting that a different formation mechanism was responsible. There are also a high number of binaries (two objects close enough in mass to be orbiting "each other") in the Kuiper belt. The most notable example is the Pluto-Charon binary, but it is estimated that over 1 percent of KBOs (a high percentage) exist in binaries.

A number of objects in the Solar System, while not technically being KBOs themselves, are believed to have originated in the Kuiper belt. 

The scattered disc is a sparsely populated region beyond the Kuiper belt, extending as far as 100 AU and farther. Scattered disc objects (SDOs) travel in highly elliptical orbits, usually also highly inclined to the ecliptic. Most models of solar system formation show icy planetoids first forming in the Kuiper belt, while later gravitational interactions, particularly with Neptune, displaced some of them outwards into the scattered disc. 

According to the Minor Planet Center, which officially catalogues all trans-Neptunian objects, a KBO, strictly speaking, is any object that orbits exclusively within the defined Kuiper belt region regardless of origin or composition. Objects found outside the belt are classed as scattered objects. However, in some scientific circles the term "Kuiper belt object" has become synonymous with any icy planetoid native to the outer solar system believed to have been part of that initial class, even if its orbit during the bulk of solar system history has been beyond the Kuiper belt (e.g. in the scattered disk region). They often describe scattered disc objects as "scattered Kuiper belt objects." Eris, the recently discovered object now known to be larger than Pluto, is often referred to as a KBO, but is technically an SDO. A consensus among astronomers as to the precise definition of the Kuiper belt has yet to be reached, and this issue remains unresolved. 

The centaurs, which are not normally considered part of the Kuiper belt, are also believed to be scattered Kuiper belt objects, the only difference being that they were scattered inward, rather than outward. The Minor Planet Center groups the centaurs and the SDOs together as scattered KBOs.

During its period of migration, Neptune is thought to have captured one of the larger KBOs and set it in orbit around itself. This is its moon Triton, which is the only large moon in the Solar System to have a retrograde orbit; it orbits in the opposite direction to Neptune's rotation. This suggests that, unlike the large moons of Jupiter and Saturn, which are thought to have coalesced from spinning discs of material encircling their young parent planets, Triton was a fully formed body that was captured from surrounding space. Gravitational capture of an object is not easy; it requires that some force act upon the object to slow it down enough to be snared by the larger object's gravity. How this happened to Triton is not well understood, though it does suggest that Triton formed as part of a large population of similar objects whose gravity could impede its motion enough to be captured. Triton is only slightly larger than Pluto, and spectral analysis of both worlds shows that they are largely composed of similar materials, such as methane and carbon monoxide. All this points to the conclusion that Triton was once a KBO that was captured by Neptune during its outward migration.

Comets in the solar system can be loosely divided into two categories: short-period and long period. Long period comets are believed to originate in the Oort cloud. There are two recognised categories of short-period comets: Jupiter-family comets and Halley-family comets. The latter group, which is named for its prototype, Halley's Comet, are believed to have emerged from the Oort cloud but to have been drawn into the inner Solar System by the gravity of the giant planets. It is the former type, the Jupiter family, that are believed to have originated from the Kuiper belt (technically, because the scattered disc is dynamically active and the Kuiper belt proper is mostly dynamically stable, the vast majority of Jupiter family comets are believed to have originated in the scattered disc, but since scattered disc objects are thought to have originally been KBOs, the Jupiter family's ultimate origin remains the Kuiper belt). The centaurs are thought to be a dynamically intermediate stage between the scattered disc and the Jupiter family. 

Despite the fact that many are universally thought to have hailed from the Kuiper belt, there exist a wide array of differences between KBOs and Jupiter-family comets. Although the centaurs share a reddish colouration with many KBOs, the nuclei of comets are far bluer, indicating a fundamental chemical or physical difference. The current hypothesis is that comet nuclei are resurfaced as they approach the Sun by subsurface materials which subsequently bury the older reddish material.

 On January 19, 2006, the first spacecraft mission to explore the Kuiper belt, New Horizons, was launched. The mission, headed by Alan Stern of the Southwest Research Institute, will arrive at Pluto on July 14 2015 and, circumstances permitting, will continue on to study another as-yet undetermined KBO. Any KBO chosen will be between 25 and 55Â miles (40 to 90Â km) in diameter and, ideally, white or grey, to contrast with Pluto's reddish colour. John Spencer, an astronomer on the New Horizons mission team, says that no target for a post-Pluto Kuiper belt encounter has yet been selected, as they are awaiting data from the Pan-STARRS survey project to ensure as wide a field of options as possible. The Pan-STARRS project, due to come fully online by 2009, will survey the entire sky with four 1.4 gigapixel digital cameras to detect any moving objects, from near-earth objects to KBOs.

As of 2006, nine stars other than the Sun are known to be circled by Kuiper belt-like structures. They appear to fall into two categories: wide belts, with radii of over 50 AU, and narrow belts (like our own Kuiper belt) with diameters of between 20 and 30 AU and relatively sharp boundaries. Most known debris discs around other stars are fairly young, but the two imaged at right, taken by the Hubble Space Telescope in January, 2006, are old enough (roughly 300 million years) to have settled into stable configurations. The left image is a "top view" of a wide belt, and the right image is an "edge view" of a narrow belt. The black central circle is produced by the camera's coronagraph which hides the central star to allow the much fainter disks to be seen.











Karl Heinrich Marx (May 5, 1818 â€?March 14, 1883) was a 19th century philosopher, political economist, and revolutionary. Often called the father of communism, Marx was both a scholar and a political activist. He addressed a wide range of political as well as social issues, and is known for, amongst other things, his analysis of history. His approach is indicated by the opening line of the Communist Manifesto (1848): â€œThe history of all hitherto existing society is the history of class strugglesâ€? Marx believed that capitalism, like previous socioeconomic systems, will produce internal tensions which will lead to its destruction. Just as capitalism replaced feudalism, capitalism itself will be displaced by communism, a classless society which emerges after a transitional period in which the state would be nothing else but the revolutionary dictatorship of the proletariat.

On the one hand, Marx argued for a systemic understanding of socioeconomic change. On this model, it is the structural contradictions within capitalism which necessitate its end, giving way to communism: 



On the other hand, Marx argued that socioeconomic change occurred through organized revolutionary action. On this model, capitalism will end through the organized actions of an international working class: "Communism is for us not a state of affairs which is to be established, an ideal to which reality [will] have to adjust itself. We call communism the real movement which abolishes the present state of things. The conditions of this movement result from the premises now in existence." (from The German Ideology)

While Marx was a relatively obscure figure in his own lifetime, his ideas began to exert a major influence on workers' movements shortly after his death. This influence was given added impetus by the victory of the Marxist Bolsheviks in the Russian October Revolution, and there are few parts of the world which were not significantly touched by Marxian ideas in the course of the twentieth century. The relation of Marx to "Marxism" is a point of controversy. Marxism remains influential and controversial in academic and political circles. 



Karl Heinrich Marx was born the third of seven children of a Jewish family in Trier, in the Kingdom of Prussia's Province of the Lower Rhine. His father, Heinrich (1777â€?838), who had descended from a long line of rabbis, converted to Christianity, despite his many deistic tendencies and his admiration of such Enlightenment figures as Voltaire and Rousseau. Marx's father was actually born Herschel Mordechai, but when the Prussian authorities would not allow him to continue practicing law as a Jew, he joined the official denomination of the Prussian state, Lutheranism, which accorded him advantages, as one of a small minority of Lutherans in a predominantly Roman Catholic region. His mother was Henrietta (nÃ©e Pressburg; 1788â€?863); his siblings were Sophie, Hermann, Henriette, Louise (m. Juta), Emilie and Caroline.

Marx was educated at home until the age of thirteen. After graduating from the Trier Gymnasium, Marx enrolled in the University of Bonn in 1835 at the age of seventeen to study law, where he joined the Trier Tavern Club drinking society and at one point served as its president; his grades suffered as a result. Marx was interested in studying philosophy and literature, but his father would not allow it because he did not believe that his son would be able to comfortably support himself in the future as a scholar. The following year, his father forced him to transfer to the far more serious and academically oriented Friedrich-Wilhelms-UniversitÃ¤t in Berlin. During this period, Marx wrote many poems and essays concerning life, using the theological language acquired from his liberal, deistic father, such as "the Deity," but also absorbed the atheistic philosophy of the Young Hegelians who were prominent in Berlin at the time. Marx earned a doctorate in 1841 with a thesis titled The Difference Between the Democritean and Epicurean Philosophy of Nature, but he had to submit his dissertation to the University of Jena as he was warned that his reputation among the faculty as a Young Hegelian radical would lead to a poor reception in Berlin. 



The Left, or Young Hegelians, consisted of a group of philosophers and journalists circling around Ludwig Feuerbach and Bruno Bauer opposing their teacher Hegel. Despite their criticism of Hegel's metaphysical assumptions, they made use of Hegel's dialectical method, separated from its theological content, as a powerful weapon for the critique of established religion and politics. Some members of this circle drew an analogy between post-Aristotelian philosophy and post-Hegelian philosophy. One of them, Max Stirner, turned critically against both Feuerbach and Bauer in his book "Der Einzige und sein Eigenthum" (1845, The Ego and Its Own), calling these atheists "pious people" for their reification of abstract concepts. Marx, at that time a follower of Feuerbach, was deeply impressed by the work and abandoned Feuerbachian materialism and accomplished what recent authors have denoted as an "epistemological break." He developed the basic concept of historical materialism against Stirner in his book "Die Deutsche Ideologie" (1846, The German Ideology), which he did not publish.Another link to the Young Hegelians was Moses Hess, with whom Marx eventually disagreed, yet to whom he owed many of his insights into the relationship between state, society and religion.

Towards the end of October 1843, Marx arrived in Paris, France. There, on August 28, 1844, at the CafÃ© de la RÃ©gence on the Place du Palais he began the most important friendship of his life, and one of the most important in history â€?he met Friedrich Engels. Engels had come to Paris specifically to see Marx, whom he had met only briefly at the office of the Rheinische Zeitung in 1842. He came to show Marx what would turn out to be perhaps Engels' greatest work, The Condition of the Working Class in England in 1844. Paris at this time was the home and headquarters to armies of German, British, Polish, and Italian revolutionaries. Marx, for his part, had come to Paris to work with Arnold Ruge, another revolutionary from Germany, on the Deutsch-FranzÃ¶sische JahrbÃ¼cher .

After the failure of the Deutsch-FranzÃ¶sische JahrbÃ¼cher, Marx, living on the Rue Vaneau, wrote for the most radical of all German newspapers in Paris, indeed in Europe, the VorwÃ¤rts, established and run by the secret society called League of the Just. Marx's topics were generally on the Jewish question and Hegel. When not writing, Marx studied the history of the French Revolution and read Proudhon. He also spent considerable time studying a side of life he had never been acquainted with before â€?a large urban proletariat.



He re-evaluated his relationship with the Young Hegelians, and as a reply to Bauer's atheism wrote On the Jewish Question. This essay was mostly a critique of current notions of civil and human rights and political emancipation, which also included several critical references to Judaism as well as Christianity from a standpoint of social emancipation. Engels, a committed communist, kindled Marx's interest in the situation of the working class and guided Marx's interest in economics. Marx became a communist and set down his views in a series of writings known as the Economic and Philosophical Manuscripts of 1844, which remained unpublished until the 1930s. In the Manuscripts, Marx outlined a humanist conception of communism, influenced by the philosophy of Ludwig Feuerbach and based on a contrast between the alienated nature of labor under capitalism and a communist society in which human beings freely developed their nature in cooperative production. 

In January 1845, after the VorwÃ¤rts expressed its hearty approval regarding the assassination attempt on the life of Frederick William IV, King of Prussia, Marx, among many others, were ordered to leave Paris. He and Engels moved on to Brussels, Belgium. 

Marx devoted himself to an intensive study of history and elaborated on his idea of historical materialism, particularly in a manuscript (published posthumously as The German Ideology), the basic thesis of which was that "the nature of individuals depends on the material conditions determining their production." Marx traced the history of the various modes of production and predicted the collapse of the present oneâ€”industrial capitalismâ€”and its replacement by communism. This was the first major work of what scholars consider to be his later phase, abandoning the Feuerbach-influenced humanism of his earlier work.

Next, Marx wrote The Poverty of Philosophy (1847), a response to Pierre-Joseph Proudhon's The Philosophy of Poverty and a critique of French socialist thought. These works laid the foundation for Marx and Engels' most famous work, The Communist Manifesto, first published on February 21, 1848, as the manifesto of the Communist League, a small group of European communists who had come to be influenced by Marx and Engels. 

Later that year, Europe experienced tremendous revolutionary upheaval. Marx was arrested and expelled from Belgium; in the meantime a radical movement had seized power from King Louis-Philippe in France, and invited Marx to return to Paris, where he witnessed the revolutionary June Days Uprising first hand.

When this collapsed in 1849, Marx moved back to Cologne and started the Neue Rheinische Zeitung ("New Rhenish Newspaper"). During its existence he was put on trial twice, on February 7, 1849 because of a press misdemeanor, and on the 8th charged with incitement to armed rebellion. Both times he was acquitted. The paper was soon suppressed and Marx returned to Paris, but was forced out again. This time he sought refuge in London.

Marx moved to London in May 1849, where he was to remain for the rest of his life. He briefly worked as correspondent for the New York Herald Tribune in 1851. In 1855, the Marx family suffered a blow with the death of their son, Edgar, from tuberculosis. Meanwhile, Marx's major work on political economy made slow progress. By 1857 he had produced a gigantic 800 page manuscript on capital, landed property, wage labour, the state, foreign trade and the world market. This work however was not published until 1941, under the title Grundrisse. In the early 1860s he worked on composing three large volumes, the Theories of Surplus Value, which discussed the theoreticians of political economy, particularly Adam Smith and David Ricardo. This work, that was published posthumously under the editorship of Karl Kautsky is often seen as the Fourth book of Capital, and constitutes one of the first comprehensive treatises on the history of economic thought. In 1867, well behind schedule, the first volume of Capital was published, a work which analyzed the capitalist process of production. Here, Marx elaborated his labor theory of value and his conception of surplus value and exploitation which he argued would ultimately lead to a falling rate of profit and the collapse of industrial capitalism. Volumes II and III remained mere manuscripts upon which Marx continued to work for the rest of his life and were published posthumously by Engels. In 1859, Marx was able to publish Contribution to the Critique of Political Economy, his first serious economic work. In his journalistic work of this period, Marx championed the Union cause in the American Civil War.

One reason why Marx was so slow to publish Capital was that he was devoting his time and energy to the First International, to whose General Council he was elected at its inception in 1864. He was particularly active in preparing for the annual Congresses of the International and leading the struggle against the anarchist wing led by Mikhail Bakunin (1814â€?876). Although Marx won this contest, the transfer of the seat of the General Council from London to New York in 1872, which Marx supported, led to the decline of the International. The most important political event during the existence of the International was the Paris Commune of 1871 when the citizens of Paris rebelled against their government and held the city for two months. On the bloody suppression of this rebellion, Marx wrote one of his most famous pamphlets, The Civil War in France, an enthusiastic defense of the Commune.

During the last decade of his life, Marx's health declined and he was incapable of the sustained effort that had characterized his previous work. He did manage to comment substantially on contemporary politics, particularly in Germany and Russia. In Germany, in his Critique of the Gotha Programme, he opposed the tendency of his followers Wilhelm Liebknecht (1826â€?900) and August Bebel (1840â€?913) to compromise with the state socialism of Ferdinand Lassalle in the interests of a united socialist party. In his correspondence with Vera Zasulich, Marx contemplated the possibility of Russia's bypassing the capitalist stage of development and building communism on the basis of the common ownership of land characteristic of the village Mir.

Karl Marx was married to Jenny von Westphalen, the educated daughter of a Prussian baron. Karl Marx's engagement to her was kept secret at first, and for several years was opposed by both the Marxes and Westphalens. Despite the objections, the two were married on June 19, 1843 in Kreuznacher Pauluskirche, Bad Kreuznach.

During the first half of the 1850s the Marx family lived in poverty and constant fear of creditors in a three room flat on Dean Street in the Soho quarter of London. Marx and Jenny already had four children and three more were to follow. Of these only three survived to adulthood. Marx's major source of income at this time was Engels, who was drawing a steadily increasing income from the family business in Manchester. This was supplemented by weekly articles written as a foreign correspondent for the New York Daily Tribune. Inheritances from one of Jenny's uncles and her mother who died in 1856 allowed the family to move to somewhat more salubrious lodgings at 9 Grafton Terrace, Kentish Town a new suburb on the then-outskirts of London. Marx generally lived a hand-to-mouth existence, forever at the limits of his resources, although this did extend to some spending on relatively bourgeois luxuries, which he felt were necessities for his wife and children given their social status and the mores of the time.

There is a disputed rumour that Marx was the father of Frederick Demuth, the son of Marx's housekeeper, Lenchen Demuth. It has been suggested that this rumour lacks any direct corroboration.

Marx's children by his wife were: Jenny Caroline (m. Longuet; 1844â€?883); Jenny Laura (m. Lafargue; 1846â€?911); Edgar (1847â€?855); Henry Edward Guy ("Guido"; 1849â€?850); Jenny Eveline Frances ("Franziska"; 1851â€?852); Jenny Julia Eleanor (1855â€?898); and one more who died before being named (July 1857).



Following the death of his wife Jenny in December 1881, Marx developed a catarrh that kept him in ill health for the last fifteen months of his life. It eventually brought on the bronchitis and pleurisy that killed him in London on March 14, 1883. He died a stateless person and was buried in Highgate Cemetery, London, on 17 March, 1883. The messages carved on Marx's tombstone are: â€œWORKERS OF ALL LANDS UNITEâ€? the final line of The Communist Manifesto, and Engels' version of the 11th Thesis on Feuerbach: 



The tombstone was a monument built in 1954 by the Communist Party of Great Britain with a portrait bust by Laurence Bradshaw; Marx's original tomb had been humbly adorned. In 1970, there was an unsuccessful attempt to destroy the monument, with a homemade bomb.

Several of Marx's closest friends spoke at his funeral, including Wilhelm Liebknecht and Friedrich Engels. Engels' speech included the words:



In addition to Engels and Liebknecht, Marx's daughter Eleanor and Charles Longuet and Paul Lafargue, Marx's two French socialist sons-in-law, also attended his funeral. Liebknecht, a founder and leader of the German Social-Democratic Party, gave a speech in German, and Longuet, a prominent figure in the French working-class movement, gave a short statement in French. Two telegrams from workers' parties in France and Spain were also read out. Together with Engels' speech, this was the entire programme of the funeral. Also attending the funeral was Friedrich Lessner, who had been sentenced to three years in prison at the Cologne communist trial of 1852; G. Lochner, who was described by Engels as "an old member of the Communist League" and Carl Schorlemmer, a professor of chemistry in Manchester, a member of the Royal Society, but also an old communist associate of Marx and Engels. Three others attended the funeral â€?Ray Lankester, Sir John Noe and Leonard Church â€?making eleven in all. 

Marx's daughter Eleanor became a socialist like her father and helped edit his works.

The American Marx scholar Hal Draper once remarked, "there are few thinkers in modern history whose thought has been so badly misrepresented, by Marxists and anti-Marxists alike." The legacy of Marx's thought is bitterly contested between numerous tendencies who claim to be Marx's most accurate interpreters, including Marxist-Leninism, Trotskyism, Maoism, and libertarian Marxism.

Marx's philosophy hinges on his view of human nature. Along with the Hegelian dialectic, Marx inherited a disdain for the notion of an underlying invariant human nature. Sometimes Marxists express their views by contrasting â€œnatureâ€?with â€œhistory.â€?Sometimes they use the phrase â€œexistence precedes consciousness.â€?In either case, a person is determined by where and when the person is â€?social context takes precedence over innate behavior; or, in other words, one of the main features of human nature is adaptability. Nevertheless, Marxian thought rests on the fundamental assumption that it is human nature to transform nature, and he calls this process of transformation "labour" and the capacity to transform nature "labour power." For Marx, this is a natural capacity for physical activity, but it is intimately tied to the active role of human consciousness:



Marx did not believe that all people worked the same way, or that how one works is entirely personal and individual. Instead, he argued that work is a social activity and that the conditions and forms under and through which people work are socially determined and change over time.

Marx's analysis of history is based on his distinction between the means / forces of production, literally those things such as land, natural resources, and technology, that are necessary for the production of material goods, and the relations of production, in other words, the social and technical relationships people enter into as they acquire and use the means of production. Together these comprise the mode of production; Marx observed that within any given society the mode of production changes, and that European societies had progressed from a feudal mode of production to a capitalist mode of production. Marx believed that the means of production change more rapidly than the relations of production (for example, we develop a new technology, such as the Internet, and only later do we develop laws to regulate that technology). For Marx this mismatch between (economic) base and (social) superstructure is a major source of social disruption and conflict.

Marx understood the "social relations of production" to comprise not only relations among individuals, but between or among groups of people, or classes. As a scientist and materialist, Marx did not understand classes as purely subjective (in other words, groups of people who consciously identified with one another). He sought to define classes in terms of objective criteria, such as their access to resources. For Marx, different classes have divergent interests, which is another source of social disruption and conflict. Conflict between social classes being something which is inherent in all human history:



Marx was especially concerned with how people relate to that most fundamental resource of all, their own labor power. Marx wrote extensively about this in terms of the problem of alienation. As with the dialectic, Marx began with a Hegelian notion of alienation but developed a more materialist conception. For Marx, the possibility that one may give up ownership of one's own labor â€?one's capacity to transform the world â€?is tantamount to being alienated from one's own nature; it is a spiritual loss. Marx described this loss in terms of commodity fetishism, in which the things that people produce, commodities, appear to have a life and movement of their own to which humans and their behavior merely adapt. This disguises the fact that the exchange and circulation of commodities really are the product and reflection of social relationships among people. Under capitalism, social relationships of production, such as among workers or between workers and capitalists, are mediated through commodities, including labor, that are bought and sold on the market.

Commodity fetishism is an example of what Engels called false consciousness, which is closely related to the understanding of ideology. By ideology they meant ideas that reflect the interests of a particular class at a particular time in history, but which are presented as universal and eternal. Marx and Engels' point was not only that such beliefs are at best half-truths; they serve an important political function. Put another way, the control that one class exercises over the means of production includes not only the production of food or manufactured goods; it includes the production of ideas as well (this provides one possible explanation for why members of a subordinate class may hold ideas contrary to their own interests). Thus, while such ideas may be false, they also reveal in coded form some truth about political relations. For example, although the belief that the things people produce are actually more productive than the people who produce them is literally absurd, it does reflect (according to Marx and Engels) that people under capitalism are alienated from their own labor-power. Another example of this sort of analysis is Marx's understanding of religion, summed up in a passage from the preface to his 1843 Contribution to the Critique of Hegel's Philosophy of Right: Whereas his Gymnasium senior thesis argued that the primary social function of religion was to promote solidarity, here Marx sees the social function in terms of political and economic inequality. Moreover, he provides an analysis of the ideological functions of religion: to reveal â€œan inverted consciousness of the world.â€?He continues: â€œIt is the immediate task of philosophy, which is in the service of history, to unmask self-estrangement in its unholy forms, once [religion,] the holy form of human self-estrangement has been unmaskedâ€? For Marx, this unholy self-estrangement, the â€œloss of man,â€?is complete for the sphere of the proletariat. His final conclusion is that for Germany, general human emancipation is only possible as a suspension of private property by the proletariat.



Marx argued that this alienation of human work (and resulting commodity fetishism) is precisely the defining feature of capitalism. Prior to capitalism, markets existed in Europe where producers and merchants bought and sold commodities. According to Marx, a capitalist mode of production developed in Europe when labor itself became a commodity â€?when peasants became free to sell their own labor-power, and needed to do so because they no longer possessed their own land. People sell their labor-power when they accept compensation in return for whatever work they do in a given period of time (in other words, they are not selling the product of their labor, but their capacity to work). In return for selling their labor power they receive money, which allows them to survive. Those who must sell their labor power are "proletarians". The person who buys the labor power, generally someone who does own the land and technology to produce, is a "capitalist" or "bourgeois". The proletarians inevitably outnumber the capitalists.

Marx distinguished industrial capitalists from merchant capitalists. Merchants buy goods in one market and sell them in another. Since the laws of supply and demand operate within given markets, there is often a difference between the price of a commodity in one market and another. Merchants, then, practice arbitrage, and hope to capture the difference between these two markets. According to Marx, capitalists, on the other hand, take advantage of the difference between the labor market and the market for whatever commodity is produced by the capitalist. Marx observed that in practically every successful industry input unit-costs are lower than output unit-prices. Marx called the difference "surplus value" and argued that this surplus value had its source in surplus labour, the difference between what it costs to keep workers alive and what they can produce.

The capitalist mode of production is capable of tremendous growth because the capitalist can, and has an incentive to, reinvest profits in new technologies. Marx considered the capitalist class to be the most revolutionary in history, because it constantly revolutionized the means of production. But Marx argued that capitalism was prone to periodic crises. He suggested that over time, capitalists would invest more and more in new technologies, and less and less in labor. Since Marx believed that surplus value appropriated from labor is the source of profits, he concluded that the rate of profit would fall even as the economy grew. When the rate of profit falls below a certain point, the result would be a recession or depression in which certain sectors of the economy would collapse. Marx understood that during such a crisis the price of labor would also fall, and eventually make possible the investment in new technologies and the growth of new sectors of the economy. 

Marx believed that this cycle of growth, collapse, and growth would be punctuated by increasingly severe crises. Moreover, he believed that the long-term consequence of this process was necessarily the enrichment and empowerment of the capitalist class and the impoverishment of the proletariat. He believed that were the proletariat to seize the means of production, they would encourage social relations that would benefit everyone equally, and a system of production less vulnerable to periodic crises. In general, Marx thought that peaceful negotiation of this problem was impracticable, and that a massive, well-organized and violent revolution would be required, because the ruling class would not give up power without violence. He theorized that to establish the socialist system, a dictatorship of the proletariat - a period where the needs of the working-class, not of capital, will be the common deciding factor - must be created on a temporary basis. As he wrote in his "Critique of the Gotha Program", "between capitalist and communist society there lies the period of the revolutionary transformation of the one into the other. Corresponding to this is also a political transition period in which the state can be nothing but the revolutionary dictatorship of the proletariat."While he allowed for the possibility of peaceful transition in some countries with strong democratic institutional structures (e.g. Britain, the US and the Netherlands), he suggested that in other countries with strong centralized state-oriented traditions, like France and Germany, the "lever of our revolution must be force."

Scholars are divided as to whether Marx was anti-Semitic. According to Edward H. Flannery, Marx was an antisemite who considered Jews worshippers of mammon, the very soul of the corrupt capitalism he fought. According to several other scholars, for Marx Jews were the embodiment of capitalism and the creators of all its evils. In their view, Marx's equation of Judaism with capitalism, together with his pronouncements on Jews, strongly influenced socialist movements and shaped their attitudes and policies toward the Jews. In those scholar's opinion, Marx's 'On the Jewish Question' influenced National Socialist, as well as Soviet and Arab anti-Semites

Hyam Maccoby has argued that Marx's early anti-Semitism is shown in his 1843 essay "On the Jewish Question." Marx wrote: 

According to Leon Boim, professor at Tel-Aviv University: 

Jonathan Sacks has written that virtually all major enlightenment philosophers were antisemitic, including Voltaire, Kant, Hegel and Nietzsche. At the time Marx wrote "On the Jewish Question", the word "antisemitism" had not yet been coined or developed a racial component, and there was little awareness of the depths of European prejudice against Jews. Marx was thus simply expressing, in Sacks's view, the commonplace thinking of his era.

Maccoby has suggested that Marx was embarrassed by his Jewish background, noting "that anyone who uses Jews as the yardstick of evil is being antisemitic". Moreover, Maccoby claims that in later years, Marx's anti-Semitism was mostly limited to private letters and conversations because of strong public identification with anti-Semitism by his political enemies both on the left (Pierre-Joseph Proudhon and Mikhail Bakunin) and on the right (aristocracy and the Church). Bernard Lewis found many instances of anti-Semitic language in Marx's later work.

In contrast, David McLellan and Francis Wheen have argued that "On the Jewish Question" must be understood in terms of Marx's debates with Bruno Bauer over the nature of political emancipation in Germany. Wheen asserts: "Those critics who see this as a foretaste of Mein Kampf overlook one essential point: in spite of the clumsy phraseology and crude stereotyping, the essay was actually written as a defence of the Jews. It was a retort to Bruno Bauer, who had argued that Jews should not be granted full civic rights and freedoms unless they were baptised as Christians." According to McLellan, Marx used the word "Judentum" in its colloquial sense of "commerce" to argue that Germans suffer, and must be emancipated from, capitalism. The second half of Marx's essay, McLellan concludes, should be read as "an extended pun at Bauerâ€™s expense.". 

Hal Draper has argued that Marx was influenced by the writing of Jewish critic Moses Hess, and that "On the Jewish Question" should be read alongside similar work by Hess:

Marx's thought was strongly influenced by: 

Marx believed that he could study history and society scientifically and discern tendencies of history and the resulting outcome of social conflicts. Some followers of Marx concluded, therefore, that a communist revolution is inevitable. However, Marx famously asserted in the eleventh of his Theses on Feuerbach that "philosophers have only interpreted the world, in various ways; the point however is to change it", and he clearly dedicated himself to trying to alter the world. Consequently, most followers of Marx are not fatalists, but activists who believe that revolutionaries must organize social change.

Marx's view of history, which came to be called historical materialism (controversially adapted as the philosophy of dialectical materialism by Engels and Lenin) is certainly influenced by Hegel's claim that reality (and history) should be viewed dialectically. Hegel believed that human history is characterized by the movement from the fragmentary toward the complete and the real (which was also a movement towards greater and greater rationality). Sometimes, Hegel explained, this progressive unfolding of the Absolute involves gradual, evolutionary accretion but at other times requires discontinuous, revolutionary leaps â€?episodal upheavals against the existing status quo. For example, Hegel strongly opposed slavery in the United States during his lifetime, and he envisioned a time when Christian nations would eliminate it from their civilization. While Marx accepted this broad conception of history, Hegel was an idealist, and Marx sought to rewrite dialectics in materialist terms. He wrote that Hegelianism stood the movement of reality on its head, and that it was necessary to set it upon its feet.

Marx's acceptance of this notion of materialist dialectics which rejected Hegel's idealism was greatly influenced by Ludwig Feuerbach. In The Essence of Christianity, Feuerbach argued that God is really a creation of man and that the qualities people attribute to God are really qualities of humanity. Accordingly, Marx argued that it is the material world that is real and that our ideas of it are consequences, not causes, of the world. Thus, like Hegel and other philosophers, Marx distinguished between appearances and reality. But he did not believe that the material world hides from us the "real" world of the ideal; on the contrary, he thought that historically and socially specific ideology prevented people from seeing the material conditions of their lives clearly. 

The other important contribution to Marx's revision of Hegelianism was Engels' book, The Condition of the Working Class in England in 1844, which led Marx to conceive of the historical dialectic in terms of class conflict and to see the modern working class as the most progressive force for revolution.

The work of Marx and Engels covers a wide range of topics and presents a complex analysis of history and society in terms of class relations. Followers of Marx and Engels have drawn on this work to propose a grand, cohesive theoretical outlook dubbed Marxism. Nevertheless, there have been numerous debates among Marxists over how to interpret Marx's writings and how to apply his concepts to current events and conditions. Moreover, it is important to distinguish between "Marxism" and "what Marx believed"; for example, shortly before he died in 1883, Marx wrote a letter to the French workers' leader Jules Guesde, and to his own son-in-law Paul Lafargue, accusing them of "revolutionary phrase-mongering" and of lack of faith in the working class. After the French party split into a reformist and revolutionary party, some accused Guesde (leader of the latter) of taking orders from Marx; Marx remarked to Lafargue, "if that is Marxism, then I am not a Marxist" (in a letter to Engels, Marx later accused Guesde of being a "Bakuninist").

Essentially, people use the word "Marxist" to describe those who rely on Marx's conceptual language (e.g. "mode of production", "class", "commodity fetishism") to understand capitalist and other societies, or to describe those who believe that a workers' revolution is the only means to a communist society. Some, particularly in academic circles, who accept much of Marx's theory, but not all its implications, call themselves "Marxian" instead.

Six years after Marx's death, Engels and others founded the "Second International" as a base for continued political activism. This organization was far more successful than the First International had been, containing mass workers' parties, particularly the large and successful Social Democratic Party of Germany, which was predominantly Marxist in outlook. This international collapsed in 1914, however, in part because some members turned to Edward Bernstein's "evolutionary socialism", and in part because of divisions precipitated by World War I.

World War I also led to the Russian Revolution of 1917 in which a left splinter of the Second International, the Bolsheviks, led by Vladimir Lenin, took power. The revolution dynamized workers around the world into setting up their own section of the Bolsheviks' "Third International". Lenin claimed to be both the philosophical and political heir to Marx, and developed a political program, called "Leninism" or "Bolshevism", which called for revolution organized and led by a centrally organized "Communist Party".

Marx believed that the communist revolution would take place in advanced industrial societies such as France, Germany and England, but Lenin argued that in the age of imperialism, and due to the "law of uneven development", where Russia had on the one hand, an antiquated agricultural society, but on the other hand, some of the most up-to-date industrial concerns, the "chain" might break at its weakest points, that is, in the so-called "backward" countries, and ignite revolution in the advanced industrial societies of Europe, where society is ready for socialism, and which could then come to the aid of the workers state in Russia.

Marx and Engels make a very significant comment in the preface to the Russian edition of the Communist Manifesto:



Marx's words served as a starting point for Lenin, who, together with Trotsky, always understood that the Russian revolution must become a "signal for a proletarian revolution in the West". Supporters of Trotsky argue that the failure of revolution in the West along the lines envisaged by Marx, to come to the aid of the Russian revolution after 1917, led to the rise of Stalinism, and set the cast of human history for seventy years. This is termed the theory of the Permanent Revolution, which became official policy in Russia until Lenin's death in 1924 and the subsequent development of the concept of "Socialism in one country" by Stalin.In China Mao Zedong also claimed to be an heir to Marx, but argued that peasants and not just workers could play leading roles in a Communist revolution, even in third world countries marked by peasant feudalism in the absence of industrial workers. Mao termed this the New Democratic Revolution. It was a departure from Marx, who had stated that the revolutionary transformation of society could take place only in countries that have achieved a capitalist stage of development with a proletarian majority. Marxism-Leninism as espoused by Mao came to be internationally known as Maoism.

Under Lenin, and particularly under Joseph Stalin, Soviet suppression of the rights of individuals in the name of the struggle against capitalism, as well as Stalinist purges themselves, came in the minds of many to be characteristic of Marxism. This impression was encouraged by capitalism-oriented western states, as well as the politics of the Cold War. There were, nonetheless, always dissenting Marxist voices â€?Marxists of the old school of the Second International, the left communists who split off from the Third International shortly after its formation, and later Leon Trotsky and his followers, who set up a "Fourth International" in 1938 to compete with that of Stalin, claiming to represent true Bolshevism.



Coming from the Second International milieu, in the 1920s and '30s, a group of dissident Marxists founded the Institute for Social Research in Germany, among them Max Horkheimer, Theodor Adorno, Erich Fromm, and Herbert Marcuse. As a group, these authors are often called the Frankfurt School. Their work is known as Critical Theory, a type of Marxist philosophy and cultural criticism heavily influenced by Hegel, Freud, Nietzsche, and Max Weber. 

The Frankfurt School broke with earlier Marxists, including Lenin and Bolshevism in several key ways. First, writing at the time of the ascendance of Stalinism, they had grave doubts as to the traditional Marxist concept of proletarian class consciousness. Second, unlike earlier Marxists, especially Lenin, they rejected economic determinism. While highly influential, their work has been criticized by both orthodox Marxists and some Marxists involved in political practice for divorcing Marxist theory from practical struggle and turning Marxism into a purely academic enterprise. 

Influential Marxists of the same period include the Third International's Georg Lukacs and Antonio Gramsci, who along with the Frankfurt School are often known by the term Western Marxism.

In 1949 Paul Sweezy and Leo Huberman founded Monthly Review, a journal and press, to provide an outlet for Marxist thought in the United States independent of the Communist Party.

In 1978, G. A. Cohen attempted to defend Marx's thought as a coherent and scientific theory of history by restating its central tenets in the language of analytic philosophy. This gave birth to Analytical Marxism, an academic movement which also included Jon Elster, Adam Przeworski and John Roemer. Bertell Ollman is another Anglophone champion of Marx within the academy, as is the Israeli Shlomo Avineri.

In Marx's 'Das Kapital' (2006), biographer Francis Wheen reiterates David McLellan's observation that since Marxism had not triumphed in the West, "it had not been turned into an official ideology and is thus the object of serious study unimpeded by government controls."

The following countries had governments at some point in the twentieth century who at least nominally adhered to Marxism (those in bold still do as of 2006): Albania, Afghanistan, Angola, Bulgaria, China, Cuba, Czechoslovakia, East Germany, Ethiopia, Hungary, Laos, Moldova, Mongolia, Mozambique, Nicaragua, North Korea, Poland, Romania, Russia, Yugoslavia, Vietnam. In addition, the Indian states of Kerala, Tripura and West Bengal have had Marxist governments.

Marxist political parties and movements have significantly declined since the fall of the Soviet Union, with some exceptions, perhaps most notably Nepal. 

Marx was ranked #27 on Michael H. Hart's list of the most influential figures in history.

In July 2005 Marx was the surprise winner of the 'Greatest Philosopher of All Time' poll by listeners of the BBC Radio 4 series In Our Time.



Many proponents of capitalism have argued that capitalism is a more effective means of generating and redistributing wealth than socialism or communism, or that the gulf between rich and poor that concerned Marx and Engels was a temporary phenomenon. Some suggest that self-interest and the need to acquire capital is an inherent component of human behavior, and is not caused by the adoption of capitalism or any other specific economic system and that different economic systems reflect different social responses to this fact. The Austrian School of economics has criticized Marx's use of the labour theory of value. In addition, the political repression and economic problems of several historical Communist states have done much to destroy Marx's reputation in the Western world, particularly following the fall of the Berlin Wall and the collapse of the Soviet Union. Some Marxists argue that the former USSR was a variant of state capitalism whose collapse does not affect the veracity of Marxism.

Friedrich Hayek provided a reply to Marx; in The Road to Serfdom (1944) Hayek shows, or attempts to show, that coordination problems in a socialist economy (the prerequisite for the subsequent pure communism and "whithering away of the state"), whether that socialist economy was democratically controlled or under Leninist direction, would necessarily create bottlenecks as the quasi-labor of "planning" replaces production for use. Followers of Hayek point to the queues and shortages that result from planned rationing (whether in Communist societies or wartime democracies such as Britain from 1939 to 1951) to demonstrate that in the short run, the socialist or Leninist economy seizes up and creates unfairness.

An intriguing critic of Marx, although he also paid tribute to many of Marx's basic ideas, was Louis Feuer, the late professor of philosophy at University of California, Berkeley. In his introduction to Selected Works on Economics and Politics by Karl Marx, published in 1960, Feuer argued strongly for the viewpoint, also expressed by others, that Marxism has many of the characteristics of a religion -- in other words, that Marxism largely depends upon a fervent kind of faith, not provable scientifically, which is typical of religious believers. Just the same, Feuer in his introduction, and in other works, pointed out that Marx has had a very enduring and positive influence on the social and economic thinking of almost every modern country, particularly in Western Europe, but also in the United States. He made the interesting comment that Marxism largely depends upon the injection of ethical thinking into economic and political analysis -- in contrast to modern trends which prefer to discuss these important areas in a totally "objective" manner without ethical values.

Marx has also been criticized from the Left. Some have argued that class is not the most fundamental inequality in history and call attention to patriarchy or race, as not being, as Marxists argue, dependent on class. It could however be argued that Marx does not suggest that class divisions are more fundamental than patriarchy, since the division between men and women, as Engels pointed out, predates class divisions, but only that the movement of history can be best understood in terms of class, and that class struggle is the mechanism of change. Anarchists, on the other hand, have always opposed Marxism, even its most libertarian forms, as being too authoritarian, and missing the basic necessity of rebellion against authority by concentrating on economic matters. (See also Anarchism and Marxism).

Some today question the theoretical and historical validity of "class" as an analytic construct or as a political actor. In this line, some question Marx's reliance on 19th century notions that linked science with the idea of "progress" (see social evolution). Many observe that capitalism has changed much since Marx's time, and that class differences and relationships are much more complex â€?citing as one example the fact that much corporate stock in the United States is owned by workers through pension funds. Critics of this analysis retort that the top 1% of stock owners still own nearly 50% of the nation's publicly traded company stocks.

Still others criticize Marx from the perspective of philosophy of science. Karl Popper has criticized Marx's theories for not being falsifiable, which he believed rendered some aspects of Marxâ€™s historical and socio-political argument unscientific; Popper's falsifiability standard has itself always been controversial. Popper also criticized Marx for historicism, that is, a relativization of truth to a particular historical period.

Some argue that while socioeconomic gaps between the bourgeoisie and proletariat remained, industrialization in countries such as the United States and Great Britain also saw the rise of a middle class not inclined to revolution, and of a welfare state that helped contain any revolutionary tendencies among the working class. While the economic devastation of the Great Depression broadened the appeal of Marxism in the developed world, future government safeguards and economic recovery led to a decline in its influence. In contrast, Marxism remained extremely influential in feudal and industrially underdeveloped societies such as Czarist Russia, where the Bolshevik Revolution was successful. 

While Marx and Engels focused almost exclusively on developments in the West following the prospective development of capitalism, this left the problems of the less developed nations, such as Russia, largely unaddressed. This perceived problem with Marxist theory - that revolutions nevertheless took place in less developed areas of the world, even rather more than within the most advanced capitalist ones - was known from the beginning of the 20th century, and much of the work of Vladimir Lenin and other Marxist and Marxian authors and theorists became dedicated to addressing it. Trotsky famously developed the theory of Permanent Revolution to show how revolutions in backward countries like Russia could succeed so long as they spread to the West. This was opposed by Stalin, who argued for "Socialism in one country". In essence, Lenin argued, taking the theory from several other contemporary Marxist writers, that through imperialism the bourgeoisie of wealthy countries is using "superprofits" from the imperial colonies to effectively bribe the working class back home in order to appease it. Nevertheless, after the Russian Revolution of 1917, Western capitalist nations did experience (unsuccessful) revolutions more or less along the "proletarian" lines that Marx envisaged, notably in Germany (1918, 1919, 1923), and Spain (leading to the Spanish Civil War) with upheavals in France, Italy, and the UK (the general strike of 1926) and elsewhere.

Critics argue that the Soviet Union's numerous internal failings and subsequent collapse were a direct result of the practical failings of Marxism. Most Marxists on the contrary claim that it was precisely the abandonment of Marxism in the Soviet Union that led to its demise, due to its isolation in a backward country not ripe for socialism according to Marx. Marx saw more advanced modes of production as growing out of mature capitalism, and needing widespread education and democratic apparatuses to allow the eventual control of the state by the people themselves (and eventually, the "withering away of the state" under a truly mature communism) - only possible with a well educated and democratic populace. Marx did not appear to suggest that a stage of economic development could simply be skipped over, as the Soviet ideology implied. Rather, no nation should realistically be able to achieve socialism (let alone a mature communism) until it had developed a modern capitalist system, and mature communism was supposed to require a level of wealth and technology that would allow the basic material needs of all citizens to be produced with very little labor, on average, per person in a given time period. That achievement would then free people's time and energies to fully participate in the democratic running of society, and then to finally overcome the alienation that the pattern of technological revolutions had caused throughout historyâ€”a giant arc in which societies developed from the "primitive communism" of small bands that had little or no structural inequality, through the great agrarian empires (usually involving slavery at one end and the richest monarchs at the other) which Marx considered to be the pinnacle of inequality, through feudalism and capitalism to the socialist organisation of society in which all can participate equally due to this technological development. The "elites" of feudal and capitalist society become less able to dominate others either through economics or ideology - their role in society is finished - as the working class develops its strength and becomes the "gravedigger" of capitalism. 

Others, like Shlomo Avineri, have argued that it was the pre-capitalist structure of 1917 Russia, as well as the strong authoritarian traditions of the Russian state and its weak civil society, which pushed the Soviet revolution towards its repressive development.

Critics have also claimed to have shown problems with the concept of historical materialism. At the base of historical materialism, they claim, is the view that the mode of production creates all historical events and changes. But critics have asked the question `Where does the mode of production come from?'. Murray Rothbard argues that "...Marx never attempts to provide an answer. Indeed he cannot, since if he attributes the state of technology or technological change to the actions of man, of individual men, his whole system falls apart. For human consciousness, and individual consciousness at that, would then be determining [the mode of production] rather than the other way round." However, Marx's famous Preface to A Contribution to the Critique of Political Economy states "In the social production of their existence, men inevitably enter into definite relations, which are independent of their will, namely relations of production appropriate to a given stage in the development of their material forces of production."  Marx clearly attributes the productive forces and their development to the actions of human beings, but emphasises the social nature of this development, based on necessity, the need to maintain their existence, which thus develops "independent of their will", as individuals, and thus impacts back on the individual in ways which reflect the given social conditions.

















A central processing unit (CPU), or sometimes just processor, is a description of a class of logic machines that can execute computer programs. This broad definition can easily be applied to many early computers that existed long before the term "CPU" ever came into widespread usage. However, the term itself and its initialism have been in use in the computer industry at least since the early 1960s . The form, design and implementation of CPUs have changed dramatically since the earliest examples, but their fundamental operation has remained much the same. 

Early CPUs were custom-designed as a part of a larger, usually one-of-a-kind, computer. However, this costly method of designing custom CPUs for a particular application has largely given way to the development of mass-produced processors that are suited for one or many purposes. This standardization trend generally began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured in very small spaces (on the order of millimeters). Both the miniaturization and standardization of CPUs have increased the presence of these digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in everything from automobiles to cell phones to children's toys.





Prior to the advent of machines that resemble today's CPUs, computers such as the ENIAC had to be physically rewired in order to perform different tasks. These machines are often referred to as "fixed-program computers," since they had to be physically reconfigured in order to run a different program. Since the term "CPU" is generally defined as a software (computer program) execution device, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.

The idea of a stored-program computer was already present during ENIAC's design, but was initially omitted so the machine could be finished sooner. On June 30, 1945, before ENIAC was even completed, mathematician John von Neumann distributed the paper entitled "First Draft of a Report on the EDVAC." It outlined the design of a stored-program computer that would eventually be completed in August 1949 . EDVAC was designed to perform a certain number of instructions (or operations) of various types. These instructions could be combined to create useful programs for the EDVAC to run. Significantly, the programs written for EDVAC were stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the large amount of time and effort it took to reconfigure the computer to perform a new task. With von Neumann's design, the program, or software, that EDVAC ran could be changed simply by changing the contents of the computer's memory.

While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, others before him such as Konrad Zuse had suggested similar ideas. Additionally, the so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also utilized a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but elements of the Harvard architecture are commonly seen as well.

Being digital devices, all CPUs deal with discrete states and therefore require some kind of switching elements to differentiate between and change these states. Prior to commercial acceptance of the transistor, electrical relays and vacuum tubes (thermionic valves) were commonly used as switching elements. Although these had distinct speed advantages over earlier, purely mechanical designs, they were unreliable for various reasons. For example, building direct current sequential logic circuits out of relays requires additional hardware to cope with the problem of contact bounce. While vacuum tubes do not suffer from contact bounce, they must heat up before becoming fully operational and eventually stop functioning altogether. Usually, when a tube failed, the CPU would have to be diagnosed to locate the failing component so it could be replaced. Therefore, early electronic (vacuum tube based) computers were generally faster but less reliable than electromechanical (relay based) computers. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely . In the end, tube based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs (see below for a discussion of clock rate). Clock signal frequencies ranging from 100 kHz to 4Â MHz were very common at this time, limited largely by the speed of the switching devices they were built with.





The design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements like vacuum tubes and electrical relays. With this improvement more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.

During this period, a method of manufacturing many transistors in a compact space gained popularity.The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or "chip." At first only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained transistor counts numbering in multiples of ten. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs. As microelectronic technology advanced, an increasing number of transistors were placed on ICs, thus decreasing the quantity of individual ICs needed for a complete CPU. MSI and LSI (medium- and large-scale integration) ICs increased transistor counts to hundreds, then thousands.

In 1964 IBM introduced its System/360 computer architecture, which was used in a series of computers that could run the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called "microcode"), which still sees widespread usage in modern CPUs . The System/360 architecture was so popular that it dominated the mainframe computer market for the next few decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In the same year (1964), Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8. DEC would later introduce the extremely popular PDP-11 line that originally was built with SSI ICs but was eventually implemented with LSI components once these became practical. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits .

Transistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. Thanks to both the increased reliability as well as the dramatically increased speed of the switching elements (which were almost exclusively transistors by this time), CPU clock rates in the tens of megahertz were obtained during this period. Additionally, while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc.



The introduction of the microprocessor in the 1970s significantly affected the design and implementation of CPUs. Since the introduction of the first microprocessor (the Intel 4004) in 1970 and the first widely used microprocessor (the Intel 8080) in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual vast success of the now ubiquitous personal computer, the term "CPU" is now applied almost exclusively to microprocessors.

Previous generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size as a result of being implemented on a single die means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, as the ability to construct exceedingly small transistors on an IC has increased, the complexity and number of transistors in a single CPU has increased dramatically. This widely observed trend is described by Moore's law, which has proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity to date.

While the complexity, size, construction, and general form of CPUs have changed drastically over the past sixty years, it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines.As the aforementioned Moore's law continues to hold true, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.

The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions called a program. Discussed here are devices that conform to the common von Neumann architecture. The program is represented by a series of numbers that are kept in some kind of computer memory. There are four steps that nearly all von Neumann CPUs use in their operation: fetch, decode, execute, and writeback.



The first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The location in program memory is determined by a program counter (PC), which stores a number that identifies the current position in the program. In other words, the program counter keeps track of the CPU's place in the current program. After an instruction is fetched, the PC is incremented by the length of the instruction word in terms of memory units. Often the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).

The instruction that the CPU fetches from memory is used to determine what the CPU is to do. In the decode step, the instruction is broken up into parts that have significance to other portions of the CPU. The way in which the numerical instruction value is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of numbers in the instruction, called the opcode, indicates which operation to perform. The remaining parts of the number usually provide information required for that instruction, such as operands for an addition operation. Such operands may be given as a constant value (called an immediate value), or as a place to locate a value: a register or a memory address, as determined by some addressing mode. In older designs the portions of the CPU responsible for instruction decoding were unchangeable hardware devices. However, in more abstract and complicated CPUs and ISAs, a microprogram is often used to assist in translating instructions into various configuration signals for the CPU. This microprogram is sometimes rewritable so that it can be modified to change the way the CPU decodes instructions even after it has been manufactured.



After the fetch and decode steps, the execute step is performed. During this step, various portions of the CPU are connected so they can perform the desired operation. If, for instance, an addition operation was requested, an arithmetic logic unit (ALU) will be connected to a set of inputs and a set of outputs. The inputs provide the numbers to be added, and the outputs will contain the final sum. The ALU contains the circuitry to perform simple arithmetic and logical operations on the inputs (like addition and bitwise operations). If the addition operation produces a result too large for the CPU to handle, an arithmetic overflow flag in a flags register may also be set (see the discussion of integer range below).

The final step, writeback, simply "writes back" the results of the execute step to some form of memory. Very often the results are written to some internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but cheaper and larger, main memory. Some types of instructions manipulate the program counter rather than directly produce result data. These are generally called "jumps" and facilitate behavior like loops, conditional program execution (through the use of a conditional jump), and functions in programs. Many instructions will also change the state of digits in a "flags" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, one type of "compare" instruction considers two values and sets a number in the flags register according to which one is greater. This flag could then be used by a later jump instruction to determine program flow.

After the execution of the instruction and writeback of the resulting data, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If the completed instruction was a jump, the program counter will be modified to contain the address of the instruction that was jumped to, and program execution continues normally. In more complex CPUs than the one described here, multiple instructions can be fetched, decoded, and executed simultaneously. This section describes what is generally referred to as the "Classic RISC pipeline," which in fact is quite common among the simple CPUs used in many electronic devices (often called microcontrollers).

!Computer architecture|-!Digital circuits

The way a CPU represents numbers is a design choice that affects the most basic ways in which the device functions. Some early digital computers used an electrical model of the common decimal (base ten) numeral system to represent numbers internally. A few other computers have used more exotic numeral systems like ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a "high" or "low" voltage.



Related to number representation is the size and precision of numbers that a CPU can represent. In the case of a binary CPU, a bit refers to one significant place in the numbers a CPU deals with. The number of bits (or numeral places) a CPU uses to represent numbers is often called "word size", "bit width", "data path width", or "integer precision" when dealing with strictly integer numbers (as opposed to floating point). This number differs between architectures, and often within different parts of the very same CPU. For example, an 8-bit CPU deals with a range of numbers that can be represented by eight binary digits (each digit having two possible values), that is, 28 or 256 discrete numbers. In effect, integer size sets a hardware limit on the range of integers the software run by the CPU can utilize.

Integer range can also affect the number of locations in memory the CPU can address (locate). For example, if a binary CPU uses 32 bits to represent a memory address, and each memory address represents one octet (8 bits), the maximum quantity of memory that CPU can address is 232 octets, or 4 GiB. This is a very simple view of CPU address space, and many designs use more complex addressing methods like paging in order to locate more memory than their integer range would allow with a flat address space.

Higher levels of integer range require more structures to deal with the additional digits, and therefore more complexity, size, power usage, and general expense. It is not at all uncommon, therefore, to see 4- or 8-bit microcontrollers used in modern applications, even though CPUs with much higher range (such as 16, 32, 64, even 128-bit) are available. The simpler microcontrollers are usually cheaper, use less power, and therefore dissipate less heat, all of which can be major design considerations for electronic devices. However, in higher-end applications, the benefits afforded by the extra range (most often the additional address space) are more significant and often affect design choices. To gain some of the advantages afforded by both lower and higher bit lengths, many CPUs are designed with different bit widths for different portions of the device. For example, the IBM System/370 used a CPU that was primarily 32 bit, but it used 128-bit precision inside its floating point units to facilitate greater accuracy and range in floating point numbers . Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.



Most CPUs, and indeed most sequential logic devices, are synchronous in nature. That is, they are designed and operate on assumptions about a synchronization signal. This signal, known as a clock signal, usually takes the form of a periodic square wave. By calculating the maximum time that electrical signals can move in various branches of a CPU's many circuits, the designers can select an appropriate period for the clock signal. 

This period must be longer than the amount of time it takes for a signal to move, or propagate, in the worst-case scenario. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the "edges" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).

However architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided in order to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue as clock rates increase dramatically is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does heat dissipation, causing the CPU to require more effective cooling solutions.

One method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire CPUs have been built without utilizing a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS. Rather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers .



The description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as subscalar, operates on and executes one instruction on one or two pieces of data at a time.

This process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result the subscalar CPU gets "hung up" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock). However, the performance is nearly always subscalar (less than one instruction per cycle).

Attempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques. Instruction level parallelism (ILP) seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the utilization of on-die execution resources), and thread level parallelism (TLP) purposes to increase the number of threads (effectively individual programs) that a CPU can execute simultaneously. Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.



One of the simplest methods used to accomplish increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is the simplest form of a technique known as instruction pipelining, and is utilized in almost all modern general-purpose CPUs. Pipelining allows more than one instruction to be executed at any given time by breaking down the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.

Pipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. To cope with this, additional care must be taken to check for these sorts of conditions and delay a portion of the instruction pipeline if this occurs. Naturally, accomplishing this requires additional circuitry, so pipelined processors are more complex than subscalar ones (though not very significantly so). A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).



Further improvement upon the idea of instruction pipelining led to the development of a method that decreases the idle time of CPU components even further. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units.  In a superscalar pipeline, multiple instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so they are dispatched to available execution units, resulting in the ability for several instructions to be executed simultaneously. In general, the more instructions a superscalar CPU is able to dispatch simultaneously to waiting execution units, the more instructions will be completed in a given cycle.

Most of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly and correctly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and gives rise to the need in superscalar architectures for significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, and out-of-order execution crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may or may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies.

In the case where a portion of the CPU is superscalar and part is not, the part which is not suffers a performance penalty due to scheduling stalls. The original Intel Pentium (P5) had two superscalar ALUs which could accept one instruction per clock each, but its FPU could not accept one instruction per clock. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the Pentium architecture, P6, added superscalar capabilities to its floating point features, and therefore afforded a significant increase in floating point instruction performance.

Both simple pipelining and superscalar design increase a CPU's ILP by allowing a single processor to complete execution of instructions at rates surpassing one instruction per cycle (IPC). Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or ISA. The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the amount of work the CPU must perform to boost ILP and thereby reducing the design's complexity.

Another strategy of achieving performance is to execute multiple programs or threads in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as Multiple Instructions-Multiple Data or MIMD. 

One technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single silicon chip, the technology is known as a multi-core microprocessor. 

It was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel.This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated in order to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as block multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly switch to another thread which is ready to run, the switch often done in one CPU clock cycle. Another type of MT is known as simultaneous multithreading, where instructions of multiple threads are executed in parallel within one CPU clock cycle. 

For several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, Out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.

CPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or program. 

This reversal of emphasis is evidenced by the proliferation of dual and multiple core CMP (chip-level multiprocessing) designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design.



A less common but increasingly important paradigm of CPUs (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as SISD (single instruction, single data) and SIMD (single instruction, multiple data), respectively. The great utility in creating CPUs that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks are multimedia applications (images, video, and sound), as well as many types of scientific and engineering tasks. Whereas a scalar CPU must complete the entire process of fetching, decoding, and executing each instruction and value in a set of data, a vector CPU can perform a single operation on a comparatively large set of data with one instruction. Of course, this is only possible when the application tends to require many steps which apply one operation to a large set of data.

Most early vector CPUs, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose CPUs has become significant. Shortly after floating point execution units started to become commonplace to include in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose CPUs. Some of these early SIMD specifications like Intel's MMX were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating point numbers. Progressively, these early designs were refined and remade into some of the common, modern SIMD specifications, which are usually associated with one ISA. Some notable modern examples are Intel's SSE and the PowerPC-related AltiVec (also known as VMX).



 













Henry Ford (July 30, 1863 â€?April 7, 1947) was the American founder of the Ford Motor Company and father of modern assembly lines used in mass production. His introduction of the Model T automobile revolutionized transportation and American industry. He was a prolific inventor and was awarded 161 U.S. patents. As owner of the Ford Company he became one of the richest and best-known people in the world. He is credited with "Fordism", that is, the mass production of large numbers of inexpensive automobiles using the assembly line, coupled with high wages for his workers. Ford had a global vision, with consumerism as the key to peace. Ford did not believe in accountants; he amassed one of the world's largest fortunes without ever having his company audited under his administration. Henry Ford's intense commitment to lowering costs resulted in many technical and business innovations, including a franchise system that put a dealership in every city in North America, and in major cities on six continents. Ford left most of his vast wealth to the Ford Foundation but arranged for his family to control the company permanently.

Ford was born July 30, 1863, on a farm next to a rural town west of Detroit, Michigan (this area is now part of Dearborn, Michigan). His father, William Ford (1826-1905), was born in County Cork, Ireland. His mother, Mary Litogot Ford (1839-1876), was born in Michigan; she was the youngest child of Belgian immigrants; her parents died when Mary was a child and she was adopted by neighbours, the O'Herns. Henry Ford's siblings include Margaret Ford (1867-1868); Jane Ford (c. 1868-1945); William Ford (1871-1917) and Robert Ford (1873-1934).

Henry took his passion for mechanics into his home. His father had given him a pocket watch in Henry's early teens. At 15, he had a reputation as a watch repairman, having dismantled and reassembled timepieces of friends and neighbors dozens of times.

Ford's mother died in 1876, which came as a devastating blow to young Henry. His father expected him to eventually take over the family farm, but Henry despised farm work. With his mother dead, little remained to keep him on the farm. He later told his father, "I never had any particular love for the farmâ€”it was the mother on the farm I loved."

In 1879, he left home for the nearby city of Detroit to work as an apprentice machinist, first with James F. Flower & Bros., and later with the Detroit Dry Dock Co. In 1882, he returned to Dearborn to work on the family farm and became adept at operating the Westinghouse portable steam engine. He was later hired by Westinghouse company to service their steam engines. 

Ford married Clara Ala Bryant (c. 1865-1950) in 1888 and supported himself by farming and running a sawmill.  They had a single child: Edsel Bryant Ford (1893-1943).

In 1891, Ford became an engineer with the Edison Illuminating Company, and after his promotion to Chief Engineer in 1893, he had enough time and money to devote attention to his personal experiments on gasoline engines. These experiments culminated in 1896 with the completion of his own self-propelled vehicle named the Ford Quadricycle, which he test-drove on June 4. After various test-drives, Ford brainstormed ways to improve the Quadricycle.

Also in 1896, Ford attended a meeting of Edison executives, where he was introduced to Thomas Edison himself. Edison approved of Ford's automobile experimentation; encouraged by Edison's approval, Ford designed and built a second vehicle, which was completed in 1898. Backed by the capital of Detroit lumber baron William H. Murphy, Ford resigned from Edison and founded the Detroit Automobile Company on August 5 1899. However, the automobiles produced were of a lower quality and higher price than Ford liked. Ultimately, the company was not successful and was dissolved in January 1901.

Ford went to work building a racer. With the help of C. Harold Wills, he designed, built, and successfully raced a 26 horsepower automobile in October 1901. With that success, Murphy and other stockholders in the Detroit Automobile Company formed the Henry Ford Company on November 30 1901, with Ford as chief engineer. However, Murphy brought in Henry M. Leland as a consultant. As a result, Ford left the company bearing his name in 1902. With Ford gone, Murphy renamed the company the Cadillac Automobile Company.

Ford once again focused on building a racecar, producing the 80+ horsepower racer "999", and getting Barney Oldfield to drive it to victory in October 1902. Ford also received the backing of an old acquaintance, Alexander Y. Malcomson, a Detroit-area coal dealer. They formed a partnership, "Ford & Malcomson, Ltd." to manufacture automobiles. Ford went to work designing an inexpensive automobile, and the duo leased a factory and contracted with a machine shop owned by John and Horace E. Dodge to supply over $160,000 in parts. Sales were slow, and a crisis arose when the Dodge brothers demanded payment for their first shipment.

In response, Malcomson brought in another group of investors and convinced the Dodge Brothers to accept a portion of the new company. On June 16, 1903, Ford & Malcomson was reincorporated as the Ford Motor Company, with $28,000 capital. The original investors included Ford and Malcomson, the Dodge brothers, Malcomson's uncle John S. Gray, Horace Rackham, and James Couzens. In a newly designed car, Ford gave an exhibition on the ice of Lake St. Clair, driving 1 mile (1.6 km) in 39.4 seconds, setting a new land speed record at 91.3 miles per hour (147.0 km/h). Convinced by this success, the race driver Barney Oldfield, who named this new Ford model "999" in honor of a racing locomotive of the day, took the car around the country, making the Ford brand known throughout the United States. Ford also was one of the early backers of the Indianapolis 500.

Ford astonished the world in 1914 by offering a $5 per day wage, which more than doubled the rate of most of his workers. The move proved extremely profitable; instead of constant turnover of employees, the best mechanics in Detroit flocked to Ford, bringing in their human capital and expertise, raising productivity, and lowering training costs. Ford called it "wage motive." The company's use of vertical integration also proved successful when Ford built a gigantic factory that shipped in raw materials and shipped out finished automobiles. 

The Model T was introduced on October 1, 1908. It had many important innovationsâ€”such as the steering wheel on the left, which every other company soon copied. The entire engine and transmission were enclosed; the four cylinders were cast in a solid block; the suspension used two semi-elliptic springs. 

The car was very simple to drive, andâ€”more importantlyâ€”easy and cheap to repair. It was so cheap at $825 in 1908 (the price fell every year) that by the 1920s a majority of American drivers learned to drive on the Model T. 

Ford created a massive publicity machine in Detroit to ensure every newspaper carried stories and ads about the new product. Ford's network of local dealers made the car ubiquitous in virtually every city in North America. As independent dealers, the franchises grew rich and publicized not just the Ford but the very concept of automobiling; local motor clubs sprang up to help new drivers and to explore the countryside. Ford was always eager to sell to farmers, who looked on the vehicle as a commercial device to help their business. Sales skyrocketedâ€”several years posted 100% gains on the previous year. Always on the hunt for more efficiency and lower costs, in 1913 Ford introduced the moving assembly belts into his plants, which enabled an enormous increase in production. Although Henry Ford is often credited with the idea, contemporary sources indicate that the concept and its development came from employees Clarence Avery, Peter E. Martin, Charles E. Sorensen, and C.H. Wills. (See Piquette Plant) 

Sales passed 250,000 in 1914. By 1916], as the price dropped to $360 for the basic touring car, sales reached 472,000. 

 

By 1918, half of all cars in America were Model T's. However, it was a monolithic block; as Ford wrote in his autobiography, "Any customer can have a car painted any colour that he wants so long as it is black". Until the development of the assembly line, which mandated black because of its quicker drying time, Model T's were available in other colors including red. The design was fervently promoted and defended by Ford, and production continued as late as 1927; the final total production was 15,007,034. This record stood for the next 45 years. 

In 1918, President Woodrow Wilson personally asked Ford to run for the United States Senate from Michigan as a Democrat. Although the nation was at war, Ford ran as a peace candidate and a strong supporter of the proposed League of Nations. 

In December 1918, Henry Ford turned the presidency of Ford Motor Company over to his son Edsel Ford. Henry, however, retained final decision authority and sometimes reversed his son. Henry started another company, Henry Ford and Son, and made a show of taking himself and his best employees to the new company; the goal was to scare the remaining holdout stockholders of the Ford Motor Company to sell their stakes to him before they lost most of their value. (He was determined to have full control over strategic decisions). The ruse worked, and Henry and Edsel purchased all remaining stock from the other investors, thus giving the family sole ownership of the company.

By the mid-1920s, sales of the Model T began to decline due to rising competition. Other auto makers offered payment plans through which consumers could buy their cars, which usually included more modern mechanical features and styling not available with the Model T. Despite urgings from Edsel, Henry steadfastly refused to incorporate new features into the Model T or to form a customer credit plan.

By 1926, flagging sales of the Model T finally convinced Henry to make a new model. Henry pursued the project with a great deal of technical expertise in design of the engine, chassis, and other mechanical necessities, while leaving the body design to his son. Edsel also managed to prevail over his father's initial objections in the inclusion of a sliding-shift transmission. 

The result was the successful Ford Model A, introduced in December 1927 and produced through 1931, with a total output of more than 4 million. Subsequently, the company adopted an annual model change system similar to that in use by automakers today. Not until the 1930s did Ford overcome his objection to finance companies, and the Ford-owned Universal Credit Corporation became a major car-financing operation.

Henry Ford was a pioneer of "welfare capitalism" designed to improve the lot of his workers and especially to reduce the heavy turnover that had many departments hiring 300 men per year to fill 100 slots. Efficiency meant hiring and keeping the best workers. 

On January 5, 1914, Ford announced his $5-per-day program. The revolutionary program called for a raise in minimum daily pay from $2.34 to $5 for qualifying workers. It also set a new, reduced workweek, although the details vary in different accounts. Ford and Crowther in 1922 described it as six 8-hour days, giving a 48-hour week, while in 1926 they described it as five 8-hour days, giving a 40-hour week. (Apparently the program started with Saturdays as workdays and sometime later made them days off.) Ford says that with this voluntary change, labor turnover in his plants went from huge to so small that he stopped bothering to measure it.

Ford had been criticized by other industrialists and by Wall Street for starting the 40-hour work week and a minimum wage. He proved, however, that paying people more would enable Ford workers to afford the cars they were producing and therefore be good for the economy. Ford explained the change in part of the "Wages" chapter of My Life and Work. He labeled the increased compensation as profit-sharing rather than wages. 

The wage was offered to employees who had worked at the company for six months or more, and, importantly, conducted their lives in a manner of which Ford's "Social Department" approved. They frowned on heavy drinking, gambling, and what we today would call "deadbeat dads". The Social Department used 50 investigators, plus support staff, to maintain employee standards; a large percentage of workers were able to qualify for this "profit-sharing." 

Ford's incursion into his employees' private lives was highly controversial, and he soon backed off from the most intrusive aspects; by the time he wrote his 1922 memoir, he spoke of the Social Department and of the private conditions for profit-sharing in the past tense, and admitted that "paternalism has no place in industry. Welfare work that consists in prying into employees' private concerns is out of date. Men need counsel and men need help, oftentimes special help; and all this ought to be rendered for decency's sake. But the broad workable plan of investment and participation will do more to solidify industry and strengthen organization than will any social work on the outside. Without changing the principle we have changed the method of payment."

Ford was adamantly against labor unions. He explained his views on unions in Chapter 18 of My Life and Work. He thought they were too heavily influenced by some leaders who, despite their ostensible good motives, would end up doing more harm than good for workers. Most wanted to restrict productivity as a means to foster employment, but Ford saw this as self-defeating because, in his view, productivity was necessary for any economic prosperity to exist. 

He believed that productivity gains that obviated certain jobs would nevertheless stimulate the larger economy and thus grow new jobs elsewhere, whether within the same corporation or in others. Ford also believed that union leaders (most particularly Leninist-leaning ones) had a perverse incentive to foment perpetual socio-economic crisis as a way to maintain their own power. Meanwhile, he believed that smart managers had an incentive to do right by their workers, because doing so would actually maximize their own profits. (Ford did acknowledge, however, that many managers were basically too bad at managing to understand this fact.) But Ford believed that eventually, if good managers such as himself could successfully fend off the attacks of misguided people from both left and right (i.e., both socialists and bad-manager reactionaries), the good managers would create a socio-economic system wherein neither bad management nor bad unions could find enough support to continue existing.

To forestall union activity, Ford promoted Harry Bennett, a former Navy boxer, to be the head of the Service Department. Bennett employed various intimidation tactics to squash union organizing. The most famous incident, in 1937, was a bloody brawl between company security men and organizers that became known as The Battle of the Overpass.

In the late 1930s and early 1940s, Edsel (who was president of the company) thought it was necessary for Ford to come to some sort of collective bargaining agreement with the unions, because the violence, work disruptions, and bitter stalemates could not go on forever. But Henry (who still had the final veto in the company on a de facto basis even if not an official one) refused to cooperate. For several years, he kept Bennett in charge of talking to the unions that were trying to organize the Ford company. Sorensen's memoir makes clear that Henry's purpose in putting Bennett in charge was to make sure no agreements were ever reached.

The Ford company was the last Detroit automaker to recognize the United Auto Workers union (UAW). A sit-down strike by the UAW union in April 1941 closed the River Rouge Plant. Sorensen said a distraught Henry Ford was very close to following through with a threat to break up the company rather than cooperate but that his wife, Clara, told him she would leave him if he destroyed the family business that she wanted to see her son and grandsons lead into the future. Henry complied with his wife's ultimatum, and Ford went literally overnight from the most stubborn holdout among automakers to the one with the most favorable UAW contract terms. The contract was signed in June 1941.

Ford, like other automobile companies, entered the aviation business during World War I, building Liberty engines. After the war, it returned to auto manufacturing until 1925, when Henry Ford acquired the Stout Metal Airplane Company.

 

Ford's most successful aircraft was the Ford 4AT Trimotorâ€”called the â€œTin Gooseâ€?because of its corrugated metal construction. It used a new alloy called Alclad that combined the corrosion resistance of aluminum with the strength of duralumin. The plane was similar to Fokker's V.VII-3m, and some say that Ford's engineers surreptitiously measured the Fokker plane and then copied it. The Trimotor first flew on June 11 1926, and was the first successful U.S. passenger airliner, accommodating about 12 passengers in a rather uncomfortable fashion. Several variants were also used by the U.S. Army. Henry Ford has been honored by the Smithsonian Institution for changing the aviation industry. About 200 Trimotors were built before it was discontinued in 1933, when the Ford Airplane Division shut down because of poor sales during the Great Depression.

President Franklin D. Roosevelt referred to Detroit as the "Arsenal of Democracy." The Ford Motor Company played a pivotal role in the Allied victory during World War I and World War II. With Europe under siege, the Ford company's genius turned to mass production for the war effort. Specifically, Ford examined the B-24 Liberator bomber, still the most-produced Allied bomber in history, which quickly shifted the balance of power. 

Before Ford, and under optimal conditions, the aviation industry could produce one Consolidated Aircraft B-24 Bomber a day at an aircraft plant. Ford showed the world how to produce one B-24 an hour at a peak of 600 per month in 24-hour shifts. Ford's Willow Run factory broke ground in April 1941. At the time, it was the largest assembly plant in the world, with over . 

Mass production of the B-24, led by Charles Sorensen and later Mead Bricker, began by August 1943. Many pilots slept on cots waiting for takeoff as the B-24 rolled off the assembly line at Ford's Willow Run facility.

Henry Ford was an Episcopalian Christian who opposed war, which he thought was a waste of time. Ford became highly critical of those who he felt financed war, and he seemed to do whatever he could to stop them. He felt time was better spent making things. 

In 1915, Jewish pacifist Rosika Schwimmer had gained the favor of Henry Ford, who agreed to fund a peace ship to Europe, where World War I was raging, for himself and about 170 other prominent peace leaders. Ford's Episcopalian pastor, Reverend Samuel S. Marquis, accompanied him on the mission. Marquis also headed Ford's Sociology Department from 1913 to 1921. Ford talked to President Wilson about the mission but had no government support. His group went to neutral Sweden and the Netherlands to meet with peace activists there. As a target of much ridicule, he left the ship as soon as it reached Sweden.

An article G. K. Chesterton wrote for the December 12 1916, issue of Illustrated London News, shows why Ford's effort was ridiculed. Referring to Ford as "the celebrated American comedian," Chesterton noted that Ford had been quoted claiming, "I believe that the sinking of the Lusitania was deliberately planned to get this country [America] into war. It was planned by the financiers of war." Chesterton expressed "difficulty in believing that bankers swim under the sea to cut holes in the bottoms of ships," and asked why, if what Ford said was true, Germany took responsibility for the sinking and "defended what it did not do." Mr. Ford's efforts, he concluded, "queer the pitch" of "more plausible and presentable" pacifists.

On the other hand H.G. Wells, in "The Shape of Things to Come", devoted an entire chapter to the Ford Peace Ship, stating that "despite its failure, this effort to stop the war will be remembered when the generals and their battles and senseless slaughter are forgotten." Wells claimed that the American armaments industry and banks, who made enormous profits from selling munitions to the warring European nations, deliberately spread lies in order to cause the failure of Ford's peace efforts. He noted, however, that when the U.S. entered the war in 1917, Ford took part and made considerable profits from the sale of munitions.

The episode was fictionalized by the British novelist Douglas Galbraith in his novel King Henry.

Ford was an admirer of Nazi Germany. Adolf Hitler kept a life-size portrait of Ford next to his desk, "I regard Henry Ford as my inspiration," Hitler told a Detroit News reporter two years before becoming the Chancellor of Germany in 1933. In July 1938, four months after the German annexation of Austria, Ford was awarded the Grand Cross of the German Eagle, the highest medal awarded by Nazi Germany to foreigners.

Ford disliked the administration of President Franklin D. Roosevelt and did not approve of U.S. involvement in the war. Therefore, from 1939 to 1943, the War Production Board's dealings with the Ford Motor Company were with others in the organization, such as Edsel Ford and Charles Sorensen, much more than with Henry Ford. During this time Henry Ford did not stop his executives from cooperating with Washington, but he himself did not get deeply involved. He watched suspiciously, focusing on his own pet side projects, as the work progressed. 

After years of the Great Depression, labor strife, and New Deal, he suspected people in Washington were conspiring to wrest the company from his control. Ironically, his paranoia was trending toward self-fulfilling prophesy, as his attitude inspired background chatter in Washington about how to undermine his control of the company, whether by wartime government fiat or by instigating some sort of coup among executives and directors. In 1945, the war ended, Henry Ford II became company president, and the storm was past.



In 1918, Ford's closest aide and private secretary, Ernest G. Liebold, purchased an obscure weekly newspaper, The Dearborn Independent for Ford. The Independent ran for eight years, from 1920 until 1927, during which Liebold was editor. The newspaper published "Protocols of the Learned Elders of Zion," which was discredited by The Times of London as a forgery during the Independent's publishing run. The American Jewish Historical Society described the ideas presented in the magazine as "anti-immigrant, anti-labor, anti-liquor, and anti-Semitic." In February 1921, the New York World published an interview with Ford, in which he said "The only statement I care to make about the Protocols is that they fit in with what is going on." During this period, Ford emerged as "a respected spokesman for right-wing extremism and religious prejudice," reaching around 700,000 readers through his newspaper. 

Along with the Protocols, anti-Jewish articles published by The Dearborn Independent also were released in the early 1920s as a set of four bound volumes, cumulatively titled The International Jew, the World's Foremost Problem. Vincent Curcio wrote of these publications that "they were widely distributed and had great influence, particularly in Nazi Germany, where no less a personage than Adolf Hitler read and admired them." Hitler, fascinated with automobiles, hung Ford's picture on his wall; Ford is the only American mentioned in Hitler's book. Steven Watts wrote that Hitler "revered" Ford, proclaiming that "I shall do my best to put his theories into practice in Germany, and modeling the Volkswagen, the people's car, on the model T."

Denounced by the Anti-Defamation League (ADL), the articles nevertheless explicitly condemned pogroms and violence against Jews (Volume 4, Chapter 80), preferring to blame incidents of mass violence on the Jews themselves. None of this work was actually written by Ford, who wrote almost nothing according to trial testimony. Friends and business associates have said they warned Ford about the contents of the Independent and that he probably never read them. (He claimed he only read the headlines.) However, court testimony in a libel suit, brought by one of the targets of the newspaper, alleged that Ford did know about the contents of the Independent in advance of publication.

A libel lawsuit brought by San Francisco lawyer and Jewish farm cooperative organizer Aaron Sapiro in response to anti-Semitic remarks led Ford to close the Independent in December 1927. News reports at the time quoted him as being shocked by the content and having been unaware of its nature. During the trial, the editor of Ford's "Own Page," William Cameron, testified that Ford had nothing to do with the editorials even though they were under his byline. Cameron testified at the libel trial that he never discussed the content of the pages or sent them to Ford for his approval. Investigative journalist Max Wallace noted that "whatever credibility this absurd claim may have had was soon undermined when James M. Miller, a former Dearborn Independent employee, swore under oath that Ford had told him he intended to expose Sapiro."

Michael Barkun observed, "That Cameron would have continued to publish such controversial material without Ford's explicit instructions seemed unthinkable to those who knew both men. Mrs. Stanley Ruddiman, a Ford family intimate, remarked that 'I don't think Mr. Cameron ever wrote anything for publication without Mr. Ford's approval.'" According to Spencer Blakeslee,



Ford's 1927 apology had been well received, "Four-Fifths of the hundreds of letters addressed to Ford in July of 1927 were from Jews, and almost without exception they praised the Industrialist." In January 1937, a Ford statement to the Detroit Jewish Chronicle disavowed "any connection whatsoever with the publicaton in Germany of a book known as the International Jew." 

In July 1938, prior to the outbreak of war, the German consul at Cleveland gave Ford, on his 75th birthday, the award of the Grand Cross of the German Eagle, the highest medal Nazi Germany could bestow on a foreigner, while James D. Mooney, vice-president of overseas operations for General Motors, received a similar medal, the Merit Cross of the German Eagle, First Class. 

Distribution of International Jew was halted in 1942 through legal action by Ford despite complications from a lack of copyright,but extremist groups often recycle the material; it still appears on antisemitic and neo-Nazi websites.

One Jewish personality who was said to have been friendly with Ford is Detroit Judge Harry Keidan. When asked about this connection, Ford replied that Keidan was only half-Jewish. A close collaborator of Henry Ford during World War II reported that Ford, at the time being more than 80 years old, was shown a movie of the Nazi concentration camps.

Ford's philosophy was one of economic independence for the United States. His River Rouge Plant became the world's largest industrial complex, even able to produce its own steel. Ford's goal was to produce a vehicle from scratch without reliance on foreign trade. He believed in the global expansion of his company. He believed that international trade and cooperation led to international peace, and he used the assembly line process and production of the Model T to demonstrate it. He opened Ford assembly plants in Britain and Canada in 1911, and soon became the biggest automotive producer in those countries. In 1912, Ford cooperated with Agnelli of Fiat to launch the first Italian automotive assembly plants. The first plants in Germany were built in the 1920s with the encouragement of Herbert Hoover and the Commerce Department, which agreed with Ford's theory that international trade was essential to world peace. In the 1920s Ford also opened plants in Australia, India, and France, and by 1929, he had successful dealerships on six continents. Ford experimented with a commercial rubber plantation in the Amazon jungle called FordlÃ¢ndia; it was one of the few failures. In 1929, Ford accepted Stalin's invitation to build a model plant (NNAZ, today GAZ) at Gorky, a city later renamed Nizhny Novgorod, and he sent American engineers and technicians to help set it up, including future labor leader Walter Reuther.  

The technical assistance agreement between Ford Motor Company, VSNH and the Soviet-controlled American Trading Organization (AMTORG) (as purchasing agent) was concluded for nine years and signed on May 31, 1929, by Ford, FMC vice-president Peter E. Martin, V. I. Mezhlauk, and the president of Amtorg, Saul G. Bron. The Ford Motor Company worked to conduct business in any nation where the United States had peaceful diplomatic relations:

By 1932, Ford was manufacturing one third of all the worldâ€™s automobiles.

Ford's image transfixed Europeans, especially the Germans, arousing the "fear of some, the infatuation of others, and the fascination among all". Germans who discussed "Fordism" often believed that it represented something quintessentially American. They saw the size, tempo, standardization, and philosophy of production demonstrated at the Ford Works as a national serviceâ€”an "American thing" that represented the culture of United States. Both supporters and critics insisted that Fordism epitomized American capitalist development, and that the auto industry was the key to understanding economic and social relations in the United States. As one German explained, "Automobiles have so completely changed the American's mode of life that today one can hardly imagine being without a car. It is difficult to remember what life was like before Mr. Ford began preaching his doctrine of salvation" For many Germans, Henry Ford embodied the essence of successful Americanism.

In My Life and Work, Ford predicted essentially that if greed, racism, and short-sightedness could be overcome, then eventually economic and technologic development throughout the world would progress to the point that international trade would no longer be based on (what today would be called) colonial or neocolonial models and would truly benefit all peoples. His ideas here were vague, but they were idealistic and they seemed to indicate a belief in the inherent intelligence of all ethnicities (which some may find somewhat suspect coming from Ford). 



Ford maintained an interest in auto racing from 1901 to 1913 and began his involvement in the sport as both a builder and a driver, later turning the wheel over to hired drivers. He entered stripped-down Model Ts in races, finishing first (although later disqualified) in an "ocean-to-ocean" (across the United States) race in 1909, and setting a one-mile (1.6 km) oval speed record at Detroit Fairgrounds in 1911 with driver Frank Kulick. In 1913, Ford attempted to enter a reworked Model T in the Indianapolis 500 but was told rules required the addition of another 1,000 pounds (450 kg) to the car before it could qualify. Ford dropped out of the race and soon thereafter dropped out of racing permanently, citing dissatisfaction with the sport's rules, demands on his time by the booming production of the Model Ts, and his low opinion of racing as a worthwhile activity.

In My Life and Work Ford speaks (briefly) of racing in a rather dismissive tone, as something that is not at all a good measure of automobiles in general. He describes himself as someone who raced only because in the 1890s through 1910s, one had to race because prevailing ignorance held that racing was the way to prove the worth of an automobile. Ford did not agree. But he was determined that as long as this was the definition of success (flawed though the definition was), then his cars would be the best that there were at racing. Throughout the book he continually returns to ideals such as transportation, production efficiency, affordability, reliability, fuel efficiency, economic prosperity, and the automation of drudgery in farming and industry, but rarely mentions, and rather belittles, the idea of merely going fast from point A to point B.

Nevertheless, Ford did make quite an impact on auto racing during his racing years, and he was inducted into the Motorsports Hall of Fame of America in 1996.

When Edsel, president of Ford Motor Company, died of cancer in May 1943, the elderly and ailing Henry Ford decided to assume the presidency. By this point in his life, he had had several cardiovascular events (variously cited as heart attack or stroke) and was mentally inconsistent, suspicious, and generally no longer fit for such a job. 

Most of the directors did not want to see him as president. But for the previous 20 years, though he had long been without any official executive title, he had always had de facto control over the company; the board and the management had never seriously defied him, and this moment was not different. The directors elected him, and he served until the end of the war. During this period the company began to decline, losing more than $10 million a month. The administration of President Franklin Roosevelt had been considering a government takeover of the company in order to ensure continued war production, but the idea never progressed.

In ill health, he ceded the presidency to his grandson Henry Ford II in September 1945 and went into retirement. He died in 1947 of a cerebral hemorrhage at age 83 in Fair Lane, his Dearborn estate, and he is buried in the Ford Cemetery in Detroit. 

Henry Ford long had an interest in materials science and engineering. He enthusiastically described his company's adoption of vanadium steel alloys and subsequent metallurgic R&D work.

Ford long had an interest in plastics developed from agricultural products, especially soybeans. He cultivated a relationship with George Washington Carver for this purpose. Soybean-based plastics were used in Ford automobiles throughout the 1930s in plastic parts such as car horns, in paint, etc. This project culminated in 1942, when Ford patented an automobile made almost entirely of plastic, attached to a tubular welded frame. It weighed 30% less than a steel car and was said to be able to withstand blows ten times greater than could steel. Furthermore, it ran on grain alcohol (ethanol) instead of gasoline. The design never caught on.

Ford was interested in engineered woods ("Better wood can be made than is grown") (at this time plywood and particle board were little more than experimental ideas); corn as a fuel source, via both corn oil and ethanol; and the potential uses of cotton. Ford was instrumental in developing charcoal briquets, under the brand name "Kingsford". His brother in law, E.G. Kingsford, used wood scraps from the Ford factory to make the briquets.

Ford maintained a vacation residence (known as the "Ford Plantation") in Richmond Hill, Georgia. He contributed substantially to the community, building a chapel and schoolhouse and employing numerous local residents.

Ford had an interest in "Americana". In the 1920s, Ford began work to turn Sudbury, Massachusetts, into a themed historical village. He moved the schoolhouse supposedly referred to in the nursery rhyme, Mary had a little lamb, from Sterling, Massachusetts, and purchased the historical Wayside Inn. This plan never saw fruition, but Ford repeated it with the creation of Greenfield Village in Dearborn, Michigan. It may have inspired the creation of Old Sturbridge Village as well. About the same time, he began collecting materials for his museum, which had a theme of practical technology. It was opened in 1929 as the Edison Institute and, although greatly modernized, remains open today.

Both Henry Ford and Karl Benz are sometimes oversimplistically credited with the "invention of the automobile", although (as is the case with most inventions) the reality of the automobile's development included many inventors. As Ford himself said, by the 1870s, the notion of a horseless carriage was "a common idea". What the following decades brought was the technical success of the idea, and the extension of the idea beyond steam power to other power sources (electric motors and internal combustion engines). Ford was, however, more influential than any other single person in changing the paradigm of the automobile from a scarce, heavy, hand-built toy for rich people into a lightweight, reliable, affordable, mass-produced mode of transportation for the masses of working people.

Both Henry Ford and Ransom E. Olds are sometimes oversimplistically credited with the "invention of the assembly line", although (as is the case with most inventions) the reality of the assembly line's development included many inventors. One prerequisite was the idea of interchangeable parts (which was another gradual technological development often mistakenly attributed to one individual or another). Ford's first moving assembly line (employing conveyor belts), after 5 years of empirical development, first began mass production on or around April 1, 1913. The idea was tried first on subassemblies, and shortly after on the entire chassis. Again, although it is inaccurate to say that Henry Ford himself "invented" the assembly line, it is accurate to say that his sponsorship of its development was central to its explosive success in the 20th century.

Ford was the winner of the award of Car Entrepreneur of the Century in 1999.

Ford held a strong dislike for activities which included touching. This belief led him to found many square-dancing clubs.

An Episcopalian, Henry Ford dressed up as Santa Claus and gave sleigh rides to children at Christmas time on his estate. 

Henry Ford was especially fond of Thomas Edison, and on Edison's deathbed, he demanded Edison's son catch his final breath in a test tube. The test tube can still be found today in Henry Ford Museum.

In 1923, Ford's pastor, and head of his sociology department, the Episcopal minister Samuel S. Marquis, claimed that Ford believed, or "once believed" in reincarnation. Though it is unclear whether or how long Ford kept such a belief, the San Francisco Examiner from August 26 1928, published a quote which described Ford's beliefs:





















Salt is a dietary mineral essential for animal life, composed primarily of sodium chloride. Salt for human consumption is produced in different forms: unrefined salt (such as sea salt), refined salt (table salt), and iodised salt. It is a crystalline solid, white, pale pink or light grey in color, normally obtained from sea water or rock deposits. Edible rock salts may be slightly greyish in color due to this mineral content.

Chloride and sodium ions, the two major components of salt, are necessary for the survival of all living creatures including humans. Some isolated cultures, such as the Yanomami in South America, have been found to consume little salt, possibly an adaptation originated in the predominantly vegetarian diet of human primate ancestors. Salt is involved in regulating the water content (fluid balance) of the body. Salt flavor is one of the basic tastes. Salt cravings may be caused by trace mineral deficiencies as well as by a deficiency of sodium chloride itself.

Overconsumption of salt increases the risk of health problems, including high blood pressure (see Health effects below). In food preparation, salt is used as a preservative and as a seasoning.

Human beings have enjoyed canning and artificial refrigeration for only a couple of centuries; for the countless millennia before then, salt provided the best-known preservative of food, especially meat.

The harvest of salt from the surface of the salt lake Yuncheng in Shanxi dates back to at least 6000 B.C., making it one of the oldest verifiable saltworks.

Salt was included among funereal offerings found in ancient Egyptian tombs from the third millennium B.C., as were salted birds and salt fish. About 2800 B.C., the Egyptians began exporting salt fish to the Phoenicians in return for Lebanon Cedar, glass, and the dye Tyrian purple; the Phoenicians traded Egyptian salt fish and salt from North Africa throughout their Mediterranean trade empire.

On the river Salzach in central Austria, within a radius of no more than 17 kilometres, lie Salzburg, Hallstatt, and Hallein. Salzach literally means "salt water" and Salzburg "salt city", both taking their names from the Germanic root for salt, salz; Hallstatt literally means "salt town" and Hallein "saltwork", taking their names from hal(l)-, a root for salt found in Celtic, Greek, and Egyptian. The root hal(l)- also gave us Gaul, the Roman exonym for the Celts. Hallstatt and Hallein in Austria, Halle and SchwÃ¤bisch Hall in Germany, Halych in Ukraine, and Galicia in Spain: this list of places named for Celtic saltworks is far from complete.

Hallstatt gave its name to the Celtic archaeological culture that began mining for salt in the area in around 1300 B.C. Around 400 B.C., the Hallstatt Celts, who had heretofore mined for salt, began flushing the salt out of mines as brine and boiling off the excess water. During the first millennium B.C., Celtic communities grew rich trading salt and salted meat to Ancient Greece and Ancient Rome in exchange for wine and other luxuries.

At times, troops in the Roman army were even paid in salt, which is the origin of the words salary and, by way of French, soldier. The word salad literally means "salted," and comes from the ancient Roman practice of salting leaf vegetables.

There are thirty-five verses which reference salt in the English translation of the Bible (King James Version), the earliest being the story of Lot's wife, who was turned into a pillar of salt when she disobediently looked back at the wicked cities of Sodom (Genesis 19:26). When King Abimelech destroyed the city of Shechem he is said to have "sowed salt on it;" a phrase expressing the completeness of its ruin. (Judges 9:45.) In the Sermon on the Mount, Jesus referred to his followers as the "salt of the earth". The apostle Paul also encouraged Christians to "let your conversation be always full of grace, seasoned with salt" (Colossians 4:6).

Salt is mandatory in the rite of the Tridentine Mass. Salt is used in the third item (which includes an Exorcism) of the Celtic Consecration (cf. Gallican rite) that is employed in the consecration of a church. Salt may be added to the water "where it is customary" in the Roman Catholic rite of Holy water.

In the native Japanese religion Shinto, salt is used for ritual purification of locations and people, such as in Sumo Wrestling.

In Aztec mythology, Huixtocihuatl was a fertility goddess who presided over salt and salt water.

Different natural salts have different mineralities, giving each one a unique flavor. Fleur de sel, natural sea salt harvested by hand, has a unique flavor varying from region to region.

Some advocates for sea salt assert that unrefined sea salt is more healthy than refined salts. However, completely raw sea salt is bitter due to magnesium and calcium compounds, and thus is rarely eaten. The refined salt industry cites scientific studies saying that raw sea and rock salts do not contain enough iodine salts to prevent iodine deficiency diseases.

Unrefined sea salts are also commonly used as ingredients in bathing additives and cosmetic products. One example are bath salts, which uses sea salt as its main ingredient and combined with other ingredients used for its healing and therapeutic effects.

Refined salt, which is most widely used presently, is mainly sodium chloride. Food grade salt accounts for only a small part of salt production in industrialised countries (3% in Europe) although world-wide, food uses account for 17.5% of salt production. The majority is sold for industrial use. Salt has great commercial value, because it is a necessary ingredient in the manufacturing of many things. A few common examples include: the production of pulp and paper, setting dyes in textiles and fabrics, and the making of soaps and detergents.

The manufacture and use of salt is one of the oldest chemical industries. Salt is also obtained by evaporation of sea water, usually in shallow basins warmed by sunlight; salt so obtained was formerly called bay salt, and is now often called sea salt or solar salt. Today, most refined salt is prepared from rock salt: mineral deposits high in salt. These rock salt deposits were formed by the evaporation of ancient salt lakes. These deposits may be mined conventionally or through the injection of water. Injected water dissolves the salt, and the brine solution can be pumped to the surface where the salt is collected.

After the raw salt is obtained, it is refined to purify it and improve its storage and handling characteristics. Purification usually involves recrystallization. In recrystallization, a brine solution is treated with chemicals that precipitate most impurities (largely magnesium and calcium salts). Multiple stages of evaporation are then used to collect pure sodium chloride crystals, which are kiln-dried.  Since the 1950s it has been common to add a trace of sodium hexacyanoferrate(II) to the brine; this acts as an anticaking agent by promoting irregular crystals. Other anticaking agents (and potassium iodide, for iodised salt) are generally added after crystallization. These agents are hygroscopic chemicals which absorb humidity, keeping the salt crystals from sticking together. Some anticaking agents used are tricalcium phosphate, calcium or magnesium carbonates, fatty acid salts (acid salts), magnesium oxide, silicon dioxide, calcium silicate, sodium alumino-silicate, and alumino-calcium silicate. Concerns have been raised regarding the possible toxic effects of aluminium in the latter two compounds; however, both the European Union and the United States Food and Drug Administration (FDA) permit their use. The refined salt is then ready for packing and distribution.

Table salt is refined salt, 99% sodium chloride. It usually contains substances that make it free flowing (anticaking agents) such as sodium silicoaluminate or magnesium carbonate. It is common practice to put a few grains of uncooked rice or half a dry cracker (such as Saltine) in salt shakers to absorb extra moisture when anticaking agents are not enough. 



Iodized salt (BrE: iodised salt) is table salt mixed with a minute amount of potassium iodide, sodium iodide, or iodate. Iodized salt is used to help reduce the chance of iodine deficiency in humans. Iodine deficiency commonly leads to thyroid gland problems, specifically endemic goiter. Endemic goiter is a disease characterized by a swelling of the thyroid gland, usually resulting in a bulbous protrusion on the neck. While only tiny quantities of iodine are required in a diet to prevent goiter, the United States Food and Drug Administration recommends (21 CFR 101.9 (c)(8)(iv)) 150 micrograms of iodine per day for both men and women, and there are many places around the world where natural levels of iodine in the soil are low and the iodine is not taken up by vegetables. 

Today, iodized salt is more common in the United States, Australia and New Zealand than in the United Kingdom.Table salt is also often iodizedâ€”a small amount of potassium iodide (in the US) or potassium iodate (in the EU) is added as an important dietary supplement. Table salt is mainly employed in cooking and as a table condiment. Iodized table salt has significantly reduced disorders of iodine deficiency in countries where it is used. Iodine is important to prevent the insufficient production of thyroid hormones (hypothyroidism), which can cause goitre, cretinism in children, and myxedema in adults.

The amount of iodine and the specific iodine compound added to salt varies from country to country. In the United States, iodized salt contains 46-77 ppm, while in the UK the iodine content of iodized salt is recommended to be 10-22 ppm.

In some European countries where drinking water fluoridation is not practiced, fluorinated table salt is available. In France, 35% of sold table salt contains either sodium fluoride or potassium fluoride. Another additive, especially important for pregnant women, is Folic acid (Vitamin B9), which gives the table salt a yellow color.

In Canada, at least one brand (Windsor salt) contains invert sugar. The reason for this is unclear.

In many East Asian cultures, salt is not traditionally used as a condiment.However, condiments such as soy sauce, fish sauce and oyster sauce tend to have a high salt content and fill much the same role as a salt-providing table condiment that table salt serves in western cultures.

Sodium is one of the primary electrolytes in the body. All four cationic electrolytes (sodium, potassium, magnesium, and calcium) are available in unrefined salt, as are other vital minerals needed for optimal bodily function. Too much or too little salt in the diet can lead to muscle cramps, dizziness, or even an electrolyte disturbance, which can cause severe, even fatal, neurological problems. Drinking too much water, with insufficient salt intake, puts a person at risk of water intoxication (hyponatremia). Salt is even sometimes used as a health aid, such as in treatment of dysautonomia.

People's risk for disease due to insufficient or excessive salt intake varies due to biochemical individuality. Some have asserted that while the risks of consuming too much salt are real, the risks have been exaggerated for most people, or that the studies done on the consumption of salt can be interpreted in many different ways.

Excess salt consumption has been linked to:

Sea salt (an unrefined form of salt made by evaporating sea water) is often sold for use as a condiment. It does not, however, have any significant health advantages above normal table salt. Certain sea salts are also used in the production of bath salts and cosmetic products.



This section summarizes the salt intake recommended by the health agencies of various countries. Recommendations tend to be similar. Note that targets for the population as a whole tend to be pragmatic (what is achievable) while advice for an individual is ideal (what is best for health). For example, in the UK target for the population is "eat no more than 6 g a day" but for a person is 4 g.

Intakes can be expressed variously as salt or sodium and in various units.

United Kingdom: In 2003, the UK's Scientific Advisory Committee on Nutrition (SACN) recommended that, for a typical adult, the Reference Nutrient Intake is 4 g salt per day (1.6 g or 70 mmol sodium). However, average adult intake is two and a half times the Reference Nutrient Intake for sodium. "Although accurate data are not available for children, conservative estimates indicate that, on a body weight basis, the average salt intake of children is higher than that of adults." SACN aimed for an achievable target reduction in average intake of salt to 6 g per day (2.4 g or 100 mmol sodium) â€?this is roughly equivalent to a teaspoonful of salt. The SACN recommendations for children are:

SACN states, "The target salt intakes set for adults and children do not represent ideal or optimum consumption levels, but achievable population goals."

Republic of Ireland: The Food Safety Authority of Ireland endorses the UK targets "emphasising that the RDA of 1.6 g sodium (4 g salt) per day should form the basis of advice targeted at individuals as distinct from the population health target of a mean salt intake of 6 g per day."(, p16)

Canada: Health Canada recommends an Adequate Intake (AI) and an Upper Limit (UL) in terms of sodium.

New Zealand

Australia: The recommended dietary intake (RDI) is 0.92 gâ€?.3 g sodium per day (= 2.3 gâ€?.8 g salt)

USA: The Food and Drug Administration itself does not make a recommendation but refers readers to Dietary Guidelines for Americans 2005. These suggest that US citizens should consume less than 2,300 mg of sodium (= 2.3 g sodium = 5.8 g salt) per day.

UK: The Food Standards Agency defines the level of salt in foods as follows: "High is more than 1.5g salt per 100g (or 0.6g sodium). Low is 0.3g salt or less per 100g (or 0.1g sodium). If the amount of salt per 100g is in between these figures, then that is a medium level of salt." In the UK, foods produced by some supermarkets and manufacturers have â€˜traffic lightâ€?colors on the front of the pack: Red (High), Amber (Medium), or Green (Low).

USA: The FDA Food Labeling Guide stipulates whether a food can be labelled as "free", "low", or "reduced/less" in respect of sodium. When other health claims are made about a food (e.g. low in fat, calories, etc.), a disclosure statement is required if the food exceeds 480mg of sodium per 'serving.'

In 2004, Britain's Food Standards Agency started a public health campaign called "Salt - Watch it", which recommends no more than 6g of salt per day; it features a character called Sid the Slug and was criticised by the Salt Manufacturers Association (SMA). The Advertising Standards Authority did not uphold the SMA complaint in its adjudication.. In March 2007, the FSA launched the third phase of their campaign with the slogan "Salt. Is your food full of it?" fronted by comedienne Jenny Eclair.

The Menzies Research Institute in Tasmania, Australia, maintains a website  dedicated to educating people about the potential problems of a salt-laden diet.

Consensus Action on Salt and Health (CASH) established in 1996, actively campaigns to raise awareness of the harmful health effects of salt. The 2008 focus includes raising awareness of high levels of salt hidden in sweet foods and marketed towards children.

Salt intake can be reduced by simply reducing the quantity of salty foods in a diet, without recourse to salt substitutes. Salt substitutes have a taste similar to table salt and contain mostly potassium chloride, which will increase potassium intake. Excess potassium intake can cause hyperkalemia. Various diseases and medications may decrease the body's excretion of potassium, thereby increasing the risk of hyperkalemia. If you have kidney failure, heart failure or diabetes, seek medical advice before using a salt substitute. A manufacturer, LoSalt, has issued an advisory statement that people taking the following prescription drugs should not use a salt substitute: Amiloride, Triamterene, Dytac, Spironolactone (Brand name Aldactone), Eplerenone and Inspra.

Salt is produced by evaporation of seawater or brine from other sources, such as brine wells and salt lakes, and by mining rock salt, called halite. In 2002, total world production was estimated at 210 million metric tonnes, the top five producers being the United States (40.3 million tonnes), China (32.9), Germany (17.7), India (14.5), and Canada (12.3). Note that these figures are not just for table salt but for sodium chloride in general.







Many other government bodies are listed in the References section above.

</div>

Chemical data









Texas hold 'em (also hold'em, holdem) is the most popular poker game in the casinos and poker card rooms across North America and Europe. Hold 'em is a community card game where each player may use any combination of the five community cards and the player's own two hole cards to make a poker hand, in contrast to poker variants like stud or draw where each player holds a separate individual hand.

After slow but steady gains in popularity throughout the 20th century, hold 'em's popularity surged in the 2000s due to exposure on television, on the Internet, and in popular literature. During this time hold 'em replaced 7 card stud as the most common game in U.S. casinos, almost totally eclipsing the once popular game. The no-limit betting form is used in the widely televised main event of the World Series of Poker (WSOP) and the World Poker Tour (WPT). 

Because each player only starts with two cards and the remaining cards are shared, it presents an opportune game for strategic analysis (including mathematical analysis). Hold 'em's simplicity and popularity have inspired a wide variety of strategy books which provide recommendations for proper play. Most of these books recommend a strategy that involves playing relatively few hands but betting and raising often with the hands one plays.

In Texas hold 'em, like all variants of poker, individuals compete for an amount of money contributed by the players themselves (called the pot). Because the cards are dealt randomly and outside the control of the players, each player attempts to control the amount of money in the pot based on the hand the player holds. 

The game is divided into a series of hands or deals; at the conclusion of each hand the pot is awarded to one or a few players. A hand ends either at the showdown (when the remaining players compare their hands), or when all but one player have folded and abandoned their claims to the pot. The pot is then awarded to the player(s) who have not folded and have the best hand. (This is usually only one player, but can be more in the case of a tie.)

The objective of winning players is not winning every individual hand, but rather making mathematically correct decisions. By making such decisions, winning poker players maximize their long run winnings, which is achieved by maximizing their expected utility on each round of betting.



Although little is known about the invention of Texas hold 'em, the Texas State Legislature officially recognizes Robstown, Texas as the game's birthplace, dating the game to the early 1900s. 

After its invention and spread throughout Texas, hold 'em was introduced to Las Vegas in 1967 by a group of Texan gamblers and card players, including Crandell Addington, Doyle Brunson, and Amarillo Slim. Addington said the first time he saw the game was in 1959. "They didn't call it Texas hold 'em at the time, they just called it hold 'em... I thought then that if it were to catch on, it would become the game. Draw poker, you only bet twice; hold 'em, you bet four times. That meant you could play strategically. This was more of a thinking man's game."

The game was later introduced to Europe by bookmakers Terry Rogers and Liam Flood. 

For several years the Golden Nugget Casino in Downtown Las Vegas was the only casino in Las Vegas to offer the game. At that time, the Golden Nugget's poker room was "truly a 'sawdust joint,' with... oiled sawdust covering the floors." Because of its location and decor, this poker room did not receive many rich drop-in clients, and as a result, professional players sought a more prominent location. In 1969, the Las Vegas professionals were invited to play Texas hold 'em at the entrance of the now-demolished Dunes Casino on the Las Vegas Strip. This prominent location, and the relative inexperience of poker players with Texas hold 'em, resulted in a very remunerative game for professional players.

After a disappointing attempt to establish a "Gambling Fraternity Convention", Tom Moore added the first ever poker tournament to the Second Annual Gambling Fraternity Convention held in 1969. This tournament featured several games including Texas hold 'em. In 1970 Benny and Jack Binion acquired the rights to this convention, renamed it the World Series of Poker, and moved it to their casino Binion's Horseshoe Casino in Las Vegas. After its first year, a journalist, Tom Thackrey, suggested that the main event of this tournament should be no-limit Texas hold 'em. The Binions agreed and ever since no-limit Texas hold 'em has been played as the main event. Interest in the Main Event continued to grow steadily over the next two decades. After receiving only 8 entrants in 1972, the numbers grew to over 100 entrants in 1982, and over 200 in 1991.

During this time, Doyle Brunson's revolutionary poker strategy guide, Super/System was first published. Despite being self-published and priced at $100 in 1978, the book revolutionized the way poker was played. It was one of the first books to discuss Texas hold 'em, and is today cited as one of the most important books on this game. A few years later, Al Alvarez published a book detailing an early World Series of Poker event. The first book of its kind, it described the world of professional poker players and the World Series of Poker. It is credited with beginning the genre of poker literature and with bringing Texas hold 'em (and poker generally), for the first time, to a wider audience.

Interest in hold 'em outside of Nevada began to grow in the 1980s as well. Although California had legal card rooms offering draw poker, Texas hold 'em was prohibited under a statute which made illegal the now unknown game "stud-horse". However in 1988, Texas hold 'em was declared legally distinct from "stud-horse" in Tibbetts v. Van De Kamp, 271 Cal. Rptr. 792 (1990). Almost immediately card rooms across the state offered Texas hold 'em.



In the first decade of the 21st century, Texas hold 'em experienced a surge in popularity worldwide. Many observers attribute this growth to the synergy of five factors: the invention of online poker, the game's appearance in film and on television, the 2004-05 NHL lockout, the appearance of television commercials advertising online cardrooms, and the 2003 World Series of Poker championship victory by online qualifier Chris Moneymaker. 



Prior to poker becoming widely televised, the movie Rounders (1998), starring Matt Damon and Edward Norton, gave moviegoers a romantic view of the game as a way of life. Texas hold 'em was the main game played during the movie and the no-limit variety was described, following Doyle Brunson, as the "Cadillac of Poker". A clip of the classic showdown between Johnny Chan and Erik Seidel from the 1988 World Series of Poker was also incorporated into the film. The 2006 James Bond movie Casino Royale also featured No-Limit Hold 'em as a major plot device.

Hold 'em first caught the public eye as a spectator sport in the United Kingdom with the Late Night Poker TV show in 1999. Fueled by the introduction of lipstick cameras, which allowed spectators to see the players' private cards, hold 'em exploded in popularity as a spectator sport in the United States and Canada in 2003. ESPN's coverage of the 2003 World Series of Poker featured the unexpected victory of Internet player Chris Moneymaker, an amateur player who gained admission to the tournament by winning a series of online tournaments. Moneymaker's victory initiated a sudden surge of interest in the World Series, based on the egalitarian idea that anyone â€?even a rank novice â€?can become a world champion.

In 2003, there were 839 entrants in the WSOP Main Event,and triple that number in 2004. The crowning of the 2004 WSOP champion, Greg "Fossilman" Raymer, a patent attorney from Connecticut, further fueled the popularity of the event among amateur (and particularly internet) players. In the 2005 Main Event, an unprecedented 5,619 entrants vied for a first prize of $7,500,000. The winner, Joe Hachem of Australia, was a semi-professional player. This growth continued in 2006, with 8,773 entrants and a first place prize of $12,000,000 (won by Jamie Gold).

Beyond the World Series, other television shows â€?including the long running World Poker Tour â€?are credited with increasing the popularity of Texas hold 'em. In addition to its presence on network and general audience cable television, poker has now become a regular part of sports networks' programming in the United States.

Twenty years after the publication of Alvarez's groundbreaking book, James McManus published a semi-autobiographical book, Positively Fifth Street (2003), which simultaneously describes the trial surrounding the murder of Ted Binion and McManus' own entry into the 2000 World Series of Poker. McManus, a poker amateur, finished 5th in the No-Limit Texas Hold 'em main event, winning over $200,000. In the book, McManus discusses events surrounding the World Series, the trial of Sandy Murphy and Rick Tabish, poker strategy, and some history of poker and the world series.

Michael Craig's 2005 book The Professor, the Banker, and the Suicide King details a series of high stakes Texas hold 'em one-on-one games between Texas banker Andy Beal and a rotating group of poker professionals. As of 2006, these games were the highest stakes ever played, reaching $100,000â€?200,000 fixed limit.





The ability to play cheaply and anonymously online has been credited as a cause of the increase in popularity of Texas hold 'em. Online poker sites both allow people to try out games and also provide an avenue for entry into large tournaments (like the World Series of Poker) via smaller tournaments known as satellites. Both the 2003 and 2004 winners of the World Series qualified by playing in these tournaments.

Although online poker grew from its inception in 1998 until 2003, Moneymaker's win and the appearance of televisions advertisements in 2003 contributed to a tripling of industry revenues in 2004.

Hold 'em is normally played using small and big blind bets â€?forced bets by two players. Antes (forced contributions by all players) may be used in addition to blinds, particularly in later stages of tournament play. A dealer button is used to represent the player in the dealer position; the dealer button rotates clockwise after each hand, changing the position of the dealer and blinds. The small blind is posted by the player to the left of the dealer and is usually equal to half of the big blind. The big blind, posted by the player to the left of the small blind, is equal to the minimum bet. In tournament poker, the blind/ante structure periodically increases as the tournament progresses. (In some cases, the small blind is some other fraction of a small bet, e.g. $10 is a common small blind when the big blind is $15. The double-blind structure described above is a commonly used and more recent adoption.)

When only two players remain, special 'head-to-head' or 'heads up' rules are enforced and the blinds are posted differently. In this case, the person with the dealer button posts the small blind, while his/her opponent places the big blind. The dealer acts first before the flop. After the flop, the dealer acts last for the remainder of the hand.

The three most common variations of hold 'em are limit hold 'em, no-limit hold 'em and pot-limit hold 'em. Limit hold 'em has historically been the most popular form of hold 'em found in casino live action games in the United States. In limit hold 'em, bets and raises during the first two rounds of betting (pre-flop and flop) must be equal to the big blind; this amount is called the small bet. In the next two rounds of betting (turn and river), bets and raises must be equal to twice the big blind; this amount is called the big bet. No-limit hold 'em is the form most commonly found in televised tournament poker and is the game played in the main event of the World Series of Poker. In no-limit hold 'em, players may bet or raise any amount over the minimum raise up to all of the chips the player has at the table (called an all-in bet). If someone wishes to re-raise, they must raise at least the amount of the previous raise. For example, if the big blind is $2 and there is a bet of $6 to a total of $8, a raise must be at least $6 more for a total of $14. If a raise or re-raise is all-in and does not equal the size of the previous raise, the initial raiser can not re-raise again. This only matters of course if there was a call before the re-raise. In pot-limit hold 'em, the maximum raise is the current size of the pot.

Most casinos that offer hold 'em also allow the player to the left of the big blind to post an optional live straddle, usually double the amount of the big blind, which then acts as the big blind. No-limit games may also allow multiple re-straddles, in any amount that would be a legal raise.



Play begins with each player being dealt two cards face down. (Like most poker games, the deck is a standard 52 card deck, no jokers.) These cards are the player's hole or pocket cards. These are the only cards each player will receive individually, and they will only (possibly) be revealed at the showdown, making Texas hold 'em a closed poker game. 

The hand begins with a "pre-flop" betting round, beginning with the player to the left of the big blind (or the player to the left of the dealer, if no blinds are used) and continuing clockwise. A round of betting continues until every player has either folded, put in all of their chips, or matched the amount put in by all other active players. See betting for a detailed account. Note that the blinds are considered "live" in the pre-flop betting round, meaning that they contribute to the amount that the blind player must contribute, and that, if all players call around to the player in the big blind position, that player may either check or raise.

After the pre-flop betting round, assuming there remain at least two players taking part in the hand, the dealer deals a flop, three face-up community cards. The flop is followed by a second betting round. This and all subsequent betting rounds begin with the player to the dealer's left and continue clockwise. 

After the flop betting round ends, a single community card (called the turn or fourth street) is dealt, followed by a third betting round. A final single community card (called the river or fifth street) is then dealt, followed by a fourth betting round and the showdown, if necessary. 

In all casinos, the dealer will burn a card before the flop, turn, and river. Because of this burn, players who are betting cannot see the back of the next community card to come, which might be marked. 

If a player bets and all other players fold, then the remaining player is awarded the pot and is not required to show his hole cards. If two or more players remain after the final betting round, a showdown occurs. On the showdown, each player plays the best five-card poker hand he can make from the seven cards comprising his two hole cards and the five community cards. A player may use both of his own two hole cards, only one, or none at all, to form his final five-card hand. If the five community cards form the player's best hand, then the player is said to be playing the board and can only hope to split the pot, since each other player can also use the same five cards to construct the same hand.

If the best hand is shared by more than one player, then the pot is split equally among them, with any extra chips going to the first players after the button in clockwise order. It is common for players to have closely-valued, but not identically ranked hands. Nevertheless, one must be careful in determining the best hand; if the hand involves fewer than five cards, (such as two pair or three of a kind), then kickers are used to settle ties (see the second example below). Note that the card's numerical rank is of sole importance; suit values are irrelevant in Hold'em.

Here's a sample showdown:

Each player plays the best 5-card hand they can make with the seven cards available. They have

In this case, Ted's full house is the best hand, with Carol in 2nd, Alice in 3rd and Bob last.



Here is a sample game involving four players. The players' individual hands will not be revealed until the showdown, to give a better sense of what happens during play:

Compulsory bets: Alice is the dealer. Bob, to Alice's left, posts a small blind of $1, and Carol posts a big blind of $2.

Pre-flop: Alice deals two hole cards face down to each player, beginning with Bob and ending with herself. Ted must act first because he is the first player after the big blind. He cannot check, since the $2 big blind plays as a bet, so he folds. Alice calls the $2. Bob adds an additional $1 to his $1 small blind to call the $2 total. Carol's blind is "live" (see blind), so she has the option to raise here, but she checks instead, ending the first betting round. The pot now contains $6, $2 from each of three players.

Flop: Alice now burns a card and deals the flop of three face-up community cards, . On this round, as on all subsequent rounds, the player on the dealer's left begins the betting. In this case it is Bob, who checks. Carol opens for $2, Ted has already folded and Alice raises another $2 (puts in $4, $2 to match Carol and $2 to raise), making the total bet now facing Bob $4. He calls (puts in $4, $2 to match Carol's initial bet and $2 to match Alice's raise). Carol calls as well, putting in her $2. The pot now contains $18, $6 from the last round and $12 from three players this round.

Turn: Alice now burns another card and deals the turn card face up. It is the . Bob checks, Carol checks, and Alice checks; the turn has been checked around. The pot still contains $18.

River: Alice burns another card and deals the final river card, the , making the final board . Bob bets $4, Carol calls, and Alice folds (Alice's holding was ; she was hoping the river card would be a club to make her hand a flush).

Showdown: Bob shows his hand of , so the best five-card hand he can make is , for three nines, with a king-queen kicker. Carol shows her cards of , making her final hand  for two pair, kings and nines, with a jack kicker. Bob wins the showdown and the $26 pot.

Because of the presence of community cards in Texas hold 'em, different players' hands can often run very close in value. As a result, it is not uncommon for kickers to be used to determine the winning hand and also for two hands to tie. A kicker is a card which is part of the five card poker hand, but is not used in determining a hand's rank. For instance, in the hand A-A-A-K-Q, the king and queen are kickers. 

The following situation illustrates the importance of breaking ties with kickers and card ranks, as well as the use of the five-card rule. After the turn, the board and players' hole cards are as follows.

At the moment, Bob is in the lead with a hand of , making two pair, queens and eights, with a king kicker. This beats Carol's hand of  by virtue of his king kicker. 

Suppose the final card was the , making the final board . Bob and Carol still each have two pair (Queens and eights), but both of them are now entitled to play the final ace as their fifth card, making their hands both two pair, queens and eights, with an ace kicker. Bob's king no longer plays, because the ace on the board plays as the fifth card in both hands, and a hand is only composed of five cards. They therefore tie and split the pot.



Most poker authors recommend a tight-aggressive approach to playing Texas hold 'em. This strategy involves playing relatively few hands (tight), but betting and raising often with those that one does play (aggressive). Although this strategy is often recommended, some professional players successfully employ other strategies as well. 

Almost all authors agree that where a player sits in the order of play (known as position) is an important element of Texas hold 'em strategy, particularly in no-limit hold'em. Players who act later have more information than players who act earlier. As a result, players typically play fewer hands from early positions than later positions. 

Because of the game's level of complexity, it has received some attention from academics. One attempt to develop a quantitative model of a Texas hold'em tournament as an isolated complex system has had some success, although the full consequences for optimal strategies remain to be explored. In addition, a group at the University of Alberta is developing a poker playing program utilizing techniques in game theory and artificial intelligence.



While most poker authors focus on playing primarily premium starting hands, some authors claim that the importance of starting hands is overstated. Because there are only two cards dealt to each player, it is easy to characterize all of the starting hands. One important factor to the strength of a starting hand is the number of players in the pot. Hand strength can be highly diluted as more players enter the pot. There are (52 Ã— 51) Ã· 2 = 1,326 distinct possible combinations of two cards from a standard 52-card deck. Because no suit is more powerful than another, many of these can be equated for the analysis of starting-hand strategy. For example, although  and  are distinct combinations of cards, they are of equal value as starting hands. 

Viewed this way there are only 169 different hole-card combinations. Thirteen of those hands would be pairs, from 2 through ace. There are 78 ways to have two cards of dissimilar rank (12 possible hands containing an ace, 11 possible hands containing a king and no ace, 10 possible hands containing a queen and no ace or king, etc.). Hole cards can both be used in a flush if they are suited, but pairs are never suited, so there would be 13 possible pairs, 78 possible suited non-pairs, and 78 possible unsuited non-pairs, for a total of 169 possible hands. Suited starting cards are usually considered stronger than unsuited hands, although the magnitude of this strength in different games is debated. 

Because of this limited number of starting hands, most strategy guides involve a detailed discussion of each of these 169 starting hands. This separates hold 'em from other poker games where the number of starting card combinations forces strategy guides to group hands into broad categories. There are many colloquial names for individual hands.

Prior to the invention of poker tournaments, poker games were played with real money where players bet actual currency (or chips which represented currency). Games which feature wagering actual money on individual hands are still very common and are referred to as "cash games" or "ring games". 

The no-limit and fixed-limit cash game versions of hold 'em are strategically very different. Doyle Brunson claims that "the games are so different that there are not many players who rank with the best in both types of hold 'em. Many no-limit players have difficulty gearing down for limit, while limit players often lack the courage and 'feel' necessary to excel at no-limit." Because the size of bets is restricted in limit games, the ability to bluff is somewhat curtailed. Since one is not (usually) risking all of one's chips in limit poker, players are sometimes advised to take more chances.

Lower stakes games also exhibit different properties than higher stakes games. Small stakes games often involve more players in each hand and can vary from extremely passive (little raising and betting) to extremely aggressive (many raises). The difference of small stakes games have resulted in several books dedicated to only those games.



Texas hold 'em is often commonly associated with poker tournaments largely because it is played as the main event in many of the famous tournaments, including the World Series of Poker's Main Event, and is the most common tournament overall. Traditionally, a poker tournament is played with chips that represent a player's stake in the tournament. Standard play allows all entrants to "buy-in" a fixed amount and all players begin with an equal value of chips. Play proceeds until one player has accumulated all the chips in play. The money pool is redistributed to the players in relation to the place they finished in the tournament. Usually only a small percentage of the players receive any money, with the majority receiving nothing. As a result the strategy in poker tournaments can be very different from a cash game.

Proper strategy in tournaments can vary widely depending on the amount of chips one has, the stage of the tournament, the amount of chips others have, and the playing styles of one's opponents. Although some authors still recommend a tight playing style, others recommend looser play (playing more hands) in tournaments than one would otherwise play in cash games. In tournaments the blinds and antes increase regularly, and can become much larger near the end of the tournament. This can force players to play hands that they would not normally play when the blinds were small, which can warrant both more loose and more aggressive play. 

There are several other poker variants which resemble Texas hold 'em. Hold 'em is a member of a class of poker games known as community card games, where some cards are available for use by all the players. There are several other games that use five community cards in addition to some private cards and are thus similar to Texas hold 'em. Royal hold 'em has the same structure as Texas hold 'em, but the deck contains only Aces, Kings, Queens, Jacks, and Tens. Pineapple and Omaha hold 'em both vary the number of cards an individual receives before the flop, but are dealt identically afterward. Alternatively, in Double-board hold'em all players receive the same number of private cards, but there are two sets of community cards. The winner is either selected for each individual board with each receiving half of the pot, or the best overall hand takes the entire pot, depending on the rules agreed upon by the players. 

Manila is a hold'em variant popular in Australia. In Manila, players receive two private cards from a reduced deck (containing no cards lower than 7). A five card board is dealt, unlike Texas hold 'em, one card at a time; there is a betting round after each card. Manila has several variations of its own, similar to the variants listed above.







Super Typhoon Tip was the largest and most intense tropical cyclone on record. The nineteenth tropical storm, twelfth typhoon, and third super typhoon of the 1979 Pacific typhoon season, Tip developed out of a disturbance in the monsoon trough on October 4 near Pohnpei. Initially, a tropical storm to its northwest hindered the development and motion of Tip, though after it tracked further north Tip was able to intensify. After passing Guam, it rapidly intensified and reached peak winds of 306Â km/h (190Â mph) and a worldwide record low sea level pressure of 870Â mbar (hPa) on October 12. At its peak strength, it was also the largest tropical cyclone on record with a diameter of 2,220Â km (1,380Â mi). It slowly weakened as it continued west-northwestward, and later turned to the northeast under the influence of an approaching trough. Tip made landfall on southern Japan on October 19, and became an extratropical cyclone shortly thereafter.

Airforce Reconaissance flew into the typhoon for 60Â missions, making Tip one of the most closely observed tropical cyclones of all time. Rainfall from the typhoon breached a flood-retaining wall at a United States Marine Corps training camp in the Kanagawa Prefecture of Japan, leading to a fire which injured 68 and killed 13Â Marines. Elsewhere in the country, it led to widespread flooding and 42Â deaths. 44 were killed or left unaccounted for due to shipwrecks offshore.

Three circulations developed within the monsoon trough that extended from the Philippines to the Marshall Islands. A disturbance to the southwest of Guam developed into Tropical Storm Roger on October 3, and later on the same day a tropical disturbance which later became Typhoon Tip organized to the south of Pohnpei. Strong flow from across the equator was drawn into the circulation of Roger, initially preventing significant development of the disturbance which would become Tip. Despite the unfavorable air pattern, the tropical disturbance near Pohnpei gradually organized as it moved westward. Due to the large-scale circulation pattern into Tropical Storm Roger, the tropical disturbance moved erratically and slowly executed a cyclonic loop a short distance to the southeast of Chuuk. A Reconnaissance Aircraft flight into the system late on October 4 confirmed the existence of a closed low-level circulation, and early on October 5 the Joint Typhoon Warning Center issued its first warning on Tropical Depression Twenty-Three.

While executing a loop near Chuuk, the tropical depression intensified into Tropical Storm Tip, though the storm initially failed to organize significantly due to the influence of Tropical Storm Roger. Reconnaissance Aircraft provided the track of the surface circulation, as satellite estimated the center being about 60Â km (38Â mi) from its true position. After drifting erratically for several days, Tip turned to a steady northwest motion on October 8. By that time, Tropical Storm Roger had become an extratropical cyclone, resulting in the southerly flow being entrained into Tip. Additionally, an area of the Tropical Upper Tropospheric Trough moved to the north of Guam, providing an excellent outflow channel the north of Tip. Initially, the storm was predicted to continue northwestward and make landfall on Guam, though early on October 9 it turned to the west, passing about 45Â km (28Â mi) south of the island. Later that day, Tip intensified to attain typhoon status.

As a result of very favorable conditions for development, Typhoon Tip rapidly intensified in the open waters of the western Pacific Ocean. Late on October 10, the typhoon attained the equivalence of a Category 4 hurricane on the Saffir-Simpson Scale, and the next day it became a super typhoon. The central pressure dropped 92Â mbar (hPa) from October 9 to 11, during which the circulation pattern of Typhoon Tip increased to a record diameter of 2,220Â km (1,380Â mi). The typhoon continued to intensify further, and early on October 12 Reconnaissance Aircraft recorded a world record-low pressure of 870Â mbar (hPa) with winds of 305Â km/h (190Â mph) while located about 840Â km (520Â mi) west-northwest of Guam. At the time of its peak strength, its eye was 15Â km (9.3Â mi) wide.

At its peak intensity, the temperature inside the eye of Typhoon Tip was 30Â°Â C (86Â°Â F) and described as exceptionally high. A weather officer on the Reconnaissance Aircraft mission at the peak intensity of Tip described an unusual double-helix spiral striation in the eye from the base of the wall cloud to the top. After peaking in intensity, Tip weakened to a 230Â km/h (145Â mph) typhoon and remained at that intensity for several days as it continued west-northwestward. For five days after reaching its peak strength, the average radius of winds stronger than 55Â km/h (35Â mph) extended over 1,100Â km (685Â mi). On October 17, Tip began to steadily weaken as its size reduced, and the next day it began recurving northeastward under the influence of a mid-level trough. After passing about 65Â km (45Â mi) east of Okinawa, it accelerated its forward motion to 75Â km/h (46Â mph), and on October 19 Tip made landfall on the Japanese island of HonshÅ« with winds of about 130Â km/h (80Â mph). The typhoon continued rapidly northeastward through the country and became an extratropical cyclone over northern HonshÅ« a few hours after moving ashore. The extratropical remnant of Tip continued to the northeastward before losing its identity on October 21 to the east of the Kamchatka Peninsula.

The typhoon produced heavy rainfall early in its lifetime while passing near Guam, including a total of 23.1Â cm (9.09Â in) at Andersen Air Force Base. The outer rainbands of the large circulation of Tip produced moderate rainfall in the mountainous regions of the Philippine island of Luzon.

During recurvature, Typhoon Tip passed about 65Â km (40Â mi) east of Okinawa. There, sustained winds reached 72Â km/h) (44Â mph) with gusts to 112Â km/h (69Â mi). Sustained winds in Japan are unknown though of at least minimal typhoon strength. The passage of the typhoon through the archipelago resulted in millions of dollars in damage to the agricultural and fishing industries of the country. Eight ships were grounded or sunk by Tip, leaving 44Â fishermen dead or unaccounted for. A Chinese freighter broke in half as a result of the typhoon, though its crew of 46 were rescued. Heavy rainfall from the typhoon breached a flood-retaining wall at Camp Fuji, a training facility for the United States Marine Corps near Yokosuka. The fuel caught on fire, killing thirteen Marines, injuring 68, and causing moderate damage to the facility. The facility's barracks were destroyed, along with fifteen huts and several other buildings. The barracks were rebuilt, and a memorial was established for those who lost their lives in the fire. The rainfall led to over 600 mudslides throughout the mountainous regions of Japan, and also flooded more than 22,000Â homes. 42Â people died throughout the country, with another 71Â missing and 283Â injured.

Typhoon Tip was the largest tropical cyclone on record with a diameter of 2,220Â km (1,380Â mi), almost double the previous record set by Typhoon Marge in August of 1951. It was also the most intense tropical cyclone on record with a pressure of 870 mbar (hPa), 6Â mbar lower than previous record set by Super Typhoon June in 1975. The records set by Tip still stand, though due to the end of routine Reconnaissance Aircraft in the western Pacific Ocean in August of 1987, modern researchers questioned if Tip is the strongest on record. After a detailed study, three researchers determined two typhoons, Angela in 1995 and Gay in 1992, maintained higher Dvorak numbers than Tip, and believed that one or both of the two may have been more intense than Tip. Additionally, Cyclone Monica of the 2005 Australian cyclone season registered an unofficial Dvorak estimate of 869 mbar. However, due to lack of direct observations, it is unknown if Tip maintains the world record or not.













The 1812 Fire of Moscow broke out on September 14, 1812 in Moscow on the day when Russian troops and most residents abandoned the city and Napoleon's vanguard troops entered the city following the Battle of Borodino. The fire raged until September 18, destroying an estimated three-quarters of Moscow. 

Before leaving Moscow, Napoleon gave orders to have the Kremlin and major public buildings (including churches and monasteries) to be either blown up or set on fire. But this was not the foremost cause of the conflagration that destroyed the city. As the bulk of the French army moved into the city, there were some fires, which historians sympathetic to Napoleon's cause traditionally blame on Russian sabotage. It is believed that Moscow governor Fyodor Rostopchin had made preparations for anything that might have been of any use to the French army - food stores, granaries, warehouses and cloth stores - to be torched once the city was evacuated by the Russians.

La Grande ArmÃ©e, that set its position in a military camp manner and was carelessly looting sellable valuables, had also its share of responsibility: many buildings caught fire from bonfires they made for cooking. In any case, the catastrophe started as many small fires, which promptly grew out of control and formed a massive blaze. Napoleon's police measures and executions of "arsonists" were put into effect after much of the city was already ablaze.

Tolstoy, in War and Peace, suggests that the fire was not deliberately set, either by the Russians or the French: the natural result of placing a wooden city in the hands of strangers in wintertime is that they will make small fires to stay warm, cook their food, and other benign purposes, and that some of those fires will get out of control. Without an efficient Fire Department, these house fires will spread to become neighborhood fires and ultimately a city-wide conflagration.

Dates in Gregorian calendar (new style) and numbers referenced to Clausewitz and Tarle





Ivan Katayev (1911) summarized losses as 3/4 of all properties in the city:

An estimated 2,000 wounded Russian soldiers perished in the fire. Moscow State University, Buturlin's library, Petrovsky and Arbatsky theaters were completely destroyed; many pieces of art, notably the original of The Tale of Igor's Campaign, were lost forever. The Moscow Orphanage near Kitai-gorod, converted to a hospital, was saved by local police. The population of Moscow in 1811 is estimated at 270,000; after the war, when residents returned to the city, it decreased to 215,000; by 1840, it had increased to 349,000 (Filippov). 

Maps compiled by Russian authorities after the war (notably the 1817 military map reprinted for the public in the 1831 guide book) show that the majority of Moscow territory had succumbed to the fire. Notable exceptions are Moscow Kremlin, the Orphanage, northern segment of Bely Gorod from Tverskaya Street to Pokrovka Street, Patriarshy Ponds in the west, as well as suburban settlements. 

The map probably exaggerates the damage, showing some surviving blocks as if they were destroyed. For instance, Bolshaya Nikitskaya Street west from Boulevard Ring retained many of its mansions: troops defended their own lodgings and the French theatre, as well as the French colony in Kuznetsky Most. On the other hand, French patronage did not help the Batashov Palace (present-day Yauzskaya Hospital), occupied by Murat's headquarters: after two days of firefighting, it was consumed by fire that razed Taganka. Still, the remaining buildings had enough space for the French army. As General de Marbot reasoned, "It is often claimed that the fire of Moscow .. was the principal cause of the failure of the 1812 campaign. This assertion seems to me to be contestable. To begin with, the destruction of Moscow was not so complete that there did not remain enough houses, palaces, churches and barracks to accommodate the entire army" (for a whole month). However, many units were stationed not in the city, but in remote suburbs like Ostankino (light cavalry) or Khimki (Italian corps); others were dispatched south to screen Russian movements.

Shortage of funds, state and private, delayed reconstruction of Moscow by at least five years. In those years, many properties were sold by ruined owners, and whole neighborhoods changed their social status; for example, all properties on once-diverse Maroseika Street were bought out by merchant class (Sytin, p.105).



The disaster allowed the authorities a unique opportunity to plan the city from scratch. In February, 1813, Alexander I of Russia set up the Commission of Building in Moscow, with the instruction to produce a viable master plan for the city. The 1813 plan by William Hastie was deemed inadequate for the task, thus the Commission hired numerous local architects and topographers who produced the final, 1817, master plan (incorporating Hastie's ideas of clearing the Central Squares of Moscow). In 1816-1830, city planners set up the Garden Ring, a circular highway in place of an old fortification rampart, and widened many other streets. 

Later in 1817, the city held groundbreaking ceremony for Alexander Witberg's Cathedral of Christ the Saviour, a monument to the 1812 War in Sparrow Hills. This project was later canceled and the actual Cathedral emerged in the center of Moscow.Reconstruction of Red Square and Kitai-gorod was handled by Joseph Bove, who designed the neoclassical Upper Trade Rows as a mirror of Matvey Kazakov's Kremlin Senate. In February 1818, Ivan Martos completed the Monument to Minin and Pozharsky, the first public monument in Moscow, placed in the center of Red Square. Bove also designed the symmetrical Theatre Square and completed Bolshoi and Maly theaters by 1825. Moscow University and other public buildings were rebuilt by Domenico Giliardi and Afanasy Grigoriev. 

Bove also handled the "faÃ§ade department", authorizing faÃ§ade designs for all new buildings. A severe shortage of brick, stone and cement forced many developers to build in wood; the city had to agree with the inevitable, on condition that the houses follow the neoclassical standards. Local craftsmen responded with mass-produced wooden imitations of classical ornaments. Most of these houses were eventually destroyed. Extant examples include a recently restored house on the corner of Glazovsky and Denezhny Lanes in Arbat District, and Vasily Pushkin house in Staraya Basmannaya Street.





Able Archer 83 was a ten-day NATO command post exercise starting on November 2 1983 that spanned Western Europe, centred on SHAPE's Headquarters situated at Casteau, north of the Belgian city of Mons. The exercise simulated a period of conflict escalation, culminating in a coordinated nuclear release. It incorporated a new, unique format of coded communication, radio silences, participation by heads of state, and a simulated DEFCON 1 nuclear alert. The realistic nature of the exercise, coupled with deteriorating relations between the United States and the Soviet Union and the anticipated arrival of Pershing II nuclear missiles in Europe, led some in the USSR to believe that Able Archer 83 was a ruse of war, obscuring preparations for a genuine nuclear first strike. In response, the Soviets readied their nuclear forces and placed air units in East Germany and Poland on alert. This relatively obscure incident is considered by many historians to be the closest the world has come to nuclear war since the Cuban Missile Crisis of 1962. The threat of nuclear war abruptly ended with the conclusion of the Able Archer 83 exercise on November 11, which, coincidentally, was also Armistice Day (alternatively called Remembrance Day or Veterans Day).

The greatest catalyst to the Able Archer war scare occurred more than two years earlier. In a May 1981 closed-session meeting of senior KGB officers and Soviet leaders, General Secretary Leonid Brezhnev and KGB chairman Yuri Andropov bluntly announced that the United States was preparing a secret nuclear attack on the USSR. To combat this threat, Andropov announced, the KGB and GRU would begin Operation RYAN. RYAN (Ð Ð¯Ð) was a Russian acronym for "Nuclear Missile Attack" (Ð Ð°ÐºÐµÑ‚Ð½Ð¾Ðµ Ð¯Ð´ÐµÑ€Ð½Ð¾Ðµ ÐÐ°Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ); Operation RYAN was the largest, most comprehensive peacetime intelligence-gathering operation in Soviet history. Agents abroad were charged with monitoring the figures who would decide to launch a nuclear attack, the service and technical personnel who would implement the attack, and the facilities from which the attack would originate. In all probability, the unlikely-sounding goal of Operation RYAN was to discover the first intent of a nuclear attack and then prevent it.

The impetus for the implementation of Operation RYAN is still largely unknown. Oleg Gordievsky, the highest-ranking KGB official ever to defect, suspected that it was born of the increased "Soviet Paranoia" coupled with "Reaganite Rhetoric". Gordievsky conjectured that Brezhnev and Andropov, who "were very, very old-fashioned and easily influencedâ€?by Communist dogmas," truly believed that an antagonistic Reagan would push the nuclear button and relegate the Soviet Union to the "ash heap of history". CIA historian Benjamin B. Fischer lists several concrete occurrences that likely led to the birth of RYAN. The first of these was the use of psychological operations (PSYOP) that began soon after President Reagan took office.

Psychological operations began mid-February 1981 and continued intermittently until 1983. These included a series of clandestine naval operations that stealthily accessed waters near the Greenland-Iceland-United Kingdom (GIUK) gap, and the Barents, Norwegian, Black, and Baltic seas, demonstrating the very close proximity NATO ships could attain to critical Soviet military bases. American bombers also flew directly towards Soviet airspace, peeling off at the last moment, occasionally several times per week. These penetrations were designed to test Soviet radar vulnerability as well as demonstrate US capabilities in a nuclear war.



In contrast to the extremely secretive PSYOPs against the Soviet Union, the Soviet attack on the Korean civilian airliner Korean Air Flight 007 (KAL 007), on September 1, 1983, brought relations between the two superpowers to a new public low. Illustrating the historically antagonistic relations between the USA and USSR in the early 1980s, the Soviet attack on KAL 007 also lends several insights into Able Archer 83. The Soviet Union, perhaps due to PSYOP penetrations, strongly guarded its territorial airspace. The disaster demonstrated the hair-trigger mindset held by many in the Soviet Union. Reagan assessed in his memoirs, "If, as some people speculated, the Soviet pilots simply mistook the airliner for a military plane, what kind of imagination did it take to think of a Soviet military man with his finger close to a nuclear push-button making an even more tragic mistake?"

The flightpath of KAL 007 sent the Boeing 747 aircraft over the northern Pacific Ocean area toward the proximal area of the Soviet Union's southern border with the Pacific. In that general area, a highly sensitive Soviet nuclear air-base was located at Sakhalin.

On March 23 1983, Reagan proposed the Strategic Defense Initiative, labeled by the media and critics as "Star Wars". While Reagan viewed the initiative as a safety net against nuclear war, leaders in the Soviet Union viewed it as a definitive departure from the relative weapons parity of dÃ©tente and an escalation of the arms race into space. Andropov â€?who had become General Secretary following Brezhnev's death in November 1982 â€?lambasted Reagan for "inventing new plans on how to unleash a nuclear war in the best way, with the hope of winning it."

Despite the enormous Soviet outcry over the "Star Wars" program, the weapons plan that generated the most alarm among the Soviet Union's leadership during Able Archer 83 was the 1979 NATO approval and subsequent deployment of intermediate-range Pershing II missiles in Western Europe. These missiles, deployed to counter Soviet SS-20 intermediate-range missiles on its own western border, represented a major threat to the Soviets. The Pershing II was capable of destroying Soviet "hard targets" such as underground missile silos and command and control bunkers. The missiles could be emplaced and launched from any surveyed site in minutes, and because the guidance system was self-correcting, the missile system possessed a genuine "first strike capability". Furthermore, it was estimated that the missiles (deployed in West Germany) could reach targets in the western Soviet Union within four to six minutes of their launch. These capabilities led Soviet leaders to believe that the only way to survive a Pershing II strike was to preempt it. This fear of an undetected Pershing II attack, according to CIA historian Benjamin B. Fischer, was explicitly linked to the mandate of Operation RYAN: to detect a decision by the United States to launch a nuclear attack and (it must be believed) to preempt it.

Thus, on November 2 1983, as Soviet intelligence services were attempting to detect the signs of a nuclear strike, NATO began to simulate one. The exercise, codenamed Able Archer, involved numerous NATO allies and simulated NATO's Command, Control, and Communications (CÂ³) procedures during a nuclear war. Some Soviet leadership, because of the preceding world events and the exercise's particularly realistic nature, believed â€?in accordance with Soviet military doctrine â€?that the exercise may have been a cover for an actual attack. Indeed, a KGB telegram of February 17 described one likely scenario as such:



The February 17 1983 KGB Permanent Operational Assignment assigned its agents to monitor several possible indicators of a nuclear attack. These included actions by "A cadre of people associated with preparing and implementing decision about RYAN, and also a group of people, including service and technical personnelâ€?those working in the operating services of installations connected with processing and implementing the decision about RYAN, and communication staff involved in the operation and interaction of these installations."

Because Able Archer 83 simulated an actual release, it is likely that the service and technical personnel mentioned in the memo were active in the exercise. More conspicuously, British Prime Minister Margaret Thatcher and West German Chancellor Helmut Kohl participated (though not concurrently) in the nuclear drill. United States President Reagan, Vice President George H. W. Bush, and Secretary of Defense Caspar Weinberger were also intended to participate. Robert McFarlane, who had assumed the position of National Security Advisor just two weeks earlier, realized the implications of such participation early in the exercise's planning and rejected it.

Another illusory indicator likely noticed by Soviet analysts was an influx of ciphered communications between the United Kingdom and the United States. Soviet intelligence was informed that "so-called nuclear consultations in NATO are probably one of the stages of immediate preparation by the adversary for RYAN." To the Soviet analysts, this burst of secret communications between the United States and the UK one month before the beginning of Able Archer may have appeared to be this "consultation". In reality, the burst of communication regarded the US invasion of Grenada on October 25, 1983, which caused a great deal of diplomatic traffic as the sovereign of the island was Queen Elizabeth II.

A further startling aspect reported by KGB agents regarded the NATO communications used during the exercise. According to the Moscow Centre's February 17 1983 memo,



Soviet Intelligence appeared to substantiate these suspicions by reporting that NATO was indeed using unique, never-before-seen procedures as well as message formats more sophisticated than previous exercises that possibly indicated the proximity of nuclear attack.

Finally, during Able Archer 83 NATO forces simulated a move through all alert phases, from DEFCON 5 to DEFCON 1. While these phases were simulated, alarmist KGB agents mistakenly reported them as actual. According to Soviet intelligence, NATO doctrine stated, "Operational readiness No 1 is declared when there are obvious indications of preparation to begin military operations. It is considered that war is inevitable and may start at any moment."

Upon learning that US nuclear activity mirrored its hypothesized first strike activity, the Moscow Centre sent its residencies a flash telegram on November 8 or 9 (Oleg Gordievsky cannot recall which), incorrectly reporting an alert on American bases and frantically asking for further information regarding an American first strike. The alert precisely coincided with the seven- to ten-day period estimated between NATO's preliminary decision and an actual strike. This was the peak of the war scare.

The Soviet Union, believing its only chance of surviving a NATO strike was to preempt it, readied its nuclear arsenal. The CIA reported activity in the Baltic Military District, in Czechoslovakia, and it determined that nuclear capable aircraft in Poland and East Germany were placed "on high alert status with readying of nuclear strike forces". Former CIA analyst Peter Vincent Pry goes further, saying he suspects that the aircraft were merely the tip of the iceberg. He hypothesizes that â€?in accordance with Soviet military procedure and history â€?ICBM silos, easily readied and difficult for the United States to detect, were also prepared for a launch.

Soviet fears of the attack ended as the Able Archer exercise finished on November 11. Upon learning of the Soviet reaction to Able Archer 83 by way of the double agent Oleg Gordievsky, a British SIS asset, President Reagan commented, "I don't see how they could believe that â€?but itâ€™s something to think about."

The double agent Oleg Gordievsky, whose highest rank was KGB resident in London, is the only Soviet source ever to have published an account of Able Archer 83. Oleg Kalugin and Yuri Shvets, who were KGB officers in 1983, have published accounts that acknowledge Operation RYAN, but they do not mention Able Archer 83. Gordievsky and other Warsaw Pact intelligence agents were extremely skeptical about a NATO first strike, perhaps because of their proximity to, and understanding of, the West. Nevertheless, agents were ordered to report their observations, not their analysis, and this critical flaw in the Soviet intelligence system â€?coined by Gordievsky as the "intelligence cycle" â€?fed the fear of US nuclear aggression.

No Soviet political figure has publicly acknowledged Able Archer 83. Marshal Sergei Akhromeyev, who at the time was Chief of the main operations directorate of the Soviet General Staff, told Cold War historian Don Orbendorfer that he had never heard of Able Archer. The lack of public Soviet response over Able Archer 83 has led some historians, including Fritz W. Ermarth in his piece, "Observations on the 'War Scare' of 1983 From an Intelligence Perch", to conclude that Able Archer 83 posed no immediate threat to the United States.

In May 1984, CIA Russian specialist Fritz W. Ermarth drafted "Implications of Recent Soviet Military-Political Activities", which concluded: "we believe strongly that Soviet actions are not inspired by, and Soviet leaders do not perceive, a genuine danger of imminent conflict with the United States." Robert Gates, Deputy Director for Intelligence during Able Archer 83, has published thoughts on the exercise that refute this conclusion:



A still-classified report written by Nina Stewart for the President's Foreign Advisory Board concurs with Gates and refutes the previous CIA reports, concluding that further analysis shows that the Soviets were, in fact, genuinely fearful of US aggression.

Some historians, including Beth B. Fischer in her book The Reagan Reversal, pin Able Archer 83 as profoundly affecting President Reagan and his turn from a policy of confrontation towards the Soviet Union to a policy of rapprochement. Most other historians say that Reagan always intended to increase the United States defensive ability and then negotiate with the Soviet Union from a position of strength. The thoughts of Reagan and those around him provide important insight upon the nuclear scare and its subsequent ripples. On October 10 1983, just over a month before Able Archer 83, President Reagan screened a television film about Lawrence, Kansas being destroyed by a nuclear attack entitled The Day After. In his diary, the president wrote that the film "left me greatly depressed."

Later in October, Reagan attended a Pentagon briefing on nuclear war. During his first two years in office, he had refused to take part in such briefings, feeling it irreverent to rehearse a nuclear apocalypse; finally, he consented to the Pentagon official requests. According to officials present, the briefing "chastened" Reagan. Weinberg said, "[Reagan] had a very deep revulsion to the whole idea of nuclear weaponsâ€?These war games brought home to anybody the fantastically horrible events that would surround such a scenario." Reagan described the briefing in his own words: "A most sobering experience with Cap W and Gen. Vessey in the Situation room, a briefing on our complete plan in the event of a nuclear attack."

These two glimpses of nuclear war primed Reagan for Able Archer 83, giving him a very specific picture of what would occur had the situation further developed. After receiving intelligence reports from sources including Gordievsky, it was clear that the Soviets were unnerved. While officials were concerned with the Soviet panic, they were hesitant about believing the proximity of a Soviet attack. Secretary of State George P. Shultz thought it "incredible, at least to us" that the Soviets would believe the US would launch a genuine attack. In general, Reagan did not share the secretary's belief that cooler heads would prevail, writing:



According to McFarlane, the president responded with "genuine anxiety", in disbelief that a regular NATO exercise could have led to an armed attack. A still-classified 1990 retroactive analysis shows the President's more alarmed reaction to be more correct than the more relaxed view of some of his staff. To the ailing Politburo â€?led from the deathbed of the terminally ill Andropov, a man with no firsthand knowledge of the United States, and the creator of Operation RYAN â€?it seemed "that the United States was preparing to launchâ€?a sudden nuclear attack on the Soviet Union." In his memoirs, Reagan, without specifically mentioning Able Archer 83 â€?he states earlier that he cannot mention classified information â€?wrote of a 1983 realization:

















Michel de Nostredame (14 December 1503 or 21 December 1503 â€?2 July 1566), usually Latinized to Nostradamus, was a French apothecary and reputed seer who published collections of prophecies that have since become famous world-wide. He is best known for his book Les Propheties, the first edition of which appeared in 1555. Since the publication of this book, which has rarely been out of print since his death, Nostradamus has attracted an enthusiastic following who, along with the popular press, credit him with predicting many major world events. 

In contrast, most academic sources maintain that the associations made between world events and Nostradamus's quatrains are largely the result of misinterpretations or mistranslations (sometimes deliberate) or else are so tenuous as to render them useless as evidence of any genuine predictive power. Moreover, none of the sources listed offers any evidence that anyone has ever interpreted any of Nostradamus's quatrains specifically enough to allow a clear identification of any event in advance. 

Nevertheless, interest in the work of this prominent figure of the French Renaissance is still considerable, especially in the media and in popular culture, and the prophecies have in some cases been assimilated to the results of applying the alleged Bible Code, as well as to other purported prophetic works.



Born on December 14 1503 (though a date of 21st December is also arguable) in Saint-RÃ©my-de-Provence in the south of France, where his claimed birthplace still exists, Michel de Nostredame was one of at least nine children of ReyniÃ¨re de St-RÃ©my and grain dealer and notary Jaume de Nostredame. The latter's family had originally been Jewish, but Jaume's father, Guy Gassonet, had converted to Catholicism in around 1455, taking the Christian name "Pierre" and the surname "Nostredame" (the latter apparently from the saint's day on which his conversion was solemnized). Michel's known siblings included Delphine, Jehan (c. 1507â€?7), Pierre, Hector, Louis (born in 1522), Bertrand, Jean and Antoine (born in 1523).

Little else is known about his childhood, although there is a persistent tradition that he was educated by his maternal great-grandfather Jean de St. RÃ©my â€?a tradition which is somewhat vitiated by the fact that the latter disappears from the historical record after 1504, when the child was only one year old.

At the age of fifteen the young Nostredame entered the University of Avignon to study for his baccalaureate. After little more than a year (when he would have studied the regular Trivium of grammar, rhetoric and logic, rather than the later Quadrivium of geometry, arithmetic, music and astronomy/astrology), he was forced to leave Avignon when the university closed its doors in the face of an outbreak of the plague. In 1529, after some years as an apothecary, he entered the University of Montpellier to study for a doctorate in medicine. He was expelled shortly afterwards when it was discovered that he had been an apothecary, a "manual trade" expressly banned by the university statutes. The expulsion document (BIU Montpellier, Register S 2 folio 87) still exists in the faculty library. However, some of his publishers and correspondents would later call him "Doctor". After his expulsion, Nostredame continued working, presumably still as an apothecary, and became famous for creating a "rose pill" that supposedly protected against the plague.

In 1531 Nostredame was invited by Jules-CÃ©sar Scaliger, a leading Renaissance scholar, to come to Agen. There he married a woman of uncertain name (possibly Henriette d'Encausse), who bore him two children. In 1534 his wife and children died, presumably from the Plague. After their death, he continued to travel, passing through France and possibly Italy. 

On his return in 1545, he assisted the prominent physician Louis Serre in his fight against a major plague outbreak in Marseille, and then tackled further outbreaks of disease on his own in Salon-de-Provence and in the regional capital, Aix-en-Provence. Finally, in 1547, he settled in Salon-de-Provence in the house which exists today, where he married a rich widow named Anne Ponsarde, with whom he had six children â€?three daughters and three sons. Between 1556 and 1567 he and his wife acquired a one-thirteenth share in a huge canal project organized by Adam de Craponne to irrigate largely waterless Salon and the nearby DÃ©sert de la Crau from the river Durance.

After another visit to Italy, Nostredame began to move away from medicine and toward the occult. Following popular trends, he wrote an almanac for 1550, for the first time Latinizing his name from Nostredame to Nostradamus. He was so encouraged by the almanac's success that he decided to write one or more annually. Taken together, they are known to have contained at least 6,338 prophecies, as well as at least eleven annual calendars, all of them starting on January 1 and not, as is sometimes supposed, in March. It was mainly in response to the almanacs that the nobility and other prominent persons from far away soon started asking for horoscopes and 'psychic' advice from him, though he generally expected his clients to supply the birth charts on which these would be based, rather than calculating them himself as a professional astrologer would have done. When obliged to attempt this himself on the basis of the published tables of the day, he always made numbers of errors, and never adjusted the figures for his clients' place or time of birth. (Refer to the analysis of these charts by Brind'Amour, 1993, and compare Gruber's comprehensive critique of Nostradamusâ€?horoscope for Crown Prince Rudolph Maximilian.)

He then began his project of writing a book of one thousand mainly French quatrains, which constitute the largely undated prophecies for which he is most famous today. Feeling vulnerable to religious fanatics, however, he devised a method of obscuring his meaning by using "Virgilianized" syntax, word games and a mixture of other languages such as Greek, Italian, Latin, and ProvenÃ§al. For technical reasons connected with their publication in three installments (the publisher of the third and last installment seems to have been unwilling to start it in the middle of a "Century," or book of 100 verses), the last fifty-eight quatrains of the seventh "Century" have not survived into any extant edition.

The quatrains, published in a book titled Les Propheties (The Prophecies), received a mixed reaction when they were published. Some people thought Nostradamus was a servant of evil, a fake, or insane, while many of the elite thought his quatrains were spiritually inspired prophecies â€?as, in the light of their post-Biblical sources (see under Nostradamus's sources below), Nostradamus himself was indeed prone to claim. Catherine de MÃ©dicis, the queen consort of King Henri II of France, was one of Nostradamus's greatest admirers. After reading his almanacs for 1555, which hinted at unnamed threats to the royal family, she summoned him to Paris to explain them and to draw up horoscopes for her children. At the time, he feared that he would be beheaded, but by the time of his death in 1566, Catherine had made him Counselor and Physician-in-Ordinary to the King.

Some accounts of Nostradamus's life state that he was afraid of being persecuted for heresy by the Inquisition, but neither prophecy nor astrology fell in this bracket, and he would have been in danger only if he had practiced magic to support them. In fact, his relationship with the Church as a prophet and healer was excellent. His brief imprisonment at Marignane in late 1561 came about purely because he had published his 1562 almanac without the prior permission of a bishop, contrary to a recent royal decree.

By 1566, Nostradamus's gout, which had plagued him painfully for many years and made movement very difficult, turned into oedema, or dropsy. In late June he summoned his lawyer to draw up an extensive will bequeathing his property plus 3,444 crowns (around $300,000 US today) â€?minus a few debts â€?to his wife pending her remarriage, in trust for her sons pending their twenty-fifth birthdays and her daughters pending their marriages. This was followed by a much shorter codicil. On the evening of July 1, he is alleged to have told his secretary Jean de Chavigny, "You will not find me alive at sunrise." The next morning he was reportedly found dead, lying on the floor next to his bed and a bench (Presage 141 [originally 152] for November 1567, as posthumously edited by Chavigny to fit). He was buried in the local Franciscan chapel (part of it now incorporated into the restaurant La Brocherie) but re-interred in the CollÃ©giale St-Laurent at the French Revolution, where his tomb remains to this day.

'''''The Prophecies'''''. In this book he compiled his collection of major, long-term predictions. The first installment was published in 1555. The second, with 289 further prophetic verses, was printed in 1557. The third edition, with three hundred new quatrains, was reportedly printed in 1558, but nowadays only survives as part of the omnibus edition that was published after his death in 1568. This version contains one unrhymed and 941 rhymed quatrains, grouped into nine sets of 100 and one of 42, called "Centuries". 

Given printing practices at the time (which included type-setting from dictation), no two editions turned out to be identical, and it is relatively rare to find even two copies that are exactly the same. Certainly there is no warrant for assuming â€?as would-be "code-breakers" are prone to do â€?that either the spellings or the punctuation of any edition are Nostradamus's originals. 

The '''''Almanacs'''''. By far the most popular of his works, these were published annually from 1550 until his death. He often published two or three in a year, entitled either Almanachs (detailed predictions), Prognostications or Presages (more generalized predictions). 

Nostradamus was not only a diviner, but a professional healer, too. It is known that he wrote at least two books on medical science. One was an alleged "translation" of Galen, and in his so-called '''''TraitÃ© des fardemens''''' (basically a medical cookbook containing, once again, materials borrowed mainly from others), he included a description of the methods he used to treat the plague â€?none of which, not even the bloodletting, apparently worked. The same book also describes the preparation of cosmetics.

A manuscript normally known as the '''''Orus Apollo''''' also exists in the Lyon municipal library, where upwards of 2,000 original documents relating to Nostradamus are stored under the aegis of Michel Chomarat. It is a purported translation of an ancient Greek work on Egyptian hieroglyphs based on later Latin versions, all of them unfortunately ignorant of the true meanings of the ancient Egyptian script, which was not correctly deciphered until the advent of Champollion in the 19th century.

Since his death only the Prophecies have continued to be popular, but in this case they have been quite extraordinarily so. Over two hundred editions of them have appeared in that time, together with over 2000 commentaries. Their popularity seems to be partly due to the fact that their vagueness and lack of dating make it easy to quote them selectively after every major dramatic event and retrospectively claim them as "hits" (see Nostradamus in popular culture).

Nostradamus claimed to base his published predictions on judicial astrology â€?the astrological assessment of the 'quality' of expected future developments â€?but was heavily criticized by professional astrologers of the day such as Laurens Videl for incompetence and for assuming that "comparative horoscopy" (the comparison of future planetary configurations with those accompanying known past events) could predict what would happen in the future.

Recent research suggests that much of his prophetic work paraphrases collections of ancient end-of-the-world prophecies (mainly Bible-based), supplemented with references to historical events and anthologies of omen reports, and then projects those into the future with the aid of comparative horoscopy. Hence the many predictions involving ancient figures such as Sulla, Gaius Marius, Nero, and others, as well as his descriptions of "battles in the clouds" and "frogs falling from the sky." Astrology itself is mentioned only twice in Nostradamus's Preface and 41 times in the Centuries themselves, but more frequently in his dedicatory ''Letter to King Henri II''.

His historical sources include easily identifiable passages from Livy, Suetonius, Plutarch and other classical historians, as well as from medieval chroniclers such as Villehardouin and Froissart. Many of his astrological references are taken almost word for word from Richard Roussat's  of 1549â€?0. 

One of his major prophetic sources was evidently the ''Mirabilis liber'' of 1522, which contained a range of prophecies by Pseudo-Methodius, the Tiburtine Sibyl, Joachim of Fiore, Savonarola and others. (His Preface contains 24 biblical quotations, all but two in the order used by Savonarola.) This book had enjoyed considerable success in the 1520s, when it went through half a dozen editions (see External links below for facsimiles and translations) but did not sustain its influence, perhaps owing to its mostly Latin text, Gothic script and many difficult abbreviations. Nostradamus was one of the first to re-paraphrase these prophecies in French, which may explain why they are credited to him. It should be noted that modern views of plagiarism did not apply in the 16th century. Authors frequently copied and paraphrased passages without acknowledgement, especially from the classics. 

Further material was gleaned from the De honesta disciplina of 1504 by Petrus Crinitus, which included extracts from Michael Psellus's De daemonibus, and the De Mysteriis Aegyptiorum (Concerning the mysteries of Egypt...), a book on Chaldean and Assyrian magic by Iamblichus, a 4th century Neo-Platonist. Latin versions of both had recently been published in Lyon, and extracts from both are paraphrased (in the second case almost literally) in his first two verses, the first of which is appended to this article. While it is true that Nostradamus claimed in 1555 to have burned all of the occult works in his library, no one can say exactly what books were destroyed in this fire. The fact that they reportedly burned with an unnaturally brilliant flame suggests, however, that some of them were manuscripts on vellum, which was routinely treated with saltpeter.

Only in the 17th century did people start to notice his reliance on earlier, mainly classical sources. This may help explain the fact that, during the same period, The Prophecies reportedly came into use in France as a classroom reader.

Nostradamus's reliance on historical precedent is reflected in the fact that he explicitly rejected the label 'prophet' (i.e. a person having prophetic powers of his own) on several occasions:Although, my son, I have used the word prophet, I would not attribute to myself a title of such lofty sublimity â€?Preface to CÃ©sar, 1555Not that I would attribute to myself either the name or the role of a prophet â€?Preface to CÃ©sar, 1555[S]ome of [the prophets] predicted great and marvelous things to come: [though] for me, I in no way attribute to myself such a title here. â€?Letter to King Henri II, 1558I do but make bold to predict (not that I guarantee the slightest thing at all), thanks to my researches and the consideration of what judicial Astrology promises me and sometimes gives me to know, principally in the form of warnings, so that folk may know that with which the celestial stars do threaten them. Not that I am foolish enough to pretend to be a prophet. â€?Open letter to Privy Councillor (later Chancellor) Birague, 15 June 1566

His rejection of the title 'prophet' also squares with the fact that he entitled his book

 

(a title that, in French, as easily means "The Prophecies, by M. Michel Nostradamus", which is precisely what they were; as "The Prophecies of M. Michel Nostradamus", which, except in a few cases, they were not, other than in the manner of their editing, expression and reapplication to the future.) Any criticism of Nostradamus for claiming to be a prophet, in other words, would have been for doing what he never claimed to be doing in the first place.

Given this reliance on literary sources, it is doubtful whether Nostradamus used any particular methods for entering a trance state, other than contemplation, meditation and incubation (i.e., ritually "sleeping on it"). His sole description of this process is contained in letter 41 of his collected Latin correspondence. The popular legend that he attempted the ancient methods of flame gazing, water gazing or both simultaneously is based on a naive reading of his first two verses, which merely liken his efforts to those of the Delphic and Branchidic oracles. The first of these is reproduced at the bottom of this article: the second can be seen by visiting the relevant facsimile site (see External Links). In his dedication to King Henri II, Nostradamus describes "emptying my soul, mind and heart of all care, worry and unease through mental calm and tranquility", but his frequent references to the "bronze tripod" of the Delphic rite are usually preceded by the words "as though" (compare, once again, External References to the original texts).

Most of the quatrains deal with disasters, such as plagues, earthquakes, wars, floods, invasions, murders, droughts, and battles â€?all undated and based on foreshadowings by the Mirabilis Liber. Some quatrains cover these disasters in overall terms; others concern a single person or small group of persons. Some cover a single town, others several towns in several countries. A major, underlying theme is an impending invasion of Europe by Muslim forces from further east and south headed by the expected Antichrist, directly reflecting the then-current Ottoman invasions and the earlier Saracen (that is, Arab) equivalents, as well as the prior expectations of the Mirabilis Liber. All of this is presented in the context of the supposedly imminent end of the world, a conviction that sparked numerous collections of end-time prophecies at the time, not least an unpublished collection by Christopher Columbus.

Some scholars believe that Nostradamus wrote not to be a prophet, but to comment on events that were happening in his own time, writing in his elusive way â€?using highly metaphorical and cryptic language â€?to avoid persecution. This is similar to the Preterist interpretation of the Book of Revelation.

Nostradamus enthusiasts have credited him with predicting numerous events in world history, from the Great Fire of London, by way of the rise of Napoleon I of France and Adolf Hitler, to the September 11, 2001 terrorist attacks on the World Trade Center, but only ever in hindsight. See Alternative views below. It is not only skeptics such as James Randi who suggest that his reputation as a prophet is largely manufactured by modern-day supporters who fit his words to events that have either already occurred or are so imminent as to be inevitable, a process sometimes known as "retroactive clairvoyance." There is no evidence in the academic literature (see Sources) to suggest that any Nostradamus quatrain has ever been interpreted as predicting a specific event before it occurred, other than in vague, general terms that could equally apply to any number of other events.

A range of quite different views are expressed in printed literature and on the Internet. At one end of the spectrum, there are extreme academic views such as those of Jacques Halbronn, suggesting at great length and with great complexity that Nostradamus's Prophecies are antedated forgeries written by later hands with a political axe to grind. Although Halbronn possibly knows more about the texts and associated archives than almost anybody else alive (he helped dig out and research many of them), most other specialists in the field reject this view. 

At the other end of the spectrum, there are numerous fairly recent popular books, and thousands of private websites, suggesting not only that the Prophecies are genuine but that Nostradamus was a true prophet. Thanks to the vagaries of interpretation, no two of them agree on exactly what he predicted, whether for our past or for our future. There is a consensus among these works, however, that he predicted the French Revolution, Napoleon Bonaparte, Adolf Hitler, both world wars, and the nuclear destruction of Hiroshima and Nagasaki. There is also a consensus that he predicted whatever major event had just happened at the time of each book's publication, from the Apollo moon landings, through the death of Diana, Princess of Wales in 1997, and the Space Shuttle ''Challenger'' disaster in 1986, to the events of 9/11: this 'movable feast' aspect appears to be characteristic of the genre. 

Possibly the first of these books to become truly popular in English was Henry C Roberts' The Complete Prophecies of Nostradamus of 1947, reprinted at least seven times during the next 40 years, which contained both transcriptions and translations, with brief commentaries. This was followed in 1961 by Edgar Leoni's unusually dispassionate Nostradamus and His Prophecies, which is almost universally regarded as the best and most comprehensive treatment and analysis of Nostradamus in English prior to 1990. After that came Erika Cheetham's well-known The Prophecies of Nostradamus, incorporating a reprint of the posthumous 1568 edition, which was reprinted, revised and republished several times from 1973 onwards, latterly as The Final Prophecies of Nostradamus. This went on to serve as the basis for Orson Welles' celebrated film/video The Man Who Saw Tomorrow. Apart from a two-part translation of Jean-Charles de Fontbrune's Nostradamus: historien et prophÃ¨te of 1980, the series could be said to have culminated in John Hogue's well-known books on the seer from about 1994 onwards, including Nostradamus: The Complete Prophecies (1999) and, most recently, Nostradamus: A Life and Myth (2003).

With the exception of Roberts, these books and their many popular imitators were almost unanimous not merely about Nostradamus's powers of prophecy, but also about various aspects of his biography. He had been a descendant of the Israelite tribe of Issachar; he had been educated by his grandfathers, who had both been physicians to the court of Good King RenÃ© of Provence; he had attended Montpellier University in 1525 to gain his first degree: after returning there in 1529 he had successfully taken his medical doctorate; he had gone on to lecture in the Medical Faculty there until his views became too unpopular; he had supported the heliocentric view of the universe; he had travelled to the north-east of France, where he had composed prophecies at the abbey of Orval; in the course of his travels he had performed a variety of prodigies, including identifying a future Pope; he had successfully cured the Plague at Aix-en-Provence and elsewhere; he had engaged in 'scrying' using either a magic mirror or a bowl of water; he had been joined by his secretary Chavigny at Easter 1554; having published the first installment of his Propheties, he had been summoned by Queen Catherine de' Medici to Paris in 1556 to discuss with her his prophecy at quatrain I.35 that her husband King Henri II would be killed in a duel; he had examined the royal children at Blois; he had bequeathed to his son a 'lost book' of his own prophetic paintings; he had been buried standing up; and he had been found, when dug up at the French Revolution, to be wearing a medallion bearing the exact date of his disinterment. 

From the 1980s onwards, however, an academic reaction set in, especially in France. The publication in 1983 of Nostradamus's private correspondence and, during succeeding years, of the original editions of 1555 and 1557 discovered by Chomarat and Benazra, together with the unearthing of much original archival material revealed that much that was claimed about Nostradamus simply didn't fit the documented facts. The academics made it clear that not one of the claims just listed was backed up by any known contemporary documentary evidence. Most of them had evidently been based on unsourced rumours retailed as 'fact' by much later commentators such as Jaubert (1656), Guynaud (1693) and Bareste (1840), on modern misunderstandings of the 16th century French texts, or on pure invention. Even the often-advanced suggestion that quatrain I.35 had successfully prophesied King Henri II's death did not actually appear in print for the first time until 1614, 55 years after the event.

On top of that, the academics, who themselves tend to eschew any attempt at 'interpretation', complained that the English translations were usually of poor quality, seemed to display little or no knowledge of 16th century French, were tendentious and, at worst, were sometimes twisted to fit the events to which they were supposed to refer (or vice versa). None of them, certainly, were based on the original editions: Roberts had based himself on that of 1672, Cheetham and Hogue on the posthumous edition of 1568. Even the relatively respectable Leoni accepted on his page 115 that he had never seen an original edition, and on earlier pages indicated that much of his biographical material was unsourced.

However, none of this research and criticism was originally known to most of the English-language commentators, by function of the dates when they were writing and, to some extent, of the language it was written in. Hogue, admittedly, was in a position to take advantage of it, but it was only in 2003 that he accepted that some of his earlier biographical material had in fact been 'apocryphal'. Meanwhile the scholars were particularly scathing about later attempts by some lesser-known authors (Hewitt, 1994; Ovason, 1997; Ramotti, 1998) to extract 'hidden' meanings from the texts with the aid of anagrams, numerical codes, graphs and other devices.



The prophecies retold and expanded by Nostradamus have figured largely in popular culture in the 20th and 21st centuries. As well as being the subject of hundreds of books (both fiction and nonfiction), Nostradamus's life has been depicted in several films and videos, and his life and writings continue to be a subject of media interest. 

There have also been several well-known internet hoaxes, where quatrains in the style of Nostradamus have been circulated by e-mail as the real thing. The best-known examples concern the collapse of the World Trade Center in the attacks of September 11, 2001, which led both to hoaxes and to reinterpretations by enthusiasts of several quatrains as supposed prophecies.

The September 11, 2001 attacks on New York City led to immediate speculation as to whether Nostradamus had predicted the events. Nostradamus enthusiasts pointed to Quatrains VI.97 and I.87 as possible predictions, but the scholars universally discounted these as irrelevant (compare the relevant sections of the Lemesurier and Snopes websites listed under External Links).





This section is dedicated to links with sites that are consistent with the established facts reported in the article only, not with the thousands of others that are, for the most part, highly speculative and/or purely fictional in character and thus in no way complementary to it.









A rifle is a firearm designed to be fired from the shoulder, with a barrel that has a helical groove or pattern of grooves ("rifling") cut into the barrel walls. The raised areas of the rifling grooves are called "lands," which make contact with the projectile (for small arms usage, called a bullet), imparting spin around an axis corresponding to the orientation of the weapon. When the projectile leaves the barrel, the conservation of angular momentum improves accuracy and range, in the same way that a properly thrown American football or rugby ball behaves. The word "rifle" originally referred to the grooving, and a rifle was called a "rifled gun." Rifles are used in warfare, hunting and shooting sports.

Typically, a bullet is propelled by the contained deflagration of an explosive compound (originally black powder, later cordite, and now nitrocellulose), although other means such as compressed air are used in air rifles, which are popular for vermin control, hunting small game, and casual shooting ("plinking").

In many armed forces units it is thought wrong to use the word "gun" to mean a rifle. Furthermore, in many works of fiction a rifle refers to any weapon that has a stock and is shouldered before firing, even if this weapon is not rifled or doesn't fire solid projectiles.



Originally, rifles were sharpshooter weapons, while the regular infantry made use of the greater firepower of massed muskets, which fired round musket balls of calibers up to 19 mm (0.75 inch). Benjamin Robins, an English mathematician, realized that an elongated bullet would retain the mass and kinetic force of a musket ball, but would slice through the air with much greater ease. The innovative work of Robins and others would take until the end of the 18th century to gain acceptance.

By the mid-19th century, however, manufacturing had advanced sufficiently that the musket was replaced by a range of riflesâ€”generally single-shot, breech-loadingâ€”designed for aimed, discretionary fire by individual soldiers. Then, as now, rifles had a stock, either fixed or folding, to be braced against the shoulder when firing. Early military rifles, such as the Baker rifle were shorter than the day's muskets, and usually the weapon of a marksman. Until the early 20th century rifles tended to be very longâ€”an 1890 Martini-Henry was almost 2 m (6 feet) in length with a fixed bayonet. The demand for more compact weapons for cavalrymen led to the carbine, or shortened rifle.

Muskets were smoothbore, large caliber weapons using ball-shaped ammunition fired at relatively low velocity. Due to the high cost and great difficulty of precision manufacturing, and the need to load readily from the muzzle, the musket ball was a loose fit in the barrel. Consequently on firing the ball bounced off the sides of the barrel when fired and the final direction on leaving the muzzle was unpredictable.

The performance of early muskets was sufficient for the styles of warfare at the time, whereby soldiers tended to stand in long, stationary lines and fire at the opposing forces. Aiming and accuracy were not necessary to hit an opponent.

The origins of rifling are difficult to trace, but some of the earliest practical experiments seem to have occurred in Europe during the fifteenth century. Archers had long realized that a twist added to the tail feathers of their arrows gave them greater accuracy. Early muskets produced large quantities of smoke and soot, which had to be cleaned from the action and bore of the musket frequently; either the action of repeated bore scrubbing, or a deliberate attempt to create "soot grooves" might also have led to a perceived increase in accuracy, although no-one knows for sure. True rifling dates from the mid-15th century, although the precision required for its effective manufacture kept it out of the hands of infantrymen for another three and a half centuries, when it largely replaced the unrifled musket as the primary infantry weapon. In the transitional nineteenth century, the term "rifled musket" was used to indicate the novel weapon.During the Napoleonic Wars the British army created several experimental units known as "Rifles." These Rifle Regiments were deployed as skirmishers during the Peninsular war in Spain and Portugal, and were more effective than skirmishers armed with muskets due to their accuracy and long range. There is also an historic account of Rifleman Thomas Plunkett of the 1st Battalion 95th Rifles shooting General Colbert at a range of between 400 m 600 m (400 to 600 yards) during this campaign.

Some early rifled guns were created with special barrels that had a twisted polygonal shape. Specially-made bullets were designed to match the shape so the bullet would grip the rifle bore and take a spin that way. These were generally limited to large caliber weapons and the ammunition still did not fit tightly in the barrel. Many experimental designs used different shapes and degrees of spiraling. Although uncommon, polygonal rifling is still used in some weapons today with one example being the Glock line of pistols (which fire standard bullets). Unfortunately, many early attempts resulted in dangerous backfiring, which could lead to destruction of the weapon and serious injury to the person firing.

Gradually, rifles appeared with cylindrical barrels cut with helical grooves, the surfaces between the grooves being called "lands". The innovation shortly preceded the mass adoption of breech-loading weapons, as it was not practical to push an overbore bullet down through a rifled barrel, only to then (try to) fire it back out. The dirt and grime from prior shots was pushed down ahead of a tight bullet or ball (which may have been a loose fit in the clean barrel before the first shot), and, of course, loading was far more difficult, as the lead had to be deformed to go down in the first place, reducing the accuracy due to deformation. Several systems were tried to deal with the problem, usually by resorting to an under-bore bullet that expanded upon firing.

The original muzzle-loading rifle, with a closely fitting ball to take the rifling grooves, was loaded with difficulty, particularly when foul, and for this reason was not generally used for military purposes. Even with the advent of rifling the bullet itself didn't change, but was wrapped in a greased, cloth patch to grip the rifling grooves.

The first half of the nineteenth century saw a distinct change in the shape and function of the bullet. In 1826 Delirque, a French infantry officer, invented a breech with abrupt shoulders on which a spherical bullet was rammed down until it caught the rifling grooves. Delirque's method, however, deformed the bullet and was inaccurate.

One of the most famous was the MiniÃ© system, which relied on a conical bullet (known as a MiniÃ© ball) with a hollow at the base of the bullet that caused the base of the round to expand from the pressure of the exploding charge and grip the rifling as the round was fired. MiniÃ© system rifles, notably the U.S. Springfield and the British Enfield of the early 1860s, featured prominently in the U.S. Civil War, due to the enhanced power and accuracy. The better seal gave more power, as less gas escaped past the bullet, which combined with the fact that for the same bore (caliber) diameter a long bullet was heavier than a round ball. Enhanced accuracy came from the expansion to grip the rifling, which spun the bullet more consistently.

Another important area of development was the way that cartridges were stored and used in the weapon. The Spencer repeating rifle was a breech-loading manually operated lever action rifle, that was adopted by the United States. Over 20,000 were used during the Civil War. It marked the first adoption of a removable magazine-fed infantry rifle by any country. The design was completed by Christopher Spencer in 1860. It used copper rimfire cartridges stored in a removable seven round tube magazine, enabling the rounds to be fired one after another. When the magazine was empty, it could be exchanged for another.

As the bullet enters the barrel, it inserts itself into the rifling, a process that gradually wears down the barrel, and also causes the barrel to heat up more rapidly. Therefore, some machine-guns are equipped with quick-change barrels that can be swapped every few thousand rounds, or in earlier designs, were water-cooled. Unlike older carbon steel barrels, which were limited to around 1,000 shots before the extreme heat caused accuracy to fade, modern stainless steel barrels for target rifles are much more resistant to wear, allowing many thousands of rounds to be fired before accuracy drops. (Many shotguns and small arms have chrome-lined barrels to reduce wear and enhance corrosion resistance. This is rare on rifles designed for extreme accuracy, as the plating process is difficult and liable to reduce the effect of the rifling.) Modern ammunition has hardened leadcore with a softer outer cladding or jacket, typically of an alloy of copper and nickel - cupro-nickel. Some ammunition is even coated with molybdenun-disulphide to further reduce internal friction - the so-called 'moly-coated' bullet.

Over the 19th century, bullet design also evolved, the bullets becoming gradually smaller and lighter. By 1910 the standard blunt-nosed bullet had been replaced with the pointed, 'spitzer' bullet, an innovation that increased range and penetration. Cartridge design evolved from simple paper tubes containing black powder and shot, to sealed brass cases with integral primers for ignition, while black powder itself was replaced with cordite, and then other nitro-cellulose-based smokeless powder mixtures, propelling bullets to higher velocities than before.

The increased velocity meant that new problems arrived, and so bullets went from being soft lead to harder lead, then to copper jacketed, in order to better engage the spiraled grooves without "stripping" them in the same way that a screw or bolt thread would be stripped if subjected to extreme forces.

As mentioned above, rifles were initially single-shot, muzzle-loading weapons. During the 18th century, breech-loading weapons were designed, which allowed the rifleman to reload while under cover, but defects in manufacturing and the difficulty in forming a reliable gas-tight seal prevented widespread adoption. During the 19th century, multi-shot repeating rifles using lever, pump or linear bolt actions became standard, further increasing the rate of fire and minimizing the fuss involved in loading a firearm. The problem of proper seal creation had been solved with the use of brass cartridge cases, which expanded in an elastic fashion at the point of firing and effectively sealed the breech while the pressure remained high, then relaxed back enough to allow for easy removal. By the end of the 19th century, the leading bolt-action design was that of Paul Mauser, whose actionâ€”wedded to a reliable design possessing a five-shot magazineâ€”became a world standard through two world wars and beyond. The Mauser rifle was paralleled by Britain's ten-shot Lee-Enfield and America's 1903 Springfield Rifle models (the latter pictured above). The American M1903 closely copied Mauser's original design.

The advent of massed, rapid firepower and of the machine gun and the rifled artillery piece was so quick as to outstrip the development of any way to attack a trench defended by riflemen and machine gunners. The carnage of World War I was perhaps the greatest vindication and vilification of the rifle as a military weapon. By World War II, military thought was turning elsewhere, towards more compact weapons. 

Experience in World War I led German military researchers to conclude that long-range aimed fire was less significant at typical battle ranges of 300 m. As mechanisms became smaller, lighter and more reliable, semi-automatic rifles, including the M1 Garand, appeared. World War II saw the first mass-fielding of such rifles, which culminated in the Sturmgewehr 44, the first assault rifle and one of the most significant developments of 20th century small-arms.

By contrast, civilian rifle design has not significantly advanced since the early part of the 20th century. Modern hunting rifles have fiberglass and carbon fiber stocks and more advanced recoil pads, but are fundamentally the same as infantry rifles from 1910. Many modern sniper rifles can trace their ancestry back for well over a century, and the Russian 7.62Â xÂ 54Â mm cartridge, as used in the front-line Dragunov Sniper Rifle (SVD), dates from 1891.

Muskets were used for comparatively rapid, unaimed volley fire, and the average conscripted soldier could be easily trained to use them. The (muzzle-loaded) rifle was originally a sharpshooter's weapon used for targets of opportunity and deliberate aimed fire. During the Napoleonic Wars, the British 95th Regiment (Green Jackets) and 60th Regiment (Royal American) used the rifle to great effect during skirmishing. Because of a slower loading time than a musket, they were not adopted by the whole army. The adoption of cartridges and breech-loading in the 19th century was concurrent with the general adoption of rifles. In the early part of the 20th century, soldiers were trained to shoot accurately over long ranges with high-powered cartridges. World War I Lee-Enfields rifles (among others) were equipped with long-range 'volley sights' for massed firing at ranges of up to 1.6 km (1.0 mile). Individual shots were unlikely to hit, but a platoon firing repeatedly could produce a 'beaten ground' effect similar to light artillery or machine guns; but experience in WWI showed that long-range fire was best left to the machine gun.

During and after WWII it became accepted that most infantry engagements occur at ranges of less than 300Â m; the range and power of the large rifles was "overkill"; and the weapons were heavier than the ideal. This led to Germany's development of the 7.92Â xÂ 33Â mm ''Kurz'' (short) round, the Karabiner 98, the MKb-42, and ultimately, the assault rifle. Today, an infantryman's rifle is optimised for ranges of 300Â m or less, and soldiers are trained to deliver individual rounds or bursts of fire within these distances. The application of accurate, long-range fire is the domain of the sniper in warfare, and of enthusiastic target shooters in peacetime. The modern sniper rifle is usually capable of accuracy better than 0.3 mrad (1 arcminute).

In recent decades, large-caliber anti-materiel rifles, typically firing 12.7 mm and 20 mm caliber cartridges, have been developed. The US Barrett M82A1 is probably the best-known such rifle. These weapons are typically used to strike critical, vulnerable targets such as computerized command and control vehicles, radio trucks, radar antennae, vehicle engine blocks and the jet engines of enemy aircraft. Anti-materiel rifles can be used against human targets, but the much higher weight of rifle and ammunition, and the massive recoil and muzzle blast, usually make them less than practical for such use. The Barrett M82 is credited with a maximum effective range of 1800Â m (1.1Â mile); and it was with a .50BMG caliber McMillan TAC-50 rifle that Canadian Master Corporal Rob Furlong made the longest recorded confirmed sniper kill in history, when he shot a Taliban insurgent at a range of 2,430 meters (1.51 miles) in Afghanistan during Operation Anaconda in 2002.

, rifles are the most common firearm in general use for hunting purposes (with the exception of bird hunting where shotguns are favored). Use in competition is also very common, and includes Olympic events. Military-style rifles in semi-automatic such as the AR-15 have become very popular in the United States and are now used for hunting all sizes of game since a selection of different calibers have become available.





















Finland, officially the Republic of Finland (), is a Nordic country situated in the Scandinavian portion of Northern Europe. It has borders with Sweden to the west, Russia to the east, and Norway to the north, while Estonia lies to its south across the Gulf of Finland. The capital city is Helsinki.

Finland has a population of 5,302,778 people, spread over an area of .Finland is the eighth largest country in Europe in terms of area, with a low population density of 16 people per square kilometer, making it the most sparsely populated country in the European Union. The majority of the population is concentrated in the southern part of the country. As their mother tongue, most Finns speak Finnish, one of the few official languages of the European Union that is not of Indo-European origin. The second official language, Swedish, is spoken natively by a 5.5 percent minority.

Formerly part of Sweden and from 1809 an autonomous Grand Duchy within the Russian Empire, Finland declared its independence in 1917. Today, Finland is a democratic, parliamentary republic and has been a member state of the United Nations since 1955 and the European Union since 1995. Finland has thriving services and manufacturing sectors and is a highly democratic welfare state with low levels of corruption, consistently ranking at or near the top in international comparisons of national performance.

Finland is eleventh on the United Nations' Human Development Index and ranked as the sixth happiest nation in the world. According to the World Audit Democracy profile, Finland is the freest nation in the world in terms of civil liberties, freedom of the press, low corruption levels and high levels of political rights. Finland is rated the sixth most peaceful country in the world by the Economist Intelligence Unit, and since 1945, Finland has been at peace, adopting neutrality in wartime.

Finland was rated the best country to live in by Reader's Digest study released in October 2007, which looked at issues such as quality of drinking water and greenhouse gas emissions as well as factors such as education and income.



According to archaeological evidence, the area now composing Finland was first settled around 8500Â BCE during the Stone Age as the ice shield of the last ice age receded. The earliest people were hunter-gatherers, living primarily off what the tundra and sea could offer. Pottery is known from around 5300Â BCE (see Comb Ceramic Culture).The arrival of the Battle Axe culture (or Cord-Ceramic Culture) in southern coastal Finland around 3200Â BCE may have coincided with the start of agriculture. However, the earliest certain records of agriculture are from the late third millenniumÂ BCE. Even with the introduction of agriculture, hunting and fishing continued to be important parts of the subsistence economy, especially in the northern and eastern parts of the country.

The Bronze Age (1500â€?00Â BCE) and Iron Age (500Â BCEâ€?200Â CE) were characterised by extensive contacts with other cultures in the Fennoscandian and Baltic regions. There is no consensus on when Finno-Ugric languages and Indo-European languages were first spoken in the area of contemporary Finland.

The first verifiable written documents appeared in the twelfth century.



Sweden established its official rule of Finland in the 13th century by the crown. Swedish became the dominant language of the nobility, administration and education; Finnish was chiefly a language for the peasantry, clergy and local courts in predominantly Finnish-speaking areas. The Bishop of Turku was usually the most important person in Finland during the Catholic era.

The Middle Ages ended with the Reformation when the Finns gradually converted to Lutheranism. In the 16th century, Mikael Agricola published the first written works in Finnish. The first university in Finland, The Royal Academy of Turku, was established in 1640. In the 18th century, wars between Sweden and Russia led to occupation of Finland twice by Russian forces, known to the Finns as the Greater Wrath (1714â€?721) and the Lesser Wrath (1742â€?743). By this time "Finland" was the predominant term for the whole area from the Gulf of Bothnia to the Russian border.

On March 29, 1809, after being conquered by the armies of Alexander I of Russia in the Finnish War, Finland became an autonomous Grand Duchy in the Russian Empire until the end of 1917. During the Russian era, the Finnish language started to gain recognition, first probably to sever the cultural and emotional ties with Sweden and thereafter, from the 1860s onwards, as a result of a strong nationalism, known as the Fennoman movement. Milestones included the publication of what would become Finland's national epic, the Kalevala, in 1835; and the Finnish language achieving equal legal status with Swedish in 1892.

Despite the Finnish famine of 1866-1868, in which about 15 percent of the population died, political and economic development was rapid from the 1860s onwards.

In 1906, universal suffrage was adopted in the Grand Duchy of Finland, the second country in the world where this happened. However, the relationship between the Grand Duchy and the Russian Empire soured when the Russian government made moves to restrict Finnish autonomy. For example, the universal suffrage was, in practice, virtually meaningless, since the emperor did not approve any of the laws adopted by the Finnish parliament. Desire for independence gained ground, first among radical nationalists and socialists.

On December 6, 1917, shortly after the Bolshevik Revolution in Russia, Finland declared its independence, which was approved by Bolshevist Russia.

In 1918, the country experienced a brief but bitter Civil War that affected domestic politics for many decades afterwards. The Civil War was fought between "the Whites", who were supported by Imperial Germany, and "the Reds", supported by Bolshevist Russia. The Reds consisted mostly of propertyless rural and industrial workers who, despite universal suffrage in 1906, felt that they lacked political influence. The White forces were mostly made up of bourgeoisie and wealthy peasantry, politically to the right. Eventually, the Whites overcame the Reds. The deep social and political enmity between the Reds and Whites remained. The civil war and activist expeditions (see Heimosodat) to the Soviet Union strained eastern relations.

After a brief flirtation with monarchy, Finland became a presidential republic, with Kaarlo Juho StÃ¥hlberg elected as its first president in 1919. The Finnishâ€“Russian border was determined by the Treaty of Tartu in 1920, largely following the historic border but granting Pechenga () and its Barents Sea harbour to Finland. Finnish democracy survived the upsurge of the extreme rightist Lapua Movement and Great Depression in the early '30s. However, legislators tended to be anti-communist and the relationship between Finland and the Soviet Union was tense.



During World War II, Finland fought the Soviet Union twice: in the Winter War of 1939â€?0 after the Soviet Union had attacked Finland and in the Continuation War of 1941â€?4, following Operation Barbarossa in which Nazi Germany invaded the Soviet Union. Following German losses on the Eastern Front and the subsequent Soviet advance, Finland was forced to make peace with the Soviet Union. This was followed by the Lapland War of 1944â€?5, when Finland forced the Germans out of northern Finland.The treaties signed in 1947 and 1948 with the Soviet Union included Finnish obligations, restraints, and reparations as well as further Finnish territorial concessions (cf. the Moscow Peace Treaty of 1940). Finland ceded most of Finnish Karelia, Salla, and Pechenga, which amounted to ten percent of its land area and twenty percent of its industrial capacity. Some 400,000 evacuees, mainly women and children, fled these areas. Establishing trade with the Western powers, such as the United Kingdom, and the reparations to the Soviet Union caused Finland to transform itself from a primarily agrarian economy to an industrialised one. Even after the reparations had been paid off, Finland continued to trade with the Soviet Union in the framework of bilateral trade.

After the Second World War, neutral Finland lay in the grey zone between the Western countries and the Soviet Union. The "YYA Treaty" (Finno-Soviet Pact of Friendship, Cooperation, and Mutual Assistance) gave the Soviet Union some leverage in Finnish domestic politics. This was extensively exploited by President Urho Kekkonen against his opponents. He maintained an effective monopoly on Soviet relations, which gave him a status of "only choice for president". There was also a tendency of self-censorship regarding Finno-Soviet relations. This phenomenon was given the name "Finlandisation" by the German press (fi. suomettuminen). However, Finland maintained a democratic government and a market economy unlike most other countries bordering the Soviet Union.

The post-war era was a period of rapid economic growth and increasing wealth and stability for Finland. In all, the war-ravaged agrarian country was transformed into a technologically advanced market economy with an extensive social welfare system. When the Soviet Union collapsed in 1991, the bilateral trade disappeared overnight. Finland was simultaneously hit by a severe depression originating from the Western markets and the Finnish economy itself that caused a structural change of the economy. The depression lasted from 1990 to 1993, but the economy survived and began growing at a high rate. Finland joined the European Union in 1995.

The name Finland (Suomi in Finnish) has uncertain origins but a strong candidate for a cognate is the proto-Baltic word *zeme meaning "land". According to an earlier theory the name was derived from suomaa (fen land) or suoniemi (fen cape).

The exonym Finland has resemblance with, e.g., the Scandinavian placenames Finnmark, Finnveden and hundreds of other toponyms starting with "Fin(n)" in Sweden and Norway. Some of these names are obviously derived from finnr, a Germanic word for a wanderer/finder and thus supposedly meaning nomadic "hunter-gatherers" or slash and burn agriculturists as opposed to the Germanic sedentary farmers and sea-faring traders and pirates. It is unknown how, why and when "Finnr" started to mean the people of Finland Proper in particular (from where the name spread from the 15th century onwards to mean the people of the whole country).

Among the first documents to mention "a land of the Finns" are two runestones. There is one in SÃ¶derby, Sweden, with the inscription finlont (U 582) and one in Gotland, a Swedish island in the Baltic Sea, with the inscription finlandi (G 319) dating from the eleventh century.



Finland is a country of thousands of lakes and islands; 187,888 lakes (larger than 500 mÂ²) and 179,584 islands to be precise. One of these lakes, Saimaa, is the fifth largest in Europe. The Finnish landscape is mostly flat with few hills and its highest point, the Halti at 1,324 metres, is found in the extreme north of Lapland at the border between Finland and Norway.

The landscape is covered mostly (seventy-five percent of land area) by coniferous taiga forests and fens, with little arable land. The most common type of rock is granite. It is a ubiquitous part of the scenery, visible wherever there is no soil cover. Moraine or till is the most common type of soil, covered by a thin layer of humus of biological origin. The greater part of the islands are found in southwest in the Archipelago Sea, part of the archipelago of the Ã…land Islands, and along the southern coast in the Gulf of Finland.

Finland is one of the few countries in the world whose surface area is still growing. Owing to the post-glacial rebound that has been taking place since the last ice age, the surface area of the country is growing by about  a year.

The distance from the most Southern point â€?Hanko â€?to the most northern point of Finland â€?Nuorgam â€?is  (driving distance), which would take approximately 18.5 hours to drive. This is very similar to Great Britain (Land's End to John o' Groats â€? and 16.5 h).

All terrestrial life in Finland was completely wiped out during the last ice age that ended some 10,000 years ago, following the retreat of the glaciers and the appearance of vegetation.

Today, there are over 1,200 species of vascular plant, 800 bryophytes and 1,000 lichen species in Finland, with flora being richest in the southern parts of the country. Plant life, like most of the Finnish ecology, is well adapted to tolerate the contrasting seasons and extreme weather. Many plant species, such as the Scots Pine, spruce, birch spread throughout Finland from Norway and only reached the western coast less than three millennia ago. Oak and maple grows in nature only in the southern part of Finland.

 Similarly, Finland has a diverse and extensive range of fauna. There are at least sixty native mammalian species, 248 breeding bird species, over seventy fish species and eleven reptile and frog species present today, many migrating from neighbouring countries thousands of years ago.

Large and widely recognised wildlife mammals found in Finland are the Brown Bear (the national animal), Gray Wolf, elk and reindeer. Other common mammals include the Red Fox, Red Squirrel, and Mountain Hare. Some rare and exotic species include the flying squirrel, Golden Eagle, Saimaa Ringed Seal and the Arctic fox, which is considered the most endangered. The Whooper Swan, the national bird of Finland, is a large Northern Hemisphere swan. The most common breeding birds are the Willow Warbler, Chaffinch and Redwing. Of some seventy species of freshwater fish, the northern pike, perch and others are plentiful. Salmon remains the favorite of fly rod enthusiasts.

The endangered Saimaa Ringed Seal, one of only three lake seal species in the world, exists only in the Saimaa lake system of southeastern Finland, down to only 300 seals today. It has become the emblem of the Finnish Association for Nature Conservation.

Due to hunting and persecution in history, many animals such as the Golden Eagle, Brown Bear and Eurasian Lynx all experienced significant declines in population. However, their numbers have increased again in the 2000s, mainly as a result of careful conservation and the establishment of vast national parks.

The climate in Southern Finland is a northern temperate climate. In Northern Finland, particularly in the Province of Lapland, a subarctic climate dominates, characterised by cold, occasionally severe, winters and relatively warm summers. The main factor influencing Finland's climate is the country's geographical position between the 60th and 70th northern parallels in the Eurasian continent's coastal zone, which shows characteristics of both a maritime and a continental climate, depending on the direction of air flow. Finland is near enough to the Atlantic Ocean to be continuously warmed by the Gulf Stream, which explains the unusually warm climate considering the absolute latitude.

A quarter of Finland's territory lies above the Arctic Circle, and as a consequence the midnight sun can be experienced â€?for more days, the farther north one travels. At Finland's northernmost point, the sun does not set for 73 consecutive days during summer, and does not rise at all for 51 days during winter.



The state organisation is divided into six administrative provinces (lÃ¤Ã¤ni, pl. lÃ¤Ã¤nit). Police, prosecutors, and other state services operate under the administration of the province, and are divided into smaller districts (formerly state local districts).

The provincial authority is part of the executive branch of the national government, and has no elected officials. This system was created in 1634, and underwent few major changes until the redivision of the country into "greater provinces" in 1997. Since then, the six provinces have been (see picture on the right):

Dialects, folklore, customs, and people's feeling of affiliation are associated with the historical provinces of Finland, although the re-settlement of 420,000 Karelians during World War II and urbanisation in the latter half of the twentieth century have made differences less pronounced. The present-day regions are subdivisions of these provinces.

The Ã…land Islands enjoy a degree of autonomy.

Legally, Finland has two levels of democratic government: the state, and 415 municipalities (as of 2008). Since 1977, no legal or administrative distinction is made between towns, cities and other municipalities. Although a municipality must follow the laws set by the state, it makes independent decisions. That is, the decisions of a municipal council, if legal, cannot be appealed. People often identify with their municipality.

Municipalities co-operate in seventy-four sub-regions and twenty regions. These are governed by the member municipalities. The Ã…land region has a permanent, democratically elected regional council, as a part of the autonomy. In the Kainuu region, there is a pilot project underway, with regional elections.

Sami people have a semi-autonomous Sami Domicile Area in Lapland for issues on language and culture.

In the following chart, the number of inhabitants includes those living in the entire municipality (kunta/kommun), not just in the built-up area. The land area is given in kmÂ², and the density in inhabitants perÂ kmÂ² (land area). The figures are as of January 1, 2007. Notice that the capital region â€?comprising Helsinki, Vantaa, Espoo and Kauniainen (see Greater Helsinki) â€?forms a continuous conurbation of one million people. However, common administration is limited to voluntary cooperation of all municipalities, e.g. in Helsinki Metropolitan Area Council.



Finland currently numbers 5,290,158 inhabitants and has an average population density of 17 inhabitants per square kilometre. This makes it, after Norway and Iceland, the most sparsely populated country in Europe. Finland's population has always been concentrated in the southern parts of the country, which is even more pronounced after twentieth-century urbanisation. The biggest and most important cities in Finland are the cities of the Greater Helsinki metropolitan area - Helsinki, Vantaa, Espoo and Kauniainen - some of the other big cities include Tampere, Turku and Oulu.

The share of immigrants in Finland is among the lowest of the European Union countries. Foreign citizens comprise 2.3 percent of the population. Most of them are from Russia, Estonia and Sweden.



Most of the Finnish people (92 percent) speak Finnish as their mother tongue. Finnish is a member of the Baltic-Finnic subgroup of the Uralic languages and is typologically between inflected and agglutinative languages. It modifies and inflects the forms of nouns, adjectives, pronouns, numerals and verbs, depending on their roles in the sentence. In practice, this means that instead of prepositions and prefixes there is a great variety of different suffixes and that compounds form a considerable percentage of the vocabulary of Finnish. It has been estimated that approximately 65â€?0 percent of all words in Finnish are compounds. A close linguistic relative to the Finnish language is Estonian, which, though similar in many aspects, is not mutually intelligible with it. These languages, together with Hungarian (all members of the Uralic language family), are the primary non-Indo-European languages spoken in Europe. Finland, together with Estonia and Hungary, is one of the three independent countries where an Uralic language is spoken by the majority.

The largest minority language is Swedish, which is the second official language in Finland, spoken by 5.5 percent of the population. Other minority languages are Russian (0.8 percent) and Estonian (0.3 percent). To the north, in Lapland, are also the Sami people, numbering around 7,000 and recognized as an indigenous people. About a quarter of them speaks a Sami language as their mother tongue. There are three Sami languages that are spoken in Finland: Northern Sami, Inari Sami and Skolt Sami. Other minority languages are Finnish Romani, Finnish Sign Language (spoken natively by 4,000â€?,000 people) and Finland-Swedish Sign Language (spoken natively by about 150 people). The rights of minority groups (in particular Sami, Swedish-speaking Finns and Romani people) to cherish their culture and language is protected by the constitution.The majority of Finns learn enough English in school and from media to be proficient in that language. Other common secondary languages are Swedish, German and French.



Most Finns are members of the Evangelical Lutheran Church of Finland (82.5 percent). A minority belongs to the Finnish Orthodox Church (1.1 percent) (see Eastern Orthodox Church). Other Protestant denominations and the Roman Catholic Church in Finland are significantly smaller, as are the Muslim, Jewish and other non-Christian communities (totaling 1.2 percent). 15.1 percent of the population is unaffiliated. The main Lutheran and Orthodox churches are the national churches of Finland. Church attendance is much lower than these figures may suggest. Most of the population holds generally secular views. A majority of members of the state Lutheran Church do not participate actively, often attending church only for special occasions like weddings and funerals.

According to a 2005 Eurobarometer Poll, 41 percent of Finnish citizens responded that "they believe there is a god", whereas 41 percent answered that "they believe there is some sort of spirit or life force" and 16 percent that "they do not believe there is any sort of spirit, god, or life force".

Finnish family life is centered on the nuclear family. Relations with the extended family are often rather distant, and Finnish people do not form politically significant clans, tribes or similar structures. According to UNICEF, Finland ranks fourth in child well-being.



In the OECD's international assessment of student performance, PISA, Finland has consistently been among the highest scorers worldwide; in 2003, Finnish 15-year-olds came first in reading literacy, science, and mathematics; and second in problem solving, worldwide. The World Economic Forum ranks Finland's tertiary education #1 in the world.

Finland has a developed public health care system. 18.9 percent of health care is funded by households themselves, 76.6 percent is publicly funded, and the rest of the funding comes from elsewhere. There are 307 residents for each doctor.

After having one of the highest death rates from heart disease in the world in the 1970s, improvements in the Finnish diet and exercise have paid off. Finland also boasts the lowest smoking rate of any country in the European Union. Finland is now one of the fittest countries in the world.

The life expectancy is 82 years for women and 75 years for men.



Finland has a semi-presidential system with parliamentarism. The president is responsible for foreign policy outside of the European Union in cooperation with the cabinet (the Finnish Council of State) where most executive power lies, headed by the Prime Minister. Responsibility for forming the cabinet is granted to a person nominated by the President and approved of by the Parliament. This person also becomes Prime Minister after formal appointment by the President. Any minister and the cabinet as a whole, however, must have continuing trust of the parliament and may be voted out, resign or be replaced. The Council of State is made up of the Prime Minister and the ministers for the various departments of the central government as well as an ex-officio member, the Chancellor of Justice.

The 200-member unicameral parliament is called the Eduskunta (Finnish) or Riksdag (Swedish). It is the supreme legislative authority in Finland. The parliament may alter the Constitution of Finland, bring about the resignation of the Council of State, and override presidential vetoes. Its acts are not subject to judicial review. Legislation may be initiated by the Council of State, or one of the Eduskunta members, who are elected for a four-year term on the basis of proportional representation through open list multi-member districts.

The judicial system of Finland is divided between courts with regular civil and criminal jurisdiction and administrative courts with responsibility for litigation between the individuals and the administrative organs of the state and the communities. Finnish law is codified and based on Swedish law and in a wider sense, civil law or Roman law. Its court system consists of local courts, regional appellate courts, and the Supreme Court. The administrative branch of justice consists of administrative courts and the Supreme Administrative Court. In addition to the regular courts, there are a few special courts in certain branches of administration. There is also a High Court of Impeachment for criminal charges (for an offence in office) against the President of the Republic, the justices of the supreme courts, members of the Council of State, the Chancellor of Justice and the Ombudsman of Parliament.

The parliament has, since equal and common suffrage was introduced in 1906, been dominated by secular Conservatives, the Centre Party (former Agrarian Union), and Social Democrats, which have approximately equal support, and represent 65â€?0 percent of voters. After 1944 Communists were a factor to consider for a few decades. The relative strengths of the parties vary only slightly in the elections due to the proportional election from multi-member districts but there are some visible long-term trends.

Like the Netherlands and the United Kingdom, Finland has no constitutional court, and courts may not strike down laws or pronounce on their constitutionality. In principle, the constitutionality of laws in Finland is verified by a simple vote in the parliament. However, the constitutional committee in the parliament reviews legistlation during the lawmaking process, and thus performs a similar role.

According to Transparency International, Finland has had the lowest level of corruption in all the countries studied in its survey for the last several years. Also according to the World Audit study, Finland is the least corrupt and most democratic country in the world as of 2006.

In its 2007 Worldwide Press Freedom Index, Reporters Without Borders ranked Finland (along with Belgium and Sweden) 5th out of 169 countries.

The President of Finland is the Head of State of Finland. Under the Constitution of Finland, executive power is vested in the President and the government, with the President possessing extensive powers. The President is elected directly by the people for a term of six years. Since 1991, no President can be elected for more than two consecutive terms. The President must be a native-born Finnish citizen. The office was established by the Constitution Act of 1919.

The current office-holder is President Tarja Halonen. She began her first term of office in 2000 and was re-elected on January 29, 2006. Her current term expires in 2012. She is the eleventh President of Finland, the first woman and first from the capital, Helsinki, to hold the office.

2000â€?012

|}

The Finnish Parliament consists of one chamber with two hundred members. The members are elected for a four-year term by direct popular vote under a system of proportional representation. According to the Constitution of Finland, the Parliament elects the Prime Minister, who is appointed to office by the President. Other Ministers are appointed by the President on the Prime Minister's proposal. The current Prime Minister of Finland, as well as Chairman of the Centre Party is Matti Vanhanen (who in the second half of 2006 was President of the European Council).

After the parliamentary elections on March 18, 2007, the seats were divided among eight parties as follows:





After the collapse of the Soviet Union in 1991, Finland freed itself from the last restrictions imposed on it by the Paris peace treaties of 1947, save for declaring that she would voluntarily continue to adhere the treaty's ban on nuclear weapons. The Finno-Soviet Agreement of Friendship, Cooperation, and Mutual Assistance (and the restrictions included therein) was annulled but Finland recognised the Russian Federation as the successor of the USSR and was quick to draft bilateral treaties of goodwill as well as reallocating Soviet debts.

Finland deepened its participation in the European integration by joining the European Union with Sweden and Austria in 1995. It could perhaps be said that the country's policy of neutrality has been moderated to "military non-alignment" with an emphasis on maintaining a competent independent defence. Peacekeeping under the auspices of the United Nations was for years the only real extra-national military responsibility which Finland undertook. Since 2006, Finland has participated in the formation of European Union Battlegroups.

The President leads Finnish foreign policy, which is implemented by the Ministry for Foreign Affairs. The current Minister for Foreign Affairs is Ilkka Kanerva. Matters related to the European Union are usually not considered part of the foreign policy.

Finland's foreign policy is based on the membership of the European Union with its customs union, military non-alliance, and neutrality. Finland is also in the Nordic Council, and has long traditions of co-operation with the Nordic countries. Finland has good relations with all its neighbours, Sweden, Norway, Russia and Estonia, and is not involved in international conflicts or border disputes.

The military doctrine is strictly self-defensive, and indeed, the Constitution of Finland only allows participation in military operations authorised by the UN or the OSCE. Public opinion is against joining any military alliances, such as NATO, although Finland is involved in the Partnership for Peace programme with NATO. Foreign trade is highly important, as about a third of the gross domestic product comes from foreign trade, and Finland depends on imports for most raw materials.

The Finnish Defence Forces is a cadre army of 16,500, of which 8,700 are professional soldiers (officers), with a standard readiness strength of 34,700 people in uniform (27,300 Army, 3,000 Navy, and 4,400 Air Force). Finland's defence budget equals about 1.4 percent of the GDP. A universal male conscription is in place, under which all men above 18 years of age serve for six, nine, eleven (unarmed service) or twelve months. Inhabitants of Finland's Ã…land Islands and Jehovah's Witnesses are exempt, but there are no other general exemptions. Non-military service for twelve months is also possible. Since 1995, Finnish women have been able to do military service as volunteers. The defence is based on a large trained reserve. During the Cold War, Finland could have mobilised 490,000 reservists in a conflict, but this number has since been reduced to some 350,000 due to ongoing budget cuts.

The Finnish Defence Forces are under the command of the Chief of Defence, who is directly subordinate to the President of the Republic in matters related to the military command. The current Chief of Defence is Admiral Juhani Kaskeala.

The military branches are Finnish Army, Finnish Navy and Finnish Air Force. The Border Guard is under the Ministry of the Interior but can be incorporated into the Defence Forces when required by defence readiness.

The Ministry of Trade and Industry is responsible for the Government's energy policy. Energy policy is of exceptional importance, for Finland needs a lot of energy because of its cold climate and the structure of its industry, but has no fossil fuel energy resources, like oil or coal. It has thus done pioneering work on developing more efficient ways of using energy. Also, Finland refines oil for export (36 percent of chemical exports) and to cover domestic needs. The Finnish corporation Neste Oil has two oil refineries. Finland is connected to the Nordpool, the Nordic electricity market.

Until the 1960s, Finnish energy policy relied on the electricity produced by hydropower stations and extensive decentralised use of wood for energy. Finland's 187,888 lakes do not lie much above sea level â€?less than 80 metres in the case of the two biggest lakes, Saimaa and PÃ¤ijÃ¤nne. Consequently, Finland has less hydropower capacity than Sweden or Norway.



Finland started planning the introduction of nuclear power in the 1950s. In 2001, eighteen percent of all electricity consumed in Finland was produced by the country's four nuclear power plants. Energy policy became a burning issue in Finland when industry applied for permission to build a new nuclear power unit, the country's fifth. On May 24, 2002, Parliament supported the application by 107 votes to 92. After the vote, the The Green League resigned from the government where they had held the environment portfolio. All the other parties were divided over the nuclear issue. The fifth nuclear power station â€?world's largest at 1600 MWe â€?is currently under construction and is scheduled to be operational by 2011. It is being built by France's AREVA and Germany's Siemens AG. After general elections held on March 18, 2007, two Finnish energy groups, Fortum and Teollisuuden Voima (TVO) started the environmental impact assessment (EIA) process concerning the sixth nuclear power plant unit.

Most of the energy is produced from fossil fuels, mainly coal and oil. Fossil fuels are, however, all imported, because Finland doesn't have any fossil fuel sources, unlike neighboring Norway with oil and Estonia with oil shale. Nevertheless, Finland fares exceptionally well with renewable energy: 25 percent of energy is renewable, which is high compared to the EU average 10 percent. About one fifth of all the energy consumed in Finland is wood-based. This is not a remnant of old ages: the pulp and paper industryÂ â€?Finland's third-largest industryÂ â€?burns its byproducts, such as black liquor residues and waste wood chippings, resulting in net production of energy. Many homeowners also own renewed forests, and use wood as an additional (but not primary) heat source. About seven percent of electricity is produced from peat harvested from Finland's extensive bogs. Peat is "bioenergy", but there is no consensus whether it is renewable (carbon neutral) or not.

Currently, some electricity is imported to Finland. In recent years, a varying amount (5â€?7 percent) of power has been imported from Russia, Sweden and Norway. The Norwegian and Swedish hydroelectric plants remain an important source for imported power. The current energy policy debate is centred on self-sustainability. There are plans to build an submarine power cable from Russia, but this is also considered a national security issue. The government has already rejected one plan for such a power cable.



Finland has a highly industrialised, free-market economy with a per capita output equal to that of other western economies such as France, Germany, Sweden or the UK. The largest sector of the economy is services at 65.7 percent, followed by manufacturing and refining at 31.4 percent. Primary production is low at 2.9 percent, reflecting the fact that Finland is a resource-poor country. With respect to foreign trade, the key economic sector is manufacturing. The largest industries are electronics (21.6 percent), machinery, vehicles and other engineered metal products (21.1 percent), forest industry (13.1 percent), and chemicals (10.9 percent). International trade is important, with exports equalling almost one-third of GDP. Except for timber and several minerals, Finland depends on imports of raw materials, energy and some components for manufactured goods.

Because of the northern climate, agricultural development is limited to maintaining self-sufficiency. Forestry, an important export earner, provides a secondary occupation for the rural population.

Finland was one of the eleven countries joining the euro monetary system (EMU) on January 1, 1999. The national currency markka (FIM), in use since 1860, was withdrawn and replaced by the euro (EUR) at the beginning of 2002 (see Finnish euro coins).

The World Economic Forum has declared Finland to be the most competitive country in the world for three consecutive years (2003â€?005) and four times since 2002. In recent years there has been national focus on innovation and research and development, with special emphasis on information technology. Nokia, the telecommunications company, is generally regarded as the single most significant cause of Finland's success.

Finnish trade relationships and politics were by large determined by avoidance of provoking first the feudally ruled Imperial Russia and then the totalitarian Soviet Union. However, the peaceful relationship with both the Soviet Union and Western powers was turned into an economic advantage. The Soviet Union conducted bilateral trade with Finland, but Western countries remained Finland's main trading partners. After the Second World War, the growth rate of the GDP was high compared to other Europe, and Finland was often called "Japan of the North". In the beginning of the 1970s, Finland's GDP per capita reached the level of Japan and the UK.

In 1991, Finland fell into a severe depression caused by economic overheating, depressed foreign markets and the dismantling of the barter system between Finland and the former Soviet Union. More than twenty percent of Finnish trade was with the Soviet Union before 1991, and in the following two years the trade practically ceased. The growth in the 1980s was based on debt, and when the defaults began rolling in, an avalanche effect increased the unemployment from a virtual full employment to one fifth of the workforce. However, civil order remained and the state alleviated the problem of funding the welfare state by taking massive debts. 1991 and again in 1992, Finland devalued the markka to promote export competitiveness. This helped stabilise the economy; the depression bottomed out in 1993, with continued growth through 1995. Since then the growth rate has been one of the highest of OECD countries, and national debt has been reduced to 41.1 percent of GDP (fulfilling the EU's Stability and Growth Pact requirement). Unfortunately, the unemployment has been persistent, and is currently at about 7 percent.

Notable Finnish companies include Nokia, the market leader in mobile telephony; Stora Enso, the largest paper manufacturer in the world; Neste Oil, an oil refining and marketing company; UPM-Kymmene, the third largest paper manufacturer in the world; Aker Finnyards, the manufacturer of the world's largest cruise ships (such as Royal Caribbean's Freedom of the Seas); Instrumentarium Imaging, the creator of the Orthopantomograph (Pan X-Ray machine) and world innovative leader of dental imaging systems and software.; KONE, a manufacturer of elevators and escalators; WÃ¤rtsilÃ¤, a producer of power plants and ship engines; and Finnair, the country's international airline.

Finland's transport network is developed. As of 2005, the country's network of main roads has a total length of 13,258Â km, and is mainly centred on the capital city of Helsinki. The total length of all public roads is 78,186Â km, of which 50,616Â km are paved. The motorway network is still to a great extent under development, and currently totals 653Â km. There are 5,865Â km of railways in the country. Helsinki has an urban rail network, and light rail systems are currently being planned in Turku and Tampere. Finland also has a considerable number of airports and large ports.

The national railway company is VR (Valtion Rautatiet, or State Railways). It offers InterCity and express trains throughout the country and the faster Pendolino trains connecting the major cities. There are large discounts (usually fifty percent) available for children (7â€?6 yr), students, senior citizens and conscripts. There are international trains to St. Petersburg (Finnish and Russian day-time trains) and Moscow (Russian over-night train), Russia. Connections to Sweden are by bus due to rail gauge differences. It's possible to take the 

There are about 25 airports in Finland with scheduled passenger services. Finnair, Blue1 and Finncomm Airlines provide air services both domestically and internationally. Helsinki-Vantaa Airport is Finland's global gateway with scheduled non-stop flights to such places as Bangkok, Beijing, Delhi, Guangzhou, Mumbai, Nagoya, New York, Osaka, Shanghai, Hong Kong and Tokyo. Helsinki has an optimal location for great circle airline traffic routes between Western Europe and the Far East. Hence, many foreign tourists visit Helsinki on a stop-over while flying from Asia to Europe or vice versa.

Tourism is an expanding industry in Finland and in recent years has become a significant aspect of its economy. In 2005, Finnish tourism grossed over â‚?.7 billion with a five percent increase from the previous year. Much of the sudden growth can be attributed to the globalisation and modernisation of the country as well as a rise in positive publicity and awareness. There are many attractions in Finland which attracted over 4 million visitors in 2005.

The Finnish landscape is covered with thick pine forests, rolling hills and complemented with a labyrinth of lakes and inlets. Much of Finland is pristine and virgin as it contains 35 national parks from the Southern shores of the Gulf of Finland to the high fells of Lapland. It is also an urbanised region with many cultural events and activities.

Commercial cruises between major coastal and port cities in the Baltic region, including Helsinki, Turku, Tallinn, Stockholm and TravemÃ¼nde, play a significant role in the local tourism industry.

Although many tourists visit for the ideal weather during the summer, winter also attracts hundreds of thousands for its Christmas festivities and winter sports and activities such as skiing, dog sledding and Nordic walking. Finland is regarded as the home of Saint Nicholas or Santa Claus. Santaâ€™s Post Office is also located in Finland, up in the northern Lapland region. Above the Arctic Circle, there is a polar night, a period when the sun doesn't rise for days or weeks, or even months. Lapland, the extreme north of Finland, is so far north that the Aurora Borealis, atmospheric fluorescence, is seen regularly in winter. This exquisite spectacle draws people from around the globe, particularly from Japan.

Throughout the summer there are a range of international festivals, markets and performing arts including song and dance. The receding snow and everlasting sunlight also provide an opportunity for an array of outdoor activities. These activities range from golf, fishing, yachting, lake cruises, hiking, kayaking among many others. At Finland's northernmost point, in the heart of summer, the Sun does not completely set for 73 consecutive days. Wildlife is abundant in Finland. Bird-watching is popular for those fond of flying fauna, however hunting is also popular. Elk, reindeer and hare are all common game in Finland. The sport is highly regulated and also helps the economy.

Finland is also a place rich in culture for history, tradition and religion. There are  and  scattered all across Finland reflecting the strong Finnish Lutheran following. There are also  and examples of ancient architecture remaining from the reign of the Swedish Empire over much of Finland. These sites allure thousands for their significance and historical insight.  from the Swedish reign are found, for example in Turku, HÃ¤meenlinna and Savonlinna. The Turku Castle is a museum. Olavinlinna in Savonlinna hosts the annual Savonlinna Opera Festival. The capital city of Helsinki, on the other hand, is famous for its Grand Duchy era architecture, which resembles that of imperial St. Petersburg.

Like the people, Finnish culture is indigenous and most prominently represented by the Finnish language. Throughout the area's prehistory and history, cultural contacts and influences have concurrently, or at varying times, come from all directions. As a result of 600 years of Swedish rule, Swedish cultural influences are still notable. Today, cultural influences from North America are prominent. Into the twenty-first century, many Finns have contacted cultures from distantly abroad, such as with those in Asia and Africa. Beyond tourism, Finnish youth in particular have been increasing their contact with peoples from outside Finland by travelling abroad to both work and study.

There are still differences between regions, especially minor differences in accents and vocabulary. Minorities, such as the Sami, Finland Swedes, Romani, and Tatar, maintain their own cultural characteristics. Many Finns are emotionally connected to the countryside and nature, as urbanisation is a relatively recent phenomenon.

Finland comfortably won the first Eurovision Dance Contest in September 2007.

Though Finnish written language could be said to exist since Mikael Agricola translated the New Testament into Finnish in the sixteenth century as a result of the Protestant Reformation, few notable works of literature were written until the nineteenth century, which saw the beginning of a Finnish national Romantic Movement. This prompted Elias LÃ¶nnrot to collect Finnish and Karelian folk poetry and arrange and publish them as Kalevala, the Finnish national epic. The era saw a rise of poets and novelists who wrote in Finnish, notably Aleksis Kivi and Eino Leino.

After Finland became independent there was a rise of modernist writers, most famously Mika Waltari. Frans Eemil SillanpÃ¤Ã¤ was awarded the Nobel Prize in Literature in 1939 â€?so far the only one for a Finnish author. The second World War prompted a return to more national interests in comparison to a more international line of thought, characterized by VÃ¤inÃ¶ Linna. Literature in modern Finland is in a healthy state, with detective stories enjoying a particular boom of popularity. Ilkka Remes, a Finnish author of thrillers, is very popular.

Finns have made major contributions to handicrafts and industrial design. Finland's best-known sculptor of the twentieth century was WÃ¤inÃ¶ Aaltonen, remembered for his monumental busts and sculptures. Finnish architecture is famous around the world. Among the top of the twentieth century Finnish architects to win international recognition are Eliel Saarinen (designer of the widely recognised Helsinki Central railway station and many other public works) and his son Eero Saarinen. Alvar Aalto, who helped bring the functionalist architecture to Finland, is also famous for his work in furniture and glassware.



Much of the music of Finland is influenced by traditional Karelian melodies and lyrics, as comprised in the Kalevala. Karelian culture is perceived as the purest expression of the Finnic myths and beliefs, less influenced by Germanic influence, in contrast to Finland's position between the East and the West. Finnish folk music has undergone a roots revival in recent decades, and has become a part of popular music.

Sami music

The people of northern Finland, Sweden and Norway, the Sami, are known primarily for highly spiritual songs called Joik. The same word sometimes refers to lavlu or vuelie songs, though this is technically incorrect.

The first Finnish opera was written by the German composer Fredrik Pacius in 1852. Pacius also wrote ''Maamme/VÃ¥rt land'' (Our Land), Finland's national anthem. In the 1890s Finnish nationalism based on the Kalevala spread, and Jean Sibelius became famous for his vocal symphony Kullervo. He soon received a grant to study runo singers in Karelia and continued his rise as the first prominent Finnish musician. In 1899 he composed Finlandia, which played its important role in Finland gaining independence. He remains one of Finland's most popular national figures and is a symbol of the nation.

Today, Finland has a very lively classical music scene. Finnish classical music has only existed for about a hundred years, and many of the important composers are still alive, such as Magnus Lindberg, Kaija Saariaho, Aulis Sallinen and Einojuhani Rautavaara. The composers are accompanied with a large number of great conductors such as Sakari Oramo, Mikko Franck, Esa-Pekka Salonen, Osmo VÃ¤nskÃ¤, Jukka-Pekka Saraste, Susanna MÃ¤lkki and Leif Segerstam. Some of the internationally acclaimed Finnish classical musicians are Karita Mattila, Soile Isokoski, Kari Kriikku, Pekka Kuusisto, RÃ©ka Szilvay and Linda Brava.

Modern Finnish popular music includes a renowned heavy metal music scene, in common with other Nordic countries, as well as a number of prominent rock bands, jazz musicians, hip hop performers, and dance music acts such as Bomfunk MCs and Darude. Finnish electronic music such as the SÃ¤hkÃ¶ Recordings record label enjoys underground acclaim. IskelmÃ¤ (coined directly from the German word Schlager, meaning hit) is a traditional Finnish word for a light popular song. Finnish popular music also includes various kinds of dance music; tango, a style of Argentinean music, is also popular. One of the most productive composers of popular music was Toivo KÃ¤rki, and the most famous singer Olavi Virta (1915â€?972). Among the lyricists, Sauvo Puhtila (born 1928), Reino Helismaa (died 1965) and Veikko "Vexi" Salmi are the most remarkable authors. The composer and bandleader Jimi Tenor is well known for his brand of retro-funk music.

Notable Finnish dance and electronic music artists include Jori Hulkkonen, Darude, JS16, DJ Proteus and DJ Orkidea.

Finnish rock-music scene emerged in 1960s with pioneers such as Blues Section and Kirka. In the 1970s Finnish rock musicians started to write their own music instead of translating international hits in Finnish. During the decade some progressive rock groups, such as Tasavallan Presidentti and Wigwam, gained respect abroad but failed to make commercial breakthrough outside Finland. This was also the fate of rock and roll group Hurriganes. Finnish punk scene produced some internationally respected names including Terveet KÃ¤det in 1980s. Hanoi Rocks was a pioneering 1980s-glam rock act that left perhaps deeper mark in the history of popular music than any other Finnish group giving inspiration for Guns 'n' Roses.

In 1990s Finnish metal music started to get international fame with such bands as The 69 Eyes, Amorphis, Children of Bodom, Ensiferum, HIM, Impaled Nazarene, Lordi, Negative, Nightwish, The Rasmus, Sentenced, Sonata Arctica, and Stratovarius. In the later 1990s the cello metal group Apocalyptica played Metallica cover versions as cello quartettos and sold half a million records worldwide. Arguably one of Finland's most domestically popular rock groups is CMX. Although this group is not widely known outside of the country, bassist Billy Gould of popular U.S. rock group Faith No More produced CMX's 1998 album Vainajala.

In 2000s also Finnish rock bands started to sell well internationally. The Rasmus finally captured Europe (and other places, like South America) in 2000s. Their 2003 album Dead Letters sold 1.5 million units worldwide and garnered them eight gold and five platinum album designations. But so far the most successful Finnish band in the United States is HIM; they were the first band from Finland to ever sell an album that was certified gold by the RIAA. Most recently, the Finnish hard rock/heavy metal music band Lordi won the 2006 Eurovision Song Contest with a record 292 points, giving Finland its first ever victory. Rock bands such as 69 Eyes and 22-pistepirkko enjoy cult following abroard despite of milder commercial success.

Tuska Open Air Metal Festival, one of the largest open-air heavy metal music festivals in the world, is held annually in Kaisaniemi, Helsinki. Ruisrock and Provinssirock are the most famous rock festivals held in Finland.

Finland has a growing film industry with a number of famous directors such as Aki KaurismÃ¤ki, Timo Koivusalo, Aleksi MÃ¤kelÃ¤ and Klaus HÃ¤rÃ¶. Hollywood film director/producer Renny Harlin (born Lauri Mauritz Harjola) was born in Finland.



Finland is one of the most advanced information societies in the world. There are 200 newspapers; 320 popular magazines, 2,100 professional magazines and 67 commercial radio stations, with one nationwide, five national public service radio channels (three in Finnish, two in Swedish, one in Sami); digital radio has three channels. Four national analog television channels (two public service and two commercial) were fully replaced by five public service and three commercial digital television channels in September 1, 2007.

Each year around twelve feature films are made, 12,000 book titles published and 12 million records sold. 79 percent of the population use the Internet.

Finns, along with other Nordic people and the Japanese, spend the most time in the world reading newspapers. The most read newspaper in Finland is Helsingin Sanomat, with a circulation of 434,000. The media group SanomaWSOY behind Helsingin Sanomat also publishes the tabloid Ilta-Sanomat and commerce-oriented Taloussanomat. It also owns the Nelonen television channel. SanomaWSOY's largest shareholder is Aatos Erkko and his family. The other major publisher Alma Media publishes over thirty magazines, including newspaper Aamulehti, tabloid Iltalehti and commerce-oriented Kauppalehti. Finland has been at the top of the worldwide Press Freedom Ranking list every year since the publication of the first index by Reporters Without Borders in 2002.

Finland's National Broadcasting Company YLE is an independent state-owned company. It has five television channels and 13 radio channels in two national languages. YLE is funded through a television license and private television broadcasting license fees. Ongoing transformation to digital TV broadcasting is in progress â€?analog broadcasts ceased on the terrestrial network 31 August, 2007 and will cease on cable at the end of February 2008. The most popular television channel MTV3 and the most popular radio channel Radio Nova are owned by Nordic Broadcasting (Bonnier and Proventus Industrier).

The people of Finland are accustomed to technology and information services. The number of cellular phone subscribers as well as the number of Internet connections per capita in Finland are among the highest in the world. According to the Ministry of Transport and Communications, Finnish mobile phone penetration exceeded fifty percent of the population as far back as August 1998 â€?first in the world â€?and by December 1998 the number of cell phone subscriptions outnumbered fixed-line phone connections. By the end of June 2007 there were 5.78 million cellular phone subscriptions, or 109 percent of the population.

Another fast-growing sector is the use of the Internet. Finland had more than 1.52 million broadband Internet connections by the end of June 2007, i.e., about 287 per 1,000 inhabitants. The Finns are not only connected; they are heavy users of Internet services. All Finnish schools and public libraries have for years been connected to the Internet.

Traditional Finnish cuisine is a combination of European, Fennoscandian and Western Russian elements; table manners are European. The food is generally simple, fresh and healthy. Fish, meat, berries and ground vegetables are typical ingredients whereas spices are not common due to their historical unavailability. In years past, Finnish food often varied from region to region, most notably between the west and east. In coastal and lakeside villages, fish was a main feature of cooking, whereas in the eastern and also northern regions, vegetables and reindeer were more common. The prototypical breakfast is oatmeal or other continental-style foods such as bread. Lunch is usually a full warm meal, served by a canteen at workplaces. Dinner is eaten at around 17.00 to 18.00 at home.

Modern Finnish cuisine combines country fare and haute cuisine with contemporary continental cooking style. Today, spices are a prominent ingredient in many modern Finnish recipes, having been adopted from the east and west in recent decades.

All official holidays in Finland are established by acts of Parliament. The official holidays can be divided into Christian and secular holidays, although some of the Christian holidays have replaced holidays of pagan origin. The main Christian holidays are Christmas, Epiphany, Easter, Ascension Day, Pentecost, and All Saints Day. The secular holidays are New Year's Day, May Day, Midsummer Day, and the Independence Day. Christmas is the most extensively celebrated holiday: usually at least 23rd to 26th of December are holidays.

In addition to this, all Sundays are official holidays, but they are not as important as the special holidays. The names of the Sundays follow the liturgical calendar and they can be categorised as Christian holidays. When the standard working week in Finland was reduced to 40 hours by an act of Parliament, it also meant that all Saturdays became a sort of de facto public holidays, though not official ones. Easter Sunday and Pentecost are Sundays that form part of a main holiday and they are preceded by a kind of special Saturdays. Retail stores are prohibited by law from doing business on Sundays, except during the summer months (May through August) and in the pre-Christmas season (November and December). Business locations that have less than 400 square metres of floor space are allowed Sunday business throughout the year, with the exception of official holidays and certain Sundays, such as Mother's Day and Father's Day.

Various sporting events are popular in Finland. PesÃ¤pallo (reminiscent of baseball) is the national sport of Finland, although the most popular sports in Finland in terms of media coverage are Formula One, ice hockey and football. The Finnish national ice hockey team is considered one of the best in the world. During the past century there has been a rivalry in sporting between Finland and Sweden, mostly in ice hockey and athletics (Finland-Sweden athletics international). Jari Kurri and Teemu SelÃ¤nne are the two Finnish-born ice hockey players to have scored 500 goals in their NHL careers. Football is also popular in Finland, though the national football team has never qualified for a finals tournament of the World Cup or the European Championships. Jari Litmanen and Sami HyypiÃ¤ are the most internationally renowned of the Finnish football players.

Relative to its population, Finland has been the number one country in the world in automobile racing, measured by international success. Finland has produced three Formula One World Champions â€?Keke Rosberg (Williams, 1982), Mika HÃ¤kkinen (McLaren, 1998 and 1999) and Kimi RÃ¤ikkÃ¶nen (Ferrari, 2007). Along with RÃ¤ikkÃ¶nen, the other Finnish Formula One driver currently active is Heikki Kovalainen (McLaren). Rosberg's son, Nico Rosberg (Williams), is also currently driving, but under his mother's German nationality. Other notable Finnish Grand Prix drivers include Leo Kinnunen, JJ Lehto and Mika Salo. Finland has also produced most of the world's best rally drivers, including the ex-WRC World Champion drivers Marcus GrÃ¶nholm, Juha Kankkunen, Hannu Mikkola, Tommi MÃ¤kinen, Timo Salonen and Ari Vatanen. The only Finn to have won a road racing World Championship, Jarno Saarinen, was killed in 1973 while racing.

Among winter sports, Finland has been the most successful country in ski jumping, with former ski jumper Matti NykÃ¤nen being arguably the best ever in that sport. Most notably, he won five Olympic medals (four gold) and nine World Championships medals (five gold). Among currently active Finnish ski jumpers, Janne Ahonen has been the most successful. Kalle Palander is a well-known alpine skiing winner, who won the World Championship and Crystal Ball (twice, in KitzbÃ¼hel). Tanja Poutiainen has won an Olympic silver medal for alpine skiing, as well as multiple FIS World Cup races.

Some of the most outstanding athletes from the past include Hannes Kolehmainen (1890â€?966), Paavo Nurmi (1897â€?973) and Ville Ritola (1896â€?982) who won eighteen gold and seven silver Olympic medals in the 1910s and 1920s. They are also considered to be the first of a generation of great Finnish middle and long-distance runners (and subsequently, other great Finnish sportsmen) often named the "Flying Finns". Another long-distance runner, Lasse VirÃ©n (born 1949), won a total of four gold medals during the 1972 and 1976 Summer Olympics.

Also, in the past, Riku Kiri, Jouko Ahola and Janne Virtanen have been the greatest strength athletes in the country, participating in the World's Strongest Man competition between 1993 and 2000.

The 1952 Summer Olympics, officially known as the Games of the XV Olympiad, were held in 1952 in Helsinki, Finland. Other notable sporting events held in Finland include the 1983 and 2005 World Championships in Athletics, among others.

Some of the most popular recreational sports and activities include floorball, Nordic walking, running, cycling and skiing.

Below are listed some of the characteristics of Finnishness. The term "Finnishness" is often referred to as the national identity of the Finnish people and its culture.

















{| width="100%" class="collapsible collapsed" style="background:transparent; margin:1em 0 0;"!style="background:#f0f0f0; border:1px solid silver; padding:0.2em 1em 0.2em 6.5em; font-size:90%;"| 







  (Medieval Latin: In the year of (the/Our) Lord), abbreviated as AD or A.D., is a designation used to number years in the Julian and Gregorian calendars. More fully, years may be also specified as Anno Domini Nostri Iesu (Jesu) Christi ("In the Year of Our Lord Jesus Christ"). 

The calendar era which it numbers is based on the traditionally reckoned year of the conception or birth of Jesus. Before Christ, abbreviated as BC or B.C., is used in the English language to denote years before the start of this epoch.

Though the Anno Domini dating system was devised in 525, it was not until the 8th century that the system began to be adopted in Western Europe. According to the Catholic Encyclopedia, even popes continued to date documents according to regnal years, and usage of AD only gradually became more common in Europe from the 11th to the 14th centuries. In 1422, Portugal became the last Western European country to adopt the Anno Domini system.

Year numbering using the Anno Domini system (or its related Common Era (CE) designation) is the most widespread numbering system in the world today. For decades, it has been the unofficial global standard, recognized by international institutions such as the United Nations and the Universal Postal Union. Its preeminence is due to the European colonisation of the Americas and the subsequent global spread of Western civilisation with the introduction of European standards in the fields of science and administration. Its association with the Gregorian calendar was another factor which promoted the spread of the numbering system.

Traditionally, English copied Latin usage by placing the abbreviation before the year number for AD, but after the year number for BC; for example: 64 BC, but AD . However, placing the AD after the year number (as in  AD) is now also common. The abbreviation is also widely used after the number of a century or millennium, as in 4th century AD or 2nd millennium AD, despite the inappropriate literal combination in this case ("in the 4th century in the year of Our Lord").

Because B.C. is an abbreviation for Before Christ, some people incorrectly conclude that A.D. must mean After Death, i.e., after the death of Jesus. 

During the first six centuries of what would come to be known as the Christian era, European countries used various systems to count years. Systems in use included consular dating, imperial regnal year dating, and Creation dating. 

Although the last non-imperial consul, Basilius, was appointed in 541 by Justinian I, later emperors through Constans II (641â€?68) were appointed consuls on the first January 1 after their accession. All of these emperors, except Justinian, used imperial postconsular years for all of the years of their reign alongside their regnal years. Long unused, this practice was not formally abolished until Novell xciv of the law code of Leo VI did so in 888.

The Anno Domini system was devised by a monk named Dionysius Exiguus (born in Scythia Minor) in Rome in 525. In his Easter table Dionysius equates the year AD 532 with the regnal year 284 of Emperor Diocletian. In Argumentum I attached to this table he equates the year AD 525 with the consulate of Probus Junior. He thus implies that Jesus' Incarnation occurred 525 years earlier, without stating the specific year during which his birth or conception occurred.

Blackburn & Holford-Strevens briefly present arguments for 2 BC, 1 BC, or AD 1 as the year Dionysius intended for the Nativity or Incarnation.

Among the sources of confusion are:

Two centuries later, the Anglo-Saxon historian Bede used another Latin term, "ante uero incarnationis dominicae tempus" ("the time before the Lord's true incarnation"), equivalent to the English "before Christ", to identify years before the first year of this era.  

Another calculation had been developed by the Alexandrian monk Annianus around the year AD 400, placing the Annunciation on March 25, AD 9 (Julian)â€”eight to ten years after the date that Dionysius was to imply. Although this Incarnation was popular during the early centuries of the Byzantine Empire, years numbered from it, an Era of Incarnation, was only used, and is still only used, in Ethiopia, accounting for the eight- or seven-year discrepancy between the Gregorian and the Ethiopian calendars. Byzantine chroniclers like Maximus the Confessor, George Syncellus and Theophanes dated their years from Annianus' Creation of the World. This era, called Anno Mundi, "year of the world" (abbreviated AM), by modern scholars, began its first year on 25 March 5492 BC. Later Byzantine chroniclers used Anno Mundi years from September 1 5509 BC, the Byzantine Era. No single Anno Mundi epoch was dominant throughout the Christian world.

"Although scholars generally believe that Christ was born some years before A.D. 1, the historical evidence is too sketchy to allow a definitive dating". According to the Gospel of Matthew (2:1,16) Herod the Great was alive when Jesus was born, and ordered the Massacre of the Innocents in response to his birth. Blackburn & Holford-Strevens fix Herod's death shortly before Passover in 4 BC, and say that those who accept the story of the Massacre of the Innocents sometimes associate the star that led the Biblical Magi with the planetary conjunction of 15 September 7 BC or Halley's comet of 12 BC; even historians who do not accept the Massacre accept the birth under Herod as a tradition older than the written gospels.

The Gospel of Luke (1:5) states that John the Baptist was at least conceived, if not born, under Herod, and that Jesus was conceived while John's mother was in the sixth month of her pregnancy (1:26). Luke's Gospel also states that Jesus was born during the reign of Augustus and while Cyrenius (or Quirinius) was the governor of Syria (2:1â€?). Blackburn and Holford-Strevens indicate Cyrenius/Quirinius' governorship of Syria began in AD 6, which is incompatible with conception in 4 BC, and say that "St. Luke raises greater difficulty....Most critics therefore discard Luke". Some scholars rely on John's Gospel to place Christ's birth in c.18 BC.

The first historian or chronicler to use Anno Domini as his primary dating mechanism was Victor of Tonnenna, an African chronicler of the 6th century. A few generations later, the Anglo-Saxon historian Bede, who was familiar with the work of Dionysius, also used Anno Domini dating in his Ecclesiastical History of the English People, finished in 731. In this same history, he was the first to use the Latin equivalent of before Christ and established the standard for historians of no year zero, even though he used zero in his computus. Both Dionysius and Bede regarded Anno Domini as beginning at the incarnation of Jesus, but "the distinction between Incarnation and Nativity was not drawn until the late 9th century, when in some places the Incarnation epoch was identified with Christ's conception, i.e., the Annunciation on 25 March" (Annunciation style).

On the continent of Europe, Anno Domini was introduced as the era of choice of the Carolingian Renaissance by Alcuin. This endorsement by Charlemagne and his successors popularizing the usage of the epoch and spreading it throughout the Carolingian Empire ultimately lies at the core of the system's prevalence until present times.

Outside the Carolingian Empire, Spain continued to date by the Era of the Caesars, or Spanish Era, which began counting from 38 BC, well into the Middle Ages,. The Era of Martyrs, which numbered years from the accession of Diocletian in 284, who launched the last yet most severe persecution of Christians, was used by the Church of Alexandria, and is still used officially by the Coptic church. It also used to be used by the Ethiopian church. Another system was to date from the crucifixion of Jesus Christ, which as early as Hippolytus and Tertullian was believed to have occurred in the consulate of the Gemini (AD 29), which appears in the occasional medieval manuscript. Most Syriac manuscripts written at the end of the 19th century still gave the date in the end-note using the "year of the Greeks" (Anno Graecorum = Seleucid era).

Even though Anno Domini was in widespread use by the 9th century, Before Christ (or its equivalent) did not become widespread until the late 15th century.

Anno Domini is sometimes referred to as the Common Era, Christian Era or Current Era (abbreviated as C.E. or CE). CE is often preferred by those who desire a term unrelated to religious conceptions of time. For example, Cunningham and Starr (1998) write that "B.C.E./C.E. ... do not presuppose faith in Christ and hence are more appropriate for interfaith dialog than the conventional B.C./A.D." The People's Republic of China, founded in 1949, adopted Western years, calling that era gÅngyuÃ¡n (å…¬å…ƒ) which literally means Common Era.

Anno Salutis (Latin: "in the year of salvation") was the term sometimes used in place of Anno Domini until the 18th century. In all other respects it operated on the same epoch, reference date, which is the Incarnation of Jesus. It was used by fervent Christians to spread the message that the birth of Jesus saved mankind from eternal damnation. It was often used in a more elaborate form such as Anno Nostrae Salutis (meaning: "in the year of our salvation"), Anno Salutis Humanae (meaning: "in the year of the salvation of men"), or Anno Reparatae Salutis (meaning: "in the year of accomplished salvation").

Common usage omits year zero. This creates a problem with some scientific calculations. Accordingly, in astronomical year numbering, a zero year is added before AD 1, and the 'AD' and 'BC' designation is dropped. In keeping with 'standard decimal numbering', a minus sign 'âˆ? is added for years before year zero: so counting down from year 2 would give 2, 1, 0, âˆ?, âˆ?, and so on. This results in a one-year shift between the two systems (eg âˆ? equals 2 BC).

Notes:References:













Frederick II (; January 24 1712 â€?August 17 1786) was a King of Prussia (1740â€?786) from the Hohenzollern dynasty. In his role as a prince-elector of the Holy Roman Empire, he was Frederick IV (Friedrich IV) of Brandenburg. He became known as Frederick the Great (Friedrich der GroÃŸe) and was nicknamed der alte Fritz ("Old Fritz").

Interested primarily in the arts during his youth, Frederick unsuccessfully attempted to flee from his authoritarian father, the "Soldier-King" Frederick William I, after which he was forced to watch the execution of a childhood friend. Upon ascending to the Prussian throne, he attacked Austria and claimed Silesia during the Silesian Wars, winning military acclaim for himself and Prussia. Near the end of his life, Frederick united most of his disconnected realm through the First Partition of Poland.

Frederick was a proponent of enlightened absolutism. For years he was a correspondent of Voltaire, with whom the king had an intimate, if turbulent, friendship. He modernized the Prussian bureaucracy and civil service and promoted religious toleration throughout his realm. Frederick patronized the arts and philosophers. Frederick is buried at his favorite residence, Sans Souci in Potsdam. Because he died childless, he was succeeded by his nephew, Frederick William II of Prussia, son of his brother, Prince Augustus William of Prussia (the second son of King Frederick William I of Prussia).

Frederick was born in Berlin, the son of King Frederick William I of Prussia and Sophia Dorothea of Hanover. The so-called "Soldier-King", Frederick William had developed a formidable army and encouraged centralization, but was also known for his authoritarianism and temper. He would strike men in the face with his cane and kick women in the street, justifying his outbursts as religious righteousness. In contrast, Sophia was well-mannered and well-educated. Her father, George, Elector of Hanover, was the heir of Queen Anne of Great Britain. George succeeded as King George I of Great Britain in 1714.

The birth of Frederick was welcomed by his grandfather with more than usual pleasure, as two of his grandsons had already died at an early age. Frederick William wished his sons and daughters to be educated not as royalty, but as simple folk. He had been educated by a Frenchwoman, Madame de Montbail, who later became Madame de Rocoulle, and he wished that she should educate his children. Frederick was brought up by Huguenot governesses and tutors and learned French and German simultaneously.

Although Frederick William was raised a devout Calvinist, he feared he was not of the elect. To avoid the possibility of Frederick having the same motives, the king ordered that his heir not be taught about predestination. Although he was largely irreligious, Frederick adopted this tenet of Calvinism, despite the king's efforts. It is unknown if the crown prince did this to spite his father, or out of genuine religious belief. 

An example of the place that Frederick holds in history as a ruler is seen in Napoleon Bonaparte, who saw the Prussian king as the greatest tactical genius of all time; after Napoleon's defeat of the Fourth Coalition in 1807, he visited Frederick's tomb in Potsdam and remarked to his officers, "Gentlemen, if this man were still alive I would not be here".

Frederick the Great's most notable and decisive military victories on the battlefield were the Battles of Hohenfriedberg, Rossbach, and Leuthen.

Empress Catherine II took the Imperial Russian throne in 1762 after the murder of her husband, Peter III. Catherine was staunchly opposed to Prussia, while Frederick disapproved of Russia, whose troops had been allowed to freely cross the Polish-Lithuanian Commonwealth during the Seven Years' War. Despite the two monarchs' dislike of each other, Frederick and Catherine signed a defensive alliance on April 11 1764 which guaranteed Prussian control of Silesia in return for Prussian support for Russia against Austria or the Ottoman Empire. Catherine's candidate for the Polish throne, StanisÅ‚aw August Poniatowski, was then elected King of Poland in September of that year.

Frederick became concerned, however, after Russia gained significant influence over Poland in the Repnin Sejm of 1767, an act which also threatened Austria and the Ottoman Turks. In the ensuing Russo-Turkish War (1768â€?774), Frederick reluctantly supported Catherine with a subsidy of 300,000 roubles, as he did not want Russia to become even stronger through the acquisitions of Ottoman territory. The Prussian king successfully achieved a rapprochement with Emperor Joseph and the Austrian chancellor Kaunitz. As early as 1731 Frederick had suggested in a letter to Field Marshal Dubislav Gneomar von Natzmer that the country would be well-served by annexing Polish Prussia in order to unite the eastern territories of the Kingdom of Prussia.

Frederick's brother Henry spent the winter of 1770â€?771 as a representative of the Prussian court at St. Petersburg. As Austria had annexed 13 towns in the Szepes region in 1769, Catherine and her advisor General Ivan Chernyshyov suggested to Henry that Prussia claim some Polish land, such as Warmia (Ermeland). After Henry informed him of the proposal, Frederick suggested a partition of the Polish borderlands by Austria, Prussia, and Russia. Kaunitz counter-proposed that Prussia take lands from Poland in return for relinquishing Silesia to Austria, but this plan was rejected by Frederick.

After Russia occupied the Danubian Principalities, Henry convinced Frederick and Maria Theresa that the balance of power would be maintained by a tripartite division of the Polish-Lithuanian Commonwealth instead of Russia taking land from the Ottomans. In the First Partition of Poland in 1772, Frederick claimed most of the Polish province of Royal Prussia. Prussia annexed 20,000 miÂ² and 600,000 inhabitants, the least of the partitioning powers.

Frederick quickly began improving the infrastructure of the new territory. The Polish administrative and legal code was replaced by the Prussian system, and education improved; 750 schools were built from 1772-1775. Both Protestant and Roman Catholic teachers taught in West Prussia, and teachers and administrators were encouraged to be able to speak both German and Polish. He also advised his successors to learn Polish, a policy followed by the Hohenzollern dynasty until Frederick III decided not to let William II learn the language.

However, Frederick looked upon many of his new citizens with scorn. He had nothing but contempt for the szlachta, the numerous Polish nobility, and wrote that Poland had "the worst government in Europe with the exception of Turkey". He considered West Prussia as uncivilized as Colonial Canada and compared the Poles to the Iroquois. In a letter to Henry, Frederick wrote about the province that "it is a very good and advantageous acquisition, both from a financial and a political point of view. In order to excite less jealousy I tell everyone that on my travels I have seen just sand, pine trees, heath land and Jews. Despite that there is a lot of work to be done; there is no order, and no planning and the towns are in a lamentable condition." Frederick invited German immigrants to redevelop the province, also hoping they would displace the Poles. Many German officials also regarded the Poles with contempt. Frederick did befriend some Poles, such as Ignacy Krasicki, whom he asked to consecrate St. Hedwig's Cathedral in 1773.



Frederick managed to transform Prussia from a European backwater to an economically strong and politically reformed state. His acquisition of Silesia was orchestrated so as to provide Prussia's fledgling industries with raw materials, and he protected these industries with high tariffs and minimal restrictions on internal trade. Canals were built, including between the Vistula and the Oder, swamps were drained for agricultural cultivation, and new crops, such as the potato and the turnip, were introduced. Frederick regarded his reclamation of land in the Oderbruch as a province conquered in peace. With the help of French experts, he reorganized the system of indirect taxes, which provided the state with more revenue than direct taxes. Frederick the Great commissioned Johann Ernst Gotzkowsky to promote the trade and - to take on the competition with France - put a silk factory where soon 1,500 persons found employment. Frederick the Great followed his recommendations in the field of toll levies and import restrictions. In 1763 when Gotzkowsky went broke during a financial crisis, which started in Amsterdam, Frederick took over his porcelain factory, known as KPM, but refused to buy more of his paintings. 

During the reign of Frederick, the effects of the Seven Years' War and the gaining of Silesia greatly changed the economy. The circulation of depreciated money kept prices high. To revalue the Thaler, the Mint Edict of May 1763 was proposed. This stabilized the rates of depreciated coins that would be accepted and provided for the payments of taxes in currency of prewar value. This was replaced in northern Germany by the Reichsthaler, worth one-fourth of a Conventionsthaler. Prussia used a Thaler containing one-fourteenth of a Cologne mark of silver. Many other rulers soon followed the steps of Frederick in reforming their own currencies â€?this resulted in a shortage of ready money.

Frederick gave his state a modern bureaucracy whose mainstay until 1760 was the able War and Finance Minister Adam Ludwig von Blumenthal, succeeded in 1764 by his nephew Joachim who ran the ministry to the end of the reign and beyond. Prussia's education system was seen as one of the best in Europe. Frederick also abolished torture and corporal punishment.

Frederick began titling himself "King of Prussia" after the acquisition of Royal Prussia (West Prussia) in 1772; the phrasing "King ''in'' Prussia" had been used since the coronation of Frederick I in KÃ¶nigsberg in 1701.

Frederick generally supported religious toleration, including the retention of Jesuits as teachers in Silesia, Warmia, and the Netze District after their suppression by Pope Clement XIV. He was interested in attracting a diversity of skills to his country, whether from Jesuit teachers, Huguenot citizens, or Jewish merchants and bankers, particularly from Spain. He wanted development throughout the country, specifically in areas that he judged as needing a particular kind of development. As an example of this practical-minded but not fully unprejudiced tolerance, Frederick wrote in his Testament politique that:

We have too many Jews in the towns. They are needed on the Polish border because in these areas Hebrews alone perform trade. As soon as you get away from the frontier, the Jews become a disadvantage, they form cliques, they deal in contraband and get up to all manner of rascally tricks which are detrimental to Christian burghers and merchants. I have never persecuted anyone from this or any other sect [sic]; I think, however, it would be prudent to pay attention, so that their numbers do not increase.

Jews on the Polish border were therefore encouraged to perform all the trade they could and received all the protection and support from the king as any other Prussian citizen. The success in integrating the Jews into those areas of society that Frederick encouraged them in can be seen by the role played by Gerson von BleichrÃ¶der in financing Bismarck's efforts to reunite Germany.

Frederick's religious tolerance seemed to be motivated by more than a simple ploy to achieve advancement for his country. At a time when much of Europe still keenly remembered the invasions of the Ottoman Empire in the 17th century, he said, "All religions are equal and good and as long as those practicing are an honest people and wish to populate our land, may they be Turks or Pagans, we will build them mosques and churches".

Frederick had famous buildings constructed in his capital, Berlin, most of which still exist today, such as the Berlin State Opera, the Royal Library (today the State Library Berlin), St. Hedwig's Cathedral, and Prince Henry's Palace (now the site of Humboldt University). However, the king preferred spending his time in his summer residence Potsdam, where he built the palace of Sanssouci, the most important work of Northern German rococo. Sanssouci, which translates from French as "carefree" or "without worry", was a refuge for Frederick. "Frederician Rococo" developed under Georg Wenzeslaus von Knobelsdorff.



Frederick was a gifted musician who played the transverse flute. He composed 100 sonatas for the flute as well as four symphonies. The Hohenfriedberger Marsch, a military march, was supposedly written by Frederick to commemorate his victory in the Battle of Hohenfriedberg during the Second Silesian War. His court musicians included C. P. E. Bach, Johann Joachim Quantz, and Franz Benda. A meeting with Johann Sebastian Bach in 1747 in Potsdam led to Bach writing The Musical Offering. 

Frederick also aspired to be a philosopher-king like the Roman emperor Marcus Aurelius. The king joined the Freemasons in 1738 and stood close to the French Enlightenment, admiring above all its greatest thinker, Voltaire, with whom he corresponded frequently. The personal friendship of Frederick and Voltaire came to an unpleasant end after Voltaire's visit to Berlin and Potsdam in 1750â€?753, although they reconciled from afar in later years.

Frederick invited Joseph-Louis Lagrange to succeed Leonhard Euler at the Berlin Academy. Other writers attracted to the philosopher's kingdom were Francesco Algarotti, d'Argens, Julien Offray de La Mettrie, and Pierre Louis Maupertuis. Immanuel Kant published religious writings in Berlin which would have been censored elsewhere in Europe.

In addition to his native language, German, Frederick spoke French, English, Spanish, Portuguese, and Italian; he also understood Latin, ancient and modern Greek, and Hebrew. Preferring instead French culture, Frederick disliked the German language, literature, and culture, explaining that German authors "pile parenthesis upon parenthesis, and often you find only at the end of an entire page the verb on which depends the meaning of the whole sentence". His criticism led many German writers to attempt to impress Frederick with their writings in the German language and thus prove its worthiness. Many statesmen, including Baron vom und zum Stein, were also inspired by Frederick's statesmanship. Johann Wolfgang von Goethe gave his opinion of Frederick during a visit to Strasbourg (Strassburg) by writing:

Well we had not much to say in favour of the constitution of the Reich; we admitted that it consisted entirely of lawful misuses, but it rose therefore the higher over the present French constitution which is operating in a maze of lawful misuses, whose government displays its energies in the wrong places and therefore has to face the challenge that a thorough change in the state of affairs is widely prophesied. In contrast when we looked towards the north, from there shone Frederick, the Pole Star, around whom Germany, Europe, even the world seemed to turnâ€?
Some historians have speculated that Frederick the Great was homosexual, bisexual, or celibate, but what is known with certainty is that he showed no interest in his wife, and his relationship with Hans Hermann von Katte was widely speculated in the Prussian court to be romantic. 

Frederick's youth was marked by his father's ill-treatment. He exposed Frederick to frequent public beatings and other humiliations. At age 18, Frederick could stand no more, and planned to flee to France with Katte, a handsome officer who shared Frederick's French tastes. The king, already suspicious of a sexual relationship between them, intercepted their escape plans, and had both arrested He ordered Katte to be beheaded outside Frederick's prison window. The weeping prince blew Katte a kiss and asked for forgiveness. Katte replied, 'No need for forgiveness, sir, I die for you with the greatest of joy', then knelt for his decapitation. Frederick fainted dead away, and suffered hallucinations for two days following.

After Katte's death, Frederick's father forced him to marry Elisabeth Christine of Brunswick-Bevern. He immediately separated from his wife when Frederick William died in 1740. Frederick was no woman-hater, however. He had at least two affairs with women during his youth, and was very close to his sister Wilhelmine. Nevertheless, he had very little in common with his bride. Morevover, he resented his political marriage as an example of the Austrian interference which had plagued Prussia since 1701. Their marriage produced no children. In later years, Frederick would pay his wife formal visits only once a year.

Outside the military milieu, Frederick spent much time in Potsdam at Sans Souci, his favourite residence, built in 1745-1747, pursuing erotic intertest in regal seclusion. The grounds there even included a Friendship Temple celebrating the homoerotic attachments of Greek Antiquity, decorated with portrats of Orestes and Pylades, among others. 

At Sans Souci Frederick entertained his most privileged guests, especially the French philospher Voltaire, whom he asked in 1750 to come to live with him and be his love. The correspondence between Frederick and Voltaire which spanned almost 50 years was marked by mutual intellectual fascination and homoeroticism. In person, however, their friendship was often contentious. Voltaire abhorred Frederick's militarism. On the other hand, Frederick, whom Voltaire once described as a 'lovable whore', was unnerved by the Frenchman's way of flirting with him and then backing off. Voltaire's jealous attack in the press on one of Frederick's literary companions made him no longer welcome in Prussia; on his return to France in 1753 he anonymously published The Private Life of the King of Prussia, wittily exposing Frederick's homosexuality and parade of male lovers. Frederick neither admitted nor denied the contents of the book. Voltaire and Frederick soon thereafter amicably resumed their correspondence, only able to love each other from a distance. 

Frederick became misanthropic and withdrawn during his old age, having outlived the ones dearest to him. After a long illness, he died at age 74, accompanied in his last days by a young Italian count, whom he rewarded with an ambassadorship  

Other historians disagree on the nature of Frederick's sexuality, saying that Frederick's writings indicate that he simply had greater priorities than women. The French professor DieudonnÃ© ThiÃ©bault declared that Frederick had mistresses at Neuruppin. Frederick's physician, Johann Georg Ritter von Zimmermann, claimed that the king let rumors of homosexuality appear to be true in order to avoid the public knowing that his genitalia were harmed by "a cruel surgical operation" to save his life from an unnamed venereal disease. Historian Christopher Clark concludes "it is impossible - and unnecessary - to reconstruct the king's sexual history; he may well have abstained from sexual acts with anyone of either sex after his accession to the throne, and possibly even before. But if he did not do it, he certainly talked about it; the conversation of the inner court circle around him was peppered with homoerotic banter." 

Near the end of his life Frederick grew increasingly solitary. His circle of friends at Sans Souci gradually died off without replacements, and Frederick became increasingly critical and arbitrary, to the frustration of the civil service and officer corps. The populace of Berlin always cheered the king when he returned to the city from provincial tours or military reviews, but Frederick took no pleasure from his popularity with the common folk, preferring instead the company of his pet greyhounds, whom he referred to as his 'marquises de Pompadour' as a jibe at Madame de Pompadour. Frederick died in an armchair in his study in the palace of Sans Souci on 17 August 1786.



Frederick had wished to be buried next to his greyhounds on the vineyard terrace on the side of the corps de logis of Sansscouci. His nephew and successor Frederick William II instead ordered the body to be buried next to the grave of his father in the church of the Potsdam garrison. During World War II, the catafalques of both Frederick and Frederick William I were transferred first to an underground bunker, later to a mineshaft close to the town of Bernrode to protect them from destruction. In 1945 the US Army transported both kings first to the Elisabeth Church (Marburg) and then on to Burg Hohenzollern close to the town of Hechingen. After the German reunification, the body of Frederick William was entombed in the Kaiser Friedrich Mausoleum in Sans Souci's Church of Peace.

There was an emotional debate in Germany whether the funeral of a former king of Prussia, who was responsible for many wars during his time and who had been exploited as a symbol both by Nazi Germany and the German Democratic Republic, should be regarded as a public matter or not.Despite numerous protests, on the 205th anniversary of his death, on 17 August 1991, Frederick's casket lay in state in the court of honor of Sans Souci, covered by a Prussian flag and escorted by a Bundeswehr guard of honour. After nightfall, Frederick's body was finally laid to rest on the terrace of the vineyard of Sans Souci, according to his last will without pomp and at night ("... Im Ã¼brigen will ich, was meine Person anbetrifft, in Sans Souci beigesetzt werden, ohne Prunk, ohne Pomp und bei Nacht..." (1757)).

Frederick remains a controversial figure in Germany and Central Europe. With the rise of German romantic nationalism in the 19th century, he was admired by German nationalists. In the 20th century, Frederick was often cited as a precursor for the Prussian and German militarism that would inspire Otto von Bismarck and Adolf Hitler.

Unlike many of his contemporaries, Frederick did not believe in the Divine Right of Kings and, disregarding the exaggerated French style of the time, often wore old military uniforms; he merely believed the crown was "a hat that let the rain in". He called himself the "first servant of the state", but the Austrian empress Maria Theresa called him "the evil man in Sans Souci." His wars against Austria weakened the Holy Roman Empire, yet gave to Prussia land and prestige that would prove vital for the 19th century unification of Germany. He was both an enlightened ruler and a ruthless despot. Through reform, war, and the First Partition of Poland in 1772, he turned the Kingdom of Prussia into a European great power.

Regarding Frederick, Lord Macaulay wrote:If he had not made conquests as vast as those of Alexander, of Caesar, and of Napoleon, if he had not, on fields of battle, enjoyed the constant success of Marlborough and Wellington, he had yet given an example unrivalled in history of what capacity and resolution can effect against the greatest superiority of power and the utmost spite of fortune.

King of Prussia, Pennsylvania, is named after the King of Prussia Inn, itself named in honor of Frederick. In popular culture, Frederick has been included in the Civilization computer game series, the computer games Age of Empires III and Empire Earth II, and the board game Friedrich. In the 2004 German film Der Untergang, Adolf Hitler is shown sitting in a dark room forlornly gazing at a painting of Frederick shortly before he takes his own life.







|-|-|-











Multiple sclerosis (abbreviated MS, also known as disseminated sclerosis or encephalomyelitis disseminata) is a chronic, inflammatory, demyelinating disease that affects the central nervous system (CNS). Disease onset usually occurs in young adults, is more common in women, and has a prevalence that ranges between 2 and 150 per 100,000 depending on the country or specific population. MS was first described in 1868 by Jean-Martin Charcot. 

MS affects the areas of the brain and spinal cord known as the white matter. These cells carry signals in between the grey matter areas, where the processing is done, and between these and the rest of the body. More specifically, MS destroys oligodendrocytes which are the cells responsible for creating and maintaining a fatty layer, known as the myelin sheath, which helps the neurons carry electrical signals. MS results in a thinning or complete loss of myelin and, less frequently, the cutting (transection) of the neuron's extensions or axons. When the myelin is lost, the neurons can no longer effectively conduct their electrical signals. The name multiple sclerosis refers to the scars (scleroses - better known as plaques or lesions) in the white matter. Loss of myelin in these lesions causes some of the symptoms, which vary widely depending upon which signals are interrupted. However, more advanced forms of imaging are now showing that much of the damage happens outside these regions. Almost any neurological symptom can accompany the disease.

MS takes several forms, with new symptoms occurring either in discrete attacks (relapsing forms) or slowly accumulating over time (progressive forms). Most people are first diagnosed with relapsing-remitting MS but develop secondary-progressive MS (SPMS) after a number of years. Between attacks, symptoms may go away completely, but permanent neurological problems often persist, especially as the disease advances. 

Although much is known about the mechanisms involved in the disease process, the cause remains elusive. The theory with the most adherents is that it results from attacks to the nervous system by the body's own immune system. Some believe it is a metabolically dependent disease while others think that it might be caused by a virus such as Epstein-Barr. Still others believe that its virtual absence from the tropics points to a deficiency of vitamin D during childhood.

The disease does not have a cure, but several therapies have proven helpful. Treatments attempt to return function after an attack, prevent new attacks, and prevent disability. As with any treatment, medications have several adverse effects, and many therapies are still under investigation. Many patients pursue alternative treatments, despite the paucity of supporting scientific study.

The prognosis, or expected course of the disease, depends on the subtype of the disease, the individual patient's characteristics, the initial symptoms, and the degree of disability the person experiences as time advances. Life expectancy of patients, however, is nearly the same as that of the unaffected population, and in many cases a normal life is possible.



MS can cause a variety of symptoms, including changes in sensation (hypoesthesia), muscle weakness, abnormal muscle spasms, or difficulty in moving; difficulties with coordination and balance (ataxia); problems in speech (dysarthria) or swallowing (dysphagia), visual problems (nystagmus, optic neuritis, or diplopia), fatigue and acute or chronic pain syndromes, bladder and bowel difficulties, cognitive impairment, or emotional symptomatology (mainly depression). Lhermitte's sign is considered a classic MS finding, but it can be seen in several other conditions as well. The main clinical measure of disability progression and severity of the symptoms is the Expanded Disability Status Scale or EDSS.

The initial attacks are often transient, mild (or asymptomatic), and self-limited. They often do not prompt a health care visit and sometimes are only identified in retrospect once the diagnosis has been made based on further attacks. The most common initial symptoms reported are: changes in sensation in the arms, legs or face (33%), complete or partial vision loss (optic neuritis) (16%), weakness (13%), double vision (7%), unsteadiness when walking (5%), and balance problems (3%); but many rare initial symptoms have been reported such as aphasia or psychosis. Fifteen percent of individuals have multiple symptoms when they first seek medical attention. For some people the initial MS attack is preceded by infection, trauma, or strenuous physical effort.



Multiple sclerosis is difficult to diagnose in its early stages. In fact, a definite diagnosis cannot be made until other disease processes (differential diagnoses) have been ruled out and, in the case of relapsing-remitting MS, there is evidence of at least two anatomically separate demyelinating events separated by at least thirty days. In the case of primary progressive, a slow progression of signs and symptoms over at least 6 months is required.

Historically, different criteria were used and the Schumacher and Poser criteria were both popular. Currently, the McDonald criteria represent international efforts to standardize the diagnosis of MS using clinical, laboratory and radiologic data.

Another test, which may become important in the future, is measurement of antibodies against myelin proteins such as myelin oligodendrocyte glycoprotein (MOG) and myelin basic protein (MBP). As of 2007, however, there is no established role for these tests in diagnosing MS. Optical coherence tomography of the eye's retina is also under study, mainly as a tool to measure response to medication and axonal degeneration.

The signs and symptoms of MS can be similar to other medical problems, such as neuromyelitis optica, stroke, brain inflammation, infections such as Lyme disease (which can produce identical MRI lesions and CSF abnormalities),tumors, and other autoimmune problems, such as lupus. Additional testing may be needed to help distinguish MS from these other problems.

The course of MS is difficult to predict, and the disease may at times either lie dormant or progress steadily. Several subtypes, or patterns of progression, have been described. Subtypes use the past course of the disease in an attempt to predict the future course. Subtypes are important not only for prognosis but also for therapeutic decisions. In 1996 the United States National Multiple Sclerosis Society standardized the following four subtype definitions:

Nevertheless the earliest clinical presentation of relapsing-remitting MS (RRMS) is the clinically isolated syndrome (CIS). In CIS, there is a subacute attack suggestive of demyelination but the person does not fullfill the criteria for multiple sclerosis. Several studies have shown that starting treatment with interferons during the initial attack can decrease the chance that a patient will develop clinical MS. Suspected cases of MS before the CIS are sometimes referred to as possible Preclinical MS cases.

Special cases of the disease with non-standard behavior have also been described although many researchers believe they are different diseases. These cases are sometimes referred to as borderline forms of multiple sclerosis and are Neuromyelitis optica (NMO), Balo concentric sclerosis, Schilder's diffuse sclerosis and Marburg multiple sclerosis.

Multiple sclerosis relapses are often unpredictable and can occur without warning with no obvious inciting factors. Some attacks, however, are preceded by common triggers. In general, relapses occur more frequently during spring and summer than during autumn and winter. Infections, such as the common cold, influenza, and gastroenteritis, increase the risk for a relapse.Emotional and physical stress may also trigger an attack, as can severe illness of any kind.Statistically, there is no good evidence that either trauma or surgery trigger relapses. People with MS can participate in sports, but they should probably avoid extremely strenuous exertion, such as marathon running. Heat can transiently increase symptoms, which is known as Uhthoff's phenomenon. This is why some people with MS avoid saunas or even hot showers.However, heat is not an established trigger of relapses.

Pregnancy can directly affect the susceptibility for relapse. The last three months of pregnancy offer a natural protection against relapses. However, during the first few months after delivery, the risk for a relapse is increased 20%â€?0%. Pregnancy does not seem to influence long-term disability. Children born to mothers with MS are not at increased risk for birth defects or other problems.

Many potential triggers have been examined and found not to influence relapse rates in MS. Influenza vaccination is safe, does not trigger relapses, and can therefore be recommended for people with MS. There is also no evidence that vaccines for hepatitis B, varicella, tetanus, or Bacille Calmette-Guerin (BCGâ€”immunization for tuberculosis) increases the risk for relapse.

Although much is known about how multiple sclerosis causes damage, the reasons why multiple sclerosis occurs are not known.

Multiple sclerosis is a disease in which the myelin (a fatty substance which covers the axons of nerve cells) degenerates. According to the view of most researchers, a special subset of lymphocytes, called T cells, plays a key role in the development of MS.

According to a strictly immunological explanation of MS, the inflammatory process is triggered by the T cells. T cells gain entry into the brain via the blood-brain barrier (a capillary system that should prevent entrance of T-cells into the nervous system). The blood brain barrier is normally not permeable to these types of cells, unless triggered by either infection or a virus, where the integrity of the tight junctions forming the blood-brain barrier is decreased. When the blood brain barrier regains its integrity (usually after infection or virus has cleared) the T cells are trapped inside the brain. These lymphocytes recognize myelin as foreign and attack it as if it were an invading virus. That triggers inflammatory processes, stimulating other immune cells and soluble factors like cytokines and antibodies. Leaks form in the blood-brain barrier. These leaks, in turn, cause a number of other damaging effects such as swelling, activation of macrophages, and more activation of cytokines and other destructive proteins such as matrix metalloproteinases. A deficiency of uric acid has been implicated in this process.

It is known that a repair process, called remyelination, takes place in early phases of the disease, but the oligodendrocytes that originally formed a myelin sheath cannot completely rebuild a destroyed myelin sheath. The newly-formed myelin sheaths are thinner and often not as effective as the original ones. Repeated attacks lead to successively fewer effective remyelinations, until a scar-like plaque is built up around the damaged axons, according to four different damage patterns. The central nervous system should be able to recruit oligodendrocyte stem cells capable of turning into mature myelinating oligodendrocytes, but it is suspected that something inhibits stem cells in affected areas.

The axons themselves can also be damaged by the attacks. Often, the brain is able to compensate for some of this damage, due to an ability called neuroplasticity. MS symptoms develop as the cumulative result of multiple lesions in the brain and spinal cord. This is why symptoms can vary greatly between different individuals, depending on where their lesions occur.

Although many risk factors for multiple sclerosis have been identified, no definitive cause has been found. MS likely occurs as a result of some combination of both environmental and genetic factors. Various theories try to combine the known data into plausible explanations. Although most accept an autoimmune explanation, several theories suggest that MS is an appropriate immune response to one or several underlying conditions (the etiology could be heterogeneous). The need for alternative theories is supported by the poor results of present therapies, since autoimmune theory predicted greater success.

The most popular hypothesis is that a viral infection or retroviral reactivation primes a susceptible immune system for an abnormal reaction later in life. On a molecular level, this might occur if there is a structural similarity between the infectious virus and some component of the central nervous system, leading to eventual confusion in the immune system.

Since MS seems to be more common in people who live farther from the equator, another theory proposes that decreased sunlight exposure and possibly decreased vitamin D production may help cause MS. This theory is bolstered by recent research into the biochemistry of vitamin D, which has shown that it is an important immune system regulator. A large, 2006 study by the Harvard School of Public Health, reported evidence of a link between vitamin D deficiency and the onset of multiple sclerosis. Other data comes from a 2007 study which concluded that sun exposure during childhood reduces the risk of suffering MS, while controlling for genetic factors.

Other theories, noting that MS is less common in children with siblings, suggest that less exposure to illness in childhood leads to an immune system which is not primed to fight infection and is thus more likely to attack the body. One explanation for this would be an imbalance between the Th1 type of helper T-cells, which fight infection, and the Th2 type, which are more active in allergy and more likely to attack the body.

Other theories describe MS as an immune response to a chronic infection. The association of MS with the Epstein-Barr virus suggests a potential viral contribution in at least some individuals. Still others believe that MS may sometimes result from a chronic infection with spirochetal bacteria, a hypothesis supported by research in which cystic forms were isolated from the cerebrospinal fluid of all MS patients in a small study. When the cysts were cultured, propagating spirochetes emerged. Another bacterium that has been implicated in MS is Chlamydophila pneumoniae; it or its DNA has been found in the cerebrospinal fluid of MS patients by several research laboratories, with one study finding that the oligoclonal bands of 14 of the 17 MS patients studied consisted largely of antibodies to Chlamydophila antigens.. Varicella zoster virus is also suspected to be involved.

Severe stress may also be a factorâ€”a large study in Denmark found that parents who had lost a child unexpectedly were 50% more likely to develop MS than parents who had not. Smoking has also been shown to be an independent risk factor for developing MS.

MS is not considered a hereditary disease. However, increasing scientific evidence suggests that genetics may play a role in determining a person's susceptibility to MS:

Some populations, such as the Roma, Inuit, and Bantus, rarely if ever develop MS. The indigenous peoples of the Americas and Asians have very low incidence rates.

In the population at large, the chance of developing MS is less than a tenth of one percent. However, if one person in a family has MS, that person's first-degree relativesâ€”parents, children, and siblingsâ€”have a one to three percent chance of getting the disease.

For identical twins, the likelihood that the second twin may develop MS if the first twin does is about 30%. For fraternal twins (who do not inherit an identical set of genes), the likelihood is closer to that for non-twin siblings, at about 4%. This pattern suggests that, while genetic factors clearly help determine the risk of MS, other factors such as environmental effects or random chance are also involved. The actual correlation may be somewhat higher than reported by these numbers as people with MS lesions remain essentially asymptomatic throughout their lives.

Further indications that more than one gene is involved in MS susceptibility comes from studies of families in which more than one member has MS. Several research teams found that people with MS inherit certain regions on individual genes more frequently than people without MS. Of particular interest is the human leukocyte antigen (HLA) or major histocompatibility complex region on chromosome 6. HLAs are genetically determined proteins that influence the immune system. However, there are other genes in this region which are not related to the immune system.

The HLA patterns of MS patients tend to be different from those of people without the disease. Investigations in northern Europe and America have detected three HLAs that are more prevalent in people with MS than in the general population. Studies of American MS patients have shown that people with MS also tend to exhibit these HLAs in combinationâ€”that is, they have more than one of the three HLAsâ€”more frequently than the rest of the population. Furthermore, there is evidence that different combinations of the HLAs may correspond to variations in disease severity and progression.

A large study examining 334,923 single nucleotide polymorphisms (small variations in genes) in 931 families showed that apart from HLA-DRA there were two genes in which polymorphisms strongly predicted MS; these were the IL2RA (a subunit of the receptor for interleukin 2) and the IL7RA (idem for interleukin 7) genes. Mutations in these genes were already known to be associated with diabetes mellitus type 1 and other autoimmune conditions; the findings circumstantially support the notion that MS is an autoimmune disease.

Studies of families with multiple cases of MS and research comparing proteins expressed in humans with MS to those of mice with EAE suggest that another area related to MS susceptibility may be located on chromosome 5. Other regions on chromosomes 2, 3, 7, 11, 17, 19, and X have also been identified as possibly containing genes involved in the development of MS.

These studies strengthen the theory that MS is the result of a number of factors rather than a single gene or other agent. Development of MS is likely to be influenced by the interactions of a number of genes, each of which (individually) has only a modest effect. Additional studies are needed to specifically pinpoint which genes are involved, determine their function, and learn how each gene's interactions with other genes and with the environment make an individual susceptible to MS.



Although there is no known cure for multiple sclerosis, several therapies have proven helpful.The primary aims of therapy are returning function after an attack, preventing new attacks, and preventing disability. As with any medical treatment, medications used in the management of MS have several adverse effects, and many possible therapies are still under investigation. At the same time different alternative treatments are pursued by many patients, despite the paucity of supporting, comparable, replicated scientific study.

During symptomatic attacks administration of high doses of intravenous corticosteroids, such as methylprednisolone, is the routine therapy for acute relapses.The aim of this kind of treatment is to end the attack sooner and leave fewer lasting deficits in the patient. Although generally effective in the short term for relieving symptoms, corticosteroid treatments do not appear to have a significant impact on long-term recovery. Potential side effects include osteoporosis and impaired memory, the latter being reversible

The earliest clinical presentation of relapsing-remitting MS (RRMS) is the clinically isolated syndrome (CIS). Several studies have shown that treatment with interferons during an initial attack can decrease the chance that a patient will develop MS.

As of 2007, six disease-modifying treatments have been approved by regulatory agencies of different countries for relapsing-remitting MS. Three are interferons: two formulations of interferon beta-1a (trade names Avonex and Rebif) and one of interferon beta-1b (U.S. trade name Betaseron, in Europe and Japan Betaferon). A fourth medication is glatiramer acetate (Copaxone). The fifth medication, mitoxantrone, is an immunosuppressant also used in cancer chemotherapy. Finally, the sixth is natalizumab (marketed as Tysabri). All six medications are modestly effective at decreasing the number of attacks and slowing progression to disability, although they differ in their efficacy rate and studies of their long-term effects are still lacking. Comparisons between immunomodulators (all but mitoxantrone) show that the most effective is natalizumab. Mitoxantrone may be the most effective of them all; however, it is generally considered not as a long-term therapy as its use is limited by severe cardiotoxicity. 

The interferons and glatiramer acetate are delivered by frequent injections, varying from once-per-day for glatiramer acetate to once-per-week (but intra-muscular) for Avonex. Natalizumab and mitoxantrone are given by IV infusion at monthly intervals.

Treatment of progressive MS is more difficult than relapsing-remitting MS. Mitoxantrone has shown positive effects in patients with a secondary progressive and progressive relapsing courses. It is moderately effective in reducing the progression of the disease and the frequency of relapses in patients in short-term follow-up. On the other hand no treatment has been proven to modify the course of primary progressive MS.

As with any medical treatment, these treatments have several adverse effects. One of the most common is irritation at the injection site for glatiramer acetate and the Interferon treatments. Over time, a visible dent at the injection site due to the local destruction of fat tissue, known as lipoatrophy, may develop. Interferons also produce symptoms similar to influenza; while some patients taking glatiramer experience a post-injection reaction manifested by flushing, chest tightness, heart palpitations, breathlessness, and anxiety, which usually lasts less than thirty minutes.. More dangerous are liver damage of interferons and mitoxantrone, the immunosuppressive effects and cardiac toxicity of the latter; or the putative link between natalizumab and some cases of progressive multifocal leukoencephalopathy in patients who had taken it in combination with interferons.

Disease-modifying treatments only reduce the progression rate of the disease but do not stop it. As multiple sclerosis progresses, the symptomatology tends to increase. The disease is associated with a variety of symptoms and functional deficits that result in a range of progressive impairments and handicap. Management of these deficits is therefore very important. Both drug therapy and neurorehabilitation have shown to ease the burden of some symptoms, even though neither influence disease progression. As for any patient with neurologic deficits, a multidisciplinary approach is key to limiting and overcoming disability; however there are particular difficulties in specifying a â€˜core teamâ€?because people with MS may need help from almost any health profession or service at some point. Similarly for each symptom there are different treatment options. Treatments should therefore be individualized depending both on the patient and the physician



Scientists continue their extensive efforts to create new and better therapies for MS. There are a number of treatments under investigation that may curtail attacks or improve function. Some of these treatments involve the combination of drugs that are already in use for multiple sclerosis, such as the combination of mitoxantrone and glatiramer acetate (Copaxone). However most treatments already in clinical trials involve drugs that are used in other diseases or medications that have been designed specifically for MS. Finally, there are also many basic investigations that try to understand better the disease and in the future may help to find new treatments.

Different alternative treatments are pursued by many patients, despite the paucity of supporting, comparable, replicated scientific study. Examples are dietary regimens,, herbal medicine, including the use of medical cannabis to help alleviate symptoms, or hyperbaric oxygenation.On the other hand the therapeutic practice of martial arts such as tai chi, relaxation disciplines such as yoga, or general exercise, seem to mitigate fatigue and improve quality of life.

The prognosis (the expected future course of the disease) for a person with multiple sclerosis depends on the subtype of the disease; the individual's sex, race, age, and initial symptoms; and the degree of disability the person experiences. The life expectancy of people with MS is now nearly the same as that of unaffected people. This is due mainly to improved methods of limiting disability, such as physical therapy, occupational therapy and speech therapy, along with more successful treatment of common complications of disability, such as pneumonia and urinary tract infections. Nevertheless half of the deaths in people with MS are directly related to the consequences of the disease, while 15% more are due to suicide.

Currently there are no clinically established laboratory investigations available that can predict prognosis or response to treatment. However, several promising approaches have been proposed. These include measurement of the two antibodies anti-myelin oligodendrocyte glycoprotein and anti-myelin basic protein, and measurement of TRAIL (TNF-related apoptosis-inducing ligand).

Epidemiology is, among other things, the study of prevalence and incidence of diseases, being incidence the percentage of cases reported each year and prevalence the total percentage of cases in the population. Prevalence is known to depend not only to incidence, but also to survival rate and migrations of affected people.

In northern Europe, continental North America, and Australasia, about one of every 1000 citizens suffers from multiple sclerosis, whereas in the Arabian peninsula, Asia, and continental South America, the frequency is much lower. In sub-Saharan Africa, MS is extremely rare. With important exceptions, there is a north-to-south gradient in the northern hemisphere and a south-to-north gradient in the southern hemisphere, with MS being much less common in people living near the equator. Climate, diet, geomagnetism, toxins, sunlight exposure, genetic factors, and infectious diseases have all been discussed as possible reasons for these regional differences. Environmental factors during childhood may play an important role in the development of MS later in life. This idea is based on several studies of migrants showing that if migration occurs before the age of fifteen, the migrant acquires the new region's susceptibility to MS. If migration takes place after age fifteen, the migrant keeps the susceptibility of his home country. However other works suggest that the age/geographical risk for developing multiple sclerosis spans a larger timescale than just the first 15 years of life.

MS occurs mainly in Caucasians. It is twentyfold lower in the Inuit people of Canada than in other Canadians living in the same region. It is also rare in the Native American tribes of North America, Australian Aborigines and the MÄori of New Zealand. Scotland appears to have the highest rate of MS in the world. The reasons for this are unknown. These few examples point out that either genetic background or lifestyle and cultural factors play an important role in the development of MS.

As observed in many autoimmune disorders, MS is more common in females than males; the mean sex ratio is about two females for every male. In children (who rarely develop MS) the sex ratio may reach three females for each male. In people over age fifty, MS affects males and females equally. Onset of symptoms usually occurs between fifteen and forty years of age, rarely before age fifteen or after age sixty.

As previously discussed, there is a genetic component to MS. On average one of every 25 siblings of individuals with MS will also develop MS. Almost half of the identical twins of MS-affected individuals will develop MS, but only one of twenty fraternal twins. If one parent is affected by MS, each child has a risk of only about one in forty of developing MS later in life.

Finally, it is important to remark that advances in the study of related diseases have shown that some cases formerly considered MS are not MS at all. In fact, all the studies before 2004 can be affected by the impossibility to distinguish MS and ''Devic's disease'' (NMO) reliably before this date. The error can be important in some areas, and is considered to be 30% in Japan.

The French neurologist Jean-Martin Charcot (1825â€?3) was the first person to recognize multiple sclerosis as a distinct, separate disease in 1868. Summarizing previous reports and adding his own important clinical and pathological observations, Charcot called the disease sclerose en plaques. The three signs of MS now known as Charcot's triad are dysarthria (problems with speech), ataxia (problems with coordination), and tremor. Charcot also observed cognition changes in MS since he described his patients as having a "marked enfeeblement of the memory" and "with conceptions that formed slowly".

Prior to Charcot, Robert Hooper (1773â€?835), a British pathologist and practicing physician, Robert Carswell (1793â€?857), a British professor of pathology, and Jean Cruveilhier (1791â€?873), a French professor of pathologic anatomy, had described and illustrated many of the disease's clinical details.

After this, several people, such as EugÃ¨ne Devic (1858â€?930), Jozsef Balo (1895â€?979), Paul Ferdinand Schilder (1886â€?940), and Otto Marburg (1874â€?948) found special cases of the disease that some authors consider different diseases and now are called the borderline forms of multiple sclerosis.

There are several historical accounts of people who probably had MS. Saint Lidwina of Schiedam (1380â€?433), a Dutch nun, may be one of the first identifiable MS patients. From the age of sixteen until her death at age 53, she suffered intermittent pain, weakness of the legs, and vision lossâ€”symptoms typical of MS. Almost a hundred years before there is a story from Iceland of a young woman called Halla. This girl suddenly lost her vision and capacity to talk; but after praying to the saints recovered them seven days after. Augustus Frederick d'Este (1794â€?848), an illegitimate grandson of King George III of Great Britain, almost certainly suffered from MS. D'Este left a detailed diary describing his 22 years living with the disease. He began his diary in 1822 and it had its last entry in 1846 (only to remain unknown until 1948). His symptoms began at age 28 with a sudden transient visual loss after the funeral of a friend. During the course of his disease he developed weakness of the legs, clumsiness of the hands, numbness, dizziness, bladder disturbances, and erectile dysfunction. In 1844, he began to use a wheelchair. Despite his illness, he kept an optimistic view of life. Another early account of MS was kept by the British diarist W. N. P. Barbellion, who maintained a detailed log of his diagnosis and struggle with MS. His diary was published in 1919 as The Journal of a Disappointed Man.















The John Bull is an English-built railroad steam locomotive, operated for the first time on September 15, 1831; it became the oldest operable steam locomotive in the world (150 years) when the Smithsonian Institution operated it in 1981. Built by Robert Stephenson and Company, the John Bull was initially purchased by and operated for the Camden and Amboy Railroad, the first railroad built in New Jersey. The railroad rostered it as locomotive number 1 and used it heavily from soon after the railroad's construction in 1833 until 1866 when it was removed from active service and placed in storage.

After the C&A's assets were acquired by the Pennsylvania Railroad (PRR) in 1871, the PRR refurbished and operated the locomotive a few times for public displays. The John Bull was steamed up for the Centennial Exposition in 1876 and again for the National Railway Appliance Exhibition in 1883. In 1884 the locomotive was purchased by the Smithsonian Institution as the museum's first major industrial exhibit.

In 1939 the employees at the PRR's Altoona, Pennsylvania, shops built an operable replica of the locomotive for further exhibition duties as the Smithsonian desired to keep the original locomotive in a more controlled environment. The Smithsonian commemorated the locomotive's 150th birthday in grand style. The locomotive became the world's oldest surviving operable steam locomotive when it ran again under its own power in 1981. Today, the original John Bull is on static display in the Smithsonian's National Museum of American History in Washington, DC, and the replica John Bull operates regularly at the Railroad Museum of Pennsylvania.

The John Bull was built in Newcastle, England by Robert Stephenson and Company for the Camden and Amboy Railroad (C&A), the first railroad built in New Jersey. It was dismantled and then shipped across the Atlantic Ocean in crates aboard the Allegheny. C&A engineer Isaac Dripps reconstructed the locomotive to the best of his knowledge (the shipment did not include any drawings or instructions to assemble the locomotive) and ran it for the first time in September 1831. On November 12 1831, Robert Stevens (then president of the C&A) repaid some political debts by inviting several members of the New Jersey legislature and some local dignitaries, including Napoleon's nephew Prince Murat, for rides behind the newly delivered locomotive over a short test track. The prince's wife made a point of hurrying onto the train so she could be declared the first woman to ride a steam-powered train in America.

Until the railroad construction was completed, the locomotive was placed in storage; horse-drawn cars served the construction efforts until 1833. The C&A applied both numbers and names to their first locomotives, giving this engine the number 1 and officially naming it Stevens (after the C&A's first president, Robert L. Stevens). However, through regular use of the engine, crews began calling it the old John Bull, a reference to the cartoon personification of England, John Bull. Eventually the informal name was shortened to John Bull and this name was much more widely used until the Stevens name fell out of use in favor of John Bull.

Stephenson built the locomotive originally as an 0-4-0 (an 0-4-0 is the Whyte notation for a steam locomotive with two powered axles and no unpowered leading or trailing axles). The locomotive's power was transmitted to the driving axles through pistons that were mounted under the boiler between the two front wheels and in front of the front axle. These inside cylinders' main rods were connected to a rear crank axle with a connecting rod between the two axles to power the front axle.

Due to poorer quality track than was the norm in its native England, the locomotive had much trouble with derailment; the C&A's engineers added a leading truck (an assembly consisting of an unpowered axle with smaller diameter wheels that was connected to the frame and pushed in front of the locomotive) to help guide the engine into curves. The leading truck's mechanism necessitated the removal of the connecting rod between the two main axles, leaving only the rear axle powered. Effectively, the John Bull became a 4-2-0 (a locomotive with two unpowered leading axles followed by one powered axle and no unpowered trailing axles). Later, the C&A also added a cowcatcher to the lead truck. The cowcatcher is an angled assembly designed to deflect animals and debris off of the railroad track in front of the locomotive. To protect the locomotive's crew from the weather, the C&A also added walls and a roof (a cab) to the rear of the locomotive where the controls were located. C&A shop crews also added safety features such as a bell and headlight.

After several years serving as a switching engine (a locomotive used for moving railroad cars around within a railroad yard; also known as a shunter) and stationary boiler, the John Bull was retired in 1866 and stored in Bordentown, NJ. Toward the end of its life in revenue service, the locomotive worked as a pump engine and as the power for a sawmill. 

The C&A was soon absorbed into the United New Jersey Railroad and Canals Company (1869) which itself was merged into the Pennsylvania Railroad (PRR) in 1871. The PRR saw the potential publicity to be gained by exhibiting such an old engine and PRR shop forces "back-dated" the engine (by replacing some original parts with parts that "looked" old or by removing them entirely). The exhaust stack was replaced with a straight tube of metal and the cab walls and roof were removed. The PRR then exhibited the engine in 1883 at the National Railway Appliance Exhibition in Chicago, IL. The following year, the Smithsonian Institution purchased John Bull from the PRR as the Institution's first large engine purchase.

At the exhibition in 1883, the Pennsylvania Railroad ended up resolving two problems at once. In the Smithsonian Institution, the railroad was able to find a home for the historic locomotive as well as a suitable new employer for a young civil engineer named J. Elfreth Watkins. Watkins had been involved in an accident on the railroad in New Jersey a few years before the exhibition. In the accident, he had lost a leg so he was no longer suited to the physical demands of railroad work (although the railroad did employ him as a clerk for a while after his accident). The PRR employed his engineering experience as an expert curator for the Smithsonian's new Arts and Industries Building which was opened in 1880. The locomotive's first public exhibition at the Smithsonian occurred on December 22 1884, where it was displayed in the East Hall of the Arts and Industries building. 

The locomotive remained on display in this location for nearly 80 years, but it was transported for display outside the museum on certain rare occasions. The most significant display in this time occurred in 1893 when the locomotive traveled to Chicago for the World's Columbian Exposition. The Pennsylvania Railroad, like many other railroads of the time, put on grand displays of their progress; the PRR arranged for the locomotive and a couple of coaches to be delivered to the railroad's Jersey City, New Jersey, shops where it would undergo a restoration of sorts to operating condition. The PRR was planning an event worthy of the locomotive's significance to American railroad historyâ€”the railroad actually planned to operate the locomotive for the entire distance between New Jersey and Chicago.

The restoration was supervised by the PRR's chief mechanical officer, Theodore N. Ely. Ely was confident enough in its 50-mile (80.5 km) test run to Perth Amboy, New Jersey (which took two hours and fifteen minutes), that the Governors of all the states that the locomotive was to pass through and then President of the United States Grover Cleveland were invited to ride behind the engine on its first leg toward Chicago. The John Bull was to pull a few passenger cars in a train that would carry dignitaries and representatives of the press. The train traveled to Philadelphia, Pennsylvania, under one crew; starting in Philadelphia, local engineers (train drivers) were employed to ride on the locomotive's footplate as pilots to advise the operators for the trip over the local engineers' territories for the rest of the journey to Chicago. Traveling at 25 to 30 mph (40 to 50 km/h), the train reached Chicago on April 22. The locomotive operated during the exhibition giving rides to the exhibition's attendees, and then the train left Chicago on December 6 for the return trip to Washington. The locomotive arrived back in Washington on December 13.

In 1927 the John Bull again traveled outside the museum. The Baltimore and Ohio Railroad was celebrating its centenary that year in its Fair of the Iron Horse in Baltimore, Maryland.Since the locomotive's original tender (fuel and water car) had deteriorated beyond repair and was dismantled in 1910, the PRR built a replica of the tender at its Altoona, Pennsylvania, shops. The locomotive was also refurbished in Altoona for operation during the fair. This fair was the last steam up for the locomotive until 1980.

After the locomotive returned to the Smithsonian, it remained on static display. The museum in 1930 commissioned the Altoona shops to build a second replica of the locomotive's tender for display with the locomotive in the museum. This time, however, the replica tender used some of the original tender's fittings that the museum still had from the tender's dismantlement twenty years before.

The Smithsonian recognized the locomotive's age in 1931. But, since the museum didn't have the funds to refurbish the locomotive for full operation again, it was decided to run the locomotive in place (with the driving wheels lifted off the rails using jacks) with compressed air. The museum borrowed an 1836 coach from the Pennsylvania Railroad to display on the track behind the newly rebuilt tender, and the locomotive's 100th birthday was officially celebrated on November 12 1931. The locomotive's semi-operation was broadcast over the CBS radio network with Stanley Bell narrating the ceremonies for the radio audience.

The PRR again borrowed the locomotive in 1939 for the Century of Progress exhibition in Chicago. Unlike its earlier jaunt to Chicago, for this trip, the railroad hauled and displayed it as a static exhibit. While this exhibit was proceeding the Altoona shops were busy again building a replica; this time the replica was an operable copy of the locomotive itself. The replica was then operated in 1940 at the New York World's Fair, while the original locomotive and rebuilt tender returned to the Smithsonian.

The original locomotive was displayed outside the museum one more time in 1939, but the museum's curators decided that the locomotive was becoming too fragile for repeated outside exhibits. It was then placed in somewhat permanent display back in the East Hall where it remained for the next 25 years. The locomotive was moved to its current home, the National Museum of American History (then called the National Museum of History and Technology), in 1964.

The John Bull had remained on static display for another 15 years, but the locomotive's significance as one of the oldest locomotives in existence, or its use on the first railroad in New Jersey was not very plainly noted in the display's literature. As 1981 and the locomotive's 150th birthday approached, the Smithsonian started discussions on how best to commemorate the locomotive's age and significance. There was very little question that special publications and exhibits would be prepared, but museum officials were left with the thought that the exhibit could still be so much more than that.

Many superficial inspections were performed on the locomotive in 1980 and it was found to be in relatively sound mechanical condition. There wasn't a significant amount of deterioration noted in these early inspections, and when the wheels were jacked off the rails, as they had been 50 years earlier, the axles were found to be freely operable. One morning in January 1980, before the museum opened to the public, museum officials used compressed air to power the cylinders and move the wheels through the connecting rods for the first time since its last semi-operation. After the compressed air blew some dirt and debris out of the locomotive's exhaust stack, it was soon running smoothly.

The running gear seemed to be in good order, but it was still unknown if the boiler could still handle the pressure of steam and a live fire again. The museum asked the Hartford Steam Boiler Inspection and Insurance Company to inspect the locomotive's boiler for operation. The inspections were conducted after hours at the museum (from 6:30 p.m. to 4:00 a.m.) over three days and included electromagnetic, ultrasonic and radiographic tests. The tests did reveal a few flaws, but it was projected that the engine could operate at a reduced boiler pressure of 50 psi (340 kPa or 3.5 kgf/cmÂ²); as delivered to the Camden & Amboy, the boiler was rated for 70 psi (480 kPa or 4.9 kgf/cmÂ²). The Smithsonian's staff, after a few further hydrostatic tests, were confident that the locomotive could again operate under its own power.

The items that needed repair were repaired and the locomotive operated under steam on September 15 1981, outside Washington, D.C.. On this exhibition, the locomotive became the oldest operable steam locomotive in the world.

The original John Bull is currently housed and displayed at the National Museum of American History in Washington, D.C.. The replica of the John Bull, built in 1939, is currently owned and operated by the Railroad Museum of Pennsylvania











This is a list of particles in particle physics, including currently known and hypothetical elementary particles, as well as the composite particles that can be built up from them.

Elementary particles are particles with no measurable internal structure; that is, they are not composed of other particles. They are the fundamental objects of quantum field theory. Elementary particles can be classified according to their spin, with fermions having half-integer spin and bosons integer spin.

The Standard Model of particle physics is the current understanding of the physics of elementary particles. All Standard Model particles except the Higgs boson have been observed.

Fermions have half-integer spin; for all known elementary fermions this is Â½. Each fermion has its own distinct antiparticle. Fermions are the basic building blocks of all matter. They are classified according to whether they interact via the color force or not. In the Standard Model, there are 12 types of elementary fermions: six quarks and six leptons.<onlyinclude>

Quarks interact via the color force. Their respective antiparticles are known as antiquarks. Quarks exist in six flavors:

Leptons do not interact via the color force. Their respective antiparticles are known as antileptons. (The antiparticle of the electron is called the positron for historical reasons.) There are six leptons, listed here with its corresponding antiparticle:-

Bosons have whole number spins. The fundamental forces of nature are mediated by gauge bosons, and mass is hypothesized to be created by the Higgs boson. According to the Standard Model (and to both linearized general relativity and string theory, in the case of the graviton) the elementary bosons are:

Supersymmetric theories predict the existence of more particles, none of which have been confirmed experimentally as of 2008:

Other theories predict the existence of additional bosons:

Mirror particles are predicted by theories that restore Parity symmetry.

Magnetic monopole is a generic name for particles with non-zero magnetic charge. They are predicted by some GUT theories.

Tachyon is a generic name for hypothetical particles that travel faster than the speed of light and have an imaginary rest mass.

The preon was a suggested substructure for both quarks and leptons, but modern collider experiments have all but disproven their existence.

Hadrons are defined as strongly interacting composite particles. Hadrons are either:

Quark models, first proposed in 1964 independently by Murray Gell-Mann and George Zweig (who called quarks "aces"), describe the known Hadrons as composed of valence quarks and/or antiquarks, tightly bound by the color force, which is mediated by gluons. A "sea" of virtual quark-antiquark pairs is also present in each Hadron. 

Notice that mesons are composite bosons, but not composed of bosons. All Hadrons, including mesons, are composed of quarks (which are fermions).



Ordinary baryons (composite fermions) contain three valence quarks or three valence antiquarks each.

Some hints at the existence of exotic baryons have been found recently; however, negative results have also been reported. Their existence is uncertain.



Ordinary mesons (composite bosons) contain a valence quark and a valence antiquark, and include the pion, kaon, the J/Ïˆ, and many other types of mesons. In quantum hadrodynamic models, the strong force between nucleons is mediated by mesons.

Exotic mesons may also exist. Positive signatures have been reported for all of these particles at some time, but their existence is still somewhat uncertain.

Atomic nuclei consist of protons and neutrons. Each type of nucleus contains a specific number of protons and a specific number of neutrons, and is called a nuclide or isotope. Nuclear reactions can change one nuclide into another. See table of nuclides for a complete list of isotopes.

Atoms are the smallest neutral particles into which matter can be divided by chemical reactions. An atom consists of a small, heavy nucleus surrounded by a relatively large, light cloud of electrons. Each type of atom corresponds to a specific chemical element.Total 118 elements(except element 117) have been discovered till date of which 111 have been officially named. Refer to the periodic table for an overview.Atoms consist of protons and neutrons within the nucleus. Within these particles, there are smaller particles still which are then made up of even smaller particles still.

Molecules are the smallest particles into which a non-elemental substance can be divided while maintaining the physical properties of the substance. Each type of molecule corresponds to a specific chemical compound. Molecules are composites of one or more atoms. See list of compounds for a list of molecules.

The field equations of condensed matter physics are remarkably similar to those of high energy particle physics. As a result, much of the theory of particle physics applies to condensed matter physics as well; in particular, there are a selection of field excitations, called quasi-particles, that can be created and explored. These include:











 Film is a term that encompasses individual motion pictures, the field of film as an art form, and the motion picture industry. Films are produced by recording images from the world with cameras, or by creating images using animation techniques or special effects. 

Films are cultural artifacts created by specific cultures, which reflect those cultures, and, in turn, affect them. Film is considered to be an important art form, a source of popular entertainment, and a powerful method for educating â€?or indoctrinating â€?citizens. The visual elements of cinema give motion pictures a universal power of communication; some movies have become popular worldwide attractions by using dubbing or subtitles that translate the dialogue. 

Traditional films are made up of a series of individual images called frames. When these images are shown rapidly in succession, a viewer has the illusion that motion is occurring. The viewer cannot see the flickering between frames due to an effect known as persistence of vision, whereby the eye retains a visual image for a fraction of a second after the source has been removed. Viewers perceive motion due to a psychological effect called beta movement.

The origin of the name "film" comes from the fact that photographic film (also called film stock) had historically been the primary medium for recording and displaying motion pictures. Many other terms exist for an individual motion picture, including picture, picture show, photo-play, flick, and most commonly, movie. Additional terms for the field in general include the big screen, the silver screen, the cinema, and the movies.

Mechanisms for producing artificially created, two-dimensional images in motion were demonstrated as early as the 1860s, with devices such as the zoetrope and the praxinoscope. These machines were outgrowths of simple optical devices (such as magic lanterns) and would display sequences of still pictures at sufficient speed for the images on the pictures to appear to be moving, a phenomenon called persistence of vision. Naturally, the images needed to be carefully designed to achieve the desired effect â€?and the underlying principle became the basis for the development of film animation.



With the development of celluloid film for still photography, it became possible to directly capture objects in motion in real time. Early versions of the technology sometimes required a person to look into a viewing machine to see the pictures which were separate paper prints attached to a drum turned by a handcrank. The pictures were shown at a variable speed of about 5 to 10 pictures per second depending on how rapidly the crank was turned. Some of these machines were coin operated. By the 1880s, the development of the motion picture camera allowed the individual component images to be captured and stored on a single reel, and led quickly to the development of a motion picture projector to shine light through the processed and printed film and magnify these "moving picture shows" onto a screen for an entire audience. These reels, so exhibited, came to be known as "motion pictures." Early motion pictures were static shots that showed an event or action with no editing or other cinematic techniques.

Ignoring Dickson's early sound experiments (1894), commercial motion pictures were purely visual art through the late 19th century, but these innovative silent films had gained a hold on the public imagination. Around the turn of the twentieth century, films began developing a narrative structure by stringing scenes together to tell narratives. The scenes were later broken up into multiple shots of varying sizes and angles. Other techniques such as camera movement were realized as effective ways to portray a story on film. Rather than leave the audience in silence, theater owners would hire a pianist or organist or a full orchestra to play music fitting the mood of the film at any given moment. By the early 1920s, most films came with a prepared list of sheet music for this purpose, with complete film scores being composed for major productions.

The rise of European cinema was interrupted by the breakout of World War I while the film industry in United States flourished with the rise of Hollywood. However in the 1920s, European filmmakers such as Sergei Eisenstein, F. W. Murnau, and Fritz Lang, along with American innovator D. W. Griffith and the contributions of Charles Chaplin, Buster Keaton and others, continued to advance the medium. In the 1920s, new technology allowed filmmakers to attach to each film a soundtrack of speech, music and sound effects synchronized with the action on the screen. These sound films were initially distinguished by calling them "talking pictures", or talkies.

The next major step in the development of cinema was the introduction of so-called "natural" color. While the addition of sound quickly eclipsed silent film and theater musicians, color was adopted more gradually as methods evolved making it more practical and cost effective to produce "natural color" films. The public was relatively indifferent to color photography as opposed to black-and-white, but as color processes improved and became as affordable as black-and-white film, more and more movies were filmed in color after the end of World War II, as the industry in America came to view color as essential to attracting audiences in its competition with television, which remained a black-and-white medium until the mid-1960s. By the end of the 1960s, color had become the norm for film makers.

Since the decline of the studio system in the 1960s, the succeeding decades saw changes in the production and style of film. New Hollywood, French New Wave and the rise of film school educated independent filmmakers were all part of the changes the medium experienced in the latter half of the 20th century. Digital technology has been the driving force in change throughout the 1990s and into the 21st century.

 

Film theory seeks to develop concise, systematic concepts that apply to the study of film as art. It was started by Ricciotto Canudo's The Birth of the Sixth Art. Formalist film theory, led by Rudolf Arnheim, BÃ©la BalÃ¡zs, and Siegfried Kracauer, emphasized how film differed from reality, and thus could be considered a valid fine art. AndrÃ© Bazin reacted against this theory by arguing that film's artistic essence lay in its ability to mechanically reproduce reality not in its differences from reality, and this gave rise to realist theory. More recent analysis spurred by Lacan's psychoanalysis and Ferdinand de Saussure's semiotics among other things has given rise to psychoanalytical film theory, structuralist film theory, feminist film theory and others.



Film criticism is the analysis and evaluation of films. In general, these works can be divided into two categories: academic criticism by film scholars and journalistic film criticism that appears regularly in newspapers and other media.

Film critics working for newspapers, magazines, and broadcast media mainly review new releases. Normally they only see any given film once and have only a day or two to formulate opinions. Despite this, critics have an important impact on films, especially those of certain genres. Mass marketed action, horror, and comedy films tend not to be greatly affected by a critic's overall judgment of a film. The plot summary and description of a film that makes up the majority of any film review can still have an important impact on whether people decide to see a film. For prestige films such as most dramas, the influence of reviews is extremely important. Poor reviews will often doom a film to obscurity and financial loss.

The impact of a reviewer on a given film's box office performance is a matter of debate. Some claim that movie marketing is now so intense and well financed that reviewers cannot make an impact against it. However, the cataclysmic failure of some heavily-promoted movies which were harshly reviewed, as well as the unexpected success of critically praised independent movies indicates that extreme critical reactions can have considerable influence. Others note that positive film reviews have been shown to spark interest in little-known films. Conversely, there have been several films in which film companies have so little confidence that they refuse to give reviewers an advanced viewing to avoid widespread panning of the film. However, this usually backfires as reviewers are wise to the tactic and warn the public that the film may not be worth seeing and the films often do poorly as a result.

It is argued that journalist film critics should only be known as film reviewers, and true film critics are those who take a more academic approach to films. This line of work is more often known as film theory or film studies. These film critics attempt to come to understand how film and filming techniques work, and what effect they have on people. Rather than having their works published in newspapers or appear on television, their articles are published in scholarly journals, or sometimes in up-market magazines. They also tend to be affiliated with colleges or universities.



The making and showing of motion pictures became a source of profit almost as soon as the process was invented. Upon seeing how successful their new invention, and its product, was in their native France, the LumiÃ¨res quickly set about touring the Continent to exhibit the first films privately to royalty and publicly to the masses. In each country, they would normally add new, local scenes to their catalogue and, quickly enough, found local entrepreneurs in the various countries of Europe to buy their equipment and photograph, export, import and screen additional product commercially. The Oberammergau Passion Play of 1898  was the first commercial motion picture ever produced. Other pictures soon followed, and motion pictures became a separate industry that overshadowed the vaudeville world. Dedicated theaters and companies formed specifically to produce and distribute films, while motion picture actors became major celebrities and commanded huge fees for their performances. Already by 1917, Charlie Chaplin had a contract that called for an annual salary of one million dollars.

In the United States today, much of the film industry is centered around Hollywood. Other regional centers exist in many parts of the world, such as Mumbai-centered Bollywood, the Indian film industry's Hindi cinema which produces the largest number of films in the world. Whether the ten thousand-plus feature length films a year produced by the Valley pornographic film industry should qualify for this title is the source of some debate. Though the expense involved in making movies has led cinema production to concentrate under the auspices of movie studios, recent advances in affordable film making equipment have allowed independent film productions to flourish.

Profit is a key force in the industry, due to the costly and risky nature of filmmaking; many films have large cost overruns, a notorious example being Kevin Costner's Waterworld. Yet many filmmakers strive to create works of lasting social significance. The Academy Awards (also known as "the Oscars") are the most prominent film awards in the United States, providing recognition each year to films, ostensibly based on their artistic merits. 

There is also a large industry for educational and instructional films made in lieu of or in addition to lectures and texts.



The nature of the film determines the size and type of crew required during filmmaking. Many Hollywood adventure films need computer generated imagery (CGI), created by dozens of 3D modellers, animators, rotoscopers and compositors. However, a low-budget, independent film may be made with a skeleton crew, often paid very little. Also, an open source film may be produced through open, collaborative processes. Filmmaking takes place all over the world using different technologies, styles of acting and genre, and is produced in a variety of economic contexts that range from state-sponsored documentary in China to profit-oriented movie making within the American studio system.

A typical Hollywood-style filmmaking Production cycle is comprised of five main stages: 

This production cycle typically takes three years. The first year is taken up with development. The second year comprises preproduction and production. The third year, post-production and distribution.



A film crew is a group of people hired by a film company, employed during the "production" or "photography" phase, for the purpose of producing a film or motion picture. Crew are distinguished from cast, the actors who appear in front of the camera or provide voices for characters in the film. The crew interacts with but is also distinct from the production staff, consisting of producers, managers, company representatives, their assistants, and those whose primary responsibility falls in pre-production or post-production phases, such as writers and editors. Communication between production and crew generally passes through the director and his/her staff of assistants. Medium-to-large crews are generally divided into departments with well defined hierarchies and standards for interaction and cooperation between the departments. Other than acting, the crew handles everything in the photography phase: props and costumes, shooting, sound, electrics (i.e., lights), sets, and production special effects. Caterers (known in the film industry as "craft services") are usually not considered part of the crew.





Independent filmmaking often takes place outside of Hollywood, or other major studio systems. An independent film (or indie film) is a film initially produced without financing or distribution from a major movie studio. Creative, business, and technological reasons have all contributed to the growth of the indie film scene in the late 20th and early 21st century.

On the business side, the costs of big-budget studio films also leads to conservative choices in cast and crew. There is a trend in Hollywood towards co-financing (over two-thirds of the films put out by Warner Bros. in 2000 were joint ventures, up from 10% in 1987). A hopeful director is almost never given the opportunity to get a job on a big-budget studio film unless he or she has significant industry experience in film or television. Also, the studios rarely produce films with unknown actors, particularly in lead roles.

Before the advent of digital alternatives, the cost of professional film equipment and stock was also a hurdle to being able to produce, direct, or star in a traditional studio film. The cost of 35Â mm film is outpacing inflation: in 2002 alone, film negative costs were up 23%, according to Variety..

But the advent of consumer camcorders in 1985, and more importantly, the arrival of high-resolution digital video in the early 1990s, have lowered the technology barrier to movie production significantly. Both production and post-production costs have been significantly lowered; today, the hardware and software for post-production can be installed in a commodity-based personal computer. Technologies such as DVDs, FireWire connections and non-linear editing system pro-level software like Adobe Premiere Pro, Sony Vegas and Apple's Final Cut Pro, and consumer level software such as Apple's Final Cut Express and iMovie make movie-making relatively inexpensive.

Since the introduction of DV technology, the means of production have become more democratized. Filmmakers can conceivably shoot and edit a movie, create and edit the sound and music, and mix the final cut on a home computer. However, while the means of production may be democratized, financing, distribution, and marketing remain difficult to accomplish outside the traditional system. Most independent filmmakers rely on film festivals to get their films noticed and sold for distribution. The arrival of internet-based video outlets such as YouTube and Veoh has further changed the film making landscape in ways that are still to be determined.



An open content film is much like an independent film, but it is produced through open collaborations; its source material is available under a license which is permissive enough to allow other parties to create fan fiction or derivative works, than a traditional copyright. Like independent filmmaking, open source filmmaking takes place outside of Hollywood, or other major studio systems.



A fan film is a film or video inspired by a film, television program, comic book or a similar source, created by fans rather than by the source's copyright holders or creators. Fan filmmakers have traditionally been amateurs, but some of the more notable films have actually been produced by professional filmmakers as film school class projects or as demonstration reels. Fan films vary tremendously in length, from short faux-teaser trailers for non-existent motion pictures to rarer full-length motion pictures.



Animation is the technique in which each frame of a film is produced individually, whether generated as a computer graphic, or by photographing a drawn image, or by repeatedly making small changes to a model unit (see claymation and stop motion), and then photographing the result with a special animation camera. When the frames are strung together and the resulting film is viewed at a speed of 16 or more frames per second, there is an illusion of continuous movement (due to the persistence of vision). Generating such a film is very labour intensive and tedious, though the development of computer animation has greatly sped up the process.

File formats like GIF, QuickTime, Shockwave and Flash allow animation to be viewed on a computer or over the Internet.

Because animation is very time-consuming and often very expensive to produce, the majority of animation for TV and movies comes from professional animation studios. However, the field of independent animation has existed at least since the 1950s, with animation being produced by independent studios (and sometimes by a single person). Several independent animation producers have gone on to enter the professional animation industry.

Limited animation is a way of increasing production and decreasing costs of animation by using "short cuts" in the animation process. This method was pioneered by UPA and popularized by Hanna-Barbera, and adapted by other studios as cartoons moved from movie theaters to television.

Although most animation studios are now using digital technologies in their productions, there is a specific style of animation that depends on film. Cameraless animation, made famous by moviemakers like Norman McLaren, Len Lye and Stan Brakhage, is painted and drawn directly onto pieces of film, and then run through a projector.

When it is initially produced, a feature film is often shown to audiences in a movie theater or cinema. The first theater designed exclusively for cinema opened in Pittsburgh, Pennsylvania in 1905. Thousands of such theaters were built or converted from existing facilities within a few years. In the United States, these theaters came to be known as nickelodeons, because admission typically cost a nickel (five cents).

Typically, one film is the featured presentation (or feature film). Before the 1970s, there were "double features"; typically, a high quality "A picture" rented by an independent theater for a lump sum, and a "B picture" of lower quality rented for a percentage of the gross receipts. Today, the bulk of the material shown before the feature film consists of previews for upcoming movies and paid advertisements (also known as trailers or "The Twenty").

Historically, all mass marketed feature films were made to be shown in movie theaters. The development of television has allowed films to be broadcast to larger audiences, usually after the film is no longer being shown in theaters. Recording technology has also enabled consumers to rent or buy copies of films on VHS or DVD (and the older formats of laserdisc, VCD and SelectaVision â€?see also videodisc), and Internet downloads may be available and have started to become revenue sources for the film companies. Some films are now made specifically for these other venues, being released as made-for-TV movies or direct-to-video movies. The production values on these films are often considered to be of inferior quality compared to theatrical releases in similar genres, and indeed, some films that are rejected by their own studios upon completion are distributed through these markets.

The movie theater pays an average of about 50-55% of its ticket sales to the movie studio, as film rental fees. The actual percentage starts with a number higher than that, and decreases as the duration of a film's showing continues, as an incentive to theaters to keep movies in the theater longer. However, today's barrage of highly marketed movies ensures that most movies are shown in first-run theaters for less than 8 weeks. There are a few movies every year that defy this rule, often limited-release movies that start in only a few theaters and actually grow their theater count through good word-of-mouth and reviews. According to a 2000 study by ABN AMRO, about 26% of Hollywood movie studios' worldwide income came from box office ticket sales; 46% came from VHS and DVD sales to consumers; and 28% came from television (broadcast, cable, and pay-per-view).

Film stock consists of transparent celluloid, acetate, or polyester base coated with an emulsion containing light-sensitive chemicals. Cellulose nitrate was the first type of film base used to record motion pictures, but due to its flammability was eventually replaced by safer materials. Stock widths and the film format for images on the reel have had a rich history, though most large commercial films are still shot on (and distributed to theaters) as 35Â mm prints.

Originally moving picture film was shot and projected at various speeds using hand-cranked cameras and projectors; though 1000 frames per minute (16â…?per second) is generally cited as a standard silent speed, research indicates most films were shot between 16 and 23 fps and projected from 18 fps on up (often reels included instructions on how fast each scene should be shown) . When sound film was introduced in the late 1920s, a constant speed was required for the sound head. 24 frames per second was chosen because it was the slowest (and thus cheapest) speed which allowed for sufficient sound quality. Improvements since the late 19th century include the mechanization of cameras â€?allowing them to record at a consistent speed, quiet camera design â€?allowing sound recorded on-set to be usable without requiring large "blimps" to encase the camera, the invention of more sophisticated filmstocks and lenses, allowing directors to film in increasingly dim conditions, and the development of synchronized sound, allowing sound to be recorded at exactly the same speed as its corresponding action. The soundtrack can be recorded separately from shooting the film, but for live-action pictures many parts of the soundtrack are usually recorded simultaneously.

As a medium, film is not limited to motion pictures, since the technology developed as the basis for photography. It can be used to present a progressive sequence of still images in the form of a slideshow. Film has also been incorporated into multimedia presentations, and often has importance as primary historical documentation. However, historic films have problems in terms of preservation and storage, and the motion picture industry is exploring many alternatives. Most movies on cellulose nitrate base have been copied onto modern safety films. Some studios save color films through the use of separation masters â€?three B&W negatives each exposed through red, green, or blue filters (essentially a reverse of the Technicolor process). Digital methods have also been used to restore films, although their continued obsolescence cycle makes them (as of 2006) a poor choice for long-term preservation. Film preservation of decaying film stock is a matter of concern to both film historians and archivists, and to companies interested in preserving their existing products in order to make them available to future generations (and thereby increase revenue). Preservation is generally a higher-concern for nitrate and single-strip color films, due to their high decay rates; black and white films on safety bases and color films preserved on Technicolor imbibition prints tend to keep up much better, assuming proper handling and storage.

Some films in recent decades have been recorded using analog video technology similar to that used in television production. Modern digital video cameras and digital projectors are gaining ground as well. These approaches are extremely beneficial to moviemakers, especially because footage can be evaluated and edited without waiting for the film stock to be processed. Yet the migration is gradual, and as of 2005 most major motion pictures are still recorded on film.

While motion picture films have been around for more than a century, film is still a relative newcomer in the pantheon of fine arts. In the 1950s, when television became widely available, industry analysts predicted the demise of local movie theaters. Despite competition from television's increasing technological sophistication over the 1960s and 1970s, such as the development of color television and large screens, motion picture cinemas continued. In the 1980s, when the widespread availability of inexpensive videocassette recorders enabled people to select films for home viewing, industry analysts again wrongly predicted the death of the local cinemas. 

In the 1990s and 2000s the development of digital DVD players, home theater amplification systems with surround sound and subwoofers, and large LCD or plasma screens enabled people to select and view films at home with greatly improved audio and visual reproduction. These new technologies provided audio and visual that in the past only local cinemas had been able to provide: a large, clear widescreen presentation of a film with a full-range, high-quality multi-speaker sound system. Once again industry analysts predicted the demise of the local cinema. Local cinemas will be changing in the 2000s and moving towards digital screens, a new approach which will allow for easier and quicker distribution of films (via satellite or hard disks), a development which may give local theaters a reprieve from their predicted demise.















The economy of the Iroquois originally focused on communal production and combined elements of both horticulture and hunter-gatherer systems. The tribes of the Iroquois Confederacy and other Northern Iroquoian-speaking peoples, including the Huron, lived in the region including what is now New York State and the Great Lakes area. The Iroquois Confederacy was composed of five different tribes â€?a sixth was added later â€?who had banded together shortly before European contact. While not Iroquois, the Huron peoples fell into the same linguistic group and shared an economy similar to the Iroquois. The Iroquois peoples were predominantly agricultural, harvesting the "Three Sisters" commonly grown by Native American groups: maize, beans, and squash. They developed certain cultural customs related to their lifestyle. Among these developments were ideas concerning the nature and management of property.

The Iroquois developed a system of economics very different from the now dominant Western variety. This system was characterized by such components as communal land ownership, division of labor by gender, and trade mostly based on gift economics. 

Contact with Europeans in the early 1600s had a profound impact on the economy of the Iroquois. At first, they became important trading partners, but the expansion of European settlement upset the balance of the Iroquois economy. By 1800 the Iroquois had been confined to reservations, and they had to adapt their traditional economic system. In the 20th century, some of the Iroquois groups took advantage of their independent status on the reservation and started Indian casinos. Other Iroquois have incorporated themselves directly into the outside economies off the reservation.

The Huron had an essentially communal system of land ownership. The French Catholic missionary Gabriel Sagard described the fundamentals. The Huron had "as much land as they need[ed]." As a result the Huron could give families their own land and still have a large amount of excess land owned communally. Any Huron was free to clear the land and farm. He maintained possession of the land as long as he continued to actively cultivate and tend the fields. Once he abandoned the land, it reverted to communal ownership, and anyone could take it up for themselves. While the Huron did seem to have lands designated for the individual, the significance of this possession may be of little relevance; the placement of corn storage vessels in the longhouses, which contained multiple families in one kinship group, suggests the occupants of a given longhouse held all production in common.

The Iroquois had a similar communal system of land distribution. The tribe owned all lands but gave out tracts to the different clans for further distribution among households for cultivation. The land would be redistributed among the households every few years, and a clan could request a redistribution of tracts when the Clan Mothers' Council gathered. Those clans that abused their allocated land or otherwise did not take care of it would be warned and eventually punished by the Clan Mothers' Council by having the land redistributed to another clan. Land property was really only the concern of the women, since it was the women's job to cultivate food and not the men's.

The Clan Mothers' Council also reserved certain areas of land to be worked by the women of all the different clans. Food from such lands, called kÄ›ndiÇ”"gwÇŽ'ge' hodi'yÄ›n'tho, would be used at festivals and large council gatherings.

The division of labor reflected the dualistic split common in Iroquois culture: The twin gods Sapling (East) and Flint (West) embodied the dualistic notion of two complementary halves. Dualism was applied to labor with each gender taking a clearly defined role that complemented the work of the other. Women did all work involving the field while men did all work involving the forest including the manufacture of anything involving wood. The Iroquois men were responsible for hunting, trading, and fighting, while the women took care of farming, food gathering, and housekeeping. This gendered division of labor was the predominant means of dividing work in Iroquois society. At the time of contact with Europeans, Iroquois women produced about 65% of the goods and the men 35%. The combined production of food made famine and hunger extremely rareâ€”early Europeans settlers often envied the success of Iroquois food production.

The Iroquois system of work matched their system of land ownership. Since the Iroquois owned property together, they worked together as well. The women performed difficult work in large groups, going from field to field helping one another work each others' land. Together they would sow the fields as a "mistress of the field" distributed a set amount of seeds to each of the women. The Iroquois women of each agricultural group would select an old but active member of their group to act as their leader for that year and agree to follow her directions. The women performed other work cooperatively as well. The women would cut their own wood, but their leader would oversee the collective carrying of the wood back to the village. The women's clans performed other work, and according to Mary Jemison, a white woman assimilated as an Indian, the collective effort averted "every jealously of one having done more or less work than another." 

The Iroquois men also organized themselves in a cooperative fashion. Of course, the men acted collectively during military actions, as there is little sense in a single individual fighting entirely alone in battle. The other jobs of men, such as hunting and fishing, also involved cooperative elements similar to women's cooperation. However, the men differed from the women in that they more often organized as a whole village rather than as a clan. The men organized hunting parties where they used extensive cooperation to kill a large amount of game. One first hand account told of a large hunting party that built a large brush fence in a forest forming a V. The hunters burned the forest from the open side of the V, forcing the animals to run towards the point where the village's hunters waited in an opening. A hundred deer could be killed at a time under such a plan.

The men also fished in large groups. Extensive fishing expeditions often took place where men in canoes with weirs and nets covered entire streams to reap large amounts of fish, sometimes a thousand in half of a day. A hunting or fishing party's takings were considered common property and would be divided among the party by the leader or taken to the village for a feast. Hunting and fishing were not always cooperative efforts, but the Iroquois generally did better in parties than as individuals.

The cooperative production and communal distribution of goods made internal trade within the Iroquois Confederacy pointless, but external trade with tribes in regions with resources the Iroquois lacked served a purpose. The Iroquois traded excess corn and tobacco for the pelts from the tribes to the north and the wampum from the tribes to the east. The Iroquois used present-giving more often than any other mode of exchange. Present-giving reflected the reciprocity in Iroquois society. The exchange would begin with one clan giving another tribe or clan a present with the expectation of some sort of needed commodity being given in return. This form of trade ties to the Iroquois culture's tendency to share property and cooperate in labor. In all cases no explicit agreement is made, but one service is performed for the community or another member of the community's good with the expectation that the community or another individual would give back. External trade offered one of the few opportunities for individual enterprise in Iroquois society. A person who discovered a new trading route had the exclusive right to trade along the same route in the future; however, clans would still collectivize trading routes to gain a monopoly on a certain type of trade.

The arrival of Europeans created the opportunity for greatly expanded trade. Furs were in demand in Europe, and they could be acquired cheaply from Indians in exchange for manufactured goods the Indians could not make themselves. Trade did not always benefit the Indians. The British took advantage of the gift-giving culture. They showered the Iroquois with European goods, making them dependent on such items as rifles and metal axes. The Iroquois had little choice but to trade for gunpowder after they had discarded their other weapons. The British primarily used these gifts to gain support among the Iroquois for fighting against the French. The Iroquois also traded for alcohol, a substance they did not have before the arrival of Europeans. Eventually, this would have a very negative impact on Iroquois society. The problem became so bad by 1753 that Scarrooyady, an Iroquois Chief, had to petition the Governor of Pennsylvania to intervene in trade: "Your Traders now bring scarce anything but Rum and Flour; they bring little powder and lead, or other valuable goods . . . and get all the skins that should go to pay the debts we have contracted for goods bought of the Fair Traders; by this means we not only ruin ourselves but them too. These wicked Whiskey Sellers, when they have once got the Indians in liquor, make them sell their very clothes from their backs. In short, if this practice be continued, we must be inevitably ruined."

The structure of the Iroquois economy created a unique property and work ethic. The threat of theft was almost nonexistent, since little was held by the individual except basic tools and implements that were so prevalent they had little value. The only goods worth stealing would have been wampum. While a theft-free society can be respected by all, communal systems such as that of the Iroquois are often criticized for not providing any incentive to work. In order for the Iroquois to succeed without an individual incentive, they had to develop a communal work ethic. Virtue became synonymous with productivity. The idealized Iroquois man was a good warrior and productive hunter while the perfect woman excelled in agriculture and housekeeping. By emphasizing an individual's usefulness to society, the Iroquois created a mindset that encouraged their members to contribute even though they received similar benefits no matter how hard they worked. 

As a result of their communal system, some would expect the Iroquois to have a culture of dependence without individuality. The Iroquois, however, had a strong tradition of autonomous responsibility. Iroquois men were taught to be self-disciplined, self-reliant, and responsible as well as stoic. The Iroquois attempted to eliminate any feelings of dependency during childhood and foster a desire for responsibility. At the same time, the child would have to participate in a communal culture, so children were taught to think as individuals but work for the community.

Many Iroquois have been fully integrated into the surrounding Western economy of the United States and Canada. For others their economic involvement is more isolated in the reservation. Whether directly involved in the outside economy or not, most of the Iroquois economy is now greatly influenced by national and world economies. The Iroquois have been involved in the steel construction industry for over a hundred years, with many men from the Mohawk nations working on such high-steel projects as the Empire State Building and World Trade Center. Inside the reservation the economic situation has often been bleak. For instance, the U.S. side of the Mohawk reservation has recently had unemployment as high as 46 percent. Many reservations have successful businesses, however. The Seneca reservation contains the City of Salamanca, New York, a center of the hardwoods industry with a Native American population of 13 percent. The Seneca make use of their independent reservation status to sell gasoline and cigarettes tax free and run high-stakes bingo operations. The Seneca's have also opened 2 Indian casinos, one in Niagara Falls, New York and one in Salamanca, New York. The Seneca Niagara Casino and the Seneca Allegany Casino are run by the Seneca. The Seneca are working on a third casino in Buffalo, it will be called the Seneca Buffalo Creek Casino.

The Oneida have already set up casinos on their reservations in New York and Wisconsin. The Oneida are one of the largest employers in northeastern Wisconsin with over 3,000 employees, including 975 people in tribal government. The Tribe manages over 16 million dollars in federal and private grant monies and a wide range of programs, including those authorized by the Indian Self-Determination and Education Assistance Act. The Oneida business ventures have brought millions of dollars into the community and improved the standard of living.

The Iroquois system of land management had to change with the coming of the Europeans and the forced isolation to reservations. The Iroquois had a system of collectively owned land free to be used as needed by their members. While this system was not wholly collective as land was distributed to individual family groups, the Iroquois lacked the Western conception of property as a commodity. After the Europeans arrived and placed the Iroquois on reservations, the natives had to adjust their property system to a more Western model. Despite the influence of Western culture, the Iroquois have maintained a unique view of property over the years. Modern-day Iroquois Doug George-Kanentiio sums up his perception of the Iroquois property view: The Iroquois have "no absolute right to claim territory for purely monetary purposes. Our Creator gave us our aboriginal lands in trust with very specific rules regarding its uses. We are caretakers of our Mother Earth, not lords of the land. Our claims are valid only so far as we dwell in peace and harmony upon her." 

Similar sentiments were expressed in a statement by the Iroquois Council of Chiefs (or Haudenosaunee) in 1981. The Council distinguished the "Western European concepts of land ownership" from the Iroquois view that "the earth is sacred" and "was created for all to use foreverâ€”not to be exploited merely for this present generation." Land is not just a commodity and "In no event is land for sale." The statement goes on, "Under Haudenosaunee law, Gayanerkowa, the land is held by the women of each clan. It is principally the women who are responsible for the land, who farm it, and who care for it for the future generations. When the Confederacy was formed, the separate nations formed one union. The territory of each nation became Confederacy land even though each nation continued to have a special interest in its historic territory." The Council's statement reflects the persistence of a unique view of property among the Iroquois.

The system of the Grand River Iroquois (two Iroquois reservations in Canada) integrated the traditional Iroquois property structure with the new way of life after being confined to a reservation. The reservation was established under two deeds in the eighteenth century. These deeds gave corporate ownership of the reservation lands to the Six Nations of the Iroquois. Individuals would then take a perpetual lease on a piece of land from the Confederacy. The Iroquois idea that land came into one's possession if cared for and reverted to public control if left alone persisted in reservation property law. In one property dispute case, the Iroquois Council sided with the claimant who had made improvements and cultivated the land over the one who had left it alone. The natural resources on the land belonged to the tribe as a whole and not to those who possessed the particular parcel. The Iroquois leased the right to extract stone from the lands in one instance and fixed royalties on all the production. After natural gas had been discovered on the reservation, the Six Nations took direct ownership of the natural gas wells and paid those who had wells on their land compensation only for damages done by gas extraction. This setup closely resembled the precontact land distribution system where the tribes actually owned the land and distributed it for use but not unconditional ownership. Another instance of traditional Iroquois property views impacting modern-day Indian life involves the purchase of land in New York State by the Seneca-Cayuga tribe, perhaps for a casino. The casino would be an additional collectively owned revenue maker. The Seneca-Cayuga already own a bingo hall, a gas station, and a cigarette factory. The later-day organization of reservation property directly reflects the influence of the precontact view of land ownership.









â‚? commemorative coins are special euro coins minted and issued by member states of the eurozone since 2004 as legal tender in all eurozone member states. The coins typically commemorate the anniversaries of historical events or draw attention to current events of special importance. As of 2008, forty-three variations of â‚? commemorative coins have been minted â€?six in 2004, eight in 2005, seven in 2006, twenty in 2007 (including the thirteen versions of the common issue) and two in 2008. At least eleven more are planned to be minted in 2008, and one more in 2009 (plus a second common issue, with sixteen states to participate, assuming Slovakia joins the eurozone on 2009-01-01 as planned). â‚? commemorative coins have become collectibles. The â‚? commemorative coins are not to be confused with commemorative coins (with a face value higher than â‚?), which are officially designated as "collector coins" and usually made of precious metal.

The basis for the commemorative coins derived from a decision of the European Council, which repealed the prohibition of changing the national obverse sides of euro coins from 1 January 2004 onwards. However, a number of recommendations and restrictions still apply.

Two restrictions concern the design. For one, nothing has changed about the fact that euro coins have a common reverse side, so only the national obverse sides may be changed. Additionally, the standard national obverse sides per se should not be changed before 2008 at the earliest, unless the head of state depicted on some of the coins changes before then. (This clause already came into effect for Monaco and the Vatican City, whose heads of state â€?Rainier III and Pope John Paul II respectively â€?died in 2005 and whose national obverse sides were changed for 2006.) The moratorium on these changes will be reviewed for extension in 2008.

Further regulations restrict the frequency and number of commemorative coin issues. Each member state shall only issue one commemorative coin per year, and it shall only be denominated as a â‚? coin. The total number of such coins put into circulation per year shouldn't surpass the higher of the following two numbers:

Another decision added two more guidelines regarding the design of the coins. The state issuing a coin should in some way clearly be identified on the obverse side, either by stating the full name or a clearly identifiable abbreviation of it; and neither name nor the denomination of the coin should be repeated on the obverse, as it is already featured on the common reverse side.

These restrictions do not apply retroactively; only new designs â€?the national obverse sides for regular issues of states newly joining the euro or of eurozone states which change their design, and â‚? commemorative coins issued from 2006 onwards â€?are subject to them. However, the five countries whose designs violated the rules (Austria, Belgium, Finland, Germany and Greece) will have to change their design in the near future, which Finland already did for 2007 and Belgium for 2008.

[[Image:European Union commemorative 2 euro coins.png|thumb|right|]]

As of March 2008, thirteen countries have independently issued â‚? commemorative coins (Austria, Belgium, Finland, France, Germany, Greece, Italy, Luxembourg, Monaco, Portugal, San Marino, Spain and the Vatican City), with Greece being the first country to issue this type of coin. Five eurozone countries have not yet issued such coins (Cyprus, Ireland, Malta, the Netherlands and Slovenia); Cyprus, Malta and Slovenia plan to do so in 2008, and there has also been common Treaty of Rome â‚? commemorative coin issued by all eurozone member states in 2007. (Another one is planned for 2009.)

The face value of the coins is typically less than their market value of between â‚? and â‚?2. The exceptions are San Marino and the Vatican City, where coins from the former are regularly sold for between â‚?0 and â‚?0, while coins from the latter are very rarely obtained for less than â‚?00.

Issued designs are made public in the Official Journal of the European Union (references to these publications are given in the tables below).

Note: In heraldry, directions are often described as they would appear to the bearer of a coat of arms, rather than as they would appear to the viewer. Therefore, the following descriptions will use "facing to the left" when it would appear to the layman that the person depicted is facing to the right.









Description: The centre part of the coin shows the treaty signed by the original six member states of the European Coal and Steel Community, on a background symbolising Michelangelo's paving on the  in Rome where the treaty was signed. The translation of EUROPE is inscribed above the book, but within the central design, whereas the translation of TREATY OF ROME 50 YEARS appears above the design. The year mark and the name of the issuing country are inscribed below the design, and the twelve stars of the European Union surround the design on the outer ring of the coin. (The location of the mint mark (and the engraver's initials, if they are shown) differs between the thirteen different versions.)

Due to special laws requiring that every coin bear the incumbent Grand Duke's portrait, the Luxembourgish edition of the common â‚? commemorative coin differs slightly from the others in addition to the translated inscriptions, since a latent image of the Grand Duke's portrait was added (as required by national law). A similar Dutch law, which requires the portrait of the current head of state of the Netherlands and the words  to appear on all coins issued by the Netherlands (for example, currently ) was amended so that the Netherlands could take part in this program; the amendment completely removed the requirement for â‚? commemorative coins. Furthermore, due to Belgium's special multilingual society, the Belgian coin features the inscription  in Latin.

The three micro-states which also use the euro due to an official agreement with the European Union (Monaco, San Marino and the Vatican City) did not issue this coin, as they are not member states of the European Union. However, some member states of the European Union which had not yet introduced the euro also took part in this program. For example, Cyprus issued a Â£1 coin and Hungary a 50 Ft coin with the same design.





Germany started the commemorative coin series  (The 16 States of the Federal Republic of Germany) in 2006, which will continue until 2021. The year in which the coin for a specific state is issued coincides with that state's Presidency of the ''''. The coins issued are:



The other thirteen coins will be issued in the following years; note that some designs are not yet finalised and still subject to change. Originally, the designs for the following states were different:



The series is similar to the United States' 50 State Quarters series, which is issuing fifty coins for its fifty constituent states, five per year between 1999 and 2008, plus six more in 2009 for the District of Columbia and five territories of the United States.

















Donald Herbert Davidson (March 6, 1917Â â€?August 30, 2003) was an American philosopher, who served as Slusser Professor of Philosophy at the University of California, Berkeley, from 1981 to 2003, after having also held substantive teaching appointments at Stanford University, Rockefeller University, Princeton University and the University of Chicago. His work has exerted considerable influence in nearly all areas of philosophy from the 1960s onward, but particularly in the philosophy of mind and philosophy of language. Although published mostly in the form of short essays which do not explicitly rely on any overriding theory, his work is nonetheless noted for a strongly unified characterâ€”the same methods and ideas are brought to bear on a host of apparently unrelated problemsâ€”and for synthesizing the work of a great number of other philosophers, including Aristotle, Kant, Wittgenstein, Frank P. Ramsey, W.V. Quine, and G. E. M. Anscombe.

Davidson was born in Springfield, Massachusetts on March 6, 1917 to Clarence ("Davie") Herbert Davidson and Grace Cordelia Anthony. The family lived in the Philippines from shortly after Davidson's birth until he was about four. Then, having lived in Amherst and Philadelphia, the family finally settled on Staten Island when Davidson was nine or ten. From this time he began to attend public school, having to begin in first grade with much younger children. He then attended the Staten Island Academy, starting in fourth grade. In high school, he tried to read Plato's Parmenides, Kant's Critique of Pure Reason and Nietzsche.

At Harvard University he switched his major from English and comparative literature (Theodore Spencer on Shakespeare and the Bible, Harry Levin on Joyce) to classics and philosophy.

Davidson was a fine pianist and always had a deep interest in music, later teaching philosophy of music at Stanford. At Harvard, he was in the same class as the conductor and composer Leonard Bernstein, with whom Davidson played four-hand piano. Bernstein wrote and conducted the musical score for the production which Davidson mounted of Aristophanes' play The Birds in the original Greek. Some of this music was later to be reused in Bernstein's ballet Fancy Free.

After graduation he went to California, where he wrote radio scripts for the private-eye drama, "Big Town," starring Edward G. Robinson. He returned to Harvard on a scholarship in classical philosophy, teaching philosophy and concurrently undergoing the intensive training of Harvard Business School. Before having the opportunity to graduate from Harvard Business School, Davidson was called up by the Navy, for which he had volunteered. He trained pilots to recognize enemy planes and participated in the invasions of Sicily, Salerno, and Enzio. After three and a half years in the Navy, he tried unsuccessfully to write a novel before returning to his philosophy studies and earning his doctorate in philosophy in 1949. Davidson wrote his dissertation, which he considered dull, on Plato's Philebus.

Under the influence of W. V. O. Quine, whom he often credits as his mentor, he began to gradually turn toward the more rigorous methods and precise problems characteristic of analytic philosophy.

During the 1950s Davidson worked with Patrick Suppes on developing an experimental approach to Decision Theory. They concluded that it was not possible to isolate a subject's beliefs and preferences independently of one another, meaning there would always be multiple ways to analyze a person's actions in terms of what they wanted, or were trying to do, or valued. This result is comparable to Quine's thesis on the indeterminacy of translation, and figures significantly in much of Davidson's later work on philosophy of mind.

His most noted work (see below) was published in a series of essays from the 1960s onward, moving successively through philosophy of action into philosophy of mind and philosophy of language, and dabbling occasionally in aesthetics, philosophical psychology, and the history of philosophy.

Davidson was widely traveled, and had a great range of interests he pursued with enormous energy. Apart from playing the piano, he had a pilot's license, built radios, and was fond of mountain climbing and surfing. He was married three times (the last time to the philosopher Marcia Cavell). Thomas Nagel elliptically eulogized him as "deeply erotic". 

He served terms as president of both the Eastern and Western Divisions of the American Philosophical Association, and held various professional positions at Queens College (now part of CUNY), Stanford, Princeton, Rockefeller University, Harvard, Oxford, and the University of Chicago. From 1981 until his death he was at the University of California, Berkeley, where he was Willis S. and Marion Slusser Professor of Philosophy. In 1995 he was awarded the Jean Nicod Prize.

Davidson's most noted work began in 1963 with an essay, "Actions, Reasons, and Causes," which attempted to refute the prevailing orthodox view, widely attributed to Wittgenstein, that an agent's reasons for acting cannot be the causes of his action (Malpas, 2005, Â§2). Instead, Davidson argued that "rationalization [the providing of reasons to explain an agent's actions] is a species of ordinary causal explanation" (1963, p. 685). In particular, an action A is explained by what Davidson called a primary reason, which involves a pro-attitude (roughly, a desire) toward some goal G and an instrumental belief that performing action A is a means to attaining G. For example, someone's primary reason for taking an umbrella with her outside on a rainy day might be that she wants to stay dry and believes that taking an umbrella is a means to stay dry today.

This view, which largely conforms to common-sense folk psychology, was held in part on the ground that while causal laws must be precise and mechanistic, explanation in terms of reasons need not. Davidson argued that the fact that the expression of a reason was not so precise, did not mean that the having of a reason could not itself be a state capable of causally influencing behavior. Several other essays pursue consequences of this view, and elaborate Davidson's theory of actions.

In "Mental Events" (1970) Davidson advanced a form of token identity theory about the mind: token mental events are identical to token physical events. One previous difficulty with such a view was that it did not seem feasible to provide laws relating mental statesâ€”for example, believing that the sky is blue, or wanting a hamburgerâ€”to physical states, such as patterns of neural activity in the brain. Davidson argued that such a reduction would not be necessary to a token identity thesis: it is possible that each individual mental event just is the corresponding physical event, without there being laws relating types (as opposed to tokens) of mental events to types of physical events. But, Davidson argued, the fact that we could not have such a reduction does not entail that the mind is anything more than the brain. Hence, Davidson called his position anomalous monism: monism, because it claims that only one thing is at issue in questions of mental and physical events; anomalous (from a-, not, and nomos, law) because mental and physical event types could not be connected by strict laws (laws without exceptions).

Davidson argued that anomalous monism follows from three plausible theses. First, he assumes the denial of epiphenomenalism--that is, the denial of the view that mental events do not cause physical events. Second, he assumes a nomological view of causation, according to which one event causes another if (and only if) there is a strict, exceptionless law governing the relation between the events. Third, he assumes the principle of the anomalism of the mental, according to which there are no strict laws that govern mental and physical event types. By these three theses, Davidson argued, it follows that the causal relations between the mental and the physical hold only between mental event tokens, but that mental events as types are anomalous. This ultimately secures token physicalism and a supervenience relation between the mental and the physical, while respecting the autonomy of the mental (Malpas, 2005, Â§2).

In 1967 Davidson published "Truth and Meaning," in which he argued that any learnable language must be statable in a finite form, even if it is capable of a theoretically infinite number of expressionsâ€”as we may assume that natural human languages are, at least in principle. If it could not be stated in a finite way then it could not be learned through a finite, empirical method such as the way humans learn their languages. It follows that it must be possible to give a theoretical semantics for any natural language which could give the meanings of an infinite number of sentences on the basis of a finite system of axioms. Following, among others, Rudolf Carnap (Introduction to Semantics, Harvard 1942, 22) Davidson also argued that "giving the meaning of a sentence" was equivalent to stating its truth conditions, so stimulating the modern work on truth-conditional semantics. In sum, he proposed that it must be possible to distinguish a finite number of distinct grammatical features of a language, and for each of them explain its workings in such a way as to generate trivial (obviously correct) statements of the truth conditions of all the (infinitely many) sentences making use of that feature. That is, we can give a finite theory of meaning for a natural language; the test of its correctness is that it would generate (if applied to the language in which it was formulated) all the sentences of the form "'p' is true if and only if p" ("'Snow is white' is true if and only if snow is white). (These are called T-sentences: Davidson derives the idea from Alfred Tarski.)

This work was originally delivered in his John Locke Lectures at Oxford, and launched a large endeavor by many philosophers to develop Davidsonian semantical theories for natural language. Davidson himself contributed many details to such a theory, in essays on quotation, indirect discourse, and descriptions of action.

After the 1970s Davidson's philosophy of mind picked up influences from the work of Saul Kripke, Hilary Putnam, and Keith Donnellan, all of whom had proposed a number of troubling counter-examples to what can be generally described as "descriptivist" theories of content. These views, which roughly originate in Bertrand Russell's Theory of Descriptions (and perhaps in the younger Wittgenstein's Tractatus Logico-Philosophicus) held that the referent of a nameâ€”which object or person that name refers toâ€”is determined by the beliefs a person holds about that object. Suppose I believe "Aristotle founded the Lyceum" and "Aristotle taught Alexander the Great." Whom are my beliefs about? Aristotle, obviously. But why? Russell would say that my beliefs are about whatever object makes the greatest number of them true. If two people taught Alexander, but only one founded the Lyceum, then my beliefs are about the one who did both. Kripke et al. argued that this was not a tenable theory, and that in fact whom or what a person's beliefs were about was in large part (or entirely) a matter of how they had acquired those beliefs, and those names, and how if at all the use of those names could be traced "causally" from their original referents to the current speaker.

Davidson picked up this theory, and his work in the 1980s dealt with the problems in relating first-person beliefs to second- and third-person beliefs. It seems that first person beliefs ("I am hungry") are acquired in very different ways from third person beliefs (someone else's belief, of me, that "He is hungry") How can it be that they have the same content?

Davidson approached this question by connecting it with another one: how can two people have beliefs about the same external object? He offers, in answer, a picture of triangulation: Beliefs about oneself, beliefs about other people, and beliefs about the world come into existence jointly. 

Many philosophers throughout history had, arguably, been tempted to reduce two of these kinds of belief and knowledge to the other one: Descartes and Hume thought that the only knowledge we start with is self-knowledge. Some of the logical positivists, (and some would say Wittgenstein, or Wilfrid Sellars), held that we start with beliefs only about the external world. (And arguably Friedrich Schelling and Emmanuel Levinas held that we start with beliefs only about other people). It is not possible, on Davidson's view, for a person to have only one of these three kinds of mental content; anyone who has beliefs of one of the kinds must have beliefs of the other two kinds.

Davidson's work is well noted for its unity, as he has brought a similar approach to a wide variety of philosophical problems. Radical interpretation is a hypothetical standpoint which Davidson regards as basic to the investigation of language, mind, action, and knowledge. Radical interpretation involves imagining that you are placed into a community which speaks a language you do not understand at all. How could you come to understand the language? One suggestion is that you know a theory that generates a theorem of the form 's means that p' for every sentence of the object language (i.e. the language of the community), where s is the name of a sentence in the object language, and p is that sentence, or a translation of it, in the metalanguage in which the theory is expressed. However, Davidson rejects this suggestion on the grounds that the sentential operator 'means that' is sensitive not only to the extensions of the terms that follow it, but also to their intensions. Hence, Davidson replaces 'means that' with a connective that is only sensitive to the extensions of sentences; since the extension of a sentence is its truth value, this is a truth functional connective. Davidson elects the biconditional - if and only if - as the connective needed in a theory of meaning. This is the obvious choice because we are aiming at equivalence of meaning between s and p. But now we have a problem: 's if and only if p' is an ungrammatical sentence because the connective must link two propositions, but s is the name of a proposition, and not a proposition itself. In order to render s a proposition we need to supply it with a predicate. Which predicate is satisfied by s if and only if the sentence named by s, or a translation of it, is the case? In other words, which predicate is satisfied by "bananas are yellow" if and only if bananas are yellow? The answer is the predicate truth. Thus, Davidson is led to the conclusion that a theory of meaning must be such that for each sentence of the object language it generates a theorem of the form 's is true if and only if p'. A theory of truth for a language can serve as a theory of meaning.

The significance of this conclusion is that it allows Davidson to draw on the work of Alfred Tarski in giving the nature of a theory of meaning. Tarski showed how we can give a compositional theory of truth for artificial languages. Thus, Davidson takes three questions to be central to radical interpretation. Firstly, can a theory of truth be given for a natural language? Secondly, given the evidence plausibly available for the radical interpreter, can they construct and verify a theory of truth for the language they wish to interpret? Thirdly, will having a theory of truth suffice for allowing the radical interpreter to understand the language? Davidson has shown, using the work of Tarski, that the first question can be answered affirmatively.

What evidence is plausibly available to the radical interpreter? Davidson points out that beliefs and meanings are inseparable. A person holds a sentence true based on what he believes and what he takes the sentence to mean. If the interpreter knew what a person believed when that person held a sentence to be true, the meaning of the sentence could then be inferred. Vice versa, if the interpreter knew what a person took a sentence to mean when that person held it to be true, the belief of the speaker could be inferred. So Davidson doesn't allow the interpreter to have access to beliefs as evidence, since the interpreter would then be begging the question. Instead, Davidson allows that the interpreter can reasonably ascertain when a speaker holds a sentence true, without knowing anything about a particular belief or meaning. This will then allow the interpreter to construct hypotheses relating a speaker and an utterance to a particular state of affairs at a particular time. The example Davidson gives is of a German speaker who utters â€œEs regnetâ€?when it is raining.

Davidson claims that even though in isolated cases a speaker might be mistaken about the state of objective reality (for example, the German speaker might utter â€œEs regnetâ€?even though it is not raining), this doesnâ€™t undermine the entire project. This is because a speakerâ€™s beliefs must be mostly correct and coherent. If they werenâ€™t, we wouldnâ€™t even identify the speaker as a speaker. This is Davidsonâ€™s famous principle of charity and it is what enables an interpreter to be confident that the evidence he gathers will allow him to verify a theory of truth for the language.

On first glance, it might seem that a theory of truth is not enough to interpret a language. After all, if truth-conditions are all that matters, then how can anomalous sentences such as â€˜â€œSchnee ist weissâ€?is true if and only if snow is white and grass is greenâ€?be verified as false? Davidson argues that because the language is compositional, it is also holistic: sentences are based on the meanings of words, but the meaning of a word depends on the totality of sentences in which it appears. This holistic constraint, along with the requirement that the theory of truth is law-like, suffices to minimize indeterminacy just enough for successful communication to occur.

In summary, then, what radical interpretation highlights is what is necessary and sufficient for communication to occur. These conditions are: that in order to recognize a speaker as a speaker, their beliefs must be mostly coherent and correct; indeterminacy of meaning doesnâ€™t undermine communication, but it must be constrained just enough.











The Second Boer War (Dutch: Tweede Boerenoorlog, Afrikaans: Tweede Boereoorlog), commonly referred to as The Boer War and also known as the South African War (outside of South Africa), the Anglo-Boer War (among most South Africans) and in Afrikaans as the Boereoorlog or Tweede Vryheidsoorlog ("Second War of Liberation"), was fought from 11 October 1899 until 31 May, 1902, between the British Empire and its (white) independent colonies of Australia, Canada and New Zealand and the two independent Boer republics of the Orange Free State and the South African Republic (Transvaal Republic). The casus belli to the conflict was the Jameson Raid, 1895-96. After a protracted, hard-fought formal and guerrilla war, the two independent Boer republics were annexed into the British Empire. 

The southern part of the African continent was dominated in the 19th century by a set of epic struggles to create within it a single unified state. While the Berlin Conference of 1884-5 sought to draw boundaries between the European powers' African possessions, it set the stage for further scrambles. The British attempt to annex first the Transvaal in 1880, and then in 1899 both the Transvaal and the Orange Free State, was their biggest incursion into southern Africa, but there were others. In 1868, the British annexed Basutoland in the Drakensberg Mountains following an appeal from Moshesh, the leader of a mixed group of African refugees from the Zulu wars, who sought British protection against the Boers. In the 1880s, Bechuanaland (modern Botswana, located north of the Orange River) became the object of dispute between the Germans to the west, the Boers to the east, and Cape Colony to the south. Although Bechuanaland had no economic value, the "Missionaries Road" passed through it towards territory farther north. After the Germans annexed Damaraland and Namaqualand (modern Namibia) in 1884, the British annexed Bechuanaland in 1885. 

â€œBritish imperialism, which often stalked its quarry with cultural and commercial feints before finally pulling down its prey through conquest and formal annexation, was for some time frustrated by the presence of the two independent Boer republics. Yet, within little more than a decade and half, the Orange Free State and the Zuid Afrikaansche Republiek had both been subjugated in the course of the bloody South African War of 1899â€?902.â€?
The Boers of the Transvaal Republic had in the 1880-1881 war proved skillful fighters in resisting the British attempt at annexation, causing several costly defeats to the British. The British government of William Gladstone had been unwilling to become bemired in a distant war demanding of substantial troop reinforcement and expense for what was at the time perceived to be minimal return. They had cut their losses, and signed an armistice to end the war, with subsequently a peace treaty with the Transvaal President Paul Kruger. 

However when, in 1886, a second major mineral find was made at an outcrop on a large ridge some thirty miles south of the Boer capital at Pretoria, it reignited British imperial interests. By 1899 Britain was again at war with the Boer republics in the Second Boer War, and this time the lure of gold was more than enough for Britain to commit the substantial troops required and keep them fighting, and bear all the cost including the loss of lives, over the three years that it would take.

The ridge, known locally as the "Witwatersrand" (literally "white water ridge"â€”a watershed) contained the world's largest deposit of gold-bearing ore. Although it was not as rich as gold finds in Canada and Australia, its consistency made it especially well-suited to industrial mining methods. With the 1886 discovery of gold in Transvaal, thousands of British and other prospectors and settlers streamed over the border from the Cape Colony (annexed by Britain earlier) and from across the globe. The city of Johannesburg sprang up as a shanty town nearly overnight as the uitlanders (foreigners) poured in and settled near the mines. The uitlanders rapidly outnumbered the Boers on the Rand, but remained a minority in the Transvaal as a whole. The Afrikaners, nervous and resentful of the uitlanders' presence, denied them voting rights and taxed the gold industry. The tax on a box of dynamite was five shillings of the cost of five pounds. These mines consumed vast quantities of explosives and President Paul Kruger gave manufacturing monopoly rights to a non-British operation of the Nobel company, which infuriated the British. 

The so-called "dynamite monopoly" became a major pretext for war. However, one of the underlying irritants for war occurred in 1894â€?5 over the railway and tariffs problems. Kruger wanted to build a railway through Portuguese East Africa to Delagoa Bay, bypassing British controlled ports in Natal and Cape Town and avoiding British tariffs. The Prime Minister of the Cape Colony was Cecil Rhodes, a man with a vision of a British controlled Africa extending from Cape to Cairo. Angered by these problems, the Uitlanders and the British mine owners sought to overthrow the Boer government. In 1895, Cecil Rhodes sponsored the failed coup d'Ã©tat backed by an armed incursion, the Jameson Raid. Of this raid, Jan C. Smuts wrote in 1906, "The Jameson Raid was the real declaration of war...And that is so in spite of the four years of truce that followed...[the] aggressors consolidated their alliance...the defenders on the other hand silently and grimly prepared for the inevitable." 

Paul Kruger and the President Martinus Theunis Steyn of the Orange Free State both understood that the failed raid was the precursor to a war and commencing in 1896 placed orders for Mauser rifles  and German Krupp artillery. 

The failure to gain improved rights for Britons became a pretext to manufacture a case for war and to justify a major military buildup in the Cape. The case for war was justified and espoused as far away as the Australian colonies. Several key British colonial leaders favoured annexation of the independent Boer republics. These figures included the Cape Colony governor Sir Alfred Milner, Cape Prime Minister Cecil Rhodes, British Colonial Secretary Joseph Chamberlain and mining syndicate owners or Randlords (nicknamed the gold bugs) such as Alfred Beit, Barney Barnato and Lionel Phillips. Confident that the Boers would be quickly defeated, they planned, schemed and organised to precipitate a war, based on the Uitlanders' real or imagined grievances.

President Steyn of the Orange Free State invited Milner and Kruger to attend a conference in Bloemfontein which started on 30 May 1899, but negotiations quickly broke down, despite Kruger's offer of concessions. In September 1899, Chamberlain sent an ultimatum demanding full equality for British citizens resident in Transvaal.

Kruger, seeing that war was inevitable, simultaneously issued his own ultimatum prior to receiving Chamberlain's. This gave the British 48 hours to withdraw all their troops from the border of Transvaal; otherwise the Transvaal, allied with the Orange Free State, would declare war.

News of the ultimatum reached London on the day it expired. Outrage and laughter were the main responses. The editor of the Times laughed out loud when he read it, saying 'an official document is seldom amusing and useful yet this was both'. The Times denounced the ultimatum as an 'extravagant farce', The Globe denounced this 'trumpery little state'. Most editorials were similar to the Daily Telegraph, which declared: 'of course there can only be one answer to this grotesque challenge. Kruger has asked for war and war he must have!'.

War was declared on 11 October 1899. The Boers had no problems with mobilisation, since the Presidents of the Transvaal and Orange Free State simply signed decrees to concentrate within a week and the Commandos could muster between 30-40,000 men. The Boers struck first by invading Cape Colony and Colony of Natal between October 1899 and January 1900. What the Boers presented was a mobile and innovative approach to warfare drawing on strategies that had first appeared in the American Civil War. The average Burghers (citizens) who made up their Commandos were farmers who had spent almost all their working life in the saddle both as farmers and hunters. They had to depend for the pot on their horse and their rifle and were skilled stalkers and marksmen. They made expert light cavalry, using every scrap of cover, from which they could pour in a destructive fire using their modern Mauser rifles. Furthermore, in preparation for hostilities the Boers had acquired around one hundred of the latest Krupp field guns, all horse drawn and dispersed among the various Commando groups, and their skill in adapting themselves to first-rate artillerymen shows them to have been a versatile adversary. 

Although it seemed a mismatch between the might of the British Empire on the one hand and farmers on the other, and the British anticipated a quick and easy victory, it became clear from the start that Britain would have problems. The quick Boer mobilisation resulted in early military successes against the scattered British forces, and although there was a large response to the call to arms over 2/3 of the British men that attempted to enlist were turned away because they were medically unfit. This staggering number had a great influence on the Liberal Welfare Reforms of 1906-14, after the Conservatives had been voted out of power.

The Boers were able to besiege the towns of Mafeking (defended by troops headed by Colonel Robert Baden-Powell), and Kimberley (defended by troops headed by Lt-Col Kekewich) on the borders of the Transvaal and Orange Free State respectively. The major British concentration was in northern Natal under Sir George White. White's troops, who were dangerously dispersed, were defeated separately, and were besieged in Ladysmith.

Siege life took its toll on both the defending soldiers and the civilians in the cities of Mafeking, Ladysmith, and Kimberley as food began to grow scarce after a few weeks. In Mafeking, Sol Plaatje wrote, "I saw horseflesh for the first time being treated as a human foodstuff." The cities under siege also dealt with constant artillery bombardment, making the streets a dangerous place. Near the end of the siege of Kimberley, it was expected that the Boers would intensify their bombardment, so a notice was displayed encouraging people to go down into the mines for protection. The townspeople panicked, and people flowed into the mineshafts constantly for a 12-hour period. Although the bombardment never came, this did nothing to diminish the distress of the civilians. Many of the townspeople, now under siege, sheltered in the local convent, now the Mcgregor museum. Since the mining that occurred there, for diamonds, was open air, the people were not able to shelter in mine shafts. The mine is now known as the Big Hole, a popular tourist attraction in the area.

Major British reinforcements were arriving under General Redvers Henry Buller. He originally intended an offensive straight up the railway line leading from Cape Town through Bloemfontein to Pretoria. Finding on arrival that the British troops already in South Africa were under siege, he split his Army Corps into several widely spread detachments, to relieve the besieged garrisons.

British commanders had trained on the lessons of the Crimean War, and were adept at battalion and regimental set pieces with columns manoeuvring in jungles, deserts and mountainous regions; what they entirely failed to comprehend was the impact of destructive fire from trench positions, and the mobility of cavalry raids both of which had been developed in the American Civil War. The British troops went to war with what would prove to be antiquated tactics, and in some cases antiquated weapons , against the mobile Boer forces with the destructive fire of their modern Mausers, the latest Krupp field guns and their innovative tactics.

The middle of December was disastrous for the British army. In a period known as Black Week (10 â€?15 December 1899), the British suffered a series of devastating losses at Magersfontein, Stormberg, and Colenso.

At the Battle of Stormberg on 10 December, British General Sir William Gatacre, who was in command of 3,000 troops protecting against Boer raids in Cape Colony, tried to recapture a railway junction about  south of the Orange River. But Gatacre chose to assault the Orange Free State Boer positions surmounting a precipitous rock face in which he lost 135 killed and wounded, as well as two guns and over 600 troops captured.

At the Battle of Magersfontein on 11 December, 14,000 British troops, under the command of Lieutenant-General Lord Methuen, attempted to fight their way to relieve Kimberley. The Boer commanders, Koos de la Rey and Piet Cronje, devised a plan to dig trenches in an unconventional place to fool the British and to give their riflemen a greater firing range. The plan worked and this tactic helped write the doctrine of the supremacy of the defensive position, using modern small arms and trench fortifications.  At Magersfontein, the British were decisively defeated, suffering the loss of 120 British soldiers killed and 690 wounded, which prevented them from relieving Kimberley and Mafeking.

From the "Battle of Magersfontein," verse by Private Smith of the Black Watch December 1899. Quoted in, â€˜Thomas Pakenhamâ€™s "The Boer War," page 115.

But the nadir of Black Week was the Battle of Colenso on 15 December where 21,000 British troops commanded by Buller himself, attempted to cross the Tugela River to relieve Ladysmith where 8,000 Transvaal Boers, under the command of Louis Botha, were awaiting them. Through a combination of artillery and accurate rifle fire, and a better use of the ground, the Boers repelled all British attempts to cross the river. The British had a further 1,126 casualties, and lost 10 artillery pieces to the Boers during the ensuing retreat. The Boer forces suffered 40 casualties.

The British suffered further defeats in their attempts to relieve Ladysmith at the Battle of Spion Kop of 19 to 24 January 1900, where Buller again attempted to cross the Tugela west of Colenso and was defeated again by Louis Botha after a hard-fought battle for a prominent hill feature which resulted in a further 1,000 British casualties and nearly 300 Boer casualties. Buller attacked Botha again on 5 February at Vaal Krantz and was again defeated.

By taking command in person in Natal, Buller allowed the overall direction of the war to drift. Because of concerns about his performance and negative reports from the field, he was replaced as Commander in Chief by Field Marshal Lord Roberts. Roberts first intended like Buller to attack directly along the Cape Town - Pretoria railway but, again like Buller, was forced to relieve the beleaguered garrisons. Leaving Buller in command in Natal, Roberts massed further reinforcements near the Orange River and on 14 February 1900, he launched a major attack to relieve Kimberley. The city was relieved on 15 February by a cavalry division under Lieutenant General John French. At the Battle of Paardeberg on 18 February to 27 February 1900, Roberts then surrounded General Piet Cronje's retreating Boer army, and forced him to surrender with 4000 men after a siege lasting a week. Meanwhile, Buller at last succeeded in forcing a crossing of the Tugela, and defeated Botha's outnumbered forces north of Colenso, allowing the Relief of Ladysmith the day after Cronje surrendered.

Roberts then advanced into the Orange Free State from the west, capturing Bloemfontein, the capital, on March 13. Meanwhile, he detached a small force to relieve Baden-Powell, and the Relief of Mafeking on May 18 1900 provoked riotous celebrations in Britain.

After being forced to delay for several weeks at Bloemfontein due to shortage of supplies and enteric fever (caused by poor hygiene, drinking bad water at Paardeburg and appalling medical care),, Roberts resumed his advance. He was forced to halt again at Kroonstad for 10 days, due once again to the collapse of his medical and supply systems, then finally captured Johannesburg on May 31 and the capital of the Transvaal, Pretoria, on June 5. The first into Pretoria, was Lt. William Watson of the New South Wales Mounted Rifles, who persuaded the Boer to surrender the capital.(Before the war, the Boers had constructed several forts south of Pretoria, but the artillery had been removed from the forts for use in the field, and in the event the Boers abandoned Pretoria without a fight.)

British observers believed the war to be all but over after the capture of the two capital cities. However, the Boers had earlier met at the temporary new capital of the Orange Free State, Kroonstad, and planned a guerrilla campaign to hit the British supply and communication lines. The first engagement of this new form of warfare was at Sanna's Post on 31 March where 1,500 Boers under the command of Christiaan De Wet attacked Bloemfontein's waterworks about  east of the city, and ambushed a heavily escorted convoy which resulted in 155 British casualties and the capture of seven guns, 117 wagons and 428 British troops.

After the fall of Pretoria, one of the last formal battles was at Diamond Hill on 11 â€?12 June, where Roberts attempted to drive the remnants of the Boer field army beyond striking distance of Pretoria. Although Roberts drove the Boers from the hill, the Boer commander, Louis Botha, did not regard it as a defeat, for he inflicted more casualties on the British (totalling 162 men) while suffering around 50 casualties.

The set-piece period of the war now largely gave way to a mobile guerrilla war, but one final operation remained. President Kruger and what remained of the Transvaal government had retreated to eastern Transvaal. Roberts, joined by troops from Natal under Buller, advanced against them, and broke their last defensive position at Bergendal on August 26. As Roberts and Buller followed up along the railway line to Komatipoort, Kruger sought asylum in Portuguese East Africa (modern Mozambique). Some dispirited Boers did likewise, and the British gathered up much war material. However, the core of the Boer fighters under Botha easily broke back through the Drakensberg mountains into the Transvaal highveld after riding north through the bushveld. Under the new conditions of the war, heavy equipment was no use to them, and therefore no great loss.

There was much sympathy for the Boers on mainland Europe and in October, President Kruger and members of the Transvaal government left South Africa on the Dutch warship De Gelderland, sent by the Queen of the Netherlands Wilhelmina, who had simply ignored the British naval blockade of South Africa. Paul Kruger's wife however was too ill to travel and remained in South Africa where she died on 20 July 1901 without seeing Paul Kruger again. President Kruger first went to Marseille and then on to The Netherlands where he stayed for a while before moving finally to Clarens, Switzerland, where he died in exile on 14 July 1904.

By September 1900, the British were nominally in control of both Republics, with the exception of the northern part of Transvaal. However, they soon discovered that they only controlled the ground that their columns physically occupied. The Boer commanders continued to fight and now adopted a guerrilla style of warfare. Each Boer commando was sent to the district from which its members had been recruited which meant that they could rely on local support and personal knowledge of the terrain and the towns within the district thereby enabling them to live off the land. Their orders were simply to act against the British whenever possible. Their strategy was to strike fast and hard causing as much damage to the enemy as possible, and then to withdraw and vanish before enemy reinforcements could arrive. The vast distances of the Republics allowed the Boer commandos considerable freedom to move about and made it impossible for the 250,000 British troops to control the territory effectively using columns alone. As soon as a British column left a town or district, British control of that area faded away. 

The Boer commandos were especially effective during the initial guerrilla phase of the war because Roberts had assumed that the war would end with the capture of the Boer capitals and the dispersal of the main Boer armies. Many British troops were therefore redeployed, and had been replaced by lower-quality contingents of Imperial Yeomanry and locally-raised irregular corps. 

The British were forced to quickly revise their tactics. They concentrated on restricting the freedom of movement of the Boer commandos, and depriving them of local support. The railway lines had provided vital lines of communication and supply, and as the British had advanced across South Africa they had used armoured trains and had established fortified blockhouses at key points. They now built many more blockhouses and fortified these. They linked the blockhouses with barbed wire fences to parcel up the wide veld into smaller areas. "New Model" drives were mounted under which a continuous line of troops could sweep an area of veld bounded by blockhouse lines, unlike the earlier inefficient scouring of the countryside by scattered columns. The British also implemented a "Scorched Earth" policy under which they targeted everything within the controlled areas that could give sustenance to the Boer guerrillas with a view to making it harder and harder for the Boers to survive. As British troops swept the countryside, they systematically destroyed crops, burned homesteads and farms, poisoned wells, and interned women, children and workers in concentration camps. Finally, the British also established their own mounted raiding columns in support of the sweeper columns. These were used to rapidly follow and relentlessly harass the Boers with a view to delaying them, and cutting off escape, while the sweeper units caught up. They also utilised the armoured trains to deliver the rapid reaction forces much more quickly to incidents (such as Boer attacks on blockhouses and columns) or to drop them off ahead of retreating Boer columns.

The Boer commandos in the Western Transvaal were very active after September 1901. Several battles of importance were fought here between September 1901 and March 1902. At Moedwil on 30 September 1901 and again at Driefontein on 24 October, Gen. De la Reyâ€™s forces attacked the British, but were forced to withdraw after the British offered strong resistance.

A time of relative quiet descended thereafter on the western Transvaal. February 1902 saw the next major battle in that region. On 25 February De la Rey attacked a British column at Ysterspruit near Wolmaransstad. De la Rey succeeded in capturing the column and a large amount of ammunition.The Boer attacks prompted Lord Methuen, the British second-in-command after Lord Kitchener, to move his column from Vryburg to Klerksdorp to deal with De la Rey. On the morning of 7 March 1902, the Boers attacked the rear guard of Methuenâ€™s moving column at Tweebosch. Confusion reigned in British ranks and Methuen was wounded and captured by the Boers.The Boer victories in the west led to stronger action by the British. In the second half of March 1902, large British reinforcements were sent to the Western Transvaal. The opportunity the British were waiting for arose on 11 April 1902 at Rooiwal, where the combined forces of Gens. Grenfell, Kekewich and Von Donop came into contact with the forces of Gen. Kemp. The British soldiers were well positioned on the mountainside and inflicted severe casualties on the Boers charging on horseback over a large distance, beating them back.

This was the end of the war in the Western Transvaal and also the last major battle of the Anglo-Boer War.

While the British occupied Pretoria, the Boer fighters in the Orange Free State had been driven into a fertile area in the north east of the Republic, known as the Brandwater Basin. This offered only temporary sanctuary, as the mountain passes leading to it could be occupied by the British, trapping the Boers. A force under General Hunter set out from Bloemfontein to achieve this in July 1900. The hard core of the Boers under Christiaan de Wet, accompanied by President Steyn, left the basin early. Those remaining fell into confusion and most failed to break out before Hunter trapped them. 4,500 Boers surrendered and much equipment was captured, but as with Robert's drive against Kruger at the same time, these losses were of relatively little consequence, as the hard core of the Boer armies and their most determined and active leaders remained at large.

From the Basin, de Wet headed west. Although hounded by British columns, he succeeded in crossing the Vaal into the Western Transvaal, to allow Steyn to travel to meet the Transvaal leaders.

Returning to the Orange Free State, de Wet inspired a series of attacks and raids from the hitherto quiet western part of the country. Many Boers who had earlier returned to their farms, sometimes giving formal parole to the British, took up arms again. In late January 1901, De Wet led a renewed invasion of Cape Colony. This was less successful, because there was no general uprising among the Cape Boers, and de Wet's men were hampered by bad weather and relentlessly pursued by British forces. They escaped across the Orange River, almost by a miracle.

From then until the final days of the war, de Wet remained comparatively quiet, partly because the Orange Free State was effectively left desolate by British sweeps. In late 1901, De Wet overran an isolated British detachment at Groenkop, inflicting heavy casualties. This prompted Kitchener to launch the first of the "New Model" drives against him.

The British had first erected lines of blockhouses to protect the railway lines. They now built fresh lines of these, linked by barbed wire fences, to prevent free Boer movement across the veld. They also allowed "New Model" drives. Unlike the earlier inefficient scouring of the countryside by scattered columns, a continuous line of troops could now effectively sweep an area of veld bounded by blockhouse lines.

De Wet escaped the first such drive, but lost 300 of his fighters. This was a severe loss, and a portent of further such attrition.

Two Boer forces fought in this area; under Botha in the south east and Ben Viljoen in the north east. Botha's forces were particularly active, raiding railways and even mounting a renewed invasion of Natal in September, 1901. After defeating British mounted infantry near Dundee, Botha was forced to withdraw by heavy rains which made movement difficult and crippled his horses. Back in the Transvaal, he attacked a British raiding column at Bakenlaagte. This made his forces the target of increasingly large and ruthless drives by British forces, and eventually, he had to abandon the high veld and retreat to a narrow enclave bordering Swaziland.

To the north, Ben Viljoen grew steadily less active. His forces mounted comparatively few attacks and as a result, the Boer enclave around Lydenburg was largely unmolested. Viljoen was eventually captured.

After he escaped across the Orange in March 1901, de Wet had left forces under Cape rebels Kritzinger and Scheepers to maintain a guerrilla campaign in the Cape Midlands. The campaign here was one of the least chivalrous, with intimidation by both sides of each other's civilian sympathisers. Several captured rebels, including Scheepers, were executed for treason by the British, some in public. In most cases though, the executions were ostensibly for capital crimes such as the murder of prisoners or of unarmed civilians.

Fresh Boer forces under Jan Christiaan Smuts, joined by the surviving rebels under Kritzinger, made another attack on the Cape in September 1901. They suffered severe hardships and were hard pressed by British columns, but eventually rescued themselves by routing some of their pursuers and capturing their equipment.

From then until the end of the war, Smuts increased his forces until they numbered 3,000. However, no general uprising took place, and the situation in the Cape remained stalemated.

Towards the end of the war, British tactics of containment, denial and harassment began to yield results. The sourcing and coordination of intelligence became increasingly efficient with regular reporting from observers in the blockhouses, from units patrolling the fences and conducting "sweeper" operations, and from native Africans in rural areas who increasingly supplied intelligence as the Scorched Earth policy took effect and they found themselves competing with the Boers for food supplies, and as old enmities resurfaced. Kitchener's forces at last began to seriously affect the Boers' fighting strength and freedom of manoeuvre, and made it harder and harder for the Boers and their families to survive. 

The counterinsurgency techniques and lessons (the restriction of movement, the containment of space, the targeting of anything and everything that could give sustenance to guerrillas, the relentless harassment through sweeper groups coupled with rapid reaction forces, the sourcing and coordination of intelligence, and the nurturing of native allies) learned during the Boer War were used by the British (and other forces) in future guerrilla campaigns including to counter Malayan communist rebels during the Malayan Emergency.



The English term "concentration camp" was first used to describe camps operated by the British in South Africa during this conflict.

The camps had originally been set up as "refugee camps" by the Army for families who had been forced to abandon their homes for one or other reason related to the war. When Kitchener succeeded Roberts as commander-in-chief in South Africa in November 29, 1900 the steady trickle of dispossessed civilians became a torrent. In an attempt to break the guerrilla campaign, Kitchener initiated plans to "flush out guerrillas in a series of systematic drives, organized like a sporting shoot, with success defined in a weekly 'bag' of killed, captured and wounded, and to sweep the country bare of everything that could give sustenance to the guerrillas, including women and children. . . . It was the clearance of civilians -- uprooting a whole nation -- that would come to dominate the last phase of the war.". 

As Boer farms were destroyed by the British under their "Scorched Earth" policy - including the systematic destruction of crops and slaughtering of livestock, the burning down of homesteads and farms, and the poisoning of wells and salting fields - many tens of thousands more non combatant women and children were forcibly moved to prevent the Boers from resupplying at their homes. The policy was ruthlessly applied; although most black Africans were not considered by the British to be hostile, they were also forcibly removed from Boer areas. 

This was not the first appearance of internment camps. The Spanish had used internment in the Ten Years' War that later led to the Spanish-American War, and the United States used them to devastate guerrilla forces during the Philippine-American War. But the Boer War concentration camp system was the first time that a whole nation had been systematically targeted, and entire regions depopulated.

Boer internees were separately held from black Africans. Eventually there were a total of 45 tented camps built for Boer internees and 64 for black Africans. Of the 28,000 Boer men captured as prisoners of war, 25,630 were sent overseas. The vast majority of Boers remaining in the local camps were women and children, but the camps established for black Africans held large numbers of men as well. A number of the black African internees who were not considered by the British to be hostile were used as a paid labour force.

The camps were poorly administered from the outset and became increasingly overcrowded when Kitchener's troops implemented the internment strategy on a wide scale. Conditions were very unhealthy with poor hygiene. The food rations were meager, and wives and children of men who were still fighting were routinely given smaller rations than others. The inadequate shelter, poor diet, inadequate hygiene and overcrowding led to malnutrition and endemic contagious diseases such as measles, typhoid and dysentery to which the children were particularly vulnerable. Coupled with a shortage of medical facilities, this led to large numbers of deaths.

Although the 1900 UK general election, also known as the "Khaki election", had resulted in a victory for the Conservative government on the back of recent British victories against the Boers, public support quickly waned as it became apparent that the war would not be easy and unease developed following reports about the treatment by the Army of the Boer civilians. Public and political opposition to Government policies in South Africa regarding Boer civilians was first expressed in Parliament in February 1901 in the form of an attack on the policy, the government, and the Army by the radical Liberal MP David Lloyd George. 

Emily Hobhouse, a delegate of the South African Women and Children's Distress Fund, visited some of the camps in the Orange Free State from January 1901 and in May, 1901 she returned to England on board the ship, the Saxon. Lord Milner, High Commissioner in South Africa, also boarded the Saxon for holiday in England but, unfortunately for both the camp internees and the British government, had no time for Miss Hobhouse, regarding her as a Boer sympathizer and "trouble maker."  On her return Emily Hobhouse did much to publicize the distress of the camp inmates. She managed to speak to the Liberal Party leader, Henry Campbell-Bannerman who professed to be suitably outraged but was disinclined to press the matter. This presumably was because his party was split between Liberal Imperialist and pro-Boer factions, and he had no wish to rock the boat. The more radical Liberals however such as David Lloyd George and John Ellis were prepared to raise the matter in Parliament and harass the government on the issue, which they duly did. St John Brodrick, the Conservative secretary of state for war, first defended the government's policy by arguing that the camps were purely 'voluntary' and that that the interned Boers were "contented and comfortable", but was somewhat undermined as he had no firm statistics to back up his argument so when that position proved untenable, he resorted to the "military necessity" argument and stated that everything possible was being done to ensure satisfactory conditions in the camps.

Emily Hobhouse published a fifteen-page report in June 1901 which contradicted Brodrick's claim, and Lloyd George then openly accused the government of "a policy of extermination" directed against the Boer population. In June, 1901, Liberal opposition party leader Campbell-Bannerman took up the assault and answered the rhetorical "When is a war not a war?" with "When it is carried on by methods of barbarism in South Africa," referring to those same camps and the policies that created them. The Hobhouse report caused uproar both domestically and in the international community. 

Although the Government had comfortably won the parliamentary debate by a margin of 252 to 149, it was stung by the criticism and concerned by the escalating public outcry, and called on Kitchener for a detailed report. In response complete statistical returns from camps were sent in July 1901. By August 1901 it was clear to Government and Opposition alike that Miss Hobhouse's worst fears were being confirmed - 93,940 Boers and 24,457 black Africans were reported to be in "camps of refuge" and the crisis was becoming a catastrophe as the death rates appeared very high, especially amongst the children. The Government responded to the growing clamour by appointing a commission. The Fawcett Commission as it became known was, uniquely for its time, an all-woman affair headed by Millicent Fawcett who despite being the leader of the women's suffrage movement was a Liberal Unionist and thus a government supporter and considered a safe pair of hands. Between August and December 1901 the Fawcett Commission conducted its own tour of the camps in South Africa. Whilst it is probable that the British Government expected the Commission to produce a report that could be used to fend off criticism, in the end it confirmed everything that Emily Hobhouse had said. Indeed, if anything the Commission's recommendations went even further than those of Emily Hobhouse; the Commission insisted that rations should be increased and that additional nurses be sent out immediately, and included a long list of other practical measures designed to improve conditions in the camp. Millicent Fawcett was quite blunt in expressing her opinion that much of the catastrophe was down to a simple failure to observe elementary rules of hygiene. 

Backed into a corner, the Colonial Secretary Joseph Chamberlain in November 1901 ordered Arthur Milner to ensure that "all possible steps are being taken to reduce the rate of mortality". The civil authority took over the running of the camps from Kitchener and British Command and by February 1902, the annual death-rate in the concentration camps for white inmates dropped to 6.9% and eventually it dropped to 2%, which was a lower rate than pertained in many British cities at the time. 

However, by then the damage had been done. A report after the war concluded that 27,927 Boers (of whom 24,074 [50% of the Boer child population] were children under 16) had died of starvation, disease and exposure in the concentration camps. In all, about one in four (25%) of the Boer inmates, mostly children, died. 

"Improvements were much slower in coming to the black camps." . It is thought that about 12% of black African inmates died (about 14,154) but the precise number of deaths of black Africans in concentration camps is unknown as little attempt was made to keep any records of the 107,000 black Africans who were interned. It is however worth noting that Emily Hobhouse and the Fawcett Commission only ever concerned themselves with the camps that held white Boer refugees. No one paid much attention to what was going on in the camps that held native refugees.

â€œThe main decisions (or their absence) had been left to the soldiers, to whom the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority. [It was only] ... ten months after the subject had first been raised in Parliamentâ€¦[and after public outcry and after the Fawcett Commission that remedial action was taken and] ... the terrible mortality figures were at last declining. In the interval, at least twenty thousand whites and twelve thousand coloured people had died in the concentration camps, the majority from epidemics of measles and typhoid that could have been avoided.â€? 

It has been argued that "this was not a deliberately genocidal policy; rather it was the result of disastrous lack of foresight and rank incompetence on part of the [British] military" . and that "Kitchener no more desired the deaths of women and children in the camps than of the wounded Dervishes after Omdurman, or of his own soldiers in the typhoid stricken hospitals of Bloemfontein." . 

However, to Kitchener and the British Command "the life or death of the 154,000 Boer and African civilians in the camps rated as an abysmally low priority" against military objectives. As the Fawcett Commission was delivering its recommendations, Kitchener wrote to St John Brodrick defending his policy of sweeps, and emphasizing that no new Boer families were being brought in unless they were in danger of starving. This was disingenious in the extreme; the countryside had by then been devastated under the "Scorched Earth" policy (the Fawcett Commission in December 1901 in its recommendations commented that: "to turn 100,000 people now being held in the concentration camps out on the veldt to take care of themselves would be cruelty") and now that the New Model counter insurgency tactics were in full swing it made cynical military sense to leave the Boer families in desperate conditions in the countryside. 

"At [the Vereeniging negotiations in May 1902] Boer leader Louis Botha stated that he had tried to send [Boer] families in to the British, but they had refused to receive them," writes S.B. Spies, who then quotes a Boer Commandant referring to Boer women and children made refugees by Britain's scorched-earth policy, "Our families are in a pitiable condition and the enemy uses those families to force us to surrender." Spies adds, "and there is little doubt that that was indeed the intention of Kitchener when he had issued instructions that no more families were to be brought into the concentration camps."

Thomas Pakenham writes of Kichener's policy u-turn, "No doubt the continued 'hullabaloo' at the death-rate in these concentration camps, and Milner's belated agreement to take over their administration, helped changed Kitchener's mind [some time at the end of 1901]. By mid-December at any rate, Kitchener was already circulating all column commanders with instructions not to bring in women and children when they cleared the country, but to leave them with the guerrillas. . . . Viewed as a gesture to Liberals, on the eve of the new session of Parliament at Westminster, it was a shrewd political move. It also made excellent military sense, as it greatly handicapped the guerrillas, now that the drives were in full swing. . . . It was effective precisely because, contrary to the Liberals' convictions, it was less humane than bringing them into camps, though this was of no great concern to Kitchener."

The incompetence of British command in administering the concentration camps, and the callousness of British command in the use of civilians to achieve military objectives, were to contribute to the Conservatives' spectacular election defeat in 1906, deepen the international isolation of Britain, and resonate across the 20th century.

The first sizable batch of Boer prisoners of war taken by the British consisted of those captured at the Battle of Elandslaagte on 21 October 1899. At first many were put on ships, but as numbers grew, the British decided they didn't want them kept locally. The capture of 400 POWs in February 1900 was a key event, which made the British realise they could not accommodate all POWs in South Africa. The British feared they could be freed by sympathetic locals. They already had trouble supplying their own troops in South Africa, and did not want the added burden of sending supplies for the POWs. Britain therefore chose to send many POWs overseas.

The first overseas (off African mainland) camps were opened in Saint Helena, which ultimately received about 5,000 POWs. About 5,000 POWs were sent to Ceylon. Other POWs were sent to Bermuda and India. Some POWs were even sent outside the British Empire, with 1443 Boers (mostly POWs) sent to Portugal. No evidence exists of Boer POWs being sent to the Dominions of the British Empire such as Australia, Canada or New Zealand.

The British offered terms of peace on various occasions, notably in March 1901, but were rejected by Botha. The last of the Boers surrendered in May 1902 and the war ended with the Treaty of Vereeniging signed on 31 May 1902. Although the British had won, this came at a cost; the Boers were given Â£3,000,000 for reconstruction and were promised eventual limited self-government granted in 1906 and 1907. The treaty ended the existence of the South African Republic and the Orange Free State as independent Boer republics and placed them within the British Empire. The Union of South Africa was established as a member of the Commonwealth in 1910.

In all, the war had cost around 75,000 lives; 22,000 British soldiers (7,792 battle casualties, the rest through disease), between 6,000 and 7,000 Boer soldiers, and, mainly in the concentration camps, between 20,000 to 28,000 Boer civilians (mainly women and children) and perhaps 20,000 black Africans (both on the battlefield and in the concentration camps).

The Second Boer War cast long shadows over the history of the region. The predominantly agrarian society of the former Afrikaner-Dutch republics was profoundly and fundamentally affected by the scorched earth policy of Roberts and Kitchner, and the devastation of both Boer and black African populations in the concentration camps and through war, and enforced exile. Many were unable to return to the farms at all; others attempted to do so but were forced to abandon the farms as unworkable given the damage caused by farm burning and salting of the fields in the course of the scorched earth policy, and given the decimation of family members. Destitute Boers and black Africans swelled the ranks of the unskilled urban poor competing with the "uitlanders" on the mines.  

The postwar reconstruction administration was presided over by Alfred (Lord) Milner and his largely Oxford trained "kindergarten". â€œIn the aftermath of the war, an imperial administration freed from accountability to a domestic electorate set about reconstructing an economy that was by then predicated unambiguously on gold. At the same time, British civil servants, municipal officials, and their cultural adjuncts were hard at work in the heartland of the former Afrikaner-Dutch republics helping to forge new identitiesâ€”first as "British South Africans" and then, later still, as white "South Africans." Some scholars, for good reasons, identify these new identities as partly underpinning the act of union that followed in 1910. Although challenged by an Afrikaner rebellion only four years later, they did much to shape South African politics between the two world wars and right up to the present dayâ€?

The Boers referred to the two wars as the Freedom Wars. The hard core of Boers who fought to the end and wanted to continue the fight were known as "bittereinders" (or irreconcilables) and at the end of the war a number of Boer fighters such as Deneys Reitz chose exile rather than sign an undertaking that they would abide by the peace terms. Over the following decade, many returned to South Africa and never signed the undertaking. Some, like Reitz, eventually reconciled themselves to the new status quo, but others could not. At the start of World War I a crisis ensued when the South African Government led by Louis Botha and including other former Boer fighters such as Jan Smuts, declared for Britain and agreed to send troops to take over the German colony of South West Africa (Namibia). Many Boers were opposed to fighting for Britain, especially against Germany which had been sympathetic to their struggle. A number of bittereinders and their allies took part in a revolt known as the Maritz Rebellion. This was quickly suppressed and in 1916, the leading Boer rebels in the Maritz Rebellion got off lightly (especially compared with the fate of leading Irish rebels of the Easter Rising), with terms of imprisonment of six and seven years and heavy fines. Two years later, they were released from prison, as Louis Botha recognised the value of reconciliation. Thereafter the bittereinders concentrated on political organisation within the constitutional system and built up what later became the National Party which took power in 1948 and dominated the politics of South Africa from the late 1940s until the early 1990s, under the apartheid system.

During the conflict, 78 Victoria Crosses (VC) â€?the highest and most prestigious award in the British armed forces for bravery in the face of the enemy â€?were awarded to British and Colonial soldiers. See List of Boer War Victoria Cross recipients.

Many Irish nationalists sympathised with the Boers, seeing them as a people oppressed by British imperialism, much like themselves. Irish miners already in the Transvaal at the start of the war formed the nucleus of two Irish commandos. The Second Irish Brigade was headed up by an Australian of Irish parents, Colonel Arthur Lynch. In addition, small groups of Irish volunteers went to South Africa to fight with the Boers â€?this despite the fact that there were many Irish troops fighting with the British army. In Britain, the "Pro-Boer" campaign expanded, with writers often idealizing the Boer society.

The war also highlighted the dangers of Britain's policy of non-alignment and deepened her isolation. The 1900 UK general election, also known as the "Khaki election", was called by the Prime Minister, Lord Salisbury, on the back of recent British victories. There was much enthusiasm for the war at this point, resulting in a victory for the Conservative government.

However, public support quickly waned as it became apparent that the war would not be easy and it dragged on, partially contributing to the Conservatives' spectacular defeat in 1906. There was public outrage at the use of scorched earth tactics â€?the forced clearance of women and children, the destruction of the countryside, burning of Boer homesteads and poisoning of wells, for example â€?and the conditions in the concentration camps. It also became apparent that there were serious problems with public health in Britain: up to 40% of recruits in Britain were unfit for military service, suffering from medical problems such as rickets and other poverty-related illnesses. This came at a time of increasing concern for the state of the poor in Britain.

Having taken the country into a prolonged war, the electorate delivered a harsh verdict at the first general election after the war was over. Balfour, succeeding his uncle Lord Salisbury in 1903 immediately after the war, took over a Conservative party that had won two successive landslide majorities but led it to a landslide defeat in 1906.

The war and its aftermath reverberated across the Empire. The importing to South Africa and use (especially on the gold mines) of Chinese labour , known as Coolies, after the war by the governor of the new crown colonies, Lord Milner as cheap labour to repress local workers and break strikes, also caused much revulsion in the UK and Australia. The Chinese workers were themselves often kept in appalling conditions, receiving only a small wage and isolated from the local population â€?revelations of homosexual acts between those forbidden contact with the local population and the services of prostitutes led to further public shock. Some believe the Chinese slavery issue can be seen as the climax of public antipathy with the war.

The vast majority of troops fighting for the United Kingdom came from the UK. However, a number did come from other parts of the Empire. These countries had their own internal disputes over whether they should remain tied to the United Kingdom, or have full independence, which carried over into the debate around the sending of forces to assist the United Kingdom. Though not fully independent on foreign affairs, these countries did have local say over how much support to provide, and the manner in which it would be provided. Ultimately, Canada, Australia, and New Zealand all sent volunteers to aid the United Kingdom. Australia provided the largest number of troops followed by Canada. Troops were also raised to fight with the British from the Cape Colony and the Colony of Natal. Some Boers fighters such as Jan Smuts and Louis Botha were technically British subjects as they came from the Cape Colony and Colony of Natal respectively.

The Australian climate and geography were far closer to that of South Africa than most other parts of the empire, so Australians could adapt quickly to service in the war. Initially the British army wanted trained foot-soldiers from Australia rather than mounted infantry.

The Swan river colony (1829-1901, 1901-today the State of Western Australia) made a decision to fight in the war because much of the population of the colony had originated from Greater Britain (England, Wales, Ireland) and they wanted to stand up for their 'country'.

From 1899 to 1901 the six separate self-governing colonies in Australia sent their own contingents. The colonies formed the Commonwealth of Australia in 1901, and the new federal government sent "Commonwealth" contingents to the war. The Boer War was thus the first war in which the Commonwealth of Australia fought.

Enlistment in all Australian contingents totalled 16,175, though about a thousand men did a second tour of duty. A total of 267 died from disease, 251 were killed in action or died from wounds sustained in battle. A further 43 men were reported missing. Another five to seven thousand Australians served in "irregular" regiments raised in South Africa. Perhaps five hundred Australian irregulars were killed. In total, then, twenty thousand or more Australians served and about a thousand were killed.

Australian troops served mostly among the army's "mounted rifles".

When the war began some Australians, like some Britons, opposed it. As the war dragged on some Australians became disenchanted, in part because the sufferings of Boer civilians were reported in the press. In an interesting twist (for Australians), when the British missed capturing President Paul Kruger, as he escaped Pretoria during its fall in June 1900, a Melbourne Punch, 21 June 1900, cartoon depicted how the War could be won, using the Kelly Gang. 

The convictions and executions of two Australian Lieutenants, Breaker Morant and Peter Handcock in 1902, and the imprisonment of a third, George Witton, had little impact on the Australian public at the time despite later legend. After the war, though, Australians joined an empire-wide campaign that saw Witton released from gaol. Much later, Australians came to see the execution of Morant and Handcock as instances of wrongful British power over Australian lives as illustrated in the 1980 Australian film Breaker Morant.

A few Australians fought on the Boer side.The most famous and colourful character was Colonel Arthur Alfred Lynch, formerly of Ballarat, Victoria, who raised the Second Irish Brigade and appears in an Australian novel by Antony O'Brien called Bye-Bye Dolly Gray.

At first, Canadian Prime Minister Wilfrid Laurier tried to keep Canada out of the war.  The Canadian government was divided between those, primarily French Canadians, who wished to stay out of the war and others, primarily English Canadians, who wanted to join with the UK in her fight. In the end, Laurier compromised by agreeing to support the British by providing volunteers, equipment and transportation to South Africa. The UK would be responsible for paying the troops and returning them to Canada at the end of their service. The Boer War marked the first occasion in which large contingents of Canadian troops served abroad. The 1st Canadian Contingent was composed of 1000 men recruited from the Canadian Militia to form the 2nd (Special Service) Battalion of The Royal Canadian Regiment. This contingent served under the command of the Permanent Force officer William Dillon Otter.

The Battle of Paardeberg in February 1900 represented the second time Canadian Troops saw battle abroad (although there was a long tradition of Canadian service in the British Army and Royal Navy), the first being the Canadian involvement in the Nile Expedition of 1884-85.

Canadians also saw action at the Battle of Faber's Put on 30 May 1900.

On November 7, 1900, the Royal Canadian Dragoons engaged the Boers in the Battle of Leliefontein, where they saved the British guns from capture during a retreat from the banks of the Komati River. The Royal Canadian Dragoons had three Victoria Cross recipients in this war: Lieutenant Turner, Lieutenant Cockburn, and Sergeant Holland, as did Strathcona's Horse in Arthur Richardson.

Ultimately, over 8,600 Canadians volunteered to fight in the South African War. However, not all saw action since many landed in South Africa after the hostilities ended while others (including the 3rd (Special Service) Battalion, The Royal Canadian Regiment) performed garrison duty in Halifax, Nova Scotia so that their British counterparts could join at the front. The 2nd Battalion, The Royal Canadian Regiment, took part in Bloody Sunday, where at the Battle of Paardeberg the British and Canadian forces suffered more casualties than on any other day of the war. Later on, contingents of Canadians served with the paramilitary South Africa Constabulary. Approximately 277 Canadians died in the South Africa War: 89 men were killed in action, 135 died of disease, and the remainder died of accident or injury. 252 were wounded.

When the Second Boer War seemed imminent, New Zealand offered its support.  On 28 September 1899, Prime Minister Richard Seddon asked Parliament to approve the offer to the imperial government of a contingent of mounted rifles and the raising of such a force if the offer were accepted and thus becoming the first British Colony to send troops to the Boer War. The British position in the dispute with the Transvaal was 'moderate and righteous', he maintained. He stressed the 'crimson tie' of Empire which bound New Zealand to the Mother-country and the importance of a strong British Empire for the colony's security.

By the time peace was concluded two and a half year later, ten contingents of volunteers, totalling nearly 6,500 men with 8,000 horses had sailed for Africa, along with doctors, nurses, veterinary surgeons and a small number of school teachers. Seventy New Zealanders died in the war as the result of action, with another 158 killed accidentally or dying by disease. 

In many ways, the South African war set the pattern for New Zealand's later involvement in the two World Wars. Specially raised units, consisting mainly of volunteers, were dispatched overseas to serve with forces from elsewhere in the British Empire. The success enjoyed by the New Zealand troops fostered the idea that New Zealanders were naturally good soldiers, who required only a modicum of training to perform creditably.

During the war, the British army also included substantial contingents from South Africa itself. There were large communities of English-speaking immigrants and settlers in Natal and Cape Colony (especially around Capetown and Grahamstown), which formed volunteer units which took the field, or local "town guards". At one stage of the war, a "Colonial Division", consisting of five light horse and infantry units under Brigadier General Edward Brabant, took part in the invasion of the Orange Free State. Part of it withstood a siege by Christiaan De Wet at Wepener on the borders of Basutoland.

Another source of volunteers was the uitlander community, many of whom hastily left Johannesburg in the days immediately preceding the war. Some of them, stung by the accusations of cowardice and treachery in the aftermath of the Jameson raid, formed the Imperial Light Horse and fought in the first engagements of the war in Natal.

Volunteers from the Empire (Australia, Canada and New Zealand) who were not selected for the official contingents from their countries travelled privately to South Africa and joined local units in South Africa, eg the Canadian Scouts or Doyleâ€™s Australian Scouts. There were also European volunteer units from India and Ceylon, though the British Government refused offers of non-white troops from the Empire. British volunteers served in the Imperial Yeomanry and the Scottish Horse.

Some Cape Coloureds also volunteered early in the war, but later some of them were effectively conscripted and kept in segregated units. As a community, they received comparatively little reward for their services. Africans were also employed as scouts, and later in the war some were armed. This was ostensibly to guard herds of oxen against poachers, but the measure infuriated the Boers.

Later during the war, Kitchener attempted to form a Boer Police Force, as part of his efforts to pacify the occupied areas and effect a reconciliation with the Boer community. The members of this force were despised as traitors by the Boers still in the field. Those Boers who attempted to remain neutral after giving their parole to British forces were derided as "hansoppers" (hands-uppers) and were often coerced into giving support to the Boer guerillas. (This was one of the reasons for the British ruthlessly scouring the countryside of people, livestock and anything else which the Boer commandos might find useful.)

Like the Canadian and particularly the Australian and New Zealand contingents, many of the volunteer units formed by the South Africans were "light horse" or mounted infantry, well suited to the countryside and manner of warfare, although as many of them were normally city-dwellers they lacked the "natural" ability of some of the country-raised Boers. Some regular British officers scorned their comparative lack of formal discipline, but the light horse units were hardier and more suited to the demands of campaigning than the overloaded British cavalry, who were still obsessed with the charge with lance or sabre.

At their peak, 24,000 South Africans (including volunteers from the Empire) served in the field in various "Colonial" units. Notable units (in addition to the Imperial Light Horse) were the South African Light Horse, Rimington's "Tigers", Kitchener's Horse and the Imperial Light Infantry.



A number of good first hand Australian writings exist on the Boer War. These include:

Sound historical works of Australians at the Boer War include:

Several fiction novels on Australians at the Boer War include:

An historical fiction book with in-depth probing into the war is;

"Antony O'Brien, ''Bye-Bye Dolly Gray'', Artillery Publishing, Hartwell, 2006" ISBN 0 9758013 2 5 (historical fiction about the war from Australian perspective)

Recommended reading:Re: New Zealand Reference involvement. Book "Soldier Boy" A young New Zealander writes home from the Boer War, compiled by Kingsley Field. First published in 2007 by New Holland Publishers (NZ) Ltd. www.newhollandpublishers.co.nz ISBN 978 186966 177 9. Letters written by Harry Gilbert to his family in New Zealand from April 1901.











Delaware () is a state located on the Atlantic Coast in the Mid-Atlantic region of the United States. The state is named after Delaware Bay and River, which were named for Thomas West, 3rd Baron De La Warr (1577â€?618). Population estimates by the Census Bureau for 2005 place the population of Delaware at 843,524. Despite ranking 45th in population, it is the seventh most densely populated state, with a population density of 320 more people per square mile than the national average, ranking ahead of states such as Florida, California, and Texas.

The state's motto, "Liberty and Independence" is inscribed on the coat of arms, which is incorporated into both the state seal and the state flag. The state's official nickname, "The First State" commemorates the fact that on December 7, 1787, Delaware became the first of the 13 original states to  the United States Constitution. Commemorating Delaware's ratification, Constitution Park (one block from where Dover's Golden Fleece Tavern once stood) features a four-foot cube upon which is inscribed the entire document as it has evolved. Delaware has also been called the "Blue Hen State", referring to the official state bird, the Blue Hen Chicken, which was carried with the Delaware Revolutionary War soldiers for cockfighting, and the "Diamond State". The ferocity of the Blue Hen Chickens carried by Captain Jonathan Caldwell's men in the Revolutionary Army and the prowess of his company led to the nickname of "Caldwell's Gamecocks" and the nickname of the University of Delaware Fightin' Blue Hens. Along with other traditional symbols such as an official state tree (the American holly) and flower (the peach blossom), legislature has adopted the Delaware Diamond, the first star on the International Star Registry ever to be registered to an American State.



Delaware is 96 miles long and ranges from 9 to 35 miles across, totaling 1,954 square miles and making it the second-smallest state in the United States after Rhode Island. 

Delaware is bounded to the north by Pennsylvania, to the east by the Delaware River, Delaware Bay, New Jersey and the Atlantic Ocean and to the west and south by Maryland. Small portions of Delaware are also situated on the far, or eastern, side of the Delaware River estuary, and these small parcels share land boundaries with New Jersey.

The state of Delaware, together with the Eastern Shore counties of Maryland and two counties of Virginia, form the Delmarva Peninsula, a geographical unit stretching far down the Mid-Atlantic Coast.

The definition of the northern boundary of the state is highly unusual. Most of the boundary between Delaware and Pennsylvania is defined by an arc extending 12 miles (19 km) from the cupola of the courthouse in New Castle, and is referred to as the Twelve-Mile Circle. This is the only true-arc political boundary in the United States. This border extends all of the way to the low-tide mark on the New Jersey shore, which continues down the shoreline until it again reaches the twelve-mile arc in the south; then the boundary continues in a more conventional way in the middle of the main channel (thalweg) of the Delaware River Estuary. A portion of this arc extends into Maryland to the west, and the remaining western border is a tangent to this arc that runs a bit to the east. The Wedge of land between the arc and the Maryland border was claimed by both Delaware and Pennsylvania until 1921, when Delaware's claim was confirmed.

Delaware is subdivided into three counties: from north to south, New Castle, Kent County and Sussex.See also: List of counties in Delaware

Delaware is on a level plain; the highest elevation, located at Ebright Azimuth, near Concord High School, Wilmington, does not even rise 450 feet above sea level. The northern part is associated with the Appalachian Piedmont and is full of hills with rolling surfaces. South of Newark and Wilmington, the state follows the Atlantic Coastal Plain with flat, sandy, and, in some parts, swampy ground. A ridge about 75 to 80 feet in altitude extends along the western boundary of the state and is the drainage divide between the two major water bodies of the Delaware River and several streams falling into Chesapeake Bay in the west.

Since almost all of Delaware is a part of the Atlantic Coastal Plain, the climate is moderated by the effects of the ocean. The state is somewhat of a transitional zone between a humid subtropical climate and a continental climate. Despite its small size (roughly 100 miles from its northernmost to southernmost points), there is significant variation in mean temperature and amount of snowfall between Sussex County and New Castle County. The southern portion of the state has a somewhat milder climate and a longer growing season than the northern portion of the State. Furthermore, the transitional climate of Delaware supports a surprising variety of vegetation. At Trap Pond State Park in Sussex County, bald cypress grow -- this is thought to be one of the northernmost stands of these trees. The vegetation in New Castle County, on the other hand, is more typical of that of the northeastern United States. All parts of Delaware have relatively hot, humid summers. While Sussex and Kent Counties are considered to fall in the humid subtropical climate zone, there is some debate about whether northern New Castle County falls in the humid subtropical climate zone or warm continental climate.



Before Delaware was settled by European colonists, the area was home to the Eastern Algonquian tribes known as the Unami Lenape or Delaware throughout the Delaware valley, and the Nanticoke along the rivers leading into the Chesapeake Bay. The Unami Lenape in the Delaware Valley were closely related to Munsee Lenape tribes along the Hudson River. They had a settled hunting and agricultural society, and they rapidly became middlemen in an increasingly frantic fur trade with their ancient enemy, the Minqua or Susquehannock. With the loss of their lands on the Delaware River and the destruction of the Minqua by the Iroquois of the Five Nations in the 1670s, the remnants of the Lenape left the region and moved over the Alleghany Mountains by the mid-18th century.

The Dutch were the first Europeans to settle in present-day Delaware by establishing a trading post at Zwaanendael, near the site of Lewes in 1631. Within a year all the settlers were killed in a dispute with area Indian tribes. In 1638, a Swedish trading post and colony was established at Fort Christina (now in Wilmington) by Dutchman Peter Minuit at the head of a group of Swedes, Finns and Dutch. Thirteen years later, the Dutch, reinvigorated by the leadership of Peter Stuyvesant, established a new fort in 1651 at present-day New Castle, and in 1655 they took over the entire Swedish colony, incorporating it into the Dutch New Netherland.

Only nine years later, in 1664, the Dutch were themselves forcibly removed by a British expedition under the direction of James, the Duke of York. Fighting off a prior claim by CÃ¦cilius Calvert, 2nd Baron Baltimore, Proprietor of Maryland, the Duke passed his somewhat dubious ownership on to William Penn in 1682. Penn strongly desired access to the sea for his Pennsylvania province and leased what then came to be known as the "Lower Counties on the Delaware" from the Duke.

Penn established representative government and briefly combined his two possessions under one General Assembly in 1682. However, by 1704 the Province of Pennsylvania had grown so large that their representatives wanted to make decisions without the assent of the Lower Counties and the two groups of representatives began meeting on their own, one at Philadelphia, and the other at New Castle. Penn and his heirs remained proprietors of both and always appointed the same person Governor for their Province of Pennsylvania and their territory of the Lower Counties. The fact that Delaware and Pennsylvania shared the same governor was not unique. During much of the colonial period, New York and New Jersey shared a governor, as did Massachusetts and New Hampshire.

Dependent in early years on indentured labor, Delaware imported more slaves as the number of English immigrants decreased with better economic conditions in England. The colony became a slave society.

Like the other middle colonies, the Lower Counties on the Delaware initially showed little enthusiasm for a break with Britain. The citizenry had a good relationship with the Proprietary government, and generally were allowed more independence of action in their Colonial Assembly than in other colonies. Nevertheless, there was strong objection to the seemingly arbitrary measures of Parliament, and it was well understood that the territory's very existence as a separate entity depended upon its keeping step with its powerful neighbors, especially Pennsylvania.

So it was that New Castle lawyer Thomas McKean denounced the Stamp Act in the strongest terms, and Kent County native John Dickinson, became the "Penman of the Revolution." Anticipating the Declaration of Independence, Patriot leaders Thomas McKean and Caesar Rodney convinced the Colonial Assembly to declare itself separated from British and Pennsylvania rule on June 15, 1776, but the person best representing Delaware's majority, George Read, could not bring himself to vote for a Declaration of Independence. Only the dramatic overnight ride of Caesar Rodney gave the delegation the votes needed to cast Delaware's vote for Independence. Once the Declaration was adopted, however, Read signed the document.

Initially led by John Haslet, Delaware provided one of the premier regiments in the Continental Army, known as the "Delaware Blues" and nicknamed the "Blue Hen Chickens." In August 1777, General Sir William Howe led a British army through Delaware on his way to a victory at the Battle of Brandywine and capture of the city of Philadelphia. The only real engagement on Delaware soil was fought on September 3, 1777, at Cooch's Bridge in New Castle County. It is believed to be the first time that the Stars and Stripes was flown in battle.

Following the Battle of Brandywine, Wilmington was occupied by the British, and State President John McKinly was taken prisoner. The British remained in control of the Delaware River for much of the rest of the war, disrupting commerce and providing encouragement to an active Loyalist portion of the population, particularly in Sussex County. Only the repeated military activities of State President Caesar Rodney were able to control them.

Following the American Revolution, statesmen from Delaware were among the leading proponents of a strong central United States government with equal representation for each state. Once the Connecticut Compromise was reachedâ€”creating a U.S. Senate and U.S. House of Representativesâ€”the leaders in Delaware were able to easily secure ratification of the U.S. Constitution on December 7, 1787, making Delaware the first state to do so.

Many colonial settlers came from Maryland and Virginia which had been experiencing a population boom. The economies of these colonies were largely based on tobacco and were increasingly dependent on slave labor. Most of the English colonists arrived as indentured servants, hiring themselves out as laborers for a fixed period to pay for their passage. In the early years the line between indentured servants and African slaves or laborers was fluid. Most of the free African-American families in Delaware before the Revolution had migrated from Maryland to find more affordable land. They were descendants chiefly of relationships or marriages between free or servant white women and enslaved, servant or free African or African-American men. As the flow of indentured laborers to the colony decreased with improving economic conditions in England, more slaves were imported. The economy's early growth and prosperity was based on slave labor, devoted first to the production of tobacco. 

At the end of the colonial period, the number of enslaved people in Delaware began to decline. Shifts in the agriculture economy from tobacco to mixed farming was less labor intensive, the efforts of local Methodists and Quakers to encourage manumission, and greater governmental regulation were all factors. Attempts to abolish slavery failed by narrow margins in the legislature. Nonetheless, by the 1860 census there were only about 1,800 slaves among a total population of 90,000 in the state. There were nearly 20,000 free African Americans. When John Dickinson freed his slaves in 1777, he was Delaware's largest slave owner with 37 slaves. By 1860 the largest slaveholder owned only 16 slaves. 

The oldest black church in the country was chartered in Delaware by former slave Peter Spencer in 1813 as the "Union Church of Africans." This is now the the A.U.M.P. Church. The Big August Quarterly began in 1814 and is the oldest such cultural festival in the country.

During the American Civil War, Delaware was a slave state that remained in the Union (Delaware voted not to secede on January 3, 1861). Delaware had been the first state to embrace the Union by ratifying the constitution and would be the last to leave it, according to Delaware's governor at the time. While most Delaware citizens who fought in the war served in the regiments of the state, some served in companies on the Confederate side in Maryland and Virginia Regiments. Delaware is notable for being the only slave state not to assemble Confederate regiments or militia groups on its own.



The five largest ancestries in Delaware are: African American (19.2%), Irish (16.6%), German (14.3%), English (12.1%), Italian (9.3%). Delaware has the highest proportion of African American residents of any state north of Maryland, and had the largest population of free blacks (17%) prior to the Civil War.

The center of population of Delaware is located in New Castle County, in the town of Townsend.

As of 2000, 90.5% of Delaware residents age 5 and older speak only English at home; 4.7% speak Spanish. French is the third most spoken language at 0.7%, followed by Chinese at 0.5% and German at 0.5%.

In 2006, legislation was proposed in Delaware that would designate English as the official language.

The religious affiliations of the people of Delaware are:

(source: American Religious Identification Survey, City University of New York)

Delaware is home to the Roman Catholic Diocese of Wilmington and the Episcopal Diocese of Delaware. The A.U.M.P. Church, the oldest African-American denomination in the nation, was founded in Wilmington and still has a very substantial presence in the state. Delaware also hosts an Islamic mosque in the Ogletown area, as well as a Hindu temple in Hockessin.

Delaware is home to approximately 20,000 Jews, who are served by the Jewish Community Center in Brandywine (outside of Wilmington) and by a number of educational, social and cultural agencies supported by the Jewish Federation of Delaware. Synagogues include Congregation Beth Emeth (Reform) in Wilmington, Congregation Beth El (Reconstructionist) in Newark, and Congregation Beth Shalom (Conservative) in Wilmington, Congregation Beth Sholom (Conservative) in Dover, and Adas Kodesh Shel Emeth (Traditional) in Wilmington. There is also a Lubavitcher community center and synagogue in Brandywine Hundred.

The gross state product of Delaware in 2003 was $49 billion. The per capita personal income was $34,199, ranking 9th in the nation. In 2005, the average weekly wage was $937, ranking 7th in the nation.

Delaware's agricultural output consists of poultry, nursery stock, soybeans, dairy products and corn. Its industrial outputs include chemical products, processed foods, paper products, and rubber and plastic products. Delaware's economy generally outperforms the national economy of the United States.

The state's largest employers are: 

Dover Air Force Base, located in the state capital of Dover, is one of the largest Air Force bases in the country and is a major employer in Delaware. In addition to its other responsibilities, the base serves as the entry point and mortuary for American military persons (and some U.S. government civilians) who die overseas.

Delaware has 6 different income tax brackets, ranging from 2.2% to 5.95%. The state does not assess sales tax on consumers. The state does, however, impose a tax on the gross receipts of most businesses. Business and occupational license tax rates range from 0.096% to 1.92%, depending on the category of business activity.

Delaware does not assess a state-level tax on real or personal property. Real estate is subject to county property taxes, school district property taxes, vocational school district taxes, and, if located within an incorporated area, municipal property taxes.

Title 4, chapter 7 of the Delaware Code stipulates that alcoholic liquor only be sold in specifically licensed establishments, and only between 9:00 AM and 1:00 AM.

The transportation system in Delaware is under the governance and supervision of the Delaware Department of Transportation, also known as "DelDOT". DelDOT manages programs such as a Delaware Adopt-a-Highway program, major road route snow removal, traffic control infrastructure (signs and signals), toll road management, Delaware Division of Motor Vehicles, the Delaware Transit Corporation (branded as "DART First State", the state government public transportation organization), among others. Almost ninety percent of the state's public roadway miles are under the direct maintenance of DelDOT which far exceeds the United States national average of twenty percent for state department of transportation maintenance responsibility; the remaining public road miles are under the supervision of individual municipalities.

One major branch of the U.S. Interstate Highway System, Interstate 95, crosses Delaware southwest-to-northeast across New Castle County. In addition to I-95, there are six U.S. highways that serve Delaware: U.S. Route 9, U.S. Route 13, U.S. Route 40, U.S. Route 113, U.S. Route 202, and U.S. Route 301. There are also several state highways that cross the state of Delaware; a few of them include Delaware Route 1, Delaware Route 9, and Delaware Route 404. U.S. 13 and DE Rt. 1 are primary north-south highways connecting Wilmington and Pennsylvania with Maryland, with DE 1 serving as the main route between Wilmington and the Delaware beaches. DE Rt. 9 is a north-south highway connecting Dover and Wilmington via a scenic route along the Delaware Bay. U.S. 40, is a primary east-west route, connecting Maryland with New Jersey. DE Rt. 404 is another primary east-west highway connecting the Chesapeake Bay Bridge in Maryland with the Delaware beaches. The state also operates two toll highways, the Delaware Turnpike, which is Interstate 95 between Maryland and New Castle and the Korean War Veterans Memorial Highway, which is DE Rt. 1 between Dover and Interstate 95 between Wilmington and Newark.

A bicycle route, Delaware Bicycle Route 1, spans the north-south length of the state from the Maryland border in Fenwick Island to the Pennsylvania border north of Montchanin. It is the first of several signed bike routes planned in Delaware.

Delaware has around 1,450 bridges, of which ninety-five percent are under the supervision of DelDOT. About thirty percent of all Delaware bridges were built prior to 1950 and about sixty percent of the number are included in the National Bridge Inventory. Some bridges not under DelDOT supervision includes the four bridges on the Chesapeake and Delaware Canal, which are under the jurisdiction of the U.S. Army Corps of Engineers, and the Delaware Memorial Bridge, which is under the bi-state Delaware River and Bay Authority.

There are three ferries that operate in the state of Delaware:

Amtrak has two stations in Delaware along the Northeast Corridor; the relatively quiet Newark Rail Station in Newark, and the busier Wilmington Rail Station in Wilmington. The Northeast Corridor is also served by SEPTA's R2 Regional Rail line, which serves Claymont, Wilmington, Churchmans Crossing, and Newark. The major freight railroad in Delaware is the Class 1 Norfolk Southern, which provides service to most of Delaware. It connects with two shortline railroads, the Delaware Coast Line Railway and the Maryland & Delaware Railroad. These two shortlines serve local customers in Sussex County. Another Class 1 railroad, CSX, passes through northern New Castle County parallel to the Amtrak Northeast Corridor.

The public transportation system, DART First State, was named "Most Outstanding Public Transportation System" in 2003 by the American Public Transportation Association. Coverage of the system is broad within northern New Castle County with close association to major highways in Kent and Sussex Counties. The system includes bus, subsidized passenger rail operated by Philadelphia transit agency SEPTA, and subsidized taxi and paratransit modes, the latter consisting of a state-wide door-to-door bus service for the elderly and disabled.

Delaware is the only state in the country without commercial air service. On June 29, 2006, Atlantic Southeast Airline, a subsidiary of Delta Air Lines, began two-a-day flights between Atlanta's Hartsfield International to New Castle Airport. However, the flights ended on September 6, 2007. This is temporary as Skybus Airlines has announced that it will commence service between its Columbus, OH and Greensboro, NC focus cities starting in March of 2008 to serve the Philadelphia, PA area from New Castle Airport.

Delaware's fourth and current constitution, adopted in 1897, provides for executive, judicial and legislative branches.

Delaware General Assembly consists of a House of Representatives with 41 members and a Senate with 21 members. It sits in Dover, the state capital. Representatives are elected to two-year terms, while senators are elected to four-year terms. The Senate confirms judicial and other nominees appointed by the governor.

The Delaware Constitution establishes a number of courts:

Minor non-constitutional courts include the Justice of the Peace Courts and Aldermen's Courts.

Significantly, Delaware has one of the few remaining Courts of Chancery in the nation, which has jurisdiction over equity cases, the vast majority of which are corporate disputes, many relating to mergers and acquisitions. The Court of Chancery and the Supreme Court have developed a worldwide reputation for rendering concise opinions concerning corporate law which generally (but not always) grant broad discretion to corporate boards of directors and officers. In addition, the Delaware General Corporation Law, which forms the basis of the Courts' opinions, is widely regarded as giving great flexibility to corporations to manage their affairs. For these reasons, Delaware is considered to have the most business-friendly legal system in the United States; therefore a great number of companies are incorporated in Delaware, including 60% of the companies listed on the New York Stock Exchange.

The executive branch is headed by the Governor of Delaware. The present governor is Ruth Ann Minner (Democrat), who was elected as the state's first female governor in 2000. The lieutenant governor is John C. Carney, Jr.. Delaware's U.S. Senators are Joseph R. Biden, Jr. (Democrat) and Thomas R. Carper (Democrat). Delaware's single US Representative is Michael N. Castle (Republican).

Delaware has three counties: Kent County, New Castle County, and Sussex County. Each county elects its own legislative body (known in New Castle and Sussex counties as County Council, and in Kent County as Levy Court), which deal primarily in zoning and development issues. Most functions which are handled on a county-by-county basis in other states â€?such as court and law enforcement â€?have been centralized in Delaware, leading to a significant concentration of power in the Delaware state government. The counties were historically divided into hundreds, which were used as tax reporting and voting districts until the 1960s, but now serve no administrative role, their only current official legal use being in real-estate title descriptions.

The Democratic Party holds a plurality of registrations in Delaware. Until the 2000 Presidential election, the state tended to be a Presidential bellwether, sending its three electoral votes to the winning candidate for over 50 years in a row. Bucking that trend, however, in 2000 and again in 2004 Delaware voted for the Democratic candidate. In the 2000 election Delaware voted with the winner of the popular vote, Al Gore, who subsequently lost the Electoral Vote to George W. Bush (see United States Presidential Election, 2000 for more information.) John Kerry won Delaware by eight percentage points with 53.5% of the vote in 2004.

Historically, the Republican Party had an immense influence on Delaware politics, due in large part to the wealthy du Pont family. Ralph Nader assembled a working group to investigate ties between Delaware's politicians and industrialists, resulting in a book published in 1968 entitled The Company State. As DuPont's political influence has declined, so has that of the Delaware Republican Party. The Democrats have won the past four gubernatorial elections and currently hold seven of the nine statewide elected offices (Governor, Lieutenant Governor, Treasurer, Insurance Commissioner, Attorney General, and two U.S. Senators), while the Republicans hold the remaining two (the state's at-large House seat and the office of Auditor). However, this belies the fact that the Democratic Party gains most of its votes from heavily-developed New Castle County, whereas the lesser-populated Kent and Sussex Counties vote Republican.

Wilmington is the state's largest city and its economic hub. It is located within commuting distance of both Philadelphia and Baltimore. Despite Wilmington's size, all regions of Delaware are enjoying phenomenal growth, with Dover and the beach resorts expanding immensely.









Ranked by per capita income



Delaware was the origin of Belton v. Gebhart, one of the four cases which was combined into Brown v. Board of Education, the Supreme Court of the United States decision that led to the end of segregated public schools. Significantly, Belton was the only case in which the state court found for the plaintiffs, thereby ruling that segregation was unconstitutional.

Unlike many states, Delaware's educational system is centralized in a state Superintendent of Education, with local school boards retaining control over taxation and some curriculum decisions.

A "three-tiered diploma" system fostered by Governor Ruth Ann Minner, which awarded "basic," "standard," and "distinguished" high-school diplomas based on a student's performance in the Delaware Student Testing Program, was discontinued by the General Assembly after many Delawareans questioned its fairness.



There are no network broadcast-television stations operating solely in Delaware. A local PBS from Philadelphia (but licensed to Wilmington), WHYY-TV, maintains a studio and broadcasting facility in Wilmington and Dover. Philadelphia's ABC affiliate, WPVI-TV, maintains a news bureau in downtown Wilmington. The northern part of the state is served by network stations in Philadelphia and the southern part by network stations in Baltimore and Salisbury, Maryland. Salisbury's CBS affiliate, WBOC-TV, maintains bureaus in Dover and Milton.

While Delaware has no places designated as national parks, national seashores, national battlefields, national memorials, or national monuments, it does have several National Historic Landmarks. In addition, there are a number of other places of interest such as , , , , , , and other . Delaware also boasts the longest twin span suspension bridge in the world. The state was playfully mocked for its lack of renown as a vacation destination in the movie Wayne's World and the TV show, The Simpsons.



In place of in-state professional sports teams, many Delawareans follow either Philadelphia or Baltimore teams, depending on their location within the state, with Philadelphia teams receiving the largest fan following, though before the Baltimore Ravens entered the NFL, the Washington Redskins had a significant fan base in Sussex County and the Baltimore Colts had a significant fan base in northern counties. In addition, the University of Delaware's football team has a loyal following throughout the state, with Delaware State University's team enjoying popularity on a much lesser scale.

Delaware is home to Dover International Speedway and Dover Downs. DIS, also known as the Monster Mile, hosts two NASCAR races each year. Dover Downs is a popular harness racing facility. In what may be the only co-located horse and car-racing facility in the nation, the Dover Downs track is located inside the DIS track.

Delaware has been home to professional wrestling outfit CZW, particularly the annual Tournament of Death, and ECWA, particularly the annual Super 8 Tournament.

Delaware is home to the Diamond State Games, an amateur Olympic-style sports festival. The event is open to athletes of all ages and is also open to residents beyond the borders of Delaware. The Diamond State Games were created in 2001 and participation levels average roughly 2500 per year in 12 contested sports.

Delaware is also the name of a Native American group (called in their own name Lenni Lenape) that was very influential in the dawning days of the United States. A band of the Nanticoke tribe of Indians still remains in Sussex County.



















Association football, commonly known as football or soccer, is a team sport played between two teams of eleven players, and is widely considered to be the most popular sport in the world. It is a football variant played on a rectangular grass or artificial turf field, with a goal at each of the short ends. The object of the game is to score by manoeuvring the ball into the opposing goal. In general play, the goalkeepers are the only players allowed to use their hands or arms to propel the ball; the rest of the team usually use their feet to kick the ball into position, occasionally using their torso or head to intercept a ball in midair. The team that scores the most goals by the end of the match wins. If the score is tied at the end of the game, either a draw is declared or the game goes into extra time and/or a penalty shootout, depending on the format of the competition.

The modern game was codified in England following the formation of The Football Association, whose 1863 Laws of the Game created the foundations for the way the sport is played today. Football is governed internationally by the FÃ©dÃ©ration Internationale de Football Association (International Federation of Association Football), commonly known by the acronym FIFA. The most prestigious international football competition is the FIFA World Cup, held every four years. This event, the most widely viewed in the world, boasts an audience twice that of the Summer Olympic Games.

Football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a single round ball, known as the football. Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw.

The primary rule is that players (other than goalkeepers) may not deliberately handle the ball with their hands or arms during play (though they do use their hands during a throw-in restart). Although players usually use their feet to move the ball around, they may use any part of their bodies other than their hands or arms.

In typical game play, players attempt to create goal scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a teammate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee. After a stoppage, play recommences with a specified restart.

At a professional level, most matches produce only a few goals. For example, the 2005â€?6 season of the English Premier League produced an average of 2.48 goals per match. The Laws of the Game do not specify any player positions other than goalkeeper, but a number of specialised roles have evolved. Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball in order to pass it to the forwards. Players in these positions are referred to as outfield players, in order to discern them from the single goalkeeper. These positions are further subdivided according to the area of the field in which the player spends most time. For example, there are central defenders, and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time. The layout of a team's players is known as a ''formation''. Defining the team's formation and tactics is usually the prerogative of the team's manager.



Games revolving around the kicking of a ball have been played in many countries throughout history. According to FIFA, the "very earliest form of the game for which there is scientific evidence was an exercise of precisely this skilful technique dating back to the 2nd and 3rd centuries BC in China (the game of cuju)." In addition, the Roman game harpastum may be a distant ancestor of football. Various forms of football were played in medieval Europe, though rules varied greatly by both period and location. 

The Cambridge Rules, first drawn up at Cambridge University in 1848, were particularly influential in the development of subsequent codes, including Association football. The Cambridge Rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world, to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857, which led to formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules.

These ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemason's Tavern in Great Queen Street, London. The only school to be represented on this occasion was Charterhouse. The Freemason's Tavern was the setting for five more meetings between October and December, which eventually produced the first comprehensive set of rules. At the final meeting, the first FA treasurer, the representative from Blackheath, withdrew his club from the FA over the removal of two draft rules at the previous meeting, the first which allowed for the running with the ball in hand and the second, obstructing such a run by hacking (kicking an opponent in the shins), tripping and holding. Other English rugby football clubs followed this lead and did not join the FA, or subsequently left the FA and instead in 1871 formed the Rugby Football Union. The eleven remaining clubs, under the charge of Ebenezer Cobb Morley, went on to ratify the original thirteen laws of the game. These rules included handling of the ball by "marks" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s with the FA absorbing some of its rules until there was little difference between the games.

The laws of the game are currently determined by the International Football Association Board (IFAB). The Board was formed in 1886 after a meeting in Manchester of The Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. The world's oldest football competition is the FA Cup, which was founded by C. W. Alcock and has been contested by English teams since 1872. The first official international football match took place in 1872 between Scotland and England in Glasgow, again at the instigation of C. W. Alcock. England is home to the world's first football league, which was founded in 1888 by Aston Villa director William McGregor. The original format contained 12 clubs from the Midlands and the North of England. The FÃ©dÃ©ration Internationale de Football Association (FIFA), the international football body, was formed in Paris in 1904 and declared that they would adhere to Laws of the Game of the Football Association. The growing popularity of the international game led to the admittance of FIFA representatives to the International Football Association Board in 1913. The board currently consists of four representatives from FIFA and one representative from each of the four British associations.

Today, football is played at a professional level all over the world. Millions of people regularly go to football stadiums to follow their favourite teams, while billions more watch the game on television. A very large number of people also play football at an amateur level. According to a survey conducted by FIFA published in 2001, over 240 million people from more than 200 countries regularly play football. Its simple rules and minimal equipment requirements have no doubt aided its spread and growth in popularity.

In many parts of the world football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations; it is therefore often claimed to be the most popular sport in the world. ESPN has spread the claim that the CÃ´te d'Ivoire national football team helped secure a truce to the nation's civil war in 2005. By contrast, football is widely considered to be the final proximate cause in the Football War in June 1969 between El Salvador and Honduras. The sport also exacerbated tensions at the beginning of the Yugoslav wars of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade devolved into rioting in March 1990.

There are seventeen laws in the official Laws of the Game. The same Laws are designed to apply to all levels of football, although certain modifications for groups such as juniors, seniors or women are permitted. The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of football. The Laws of the Game are published by FIFA, but are maintained by the International Football Association Board, not FIFA itself.



Each team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team; this is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws.

The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials.

A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is three, though the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match.

A game is officiated by a referee, who has "full authority to enforce the Laws of the Game in connection with the match to which he has been appointed" (Law 5), and whose decisions are final. The referee is assisted by two assistant referees. In many high-level games there is also a fourth official who assists the referee and may replace another official should the need arise.



As the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though popular use tends to continue to use traditional units in English-speaking countries with a relatively recent history of metrication, such as Britain.

The length of the pitch for international adult matches is in the range 100â€?10Â m (110â€?20Â yd) and the width is in the range 64â€?5Â m (70â€?0Â yd). Fields for non-international matches may be 91â€?20Â m (100â€?30Â yd) length and 45â€?1Â m (50â€?01Â yd) in width, provided that the pitch does not become square. The longer boundary lines are touchlines or sidelines, while the shorter boundaries (on which the goals are placed) are goal lines. A rectangular goal is positioned at the middle of each goal line. The inner edges of the vertical goal posts must be 7.3Â m (8Â yd) apart, and the lower edge of the horizontal crossbar supported by the goal posts must be 2.44Â m (8Â ft) above the ground. Nets are usually placed behind the goal, but are not required by the Laws.

In front of each goal is an area known as the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5Â m (18Â yd) from the goalposts and extending 16.5Â m (18Â yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks.

A standard adult football match consists of two periods of 45 minutes each, known as halves. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute "half-time" break between halves. The end of the match is known as full-time.

The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is commonly referred to as stoppage time or injury time, and is at the sole discretion of the referee. The referee alone signals the end of the match. In matches where a fourth official is appointed, toward the end of the half the referee signals how many minutes of stoppage time he intends to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee.

In league competitions, games may end in a draw, but in some knockout competitions if a game is tied at the end of regulation time it may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shootouts (known officially in the Laws of the Game as "kicks from the penalty mark") to determine which team will progress to the next stage of the tournament. Goals scored during extra time periods count toward the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament (with goals scored in a penalty shootout not making up part of the final score).

Competitions held over two legs (in which each team plays at home once) may use the away goals rule to determine which team progresses in the event of equal aggregate scores. If the result is still equal, kicks from the penalty mark are usually required, though some competitions may require a tied game to be replayed.

In the late 1990s, the IFAB experimented with ways of creating a winner without requiring a penalty shootout, which was often seen as an undesirable way to end a match. These involved rules ending a game in extra time early, either when the first goal in extra time was scored (golden goal), or if one team held a lead at the end of the first period of extra time (silver goal). Golden goal was used at the World Cup in 1998 and 2002. The first World Cup game decided by a golden goal was France's victory over Paraguay in 1998. Germany was the first nation to score a golden goal in a major competition, beating Czech Republic in the final of Euro 1996. Silver goal was used in Euro 2004. Both these experiments have been discontinued by IFAB.



Under the Laws, the two basic states of play during a game are ball in play and ball out of play. From the beginning of each playing period with a kick-off (a set kick from the centre-spot by one team) until the end of the playing period, the ball is in play at all times, except when either the ball leaves the field of play, or play is stopped by the referee. When the ball becomes out of play, play is restarted by one of eight restart methods depending on how it went out of play:

A foul occurs when a player commits an offence listed in the Laws of the Game while the ball is in play. The offences that constitute a foul are listed in Law 12. Handling the ball deliberately, tripping an opponent, or pushing an opponent, are examples of "penal fouls", punishable by a direct free kick or penalty kick depending on where the offence occurred. Other fouls are punishable by an indirect free kick.The referee may punish a player or substitute's misconduct by a caution (yellow card) or sending-off (red card). A second yellow card at the same game leads to a red card, and therefore to a sending-off. If a player has been sent-off, no substitute can be brought on in their place. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of "unsporting behaviour" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute or substituted player. Non-players such as managers and support staff cannot be shown the yellow or red card, but may be expelled from the technical area if they fail to conduct themselves in a responsible manner.

Rather than stopping play, the referee may allow play to continue if doing so will benefit the team against which an offence has been committed. This is known as "playing an advantage". The referee may "call back" play and penalise the original offence if the anticipated advantage does not ensue within a short period, typically taken to be four to five seconds. Even if an offence is not penalised due to advantage being played, the offender may still be sanctioned for misconduct at the next stoppage of play.

The most complex of the Laws is offside. The offside law limits the ability of attacking players to remain forward (i.e. closer to the opponent's goal line) of the ball, the second-to-last defending player (which can include the goalkeeper), and the half-way line.

The recognised international governing body of football (and associated games, such as futsal and beach soccer) is the FÃ©dÃ©ration Internationale de Football Association (FIFA). The FIFA headquarters are located in ZÃ¼rich.

Six regional confederations are associated with FIFA; these are:

National associations oversee football within individual countries. These are affiliated both with FIFA and with their respective continental confederations.

The major international competition in football is the World Cup, organised by FIFA. This competition takes place over a four-year period. More than 190 national teams compete in qualifying tournaments within the scope of continental confederations for a place in the finals. The finals tournament, which is held every four years, involves 32 national teams competing over a four-week period. The 2006 FIFA World Cup took place in Germany; in 2010 it will be held in South Africa.

There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles. Before the inception of the World Cup, the Olympics (especially during the 1920s) had the same status as the World Cup. Originally, the event was for amateurs only, however, since the 1984 Summer Olympics professional players have been permitted, albeit with certain restrictions which prevent countries from fielding their strongest sides. Currently, the Olympic men's tournament is played at Under-23 level. In the past the Olympics have allowed a restricted number of over-age players per team; but that practice will cease in the 2008 Olympics. The Olympic competition is not generally considered to carry the same international significance and prestige as the World Cup. A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the womenâ€™s Olympic tournament. It thus carries international prestige considered comparable to that of the FIFA Women's World Cup.

After the World Cup, the most important football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa AmÃ©rica (CONMEBOL), African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC). The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example the UEFA Champions League in Europe and the Copa Libertadores de AmÃ©rica in South America. The winners of each continental competition contest the FIFA Club World Cup.



The governing bodies in each country operate league systems, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team are declared the champions. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division. The teams finishing at the top of a country's league may be eligible also to play in international club competitions in the following season. The main exceptions to this system occur in some Latin American leagues, which divide football championships into two sections named Apertura and Clausura, awarding a champion for each.

The majority of countries supplement the league system with one or more cup competitions. These are organised on a knock-out basis, the winner of each match proceeding to the next round; the loser takes no further part in the competition. 

Some countries' top divisions feature highly-paid star players; in smaller countries and lower divisions, players may be part-timers with a second job, or amateurs. The five top European leaguesâ€”the Premier League (England), the Bundesliga (Germany), La Liga (Spain), Ligue 1 (France) and Serie A (Italy)â€”attract most of the world's best players.



Today the sport is generally known simply as football in countries where it is the most popular football code. In countries where other codes are more popular, the sport is more commonly referred to as soccer, and indeed is referred to as such in the official names of the governing bodies in the United States and Canada. FIFA, the sport's world governing body, defines the sport they govern as association football in their statutes, but the term most commonly used by FIFA and the International Olympic Committee is football.















 

[[Image:Jacques Lipchitz, Birth of the Muses (1944-1950), MIT Campus.JPG|thumb|300px|<center>Birth of the Muses by Jacques Lipchitz]]Sculpture is three-dimensional artwork created by shaping hard or plastic material, commonly stone (either rock or marble), metal, or wood. Some sculptures are created directly by carving; others are assembled, built up and fired, welded, molded, or cast. A person who creates sculptures is called a sculptor. Because sculpture involves the use of materials that can be moulded or modulated, it is considered one of the plastic arts.

Some common forms of sculpture are:

The majority of public art is sculpture. Many sculptures together in a garden setting may be referred to as a sculpture garden.

Sculptors have generally sought to produce works of art that are as permanent as possible, working in durable and frequently expensive materials such as bronze and stone: marble, limestone, porphyry, and granite. More rarely, precious materials such as gold, silver, jade, and ivory were used for chryselephantine works. More common and less expensive materials were used for sculpture for wider consumption, including hardwoods (such as oak, box/boxwood, and lime/linden); terra cotta and other ceramics, and cast metals such as pewter and zinc (spelter).

Many sculptors seek new ways and materials to make art. Jim Gary used stained glass and automobile parts, tools, machine parts, and hardware. One of Pablo Picasso's most famous sculptures included bicycle parts. Alexander Calder and other modernists made spectacular use of painted steel. Since the 1960s, acrylics and other plastics have been used as well. Andy Goldsworthy makes his unusually ephemeral sculptures from almost entirely natural materials in natural settings. Some sculpture, such as ice sculpture, sand sculpture, and gas sculpture, is deliberately short-lived.

Sculptors often build small preliminary works called maquettes of ephemeral materials such as plaster of Paris, wax, clay, or plasticine, as Alfred Gilbert did for 'Eros' at Piccadilly Circus, London. In Retroarchaeology, these materials are generally the end product.

Many different forms of sculpture were used in Asia, with many pieces being religious art based around Hinduism and Buddhism (Buddhist art). A great deal of Cambodian Hindu sculpture is preserved at Angkor, however organized looting has had a heavy impact on many sites around the country. In Thailand, sculpture was almost exclusively of Buddha images. Many Thai sculptures or temples are gilded, and on occasion enriched with inlays. See also Thai art

The first known sculptures are from the Indus Valley civilization (3300â€?700 BC), found in sites at Mohenjo-daro and Harappa in modern-day Pakistan. These are among the earliest known instances of sculpture in the world. Later, as Hinduism, Buddhism, and Jainism developed further, India produced bronzes and stone carvings of great intricacy, such as the famous temple carvings which adorn various Hindu, Jain and Buddhist shrines. Some of these, such as the cave temples of Ellora and Ajanta, are examples of Indian rock-cut architecture, perhaps the largest and most ambitious sculptural schemes in the world. 

During the 2nd to 1st century BC in northern India, in what is now southern Afghanistan and northern Pakistan, sculptures became more anatomically realistic, often representing episodes of the life and teachings of Gautama Buddha. Although India had a long sculptural tradition and a mastery of rich iconography, the Buddha was never represented in human form before this time, but only through symbols such as the stupa. This alteration in style may have occurred because Gandharan Buddhist sculpture in ancient Afghanistan acquired Greek and Persian influence. Artistically, the Gandharan school of sculpture is characterized by wavy hair, drapery covering both shoulders, shoes and sandals, and acanthus leaf decorations, among other things. 

The pink sandstone sculptures of Mathura evolved during the Gupta Empire period (4th-6th century AD) to reach a very high fineness of execution and delicacy in the modeling. Gupta period art would later influence Chinese styles during the Sui dynasty, and the artistic styles across the rest of east Asia. Newer sculptures in Afghanistan, in stucco, schist or clay, display very strong blending of Indian post-Gupta mannerism and Classical influence. The celebrated bronzes of the Chola dynasty (c. 850-1250) from south India are of particular note; the iconic figure of Nataraja being the classic example. The traditions of Indian sculpture continue into the 20th and 21st centuries with for instance, the granite carving of Mahabalipuram derived from the Pallava dynasty. Contemporary Indian sculpture is typically polymorphous but includes celebrated figures such as Dhruva Mistry.



Artifacts from China date back as early as 10,000 BC and skilled Chinese artisans had been active very early in history, but the bulk of what is displayed as sculpture comes from a few select historical periods. The first period of interest has been the Western Zhou Dynasty (1050-771 BC), from which come a variety of intricate cast bronze vessels. The next period of interest was the Han Dynasty (206 BC-220 AD), beginning with the spectacular Terracotta Army assembled for the tomb of Qin Shi Huang, the first emperor of the important but short-lived Qin Dynasty that preceded the Han. Tombs excavated from the Han period have revealed many figures found to be vigorous, direct, and appealing 2000 years later.

The first Buddhist sculpture is found dating from the Three Kingdoms period (3rd century), while the sculpture of the Longmen Grottoes near Luoyang, Henan Province (Northern Wei, 5th and 6th century) has been widely recognized for its special elegant qualities.

The period now considered to be China's golden age is the Tang Dynasty, coinciding with what in Europe is sometimes called the Dark Ages). Decorative figures like those shown below became very popular in 20th century Euro-American culture, and were made available in bulk, as warlords in the Chinese civil wars exported them to raise cash. Considered especially desirable, and even profound, was the Buddhist sculpture, often monumental, begun in the Sui Dynasty, inspired by the Indian art of the Gupta period, and many are considered treasures of world art.

Following the Tang, Western interest in Chinese artifacts drops off dramatically, except for what might be considered as ornamental furnishings, and especially objects in jade. Pottery from many periods has been collected, and again the Tang period stands out apart for its free, easy feeling. Chinese sculpture has no nudes --other perhaps than figures made for medical training or practice -- and very little portraiture compared with the European tradition. One place where sculptural portraiture was pursued, however, was in the monasteries.

Almost nothing, other than jewelry, jade, or pottery is collected by art museums after the Ming Dynasty ended in the late 17th century -- and absolutely nothing has yet been recognized as sculpture from the tumultuous 20th century, although there was a school of Soviet-influenced social realist sculpture in the early decades of the Communist regime, and as the century turned, Chinese craftsmen began to dominate commercial sculpture genres (the collector plates, figurines, toys, etc) and avant garde Chinese artists began to participate in the Euro-American enterprise of contemporary art.



Countless paints and sculpture were made, often under governmental sponsorship. Most Japanese sculpture is associated with religion, and the medium' use declined with the lessening importance of traditional Buddhism. During the Kofun period of the third century, clay sculptures called haniwa were erected outside tombs. Inside the Kondo at HÅryÅ«-ji is a Shaka Trinity (623), the historical Buddha flanked by two bodhisattvas and also the Guardian Kings of the Four DirectionsThe wooden image ( 9th c.) of Shakyamuni, the "historic" Buddha, enshrined in a secondary building at the Muro-ji, is typical of the early Heian sculpture, with its ponderous body, covered by thick drapery folds carved in the hompa-shiki (rolling-wave) style, and its austere, withdrawn facial expression. The Kei school of sculptors, particularly Unkei, created a new, more realistic style of sculpture.

African art has an emphasis on Sculpture - African artists tend to favor three-dimensional artworks over two-dimensional works. Although anthropologists argue that the earliest known sculptures in Africa are from the Nok culture of Nigeria that date around 500 BC, the art of Pharaonic Africa date much earlier than the Nok period. Metal sculptures from the eastern portions of west Africa such as Benin, are considered among the best ever produced.

Art plays an essential role in the lives of the African peoples and communities across the continent. The beauty of African art is simply in meaning. These objects mean a great deal to the people and they are of significant meaning to the traditions that produce them. Their beauty and content protect the community and the individual artists, and tell much of the artists who use them. Later exhibitions of African art in the West have been able to get much detailed catalogues that attempt to cover the art of the whole continent.

African Sculptures 

Sculptures are created to symbolize and reflect the regions from which they are made. Right from the materials and techniques used, the pieces have functions that are very different from one region to the other. 

In West Africa, the figures have elongated bodies, angular shapes, and facial features that represent an ideal rather than an individual. These figures are used in religious rituals. They are made to have surfaces that are often coated with materials placed on them for ceremonial offerings. In contrast to these sculptures of West Africa are the ones of Mande-speaking peoples of the same region. The Mande pieces are made of wood and have broad, flat surfaces. Their arms and legs are shaped like cylinders. 

In Central Africa, however, the key characteristics include heart shaped faces that are curve inward and display patterns of circles and dots. Although some groups prefer more of geometric and angular facial forms, not all pieces are exactly the same. Also, not all pieces are made of the same material. The materials used range from mostly wood all the way to ivory, bone, stone, clay, and metal. Overall, though, the Central African region has very striking styles that is very easy to identify. With the distinctive style, one can easily tell which area the sculpture was produced in.Eastern Africa is not known for their sculptures but one type that is done in this area is pole sculptures. These are a pole carved in a human shape and decorated with geometric forms, while the tops are carved with figures of animals, people, and various objects. These poles are then placed next to graves and are associated with death and the ancestral world.

Southern Africaâ€™s oldest known clay figures date from 400 to 600 A.D. and have cylindrical heads. These clay figures have a mixture of human and animal features. Other than clay figures, there are also wooden headrests that were buried with their owners. The headrests had styles ranging from geometric shapes to animal figures.Each region had a unique style and meaning to their sculptures. The type of material and purpose for creating sculpture in Africa reflect the region from which the pieces are created.

The monumental sculpture of Ancient Egypt is world-famous, but refined and delicate small works are also a feature. The ancient art of Egyptian sculpture evolved to represent the ancient Egyptian gods, and Pharaohs, the divine kings and queens, in physical form. Very strict conventions were followed while crafting statues: male statues were darker than the female ones; in seated statues, hands were required to be placed on knees and specific rules governed appearance of every Egyptian god. Artistic works were ranked according to exact compliance with all the conventions, and the conventions were followed so strictly that over three thousand years, very little changed in the appearance of statues except during a brief period during the rule of Akhenaten and Nefertiti when naturalistic portrayal was encouraged.



Sculpture in what is now Latin America developed in two separate and distinct areas, Mesoamerica in the north and Peru in the south. In both areas, sculpture was initially of stone, and later of terra cotta and metal as the civilizations in these areas became more technologically proficient.  The Mesoamerican region produced more monumental sculpture, from the massive block-like works of the Olmec and Toltec cultures, to the superb low reliefs that characterize the Mayan and Aztec cultures. In the Andean region, sculptures were typically small, but often show superb skill. In North America, wood was sculpted for totems, totem poles, masks, and boats. The arrival of European Catholic culture readily adapted local skills to the prevailing Baroque style, producing enormously elaborate retablos and other church sculptures in a slightly hybrid style. Later, artists trained in the Western academic tradition followed European styles until in the late nineteenth century they began to draw again on indigenous influences. 

The history of sculpture in the United States after Europeans' arrival reflects the country's 18th-century foundation in Roman republican civic values and Protestant Christianity. Compared to areas colonized by the Spanish, sculpture got off to an extremely slow start in the British colonies, with next to no place in churches, and was only given impetus by the need to assert nationality after independence. American sculpture of the mid- to late-19th century was often classical, often romantic, but showed a bent for a dramatic, narrative, almost journalistic realism. Public buildings of the first half of the 20th century often provided an architectural setting for sculpture, especially in relief. By the 1950s, traditional sculpture education would almost be completely replaced by a Bauhaus-influenced concern for abstract design. Minimalist sculpture often replaced the figure in public settings. Modern sculptors use both classical and abstract inspired designs. Beginning in the 1980s, there was a swing back toward figurative public sculpture; by 2000, many of the new public pieces in the United States were figurative in design.

Features unique to the European Classical tradition:

Features that the European Classical tradition shares with many others:

An unadorned figure in Greek classical sculpture was a reference to the status or role of the depicted person, deity or other being. Athletes, priestesses and gods could be identified by their adornment or lack of it.

The Renaissance preoccupation with Greek classical imagery, such as the 4th century B.C. Doryphoros of Polykleitos, led to nude figurative statues being seen as the 'perfect form' of representation for the human body. Subsequently, nudity in sculpture and painting has represented a form of ideal, be it innocence, openness or purity. Nude sculptures are still common. As in painting, they are often made as exercises in efforts to understand the anatomical structure of the human body and develop skills that will provide a foundation for making clothed figurative work.

Nude statues are usually widely accepted by most societies, largely due to the length of tradition that supports this form. Occasionally, the nude form draws objections, often by fundamentalist moral or religious groups. Classic examples of this are the removal of penises from the Vatican collection of Greek sculpture and the addition of a fig leaf to a plaster cast of Michelangelo's sculpture of David for Queen Victoria's visit to the British Museum.

The topic of social status

Worldwide, sculptors are usually tradesmen whose work is unsigned. But in the Classical tradition, some sculptors began to receive individual recognition in Periclean Athens and more so in the Renaissance revival 2000 years later, culminating in the career of Michelangelo who entered the circle of princes. Sculpture was still a trade, but exceptional sculptors were recognized on a level with exceptional poets and painters. In the 19th century, sculpture also became a bourgeois/upper class avocation, as poetry and painting had been, and the classical work of women sculptors began to appear.



Gothic sculpture evolved from the early stiff and elongated style, still partly Romanesque, into a spatial and naturalistic feel in the late 12th and early 13th century. The architectural statues at the Western (Royal) Portal at Chartres Cathedral (c. 1145) are the earliest Gothic sculptures and were a revolution in style and the model for a generation of sculptors. Prior to this there had been no sculpture tradition in Ile-de-Franceâ€”so sculptors were brought in from Burgundy. Bamberg Cathedral had the largest assemblage of 13th century sculpture. In England sculpture was more confined to tombs and non-figurine decorations. In Italy there was still a Classical influence, but Gothic made inroads in the sculptures of pulpits such as the Pisa Baptistery pulpit (1269) and the Siena pulpit. Dutch-Burgundian sculptor Claus Sluter and the taste for naturalism signaled the beginning of the end of Gothic sculpture, evolving into the classicistic Renaissance style by the end of the 15th century.

 Although the Renaissance began at different times around Europe (some areas created art longer in the Gothic style than other areas) the transition from Gothic to Renaissance in Italy was signalled by a trend toward naturalism with a nod to classical sculpture. One of the most important sculptors in the classical revival was Donatello. The greatest achievement of what art historians refer to as his classic period is the bronze statue entitled David (not to be confused with Michelangelo's David), which is currently located at the Bargello in Florence. At the time of its creation, it was the first free-standing nude statue since ancient times. Conceived fully in the round and independent of any architectural surroundings, it is generally considered to be the first major work of Renaissance sculpture. 

During the High Renaissance, the time from about 1500 to 1520, Michelangelo was an active sculptor with works such as David and the PietÃ , as well as the Doni Virgin, Bacchus, Moses, Rachel, Orgetorix, and members of the Medici family. Michelangelo's David is possibly the most famous sculpture in the world, which was unveiled on September 8, 1504. It is an example of the contrapposto style of posing the human figure, which again borrows from classical sculpture. Michelangelo's statue of David differs from previous representations of the subject in that David is depicted before his battle with Goliath and not after the giant's defeat. Instead of being shown victorious over a foe much larger than he, David looks tense and ready for combat.

 the Mannerist period, more abstract representations were praised, giving more thought to color and composition rather than realistic portrayal of the subjects in the piece. This is exemplified in Giambologna's Abduction/Rape of the Sabine Women, where the figures are not positioned in a way which is at all comfortable, or even humanly possible, but the position and emotion still come across. Another exemplar of the form is Benvenuto Cellini's 1540 salt cellar of gold and ebony, featuring Neptune and Amphitrite (earth and water) in elongated form and uncomfortable positions.

 Baroque sculpture, groups of figures assumed new importance, and there was a dynamic movement and energy of human formsâ€?they spiralled around an empty central vortex, or reached outwards into the surrounding space. For the first time, Baroque sculpture often had multiple ideal viewing angles. The characteristic Baroque sculpture added extra-sculptural elements, for example, concealed lighting, or water fountains. Often, Baroque artists fused sculpture and architecture seeking to create a transformative experience for the viewer. Gian Lorenzo Bernini was undoubtedly the most important sculptor of the Baroque period. His first works were inspired by Hellenistic sculpture of ancient Greece and imperial Rome. One of his most famous works is Ecstasy of St Theresa

 The sculpture examples they actually embraced were more likely to be Roman copies of Hellenistic sculptures. In sculpture, the most familiar representatives are the Italian Antonio Canova, the Englishman John Flaxman and the Dane Bertel Thorvaldsen. The European neoclassical manner also took hold in the United States, where its high tide occurred somewhat later and is exemplified in the sculptures of William Henry Rinehart (1825-1874).

Modern Classicism contrasted in many ways with theclassical sculpture of the 19th century which was characterized by commitments to naturalism (Antoine-Louis Barye) -- the melodramatic (FranÃ§ois Rude) sentimentality (Jean Baptiste Carpeaux)-- or a kind of stately grandiosity (Lord Leighton) Several different directions in the classical tradition were taken as the century turned, but the study of the live model and the post-Renaissance tradition was still fundamental to them.

Auguste Rodin was the most renowned European sculptor of the early 20th century. He might be considered as sui generis -- that is, if anyone successfully composed in his turbulent, virtuosic style, they have yet to be discovered. But he is often considered a sculptural Impressionist, as are Medardo Rosso, Count Troubetski, and Rik Wouters, attempting to frame the charm of a fleeting moment of daily life.

 Modern Classicism showed a lesser interest in naturalism and a greater interest in formal stylization. Greater attention was paid to the rhythms of volumes and spaces - as well as greater attention to the contrasting qualities of surface (open, closed, planar, broken etc) while less attention was paid to story-telling and convincing details of anatomy or costume. Greater attention was given to psychological realism than to physical realism. Greater attention was given to showing what was eternal and public, rather than what was momentary and private. Greater attention was given to examples of ancient and Medieval sacred arts:Egyptian, Middle Eastern, Asian, African, and Meso-American. Grandiosity was still a concern, but in a broader, more world-wide context.

Early masters of modern classicism included: Aristide Maillol, Alexander Matveev, Joseph Bernard, Antoine Bourdelle, Georg Kolbe, Libero Andreotti, Gustav Vigeland, Jan Stursa, Constantin Brancusi.

As the century progressed, modern classicism was adopted as the national style of the two great European totalitarian empires: Nazi Germany and Soviet Russia, who co-opted the work of early masters, like Kolbe and Arno Breker in Germany, and Matveev in Russia. Nazi Germany had a 15-year run; but over the 70 years of the USSR, new generations of sculptors were trained and chosen within their system, and a distinct style, socialist realism, developed, that returned to the 19th century's emphasis on melodrama and naturalism.

Classical training was rooted out of art education in Western Europe (and the Americas) by 1970 and the classical variants of the 20th century were marginalized in the history of modernism. But classicism continued as the foundation of art education in the Soviet academies until 1990, providing a foundation for expressive figurative art throughout eastern Europe and parts of the Middle East. By the year 2000, the European classical tradition maintains a wide appeal to viewers - especially tourists - and especially for the ancient, Renaissance, Baroque, and 19th century periods -- but awaits an educational tradition to revive its contemporary development.

In the rest of Europe, the modern classical became either more decorative/art deco (Paul Manship, Carl Milles) or more abstractly stylized or more expressive (and Gothic) (Anton Hanak, Wilhelm Lehmbruck, Ernst Barlach, Arturo Martini) -- or turned more to the Renaissance (Giacomo Manzu, Venanzo Crocetti) or stayed the same (Charles Despiau, Marcel Gimond).

In the early days of the 20th century Pablo Picasso revolutionized the art of sculpture when he began creating his constructions fashioned by combining disparate objects and materials into one constructed piece of sculpture, - by addition. Picasso reinvented the art of sculpture with his innovative use of constructing a work in three dimensions with disparate material. Just as collage was a radical development in two dimensional art; so was construction a radical development in three dimensional sculpture.

In Europe, by the 1930s and 1940s modernism in sculpture became more abstract and stylized, exemplified by Picasso, Henry Moore, Alberto Giacometti, and Julio GonzÃ¡lez. Eventually artists like Isamu Noguchi, David Smith, Alexander Calder, Richard Lippold, George Rickey and Louise Nevelson came to characterize the look of modern sculpture. By the 1960s Abstract expressionism, Geometric abstraction and Minimalism as exemplified by the Cubi's of David Smith, and the welded steel work of Sir Anthony Caro, the large scale work of Mark di Suvero, and the Minimalist works by Tony Smith, Robert Morris, Donald Judd and others led contemporary abstract sculpture in new directions.Also during the 1960s artists as diverse as Claes Oldenburg, George Segal, Edward Kienholz, Duane Hanson, and John DeAndrea explored imagery and figuration through installation art in new ways.

Since the 1950s Modernist trends dominated the public imagination and the popularity of Modernist sculpture has all but eliminated the traditional approach.

Modernist sculpture movements include Geometric abstraction, De Stijl, Suprematism, Constructivism, Dadaism, Cubism, Surrealism, Futurism, Minimalism, Formalism Abstract expressionism, Pop-Art, and Installation art among others.

Post-modern sculpture occupies a broader field of activities than Modernist sculpture, as Rosalind Krauss has observed. Her idea of sculpture in the expanded field identified a series of oppositions that describe the various sculpture-like activities that are post-modern sculpture:

Krauss' concern was creating a theoretical explanation that could adequately fit the developments of Land art, Minimalist sculpture, and Site-specific art into the category of sculpture. To do this, her explanation created a series of oppositions around the work's relationship to its environment. 

Some modern sculpture forms are now practiced outdoors, and often in full view of spectators, thus giving them kinship to performance art in the eyes of some. Ice sculpture is a form of sculpture that uses ice as the raw material. It's popular in China, Japan, Canada, Sweden, and Russia. Ice sculptures feature decoratively in some cuisines, especially in Asia. Kinetic sculptures are sculptures that are designed to move, which include Mobiles. Snow sculptures are usually carved out of a single block of snow about 6 to  on each side and weighing about 20 - 30 tons. The snow is densely packed into a form after having been produced by artificial means or collected from the ground after a snowfall. Sound sculptures take the form of indoor sound installations, outdoor installations such as aeolian harps, automatons, or be more or less near conventional musical instruments. Sound sculpture is often site-specific. A Sand castle can be regarded as a sand sculpture. Weightless Sculpture (in outer space) as a concept is created in 1985 by the Dutch artist Martin Sjardijn. LEGO brick sculpting involves the use of common LEGO bricks to build realistic or artistic sculptures sometimes using hundreds of thousands of bricks.

Other arts which can be regarded as sculptures include:













The Cristero War (also known as the Cristiada) of 1926 to 1929 was an uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.

After a period of peaceful resistance, a number of skirmishes took place in 1926. The formal rebellions began on January 1, 1927 with the rebels calling themselves Cristeros because they felt they were fighting for Christ himself. Just as the Cristeros began to hold their own against the federal forces, the rebellion was ended by diplomatic means, brokered by the U.S. Ambassador Dwight Whitney Morrow, father-in-law of aviator Charles Lindbergh.

The 1917 Constitution of Mexico, resulting from the Mexican Revolution, as well as a similar one instituted by Benito Juarez in 1857 (1857 Constitution of Mexico), reflected a long-standing discontent among some Mexican leaders about what they considered abuses by the Catholic clergy, as well as the view that the Catholic hierarchy had a long history of aligning itself with the wealthy classes at the expense of the poorest classes. This latter cause corresponded with the grievances which underlay the Mexican Revolution.  

Five articles of the 1917 Constitution were particularly aimed at suppression of the Roman Catholic Church in Mexican political and civil life. Article 3 mandated secular education in schools. Article 5 outlawed monastic religious orders. Article 24 forbade public religious worship outside temples (which included churches), while Article 27 restricted religious organizations' rights to own property. Finally, Article 130 took away basic civil rights of members of the clergy: priests and religious leaders were prevented from wearing their religious garb in public, were denied the right to vote, and were not permitted to comment on public affairs in the press.

The anticlerical mindset of the government extended also to superficial changes made to place names to secularize them. For instance, the state of Vera Cruz (true cross) was renamed Veracruz.

When the anti-clerical measures were enacted in 1917, the President of Mexico was Venustiano Carranza. Carranza was overthrown by the machinations of his one-time ally Ãlvaro ObregÃ³n in 1919, who succeeded to the presidency in late 1920. Although he shared Carranza's anti-clerical sentiments, he applied the measures selectively, only in areas where Catholic sentiment was weakest.

This uneasy "truce" between the government and the Church ended with the 1924 election of Plutarco ElÃ­as Calles, a strident atheist.  Calles applied the anti-clerical laws stringently throughout the country and added his own anti-clerical legislation. In June 1926, he signed the "Law for Reforming the Penal Code", known unofficially as the "Calles Law". This provided specific penalties for priests and individuals who violated the provisions of the 1917 Constitution. For instance, wearing clerical garb in public (i.e., outside Church buildings) earned a fine of 500 pesos (approximately 250 U.S. dollars at the time); a priest who criticized the government could be imprisoned for five years.  Some states enacted oppressive measures. Chihuahua, for example, enacted a law permitting only a single priest to serve the entire Catholic congregation of the state. Calles seized church property, expelled all foreign priests, closed the monasteries and convents and religious schools.

Calles was also a Freemason, and (according to one scholar) on May 28, 1926 the Masons awarded him a medal of merit for his persecution of Catholics.  On July 12, 1926, the following communique appeared in the press: "International Masonry accepts responsibility for everything that is happening in Mexico, and is preparing to mobilize all its forces for the methodic, integral application of the agreed upon program for this country." 

In response to these measures, Catholic organizations began to intensify their resistance. The most important of these groups was the National League for the Defense of Religious Liberty, founded in 1924. This was joined by the Mexican Association of Catholic Youth (founded 1913) and the Popular Union, a Catholic political party founded in 1925.

On July 11, 1926, the Mexican bishops voted to suspend all public worship in Mexico in response to the Calles Law. This suspension was to take place on August 1. On July 14, they endorsed plans for an economic boycott against the government, which was particularly effective in west-central Mexico (the states of Jalisco, Guanajuato, Aguascalientes, Zacatecas). Catholics in these areas stopped attending movies and plays and using public transportation and Catholic teachers stopped teaching in secular schools.

However, this boycott collapsed by October 1926, in large part due to lack of support among wealthy Catholics, who were themselves losing money due to the boycott. The wealthy were generally disliked because of this, and the reputation was worsened when they paid the federal army for protection and called on the police to break the picket lines.

The Catholic bishops meanwhile worked to have the offending articles of the Constitution amended. Pope Pius XI explicitly approved this means of resistance. However, the Calles government considered this seditious behavior and had many churches closed. In September the episcopate submitted a proposal for the amendment of the constitution, but this was rejected by Congress on September 22, 1926.

In Guadalajara, Jalisco, on August 3, 1926, some 400 armed Catholics shut themselves up in the Church of Our Lady of Guadalupe in that city. There, they were involved in a shootout with federal troops, and surrendered only when they ran out of ammunition. According to U.S. consular sources, this battle resulted in 18 dead and 40 injured.

The following day, August 4, in Sahuayo, MichoacÃ¡n, 240 government soldiers stormed the parish church. The parish priest and his vicar were killed in the ensuing violence. On August 14, government agents staged a purge of the Chalchihuites, Zacatecas, chapter of the Association of Catholic Youth and executed their spiritual adviser, Father Luis BÃ¡tiz Sainz. This execution caused a band of ranchers, lead by Pedro Quintanar, to seize the local treasury and declare themselves in rebellion. At the height of their rebellion, they held a region including the entire northern part of Jalisco.

Another uprising was led by the mayor of PÃ©njamo, Guanajuato, Luis Navarro Origel, beginning on September 28. His men were defeated by federal troops in the open land around the town, but retreated into the mountains, where they continued as guerrillas. This was followed by an uprising in Durango led by Trinidad Mora on September 29 and an October 4 rebellion in southern Guanajuato, led by former general Rodolfo Gallegos. Both of these rebel leaders were forced to adopt guerrilla tactics, as they were no match for the federal troops and airforce on open ground.

Meanwhile, the rebels in Jalisco (particularly the region northeast of Guadalajara) quietly began gathering forces. This region became the main focal point of the rebellion led by 27-year-old RenÃ© Capistran Garza, leader of the Mexican Association of Catholic Youth.

The formal rebellion began on January 1, 1927 with a manifesto sent by Garza on New Year's Day, titled A la NaciÃ³n (To the Nation). This declared that "the hour of battle has sounded" and "the hour of victory belongs to God". With the declaration, the state of Jalisco, which had seemed to be quiet since the Guadalajara church uprising, exploded. Bands of rebels moving in the "Los Altos" region northeast of Guadalajara began seizing villages, often armed with only ancient muskets and clubs. The Cristeros' battle cry was Â¡Viva Cristo Rey! Â¡Viva la Virgen de Guadalupe! ("Long live Christ the King! Long live the Virgin of Guadalupe!"). The rebels were an unusual army in that they had no logistical supplies, and relied heavily on raids to towns, trains and ranches in order to supply themselves with money, horses, ammunition and food. 

The Calles government did not take the threat very seriously at first. The rebels did well against the agraristas (a rural militia recruited throughout Mexico) and the Social Defense forces (local militia), but were always defeated by the federal troops who guarded the important cities. At this time, the federal army numbered 79,759 men. When Jalisco federal commander General JesÃºs Ferreira moved on the rebels, he calmly stated that "it will be less a campaign than a hunt." 

However, these rebels, who had had no previous military experience for the most part, planned their battles well. The most successful rebel leaders were JesÃºs Degollado (a druggist), Victoriano RamÃ­rez (a ranch hand), and two priests, Aristeo Pedroza and JosÃ© Reyes Vega. At least five priests took up arms, while many more supported them in various ways.

Recent scholarship suggests that for many Cristeros, religious motivations for rebellion were reinforced by other political and material concerns. Participants in the uprising often came from rural communities that had suffered from the government's land reform policies since 1920, or otherwise felt threatened by recent political and economic changes. Many agraristas and other government supporters were also fervent Catholics.

Whether the Cristeros' actions were or were not supported by the episcopate or the Pope has been a subject of controversy. Officially, the Mexican episcopate never supported the rebellion, but by several accounts, the rebels had the episcopate's acknowledgement that their cause was legitimate. The episcopate did not, in any event, condemn the rebels. Bishop JosÃ© Francisco Orozco y JimÃ©nez of Guadalajara remained with the rebels; while formally rejecting armed rebellion, he was unwilling to leave his flock. Many modern historians consider him to have been the real head of the movement. 

On February 23, 1927, the Cristeros defeated federal troops for the first time at San Francisco del RincÃ³n, Guanajuato, followed by another victory at San JuliÃ¡n, Jalisco. The rebellion was almost extinguished, however, on April 19, when Father Vega led a raid against a train thought to be carrying a shipment of money. In the shootout, his brother was killed, and Father Vega had the train cars doused in gasoline and set afire, killing 51 civilians. 

This atrocity turned public opinion against the Cristeros. The government began moving the civilians back into the population centers and prevented them from providing supplies to the rebels. By the summer, the rebellion was almost completely quelled. Garza resigned from his position at the head of the rebellion in July, after a failed attempt to raise funds in the United States of America.

The rebellion was given new life by the efforts of Victoriano RamÃ­rez, generally known as "El Catorce" (the fourteen). Legend has it the nickname originated because during jailbreak he killed all fourteen members of the posse sent after him. He then sent a message to the mayorâ€”his uncleâ€”telling him that in the future he should send more men.

El Catorce was illiterate, but a natural guerrilla leader. He brought the rebellion back to life, enabling the National League for the Defense of Religious Liberty to select a general, a mercenary who demanded twice the salary of a federal general. Enrique Gorostieta was so alienated from Catholicism that he made fun of his own troops' religion. Despite his lack of piety, he trained the rebel troops well, producing disciplined units and officers. Gradually, the Cristeros began to gain the upper hand.

Both priest-commanders, Father Vega and Father Pedroza, were born soldiers. Father Vega was not a typical priest, and was reputed to drink heavily and routinely ignore his vow of chastity. Father Pedroza, by contrast, was rigidly moral and faithful to his priestly vows. However, the fact that the two took up arms at all is problematic from the point of view of Catholic sacramental theology.

On June 21, 1927, the first brigade of female Cristeros was formed in Zapopan. They named themselves for Saint Joan of Arc. The brigade began with 17 women, but soon grew to 135 members. Its mission was to obtain money, weapons, provisions and information for the combatant men; they also cared for the wounded. By March 1928, there were some 10,000 women involved. Many smuggled weapons into the combat zones by carrying them in carts filled with grain or cement. By the end of the war, they numbered some 25,000.

The Cristeros maintained the upper hand throughout 1928, and in 1929, the federal government faced a new crisis: a revolt within Army ranks, led by Arnulfo R. GÃ³mez in Veracruz. The Cristeros tried to take advantage of this with an attack on Guadalajara in late March. This failed, but the rebels did manage to take TepatitlÃ¡n on April 19. Father Vega was killed in that battle.

However, the military rebellion was met with equal cruelty and force, and the Cristeros were soon facing divisions within their own ranks. Mario ValdÃ©s, widely believed by historians to have been a federal spy, managed to stir up sentiment against El Catorce leading to his execution before a rigged court-martial.

On June 2, Gorostieta was killed when he was ambushed by a federal patrol. However the rebels had some 50,000 men under arms by this point and seemed poised to draw out the rebellion for a long time.

Before and after the successes had by the rebels and the support of Bishop Orozco y JimÃ©nez, the Mexican bishops supported the Cristeros (this is in dispute- the only comprehensive history of this movement, "The Cristero Rebellion" indicates that with a couple of exceptions the episcopacy was hostile to the movement). The bishops were expelled from Mexico after Father Vega's attack on the train, but continued to try and influence the war's outcome from outside the country.

The U.S. ambassador to Mexico, in October 1927, was Dwight Whitney Morrow. He initiated a series of breakfast meetings with President Calles where the two would discuss a whole range of problems, from the religious uprising, to oil and irrigation. This earned him the nickname "ham and eggs diplomat" in U.S. papers. Morrow wanted the conflict to come to an end both for regional safety, and to help find a solution to the oil problem in the U.S. He was aided in his efforts by Father John Burke of the National Catholic Welfare Conference. The Vatican was also actively suing for peace. 

Calles' term as president was coming to an end and president-elect Ãlvaro ObregÃ³n was scheduled to take office on December 1. However, he was assassinated by a Catholic radical two weeks after his election, gravely damaging the peace process.

Congress named Emilio Portes Gil interim president in September, with an election to be held in November 1929. Portes Gil was more open to the Church than Calles had been, allowing Morrow and Burke to reinitiate their peace initiative. Portes Gil told a foreign correspondent on May 1 that "the Catholic clergy, when they wish, may renew the exercise of their rites with only one obligation, that they respect the laws of the land."

The next day, exiled Archbishop Leopoldo RuÃ­z y Flores issued a statement the bishops would not demand the repeal of the laws, only their more lenient application.

Morrow managed to bring the parties to agreement on June 21, 1929. The pact he drafted, called the arreglos (agreement) would allow worship to resume in Mexico and granted three concessions to the Catholics: only priests who were named by hierarchical superiors would be required to register, religious instruction in the churches (but not in the schools) would be permitted, and all citizens, including the clergy, would be allowed to make petitions to reform the laws. But the most important part of it was that the church would recover the right to use its properties, and priests recovered their rights to live on such property. Legally speaking, the church was not allowed to own real estate, and its former facilities remained federal property. However, the church took control over them, and the government never again tried to take these properties back. It was a convenient arrangement for both parties and, Church support for the rebels ended.

The agreement led to an unusual end to the war. In the last two years, many more anticlerical officers who were hostile to the federal government for other reasons had joined the rebels. When the agreement was made known, only a minority of the rebels went home, those who felt their battle had been won. As the rebels themselves were not consulted in these talks, most of them felt betrayed and some continued to fight. The church then threathened rebels with excommunication, and gradually the rebellion died out.

The end of the Cristero War affected emigration to the United States. "In the aftermath of their defeat, many of the Cristeros â€?by some estimates as much as 5 percent of Mexico's population â€?fled to America. Many of them made their way to Los Angeles, where they found a protector in John Joseph Cantwell, the bishop of what was then the Los Angeles-San Diego diocese." (Rieff, David. "Nuevo Catholics." The New York Times Magazine. 24 Dec. 2006.)

The officers, fearing that they would be tried as traitors, tried to keep the rebellion alive. This attempt failed and many were captured and shot, while others escaped to San Luis PotosÃ­, where General Saturnino Cedillo gave them refuge.

On June 27, 1929, the church bells rang in Mexico for the first time in almost three years.

The war had claimed the lives of some 90,000 people: 56,882 on the federal side, 30,000 Cristeros, and numerous civilians and Cristeros who were killed in anticlerical raids after the war's end. As promised by Portes Gil, the Calles Law remained on the books, but no organized federal attempts to enforce it were put into action. Nonetheless, in several localities, persecution of Catholic priests continued based on local officials' interpretations of the law. The anticlerical provisions of the Constitution remain in place as of 2008, though they are no longer enforced.

The effects of the war on the Church were profound. Between 1926 and 1934 at least 40 priests were killed. Where there were 4,500 priests serving the people before the rebellion, in 1934 there were only 334 priests licensed by the government to serve fifteen million people, the rest having been eliminated by emigration, expulsion and assassination.   By 1935, 17 states had no priest at all. 





The Catholic Church has recognized several of those killed in connection with the Cristero rebellion as martyrs. Perhaps the best-known is Blessed Miguel Pro, SJ. This Jesuit priest was executed by firing squad on November 23, 1927, without benefit of a trial, on the grounds that his priestly activities were in defiance of the government. The Calles government hoped to use images of the execution to scare the rebels into surrender, but the photos had the opposite effect. Upon seeing the photos, which the government had printed in all the newspapers, the Cristeros were inspired with a desire to follow Father Pro into martyrdom for Christ. His beatification occurred in 1988.

On May 21, 2000, Pope John Paul II canonized a group of 25 martyrs from this period. (They had been beatified on November 22, 1992.) For the most part, these were priests who did not take up arms, but refused to leave their flocks, and were executed by federal forces. 

To cite just one example (mentioned above in the history), Father Luis BÃ¡tiz Sainz was the parish priest in Chalchihuites and a member of the Knights of Columbus. He was known for his devotion to the Eucharist and for his prayer for martyrdom: "Lord, I want to be a martyr; even though I am your unworthy servant, I want to pour out my blood, drop by drop, for your name." In 1926, shortly before the closing of the churches, he was denounced as a conspirator against the government because of his connections with the National League for the Defense of Religious Liberty, which was preparing an armed uprising. A squad of soldiers raided the private house he was staying in on August 14, taking him captive. He was executed without trial together with three youths of the Mexican Association of Catholic Youth.

Thirteen additional victims of the anti-Catholic regime have been declared martyrs by the Catholic Church, paving the way to their beatification. These are primarily lay people, including the 14-year-old JosÃ© SÃ¡nchez del RÃ­o. The requirement that they did not take up arms, which was applied to the priest martyrs, does not apply to the lay people, though it had to be shown that they were taking up arms in self-defense.

On November 20, 2005 on Jalisco Stadium in Guadalajara, Mexico, these 13 martyrs were beatified by Cardinal Jose Saraiva Martins.









The Battle of Stalingrad, conducted between Germany and its allies and the Soviet Union for the Soviet city of Stalingrad, took place between August 21 1942 and February 2 1943, during the Second World War. Stalingrad was known as Tsaritsyn until 1925 and today is known as Volgograd. 

This battle is often cited as one of the turning points of the war in the European Theater and was arguably the bloodiest battle in human history, with combined casualties estimated to be above 1.5 million. The battle was marked by brutality and disregard for military and civilian casualties by both sides. The struggle included the German siege of Stalingrad, the battle inside the city, and the Soviet counter-offensive which eventually trapped and destroyed the German Sixth Army and other Axis forces around the city.

On June 22, 1941, Nazi Germany launched Operation Barbarossa (Unternehmen Barbarossa). The armed forces of Germany and its allies invaded the Soviet Union, quickly advancing deep into Soviet territory. During December, having suffered multiple defeats during the summer and autumn, Soviet forces counter-attacked during the Battle of Moscow and successfully drove the German Army (Wehrmacht Heer) from the environs of Moscow.

By spring 1942, the Germans had stabilized their front and were confident they could master the Red Army when winter weather no longer impeded their mobility. There was some substance to this belief: while Army Group Centre (Heeresgruppe Mitte) had suffered heavy punishment, 65 percent of its infantry had not been engaged during the winter fighting, and had been rested and reequipped. Part of the German military philosophy was to attack where least expected so that rapid gains could be made. An attack on Moscow was seen as too predictable by some, most notably German dictator Adolf Hitler. Along with this, the German High Command (Oberkommando des Heeres  or OKH) knew that time was running out for them, as the United States had entered the war following Germany's declaration of war in support of its Japanese ally. Hitler wanted to end the fighting on the Eastern Front, or at least minimize it, before the Americans had a chance to get deeply involved in the war in Europe. 

The capture of Stalingrad was important to Hitler for primarily two reasons. First, it was a major industrial city on the Volga River â€?a vital transport route between the Caspian Sea and Northern Russia. Secondly, its capture would secure the left flank of the German armies as they advanced into the oil-rich Caucasus region â€?with a goal of cutting off fuel to Stalin's war machine. That the city bore the name of Hitlerâ€™s nemesis, Joseph Stalin, would make the cityâ€™s capture an ideological and propaganda coup. Stalin realized this and ordered anyone that was strong enough to hold a rifle be sent out to war. Stalin also had an ideological and propaganda interest in defending the city which bore his name in honor of his defense of the city during the Russian Civil War, but the fact remains that Stalin was under tremendous constraints of time and resources. The Red Army, at this stage of the war, was less capable of highly mobile operations than the German Army, but the prospect of combat inside a large urban area, which would be dominated by short-range small firearms and artillery rather than armored and mechanized tactics, minimized the Red Armyâ€™s disadvantages against the Germans.



Army Group South was selected for a sprint forward through the southern Russian steppes into the Caucasus to capture the vital Soviet oil fields there. Instead of focusing his attention on the Soviet Capital of Moscow as his general staff advised, Hitler continued to send forces and supplies to the eastern Ukraine. The planned summer offensive was code-named Fall Blau (trans.: â€œCase Blueâ€?. It was to include the German Sixth Army, Seventeenth Army, Fourth Panzer Army and First Panzer Army. Army Group South had overrun the Ukrainian SSR in 1941. Poised in the Eastern Ukraine, it was to spearhead the offensive. 

Hitler intervened, however, ordering the Army Group to be split in two. Army Group South (A), under the command of Wilhelm List, was to continue advancing south towards the Caucasus as planned with the Seventeenth Army and First Panzer Army. Army Group South (B), including Friedrich Paulusâ€™s Sixth Army and Hermann Hoth's Fourth Panzer Army, was to move east towards the Volga and the city of Stalingrad. Army Group B was commanded initially by Field Marshal Fedor von Bock and later by General Maximilian von Weichs.

The start of Operation Blau had been planned for late May 1942. However, a number of German and Romanian units that were involved in Blau were then in the process of besieging Sevastopol on the Crimean Peninsula. Delays in ending the siege pushed back the start date for Blau several times, and the city did not fall until the end of June. A smaller action was taken in the meantime, pinching off a Soviet salient in the Second Battle of Kharkov, which resulted in the pocketing of a large Soviet force on 22 May.

Blau finally opened as Army Group South began its attack into southern Russia on June 28, 1942. The German offensive started well. Soviet forces offered little resistance in the vast empty steppes and started streaming eastward in disarray. Several attempts to re-establish a defensive line failed when German units outflanked them. Two major pockets were formed and destroyed: the first northeast of Kharkov on July 2 and a second, around Millerovo, Rostov Oblast, a week later.

Meanwhile, the Hungarian Second Army and the German 4th Panzer Army had launched an assault on Voronezh, capturing the city on the 5th of July.[[Image:Eastern Front 1942-05 to 1942-11.png|thumb|right|300px|Operation Blau: German advances from 7 May 1942 to 18 November 1942]]The initial advance of the Sixth Army was so successful that Hitler intervened and ordered the Fourth Panzer Army to join Army Group South (A) to the south. A massive traffic jam resulted when the Fourth Panzer and the Sixth both required the few roads in the area. Both armies were stopped dead while they attempted to clear the resulting mess of thousands of vehicles. The delay was long, and it is thought that it cost the advance at least one week. With the advance now slowed, Hitler changed his mind and re-assigned the Fourth Panzer Army back to the attack on Stalingrad.

By the end of July, the Germans had pushed the Soviets across the Don River. At this point, the Germans began using the armies of their Italian, Hungarian, and Romanian allies to guard their left (northern) flank. The German Sixth Army was only a few dozen kilometers from Stalingrad, and Fourth Panzer Army, now to their south, turned northwards to help take the city. To the south, Army Group A was pushing far into the Caucasus, but their advance slowed as supply lines grew overextended. The two German army groups were not positioned to support one another due to the great distances involved.

After German intentions became clear in July, Stalin appointed Marshall Andrei Yeremenko as commander of the Southeastern Front on August 1, 1942. Yeremenko and Commissar Nikita Krushchev were tasked with planning the defense of Stalingrad  . The eastern border of Stalingrad was the wide Volga River, and over the river additional Soviet units were deployed. This combination of units became the newly formed 62nd Army, which Yeremenko placed under the command of Lt. Gen. Vasiliy Chuikov on September 11, 1942. The 62nd Army's mission was to defend Stalingrad at all costs.

Before the Wehrmacht reached the city itself, the Luftwaffe had rendered the Volga River, vital for bringing supplies into the city, virtually unusable to Soviet shipping. Between 25 July and 31 July, 32 Soviet ships were sunk with another nine crippled.The battle began with the heavy bombing of the city by the Generaloberst von Richthofen's Luftflotte 4, which in the summer and autumn of 1942 was the mightiest single air command in the world. Some 1,000 tons were dropped. The city was quickly turned to rubble, although some factories survived and continued production whilst workers joined in the fighting.

Stalin prevented civilians from leaving the city on the premise that their presence would encourage greater resistance from the city's defenders. Civilians, including women and children, were put to work building trenchworks and protective fortifications. A massive German air bombardment on August 23 caused a firestorm, killing thousands and turning Stalingrad into a vast landscape of rubble and burnt ruins. Ninety percent of the living space in the Voroshilovskiy area was destroyed.

The Soviet Air Force, the Voenno-Vozdushnye Sily (VVS), was swept aside by the Luftwaffe. The VVS unit in the immediate area lost 201 aircraft from 23-31 August, and despite meager reinforcements of some 100 aircraft in August, it was with just 192 servicable aircraft which included just 57 fighters. The Soviets poured aerial reinforcements into the Stalingrad area in late September but continued to suffer appalling losses. The Luftwaffe had complete control of the skies.

The burden of the initial defense of the city fell on the 1077th Anti-Aircraft (AA) Regiment, a unit made up mainly of young women volunteers who had no training on engaging ground targets. Despite this, and with no support available from other Soviet units, the AA gunners stayed at their posts and took on the advancing Panzers. The German 16th Panzer Division reportedly had to fight the 1077thâ€™s gunners "shot for shot" until all 37 AA batteries were destroyed or overrun. In the beginning, the Soviets relied extensively on "Workers militias" composed of workers not directly involved in war production. For a short time, tanks continued to be produced and then manned by volunteer crews of factory workers. They were driven directly from the factory floor to the front line, often without paint or even gunsights.

By the end of August, Army Group South (B) had finally reached the Volga, north of Stalingrad. Another advance to the river south of the city followed. By September 1, the Soviets could only reinforce and supply their forces in Stalingrad by perilous crossings of the Volga, under constant bombardment by German artillery and aircraft.

On September 5, the Soviet 24th and 66th Armies organised a massive attack against XIV. Panzerkorps. The Luftwaffe helped the German forces repulse the offensive by subjecting Soviet artillery positions and defensive lines to heavy attack. The Soviets were forced to withdraw at midday after only a few hours. Of the 120 tanks the Soviets committed, 30 were lost to air attack.Soviet operations were constantly hampered by the Luftwaffe. On 18 September, the Soviet 1st Guards and 24th Army launched an offensive against VIII Armeekorps at Kotluban. VIII. Fliegerkorps dispatched wave after wave of Stuka dive-bombers to prevent a breakthrough. The offensive was repulsed, and the Stukas claimed 41 of the 106 Soviet tanks knocked out that morning while escorting Bf 109s destroyed 77 Soviet aircraft, shattering their remaining strength.Amid the debris of the wrecked city, the Soviet 62nd and 64th Armies, which included the Soviet 13th Guards Rifle Division, anchored their defense lines with strongpoints in houses and factories. Fighting was fierce and desperate. The life expectancy of a newly-arrived Soviet private in the city dropped to less than 24 hours, while that of a Soviet officer was about 3 days. Stalin's Order No. 227 of July 27 1942, decreed that all commanders who order unauthorized retreat should be subjects of a military tribunal. â€œNot a step back!â€?was the slogan. The Germans pushing forward into Stalingrad suffered heavy casualties.German military doctrine was based on the principle of combined-arms teams and close cooperation by tanks, infantry, engineers, artillery, and ground-attack aircraft. To counter this, Soviet commanders adopted the simple expedient of always keeping the front lines as close together as physically possible. Chuikov called this tactic "hugging" the Germans. This forced the German infantry to either fight on their own or risk taking casualties from their own supporting fire; it neutralized German close air support and weakened artillery support. Bitter fighting raged for every street, every factory, every house, basement and staircase. The Germans, calling this unseen urban warfare Rattenkrieg ("war of the rats"), bitterly joked about capturing the kitchen but still fighting for the living-room.

Fighting on Mamayev Kurgan, a prominent, blood-soaked hill above the city, was particularly merciless. The position changed hands many times. During one Soviet counter-attack, the Russians lost an entire division of 10,000 men in one day. At the Grain Silo, a huge grain-processing complex dominated by a single enormous silo, combat was so close that at times Soviet and German soldiers could hear each other breathe. Combat raged there for weeks. When German soldiers finally took the position, only forty Soviet bodies were found, though the Germans had thought there to be many more Soviet soldiers present due to the strength of Soviet resistance. In another part of the city, a Soviet platoon under the command of Yakov Pavlov turned an apartment building into an impenetrable fortress. The building, later called â€œPavlov's House,â€?oversaw a square in the city center. The soldiers surrounded it with minefields, set up machine-gun positions at the windows, and breached the walls in the basement for better communications.

With no end in sight, the Germans started transferring heavy artillery to the city, including the gigantic 800Â mm railroad gun nicknamed Dora. The Germans made no effort to send a force across the Volga, allowing the Soviets to build up a large number of artillery batteries there. Soviet artillery on the eastern bank continued to bombard the German positions. The Soviet defenders used the resulting ruins as defensive positions. German tanks became useless amid heaps of rubble up to 8 meters high. When they were able to move forward, they came under Soviet antitank fire from wrecked buildings.

Soviet snipers also successfully used the ruins to inflict heavy casualties on the Germans. The most successful sniper was Vasily Grigoryevich Zaytsev who is also the most famous . Vasiliy GrigorÂ´yevich Zaytsev was credited with 242 kills during the battle; he was also credited with killing a specially-sent German sniper named Heinz Thorvald (although questions have been raised regarding the historical evidence for this event).

For both Stalin and Hitler, the battle of Stalingrad became a prestige issue in addition to the actual strategic significance of the battle. The Soviet command moved the Red Army's strategic reserves from the Moscow area to the lower Volga, and transferred aircraft from the entire country to the Stalingrad region. The strain on both military commanders was immense: Paulus developed an uncontrollable tic in his eye, while Chuikov experienced an outbreak of eczema that required him to bandage his hands completely. Troops on both sides faced the constant strain of close-range combat.



Determined to crush Soviet resistance, Luftflotte 4s Stukawaffe flew 700 individual sorties against Soviet positions at the Dzherzhinskiy Tractor Factory on 5 October. Several Soviet regiments were wiped out; the entire staff of the Soviet 339th Infantry Regiment were killed the following morning during an air raid.

By mid-October, the Luftwaffe intensified its efforts against remaining Red Army positions holding the west bank. By now, Soviet aerial resistance had ceased to be effective. Luftflotte 4 flew 2,000 sorties on 14 October and 600 tons of bombs were dropped while German infantry surrounded the three factories. Stukageschwader 1, 2, and 77 had silenced Soviet artillery on the eastern bank of the Volga to a large degree before turning their attention to the shipping that was once again trying to reinforce the narrowing Soviet pockets of resistance. The 62nd Army had been cut in two, and, due to intensive air attack against its supply ferries, were now being paralyzed.

With the Soviets forced into a  strip of land on the western bank of the Volga, over 1,208 Stuka missions were flown in an effort to eliminate them. Despite the heavy air bombardment (Stalingrad suffered heavier bombardment than that inflicted on Sedan and Sevastopol), the Soviet 62 Army, with just 47,000 men and 19 tanks, prevented the VI. Armee and IV.Panzerarmee from wrestling the west bank out of Soviet control.

The Luftwaffe remained in command of the sky into early November, and Soviet aerial resistance during the day was nonexistent, but after flying 20,000 individual sorties, its original strength of 1,600 serviceable aircraft had fallen 40% to 950. The Kampfwaffe (bomber force) had been hardest hit, having only 232 out of a force of 480 left. Despite enjoying qualitative superiority against the VVS and possessing eighty percent of the Luftwaffe's resources on the Eastern Front, Luftflotte 4 could not prevent Soviet aerial power from growing. By the time of the counter-offensive, the Soviets were superior numerically.

The Soviet bomber force, the Aviatsiya Dalnego Destviya (ADD), having taken crippiling losses over the past 18 months, was restricted to flying at night. The Soviets flew 11,317 sorties in this manner, from 17 July to 19 November over Stalingrad and the Don-bend sector. These raids caused little damage and were of nuisance value only. The situation for the Luftwaffe was now becoming increasingly difficult. On 8 November substantial units from Luftflotte 4 were removed to combat the American landings in North Africa. The German air-arm found itself spread thin across Europe, and struggling to maintain its strength in the other southern sectors of the Soviet-German front.

After three months of carnage and slow and costly advance, the Germans finally reached the river banks, capturing 90% of the ruined city and splitting the remaining Soviet forces into two narrow pockets. In addition, ice-floes on the Volga now prevented boats and tugs from supplying the Soviet defenders across the river. Nevertheless, the fighting, especially on the slopes of Mamayev Kurgan and inside the factory area in the northern part of the city, continued as fiercely as ever. The battles for the Red October Steel Factory, the Dzerzhinsky tractor factory, and the Barrikady gun factory became world famous. While Soviet soldiers defended their positions and took the Germans under fire, factory workers repaired damaged Soviet tanks and other weapons close to the battlefield, sometimes on the battlefield itself.

During the siege, the German, Italian, Hungarian, and Romanian armies protecting Army Group South (B)'s flanks had pressed their headquarters for support. The Hungarian Second Army, consisting of mainly ill-equipped and ill-trained units, was given the task of defending a 200Â km section of the front north of Stalingrad. This resulted in a very thin line of defense with some parts where 1â€?Â km stretches were being guarded by a single platoon. Soviet forces held several points on the south bank of the river and presented a potentially serious threat to Army Group South (B). However, Hitler was so focused on the city itself that requests from the flanks for support were refused. The chief of the Army General Staff, Franz Halder, expressed concerns about Hitler's preoccupation with the city, pointing at the Germans' weak flanks, claiming that if the situation on the flanks was not rectified then 'there would be a disaster'. Hitler had claimed to Halder that Stalingrad would be captured and the weakened flanks would be held with 'national socialist ardour, clearly I cannot expect this of you (Halder)'. Halder was then replaced in mid-October with General Kurt Zeitzler.

[[Image:Battle Of Stalingrad.svg|thumbnail|right|300px|The Soviet counter-attack at Stalingrad]]

In autumn the Soviet generals Aleksandr Vasilyevskiy and Georgy Zhukov, responsible for strategic planning in the Stalingrad area, concentrated massive Soviet forces in the steppes to the north and south of the city. The German northern flank was particularly vulnerable, since it was defended by Italian, Hungarian, and Romanian units that suffered from inferior training, equipment, and morale when compared with their German counterparts. This weakness was known and exploited by the Soviets, who preferred to face off against non-German troops whenever it was possible, just as the British preferred attacking Italian troops, instead of German ones, whenever possible, in North Africa. The plan was to keep pinning the Germans down in the city, then punch through the overstretched and weakly defended German flanks and surround the Germans inside Stalingrad. During the preparations for the attack, Marshal Zhukov personally visited the front, which was rare for such a high-ranking general. The operation was code-named â€œUranusâ€?and launched in conjunction with Operation Mars, which was directed at Army Group Center. The plan was similar to Zhukov's victory at Khalkin Gol three years before, where he had sprung a double envelopment and destroyed the 23rd Division of the Japanese army.

On November 19, the Red Army unleashed Uranus. The attacking Soviet units under the command of Gen. Nikolay Vatutin consisted of three complete armies, the 1st Guards Army, 5th Tank Army, and 21st Army, including a total of 18 infantry divisions, eight tank brigades, two motorized brigades, six cavalry divisions and one anti-tank brigade. The preparations for the attack could be heard by the Romanians, who continued to push for reinforcements, only to be refused again. Thinly spread, outnumbered and poorly equipped, the Romanian Third Army, which held the northern flank of German Sixth Army, was shattered. On November 20, a second Soviet offensive (two armies) was launched to the south of Stalingrad, against points held by the Romanian IV Corps. The Romanian forces, made up primarily of infantry, collapsed almost immediately. Soviet forces raced west in a pincer movement, and met two days later near the town of Kalach, sealing the ring around Stalingrad. The Russians later reconstructed the link up for use as propaganda, and the piece of footage achieved worldwide fame.

Because of the Soviet pincer attack, about 230,000 German and Romanian soldiers, as well as some Croatian units and volunteer subsidiary troops found themselves trapped inside the resulting pocket. Inside the pocket (German: kessel) there also were the surviving Soviet civiliansâ€”around 10,000, and several thousand Soviet soldiers the Germans had taken captive during the battle. Not all German soldiers from Sixth Army were trapped; 50,000 were brushed aside outside the pocket. The encircling Red Army units immediately formed two defensive fronts: a circumvallation facing 'inward', to defend against breakout attempt, and a contravallation facing 'outward' to defend against any relief attempt.

Adolf Hitler had declared in a public speech (in the Berlin Sportpalast) on September 30 that the German army would never leave the city. At a meeting shortly after the Soviet encirclement, German army chiefs pushed for an immediate breakout to a new line on the west of the Don. But Hitler was at his Bavarian retreat of Obersalzberg in Berchtesgaden with the head of the Luftwaffe, GÃ¶ring. When asked by Hitler, GÃ¶ring replied, after being convinced by Hans Jeschonnek, that the Luftwaffe could supply the Sixth Army with an "air bridge". This would allow the Germans in the city to fight on while a relief force was assembled.

A similar plan had been used successfully a year earlier at the Demyansk Pocket, albeit on a much smaller scale: it had been only an army corps at Demyansk as opposed to an entire army. Also, Soviet fighter forces had improved considerably in both quality and quantity in the intervening year. But the mention of the successful Demyansk air supply operation reinforced Hitler's own views, and was endorsed by Hermann GÃ¶ring several days later.

The head of the Fourth Air Fleet (Luftflotte 4), Wolfram von Richthofen, tried to have this decision overturned without success. The Sixth Army would be supplied by air. The Sixth Army was the largest unit of this type in the world, almost twice as large as a regular German army. Also trapped in the pocket was a corps of the Fourth Panzer Army. It should have been clear that supplying the pocket by air was impossible, and the maximum 117.5 tons they could deliver a day would be less than the 800 needed by the pocket. To supplement the limited number of Junkers Ju 52 transports, bomber units equipped with aircraft wholly inadequate for the role, such as the He-177, (the Heinkel He-111 proved to be quite capable and was a lot faster than the Ju 52) were pressed into service. But Hitler backed GÃ¶ring's plan and reiterated his order of "no surrender" to his trapped armies.

The air supply mission failed. Appalling weather conditions, technical failures, heavy Soviet anti-aircraft fire and fighter interceptions led to the loss of 488 German aircraft. The Luftwaffe failed to achieve even the maximum supply capacity of 117 tons that it was capable of. An average of 94 tons of supplies per day was delivered to the trapped German Army. Even then, it was often inadequate or unnecessary; one aircraft arrived with 20 tonnes of Vodka and summer uniforms, completely useless in their current situation The transport aircraft that did land safely were used to evacuate technical specialists and sick or wounded men from the besieged enclave (some 42,000 were evacuated in all). The Sixth Army slowly starved. Pilots were shocked to find the troops assigned to offloading the planes too exhausted and hungry to unload food. General Zeitzler, moved by the troops' plight at Stalingrad, began to limit himself to their slim rations at meal times. After a few weeks of such a diet he'd grown so emaciated that Hitler, annoyed, personally ordered him to start eating regular meals again.

The expense to the Transportgruppen was heavy. Some 266 Junkers Ju 52s were destroyed, one-third of the fleets strength on the Soviet-German front. The He 111 gruppen lost 165 aircraft in transport operations. Other losses included 42 Junkers Ju 86s, nine Fw 200 "Condors", five He 177 bombers and a single Ju 290. The Luftwaffe also lost close to 1,000 highly experienced bomber crew personnel.

So heavy were the Luftwaffe's losses that four of Luftflotte 4s transport units (KGrzbV 700, KGrzbV 900, I./KGrzbV 1 and II./KGzbV 1) were "formally dissolved".

Soviet forces consolidated their positions around Stalingrad, and fierce fighting to shrink the pocket began. An attack by a German battlegroup formed to relieve the trapped armies from the South, Operation Wintergewitter (â€œOperation Winter Thunderstormâ€? was successfully fended off by the Soviets in December. The full impact of the harsh Russian winter set in. The Volga froze solid, allowing the Soviets to supply their forces in the city more easily. The trapped Germans rapidly ran out of heating fuel and medical supplies, and thousands started dying of frostbite, malnutrition and disease.

On December 16, the Soviets launched a second offensive, Operation Saturn, which attempted to punch through the Axis army on the Don and take Rostov. If successful, this offensive would have trapped the remainder of Army Group South, one third of the entire German Army in Russia, in the Caucasus. The Germans set up a "mobile defense" in which small units would hold towns until supporting armor could arrive. The Soviets never got close to Rostov, but the fighting forced von Manstein to extract Army Group A from the Caucasus and restabilize the frontline some 250Â km away from the city. The Tatsinskaya Raid also caused significant losses to Luftwaffeâ€™s transport fleet. The Sixth Army now was beyond all hope of German reinforcement. The German troops in Stalingrad were not told this, however, and continued to believe that reinforcements were on their way. Some German officers requested that Paulus defy Hitlerâ€™s orders to stand fast and instead attempt to break out of the Stalingrad pocket. Paulus refused, as he abhorred the thought of disobeying orders. Also, whereas a breakout may have been possible in the first few weeks, at this late stage, Sixth Army was short of the fuel required for such a breakout. The German soldiers would have faced great difficulty breaking through the Soviet lines on foot in harsh winter conditions.



The Germans inside the pocket retreated from the suburbs of Stalingrad to the city itself. The loss of the two airfields at Pitomnik on 16 January and Gumrak on the 25 January meant an end to air supplies and to the evacuation of the wounded. The Germans were now not only starving, but running out of ammunition. Nevertheless they continued to resist stubbornly, partly because they believed the Soviets would execute those who surrendered. In particular, the so-called "HiWis", Soviet citizens fighting for the Germans, had no illusions about their fate if captured. The Soviets, in turn, were initially surprised by the large number of German forces they had trapped, and had to reinforce their encircling forces. Bloody urban warfare began again in Stalingrad, but this time it was the Germans who were pushed back to the banks of the Volga. A Soviet envoy made Paulus a generous surrender offerâ€”that if he surrendered within 24 hours, the Germans would receive a guarantee of safety for all prisoners, medical care for the German sick and wounded, a promise that prisoners would be allowed to keep their personal belongings, "normal" food rations, and repatriation to whatever country they wished to go to after the warâ€”but Paulus, ordered not to surrender by Adolf Hitler, did not reply, ensuring the destruction of the 6th Army.

Hitler promoted Friedrich Paulus to Generalfeldmarschall on January 30, 1943, (the 10th anniversary of Hitler coming to power). Since no German Field Marshal had ever been taken prisoner, Hitler assumed that Paulus would fight on or take his own life. Nevertheless, when Soviet forces closed in on Paulus' headquarters in the ruined GUM department store the next day, Paulus surrendered. The remnants of the German forces in Stalingrad surrendered on February 2; 91,000 tired, ill, and starving Germans were taken captive. To the delight of the Soviet forces and the dismay of the Third Reich, the prisoners included 22 generals. Hitler was furious at the Field Marshalâ€™s surrender and confided that "Paulus stood at the doorstep of eternal glory but made an about-face". According to the German documentary film Stalingrad, over 11,000 German and Axis soldiers refused to lay down their arms at the official surrender, seemingly believing that fighting to the death was better than what seemed like a slow end in Soviet camps. These forces continued to resist until early March 1943, hiding in cellars and sewers of the city with their numbers being diminished at the same time by Soviet forces clearing the city of remaining enemy resistance. By March, what remained of these forces were small and isolated pockets of resistance that surrendered. According to Soviet intelligence documents shown in the documentary, 2,418 of the men were killed, and 8,646 were captured.

Only 5,000 of the 91,000 German prisoners of war survived their captivity and returned home. Already weakened by disease, starvation and lack of medical care during the encirclement, they were sent to labour camps all over the Soviet Union, where most of them died of overwork and malnutrition. A handful of senior officers were taken to Moscow and used for propaganda purposes and some of them joined National Committee for a Free Germany. Some, including Paulus, signed anti-Hitler statements which were broadcast to German troops. General Walther von Seydlitz-Kurzbach offered to raise an anti-Hitler army from the Stalingrad survivors, but the Soviets did not accept this offer. It was not until 1955 that the last of the handful of survivors were repatriated.

The German public was not officially told of the disaster until the end of January 1943, though positive reports in the German propaganda media about the battle had stopped in the weeks before the announcement. It was not the first major setback of the German military, but the crushing defeat at Stalingrad was unmatched in scale. On February 18, the minister of propaganda, Joseph Goebbels, gave his famous Sportpalast speech in Berlin, encouraging the Germans to accept a total war which would claim all resources and efforts from the entire population.



The battle of Stalingrad was one of the largest battles in human history. It raged for 199 days. Numbers of casualties are difficult to compile due to the vast scope of the battle and the fact that the Soviet government did not allow estimates to be made, for fear the cost would be shown to be too high. In its initial phases, the Germans inflicted heavy casualties on Soviet formations; but the Soviet encirclement by punching through the German flank, mainly held by Romanian troops, effectively besieged the remainder of German Sixth Army, which had taken heavy casualties in street fighting prior to this. At different times the Germans had held up to 90% of the city, yet the Soviet soldiers and officers fought on fiercely. Some elements of the German Fourth Panzer Army also suffered casualties in operations around Stalingrad during the Soviet counter offensive.

Various scholars have estimated the Axis suffered 850,000 casualties of all types (wounded, killed, captured...etc) among all branches of the German armed forces and its allies, many of which were POWs who died in Soviet captivity between 1943 and 1955,: 400,000 Germans, 200,000 Romanians, 130,000 Italians, and 120,000 Hungarians were killed, wounded or captured. Of the 91,000 German POW's taken at Stalingrad 27,000 died within weeks and only 5,000 returned to Germany in 1955. The remainder of the POWs died in Soviet captivity. In the whole Stalingrad area the Axis lost 1.5 million killed, wounded or captured . 50,000 ex-Soviets Hiwis (local volunteers incorporated into the German forces in supporting capacities) were killed or captured by the Red Army. According to archival figures, the Red Army suffered a total of 1,129,619 total casualties ; 478,741 men killed and captured and 650,878 wounded. These numbers are for the whole Stalingrad Area, in the city itself 750,000 were killed, captured, or wounded. Also, more than 40,000 Soviet civilians died in Stalingrad and its suburbs during a single week of aerial bombing as the German Fourth Panzer and Sixth armies approached the city; the total number of civilians killed in the regions outside the city is unknown. In all, the battle resulted in an estimated total of 1.7 million to 2 million Axis and Soviet casualties.

Besides being a turning point in the war, Stalingrad was also revealing in terms of the discipline and determination of both the German Wehrmacht and the Soviet Red Army, though this was often maintained by brutal enforcement of commands. The Soviets first defended Stalingrad against a fierce German onslaught. So great were Soviet losses that at times, the life expectancy of a newly arrived soldier was less than a day, and the life expectancy of a Soviet officer was three days. Their sacrifice is immortalised by a soldier of General Rodimtsev, about to die, who scratched on the wall of the main railway station (which changed hands 15 times during the battle) â€œRodimtsevâ€™s Guardsmen fought and died here for their Motherland.â€?
For the heroism of the Soviet defenders of Stalingrad, the city was awarded the title Hero City in 1945. After the war, in the 1960s, a colossal monument of â€œMother Motherlandâ€?was erected on Mamayev Kurgan, the hill overlooking the city. The statue forms part of a memorial complex which includes ruined walls deliberately left the way they were after the battle. The Grain Silo, as well as Pavlov's House, the apartment building whose defenders eventually held out for two months until they were relieved, can still be visited. Even today, one may find bones and rusty metal splinters on Mamayev Kurgan, symbols of both the human suffering during the battle and the successful yet costly resistance against the German invasion.

On the other side, the German Army showed remarkable discipline after being surrounded. It was the first time that it had operated under adverse conditions on such a scale. Short of food and clothing, during the latter part of the siege, many German soldiers starved or froze to death. Yet, discipline and obedience to authority prevailed until the very end, when resistance no longer served any useful purpose. Generalfeldmarschall Friedrich Paulus obeyed Hitler's orders, against many of Hitler's top generals' counsel and advice (including that of von Manstein) to not attempt to break out of the city before German ammunition, supplies, and food became totally exhausted. Hitler ordered Paulus to stay, and then promoted him to Field Marshal. Hitler, acting on GÃ¶ring's advice, believed the German 6th Army could be supplied by air; the Luftwaffe had successfully accomplished an aerial resupply in January 1942, when a German garrison was surrounded in Demyansk for four months by the Red Army. In this case, however, there were obvious differences. The encircled forces at Demyansk were a much smaller garrison, while an entire army was trapped in the Stalingrad pocket. By the time Hitler made him a Field Marshal, Paulus knew Stalingrad was lost and the airlift had failed. Hitler thought that Paulus would commit suicide, the traditional German general's method of surrender; promoting him was a consolatory gesture, and further impetus for Paulus to avoid being taken by the Soviets alive. Paulus would have been the highest ranking German commander to be captured, and that was not acceptable to Hitler. However, Paulus disobeyed Hitler, shortly after being promoted to Field Marshal, saying that as a Christian he could not, in good faith, kill himself. Hitler did not find this reasonable, and openly lambasted Paulus for being the only Field Marshal in German history to surrender alive.

The extreme conditions of the battle, including the paralyzing Soviet winter that precipitated massive German fatalities due to starvation and freezing, have been immortalized in several films of German, Russian, and American origin. The struggle is also remembered and reflected upon in countless books, for its significance in thwarting the German invasion, as well as its significance as a landmark of military barbarism and human suffering in which the loss of life was unprecedented.













Ethel Merman (January 16, 1908 â€?February 15, 1984) was a Tony Award- and Grammy Award-winning American star of stage and film musicals, well known for her powerful voice, often hailed by critics as "The Grande Dame of the Broadway stage".

Merman was born Ethel Agnes Zimmermann in her maternal grandmother's house at 359 4th Avenue, Astoria, Queens, New York. Her father, Edward Zimmermann, was an accountant, and her mother, Agnes (nÃ©e Gardner), was a school teacher. Merman's father was German American and Lutheran, and her mother was Scottish American and Presbyterian; she was baptized Episcopalian. She attended PS 6 on Steinway Street in Astoria. She used to stand outside the Famous Players-Lasky Studios and wait to see her favorite Broadway star, Alice Brady. Ethel loved to sing songs like "By the Light of the Silvery Moon" and "Alexander's Ragtime Band" while her adoring father accompanied her on the piano. William Cullen Bryant High School in Astoria named its auditorium Ethel Merman Theater.

Merman was known for her powerful, belting mezzo-soprano â€?alto voice, precise enunciation, and pitch. Because stage singers performed without microphones when she began singing professionally, she had great advantages in show business, despite the fact that she never received any singing lessons. In fact, Broadway lore holds that George Gershwin warned her never to take a singing lesson after seeing her opening reviews for Girl Crazy. Stephen Sondheim, who wrote the lyrics for Merman's , remembered that she could become "mechanical" after a while. "She performed the dickens out of the show when the critics were there," he said. He added, "or if she thought there was a celebrity in the audience. So we used to spread a rumor that Frank Sinatra was out front. That whoever, Judy Garland was out front. I'll tell you one thing [Merman] did do, she steadily upstaged everybody. Every night, she would be about one more foot upstage, so finally they were all playing with their backs to the audience. I don't think it was conscious. Ethel was not big on brains. But she sure knew her way around a stage, and it was all instinctive."



Merman began singing while working as a secretary for the B-K Booster (automobile) Vacuum Brake Company in Queens. She eventually became a full time vaudeville performer and played the pinnacle of vaudeville, the Palace Theatre in New York City. She had already been engaged for Girl Crazy, a musical with songs by George and Ira Gershwin, which also starred a very young Ginger Rogers (19 years old) in 1930. Although third billed, her rendition of "I Got Rhythm" in the show was popular, and by the late 1930s, she had become the first lady of the Broadway musical stage. Many consider her the leading Broadway musical performer of the Twentieth Century, with her signature song being "There's No Business Like Show Business" (from Annie Get Your Gun).

Merman starred in five Cole Porter musicals, among them Anything Goes in 1934, where she introduced "I Get a Kick Out of You", "Blow Gabriel Blow", and the title song. Her next musical with Porter was Red, Hot and Blue, in which she co-starred with Bob Hope and Jimmy Durante and introduced "It's Delovely" and "Down in the Depths (on the 90th floor)". In 1939's DuBarry Was a Lady, Porter provided Merman with a "can you top this" duet with Bert Lahr, "Friendship". Like "You're the Top" in Anything Goes, this kind of duet became one of her signatures. Porter's lyrics also helped showcase her comic talents in duets in Panama Hattie ("Let's Be Buddies", "I've Still Got My Health"), and Something for the Boys ("By the Mississinewah", "Hey Good Lookin'").

Irving Berlin supplied Merman with equally memorable duets, including counterpoint songs "An Old-Fashioned Wedding" with Bruce Yarnell, written for the 1966 revival of Annie Get Your Gun, and "You're Just in Love" with Russell Nype in Call Me Madam. Merman won the 1951 Tony Award for Best Actress for her performance as Sally Adams in Call Me Madam. She reprised her role in the lively Walter Lang film version.

Perhaps Merman's most revered performance was in  as Gypsy Rose Lee's mother Rose. Merman introduced "Everything's Coming Up Roses" and "Some People" and ended the show with the wrenching "Rose's Turn". Critics and audiences saw her creation of Madame Rose as the performance of her career. She did not get the role in the movie version, however, which went to movie actress Rosalind Russell, and an infuriated Merman was quoted as saying: "There's a name for women like her but it's seldom used in society outside [of] a kennel." (Since this is a line from the film The Women, in which Russell appeared, the story may be apocryphal.) She also insulted Russell's husband, Freddie Brisson, by calling him the "Lizard of Roz". [citation needed] Merman decided to take Gypsy on the road and trumped the motion picture as a result.

Merman lost the Tony Award to Mary Martin, who was playing Maria in The Sound of Music. "How can you buck a nun?" mused Merman. The competitiveness notwithstanding, Merman and Martin were friends off stage and starred in a legendary musical special on television.Merman retired from Broadway in 1970, when she appeared as the last Dolly Levi in Hello, Dolly!, a show initially written for her. No longer willing to "take the veil," as she described being in a Broadway role, Merman preferred to act in television specials and movies. Despite having a reputation for a salty tongue and having introduced ribald Cole Porter lyrics, Merman was known to dislike 1970s theatre fare like Oh! Calcutta! for being lewd.

Merman's film career was not as distinguished as her stage roles. Though she reprised her roles in Anything Goes and Call Me Madam, film executives would not select her for Annie Get Your Gun or Gypsy. Some critics state the reason for losing the roles was that her outsized stage persona did not fit well on the screen. Others have said that after her behavior on the set of Twentieth-Century Fox's There's No Business Like Show Business, Jack Warner refused to have her in any of his motion pictures, thereby causing her to lose the role of Rose in Gypsy, though some believe Rosalind Russell's husband and agent, Freddie Brisson, negotiated the rights away from Merman for his wife. Nonetheless, Stanley Kramer decided to cast her as the battle-axe Mrs. Marcus, mother-in-law of Milton Berle, in the madcap It's a Mad, Mad, Mad, Mad World.

Merman's last movie role was a self-parody in the comedy movie Airplane!, appearing as a soldier, Lieutenant Hurwitz. Hurwitz is suffering from shell shock and thinks he is Ethel Merman. Merman sings "Everything's Coming Up Roses", while the nurses drag her back to bed and give her a sedative. In 1979, she recorded the infamous The Ethel Merman Disco Album, with many of her signature show-stoppers set to a disco beat. 

Merman was married and divorced four times:

Merman co-wrote two volumes of memoirs, Who Could Ask for Anything More in 1955 and Merman in 1978. In a radio interview, Merman commented on her many marriages, saying that "We all make mistakes, that's why they put rubbers on pencils, and that's what I did. I made a few loo-loos!" In the latter book, the chapter entitled "My Marriage to Ernest Borgnine" consists of one blank page. 

Merman was pre-deceased by one of her two children, daughter Ethel Levitt (known as "Ethel, Jr." and "Little Bit"). In 1983, as she was preparing to go to Los Angeles to appear at the Oscars that year, Merman collapsed. Although the original physician assessment was that Merman had suffered a stroke, tests later revealed an inoperable brain tumor. The severity of her condition was kept out of the press, and only a few close friends were allowed to visit. As her condition deteriorated, she was cared for by her son, Bobby. She died February 15, 1984--less than a month after her 76th birthday.

On February 20, 1984, Ethel's son, Robert Levitt, Jr., held his mother's ashes as he rode down Broadway. He passed the Imperial, the Broadway and the Majestic theatres, where Merman had performed all her life. A minute before the curtains of these theatres opened that night, all of the Broadway marquees dimmed their lights in remembrance of her. It was Robert's sentimental gesture that led to the legend that Merman's ashes were "scattered over Broadway," a rather unusual plan and an illegal method of disposing of human remains.

Merman is mentioned in the Broadway musical The Producers. During the song "Springtime for Hitler", Hitler sings "Heil myself, Watch my show! I'm the German Ethel Merman, don't ya know!"

Merman was also mentioned by Nellie McKay in her song "Change The World". McKay sings, "God, I'm so German, have to have a plan. Please, Ethel Merman, help me out this jam."

Merman had a cameo appearance in the movie Airplane! when a combat veteran suffering from "severe shell-shock" believed he was Ethel Merman. During the course of the joke she sat up in bed and sang a few bars of "Everything's Coming Up Roses".

The British Psychobillyband The Meteors recorded an instrumental called "Return Of The Ethel Merman" for their 1986 album "Sewertime Blues".

Merman is mentioned a lot in the musical series Forbidden Broadway making fun of the wireless microphones and soft singing used in The Phantom of the Opera (1986 musical).

In the 1987 film Good Morning, Vietnam, Army radio disc jockey Adrian Cronauer (played by Robin Williams) alluded to Merman's distinctive, brassy style and powerful voice during one of his improvised comic news bulletins. "Ethel Merman has been used to jam Russian radar systems. {belting in imitation of Merman} 'I've got a feeling that love is here to stay!' When asked for a reply, the Russians said 'Vat de hell vas dat?'"

A San Francisco Female Impersonator named Mark Sargent has a successful act called"The Ethel Merman Experience" where Mr. Sargent performs popular rock and roll songs'in the style of Ethel Merman' while in drag dressed as Ethel Merman.Ethel Merman Experience pageEthel Merman Experience video clip

A blooper recording announced "the robust voice of Ethel Murmur"!

Courtesy of NPRWindows Media Player Required











 Law is a system of rules usually enforced through a set of institutions. It affects politics, economics and society in numerous ways. Contract law regulates everything from buying a bus ticket to trading swaptions on a derivatives market. Property law defines rights and obligations related to transfer and title of personal and real property, for instance, in mortgaging or renting a home. Trust law applies to assets held for investment and financial security, such as pension funds. Tort law allows claims for compensation when someone or their property is injured or harmed. If the harm is criminalised in a penal code, criminal law offers means by which the state prosecutes and punishes the perpetrator. Constitutional law provides a framework for creating laws, protecting people's human rights, and electing political representatives. Administrative law relates to the activities of administrative agencies of government. International law regulates affairs between sovereign nation-states in everything from trade to the environment to military action. "The rule of law", wrote the ancient Greek philosopher Aristotle in 350 BC, "is better than the rule of any individual."

Legal systems around the world elaborate legal rights and responsibilities in different ways. A basic distinction is made between civil law jurisdictions and systems using common law. Some countries persist in basing their law on religious texts. Scholars investigate the nature of law through many perspectives, including legal history and philosophy, or social sciences such as economics and sociology. The study of law raises important questions about equality, fairness and justice, which are not always simple. "In its majestic equality", said the author Anatole France in 1894, "the law forbids rich and poor alike to sleep under bridges, beg in the streets and steal loaves of bread." The most important institutions for law are the judiciary, the legislature, the executive, its bureaucracy, the military and police, the legal profession and civil society. 

Though all legal systems must deal with similar issues, different countries often categorise and name legal subjects in different ways. Quite common is the distinction between "public law" subjects, which relate closely to the state (including constitutional, administrative and criminal law), and "private law" subjects (including contract, tort and property). In civil law systems, contract and tort fall under a general law of obligations and trusts law is dealt with under statutory regimes or international conventions. International, constitutional and administrative law, criminal law, contract, tort, property law and trusts are regarded as the "traditional core subjects", although there are many further disciplines which might be of greater practical importance. 

In a global economy, law is globalising too. International law can refer to three things: public international law, private international law or conflict of laws and the law of supranational organisations.



Constitutional and administrative law govern the affairs of the state. Constitutional law concerns both the relationships between the executive, legislature and judiciary and the human rights or civil liberties of individuals against the state. Most jurisdictions, like the United States and France, have a single codified constitution, with a Bill of Rights. A few, like the United Kingdom, have no such document; in those jurisdictions the constitution is composed of statute, case law and convention. A case named Entick v. Carrington illustrates a constitutional principle deriving from the common law. Mr Entick's house was searched and ransacked by Sheriff Carrington. When Mr Entick complained in court, Sheriff Carrington argued that a warrant from a Government minister, the Earl of Halifax, was valid authority. However, there was no written statutory provision or court authority. The leading judge, Lord Camden, stated that,

"The great end, for which men entered into society, was to secure their property. That right is preserved sacred and incommunicable in all instances, where it has not been taken away or abridged by some public law for the good of the wholeâ€?If no excuse can be found or produced, the silence of the books is an authority against the defendant, and the plaintiff must have judgment."

The fundamental constitutional principle, inspired by John Locke, is that the individual can do anything but that which is forbidden by law, and the state may do nothing but that which is authorised by law. Administrative law is the chief method for people to hold state bodies to account. People can apply for judicial review of actions or decisions by local councils, public services or government ministries, to ensure that they comply with the law. The first specialist administrative court was the Conseil d'Ã‰tat set up in 1799, as Napoleon assumed power in France.



Criminal law is the body of law that defines criminal offences and the penalties for convicted offenders. Apprehending, charging, and trying suspected offenders is regulated by the law of criminal procedure. The paradigm case of a crime lies in the proof, beyond reasonable doubt, that a person is guilty of two things. First, the accused must commit an act which is deemed by society to be criminal, or actus reus (guilty act). Second, the accused must have the requisite malicious intent to do a criminal act, or mens rea (guilty mind). However for so called "strict liability" crimes, which include cases like dangerous driving, proof of mens rea is not necessary. An actus reus is enough.

Examples of different kinds of crime include murder, assault, fraud or theft. In exceptional circumstances, defences can exist to some crimes, such as killing in self defence, or pleading insanity. Another example is in the 19th century English case of ''R v. Dudley and Stephens'', which tested a defence of "necessity". The Mignotte, sailing from Southampton to Sydney, sank. Three crew members and a cabin boy were stranded on a raft. They were starving and the cabin boy was close to death. Driven to extreme hunger, the crew killed and ate the cabin boy. The crew survived and were rescued, but put on trial for murder. They argued it was necessary to kill the cabin boy to preserve their own lives. Lord Coleridge, expressing immense disapproval, ruled, "to preserve one's life is generally speaking a duty, but it may be the plainest and the highest duty to sacrifice it." The men were sentenced to hang, but public opinion, especially among seafarers, was outraged and overwhelmingly supportive of the crew's right to preserve their own lives. In the end, the Crown commuted their sentences to six months in jail.

Criminal law offences are viewed as offences against not just individual victims, but the community as well. The state, usually with the help of police, takes the lead in prosecution, which is why in common law countries cases are cited as "The People v. â€? or "R. (for Rex or Regina) v. â€? Also, lay juries are often used to determine the guilt of defendants on points of fact: juries cannot change legal rules. Some developed countries still condone capital punishment for criminal activity, but the normal punishment for a crime will be imprisonment, fines, state supervision (such as probation), or community service. Modern criminal law has been affected considerably by the social sciences, especially with respect to sentencing, legal research, legislation, and rehabilitation. On the international field, 105 countries have signed the enabling treaty for the International Criminal Court, which was established to try people for crimes against humanity.



The concept of a "contract" is based on the Latin phrase pacta sunt servanda (agreements must be kept). Contracts can be simple everyday buying and selling or complex multi-party agreements. They can be made orally (e.g. buying a newspaper) or in writing (e.g. signing a contract of employment). Sometimes formalities, such as writing the contract down or having it witnessed, are required for the contract to take effect (e.g. when buying a house).

In common law jurisdictions, there are three key elements to the creation of a contract. These are offer and acceptance, consideration and an intention to create legal relations. For example, in Carlill v. Carbolic Smoke Ball Company a medical firm advertised that its new wonder drug, the smokeball, would cure people's flu, and if it did not, the buyers would get Â£100. Many people sued for their Â£100 when the drug did not work. Fearing bankruptcy, Carbolic argued the advert was not to be taken as a serious, legally binding offer. It was an invitation to treat, mere puff, a gimmick. But the court of appeal held that to a reasonable man Carbolic had made a serious offer. People had given good consideration for it by going to the "distinct inconvenience" of using a faulty product. "Read the advertisement how you will, and twist it about as you will", said Lord Justice Lindley, "here is a distinct promise expressed in language which is perfectly unmistakable".

"Consideration" means all parties to a contract must exchange something of value to be able to enforce it. Some common law systems, like Australia, are moving away from consideration as a requirement for a contract. The concept of estoppel or culpa in contrahendo can be used to create obligations during pre-contractual negotiations. In civil law jurisdictions, consideration is not a requirement for a contract at all. In France, an ordinary contract is said to form simply on the basis of a "meeting of the minds" or a "concurrence of wills". Germany has a special approach to contracts, which ties into property law. Their 'abstraction principle' (Abstraktionsprinzip) means that the personal obligation of contract forms separately from the title of property being conferred. When contracts are invalidated for some reason (e.g. a car buyer is so drunk that he lacks legal capacity to contract) the contractual obligation to pay can be invalidated separately from the proprietary title of the car. Unjust enrichment law, rather than contract law, is then used to restore title to the rightful owner.



Torts, sometimes called delicts, are civil wrongs. To have acted tortiously, one must have breached a duty to another person, or infringed some pre-existing legal right. A simple example might be accidentally hitting someone with a cricket ball. Under negligence law, the most common form of tort, the injured party could potentially claim compensation for his injuries from the party responsible. The principles of negligence are illustrated by Donoghue v. Stevenson. A friend of Mrs Donoghue ordered an opaque bottle of ginger beer (intended for the consumption of Mrs Donoghue) in a cafÃ© in Paisley. Having consumed half of it, Mrs Donoghue poured the remainder into a tumbler. The decomposing remains of a snail floated out. She claimed to have suffered from shock, fell ill with gastroenteritis and sued the manufacturer for carelessly allowing the drink to be contaminated. The House of Lords decided that the manufacturer was liable for Mrs Donoghue's illness. The case was furtherly complicated in that, the contract of the purchase was between Mrs Donoghue's friend, as the buyer of the ginger beer, rather than Mrs Donoghue herself. Lord Atkin took a distinctly moral approach, and said,

"The liability for negligenceâ€?is no doubt based upon a general public sentiment of moral wrongdoing for which the offender must payâ€?The rule that you are to love your neighbour becomes in law, you must not injure your neighbour; and the lawyer's question, Who is my neighbour? receives a restricted reply. You must take reasonable care to avoid acts or omissions which you can reasonably foresee would be likely to injure your neighbour."

This became the basis for the four principles of negligence; (1) Mr Stevenson owed Mrs Donoghue a duty of care to provide safe drinks (2) he breached his duty of care (3) the harm would not have occurred but for his breach and (4) his act was the proximate cause, or not too remote a consequence, of her harm. Another example of tort might be a neighbour making excessively loud noises with machinery on his property. Under a nuisance claim the noise could be stopped. Torts can also involve intentional acts, such as assault, battery or trespass. A better known tort is defamation, which occurs, for example, when a newspaper makes unsupportable allegations that damage a politician's reputation. More infamous are economic torts, which form the basis of labour law in some countries by making trade unions liable for strikes, when statute does not provide immunity.

Property law governs everything that people call 'theirs'. Real property, sometimes called 'real estate' refers to ownership of land and things attached to it. Personal property, refers to everything else; movable objects, such as computers, cars, jewelry, and sandwiches, or intangible rights, such as stocks and shares. A right in rem is a right to a specific piece of property, contrasting to a right in personam which allows compensation for a loss, but not a particular thing back. Land law forms the basis for most kinds of property law, and is the most complex. It concerns mortgages, rental agreements, licences, covenants, easements and the statutory systems for land registration. Regulations on the use of personal property fall under intellectual property, company law, trusts and commercial law. An example of a basic case of most property law is Armory v. Delamirie. A chimney sweep's boy found a jewel encrusted with precious stones. He took it to a goldsmith to have it valued. The goldsmith's apprentice looked at it, sneakily removed the stones, told the boy it was worth three halfpence and that he would buy it. The boy said he would prefer the jewel back, so the apprentice gave it to him, but without the stones. The boy sued the goldsmith for his apprentice's attempt to cheat him. Lord Chief Justice Pratt ruled that even though the boy could not be said to own the jewel, he should be considered the rightful keeper until the original owner is found. In fact the apprentice and the boy both had a right of possession in the jewel (a technical concept, meaning evidence that something could belong to someone), but the boy's possessory interest was considered better, because it could be shown to be first in time. Physical possession is nine tenths of the law, but not all.

This case is used to support the view of property in common law jurisdictions, that person who can show the best claim to a piece of property, against any contesting party, is the owner.By contrast, the classic civil law approach to property, propounded by Friedrich Carl von Savigny, is that it is a right good against the world. Obligations, like contracts and torts are conceptualised as rights good between individuals. The idea of property raises many further philosophical and political issues. The English philosopher John Locke argued that our "lives, liberties and estates" are our property because we own our bodies and mix our labour with our surroundings. The idea of privately owned property has been contentious in the view of a number of thinkers. Pierre Proudhon, an anarchist thinker, argued in 1840 that "property is theft".

Equity is a body of rules that developed in England separately from the "common law". The common law was administered by judges. The Lord Chancellor on the other hand, as the King's keeper of conscience, could overrule the judge made law if he thought it equitable to do so. This meant equity came to operate more through principles than rigid rules. For instance, whereas neither the common law nor civil law systems allow people to split the ownership from the control of one piece of property, equity allows this through an arrangement known as a 'trust'. 'Trustees' control property, whereas the 'beneficial' (or 'equitable') ownership of trust property is held by people known as 'beneficiaries'. Trustees owe duties to their beneficiaries to take good care of the entrusted property. In the early case of Keech v. Sandford a child had inherited the lease on a market in Romford, London. Mr Sandford was entrusted to look after this property until the child matured. But before then, the lease expired. The landlord had (apparently) told Mr Sandford that he did not want the child to have the renewed lease. Yet the landlord was happy (apparently) to give Mr Sandford the opportunity of the lease instead. Mr Sandford took it. When the child (now Mr Keech) grew up, he sued Mr Sandford for the profit that he had been making by getting the market's lease. Mr Sandford was meant to be trusted, but he put himself in a position of conflict of interest. The Lord Chancellor, Lord King, agreed and ordered Mr Sandford should disgorge his profits. He wrote,

"I very well see, if a trustee, on the refusal to renew, might have a lease to himself few trust-estates would be renewedâ€?This may seem very hard, that the trustee is the only person of all mankind who might not have the lease; but it is very proper that the rule should be strictly pursued and not at all relaxed."

Of course, Lord King LC was worried that trustees might exploit opportunities to use trust property for themselves instead of looking after it. Business speculators using trusts had just recently caused a stock market crash. Strict duties for trustees made their way into company law and were applied to directors and chief executive officers. Another example of a trustee's duty might be to invest property wisely or sell it. This is especially the case for pension funds, the most important form of trust, where investors are trustees for people's savings until retirement. But trusts can also be set up for charitable purposes, famous examples being the British Museum or the Rockefeller Foundation.

Law spreads far beyond the core subjects into virtually every area of life. Three categories are presented for convenience, though the subjects intertwine and flow into one another.







In general, legal systems around the world can be split between civil law jurisdictions, on the one hand, and systems using common law and equity, on the other. The term civil law, referring to a legal system, should not be confused with civil law as a group of legal subjects, as distinguished from criminal law or public law. A third type of legal system â€?still accepted by some countries in part, or even in whole â€?is religious law, based on scriptures and interpretations thereof. The specific system that a country follows is often determined by its history, its connection with countries abroad, and its adherence to international standards. The sources that jurisdictions recognise as authoritatively binding are the defining features of legal systems. Yet classification of different systems is a matter of form rather than substance, since similar rules often prevail.

Civil law is the legal system used in most countries around the world today. In civil law the sources recognised as authoritative are, primarily, legislation â€?especially codifications in constitutions or statutes passed by government â€?and, secondarily, custom. Codifications date back millennia, with one early example being the ancient Babylonian Codex Hammurabi, but modern civil law systems essentially derive from the legal practice of the Roman Empire, whose texts were rediscovered in medieval Europe. Roman law in the days of the Roman Republic and Empire was heavily procedural, and there was no professional legal class. Instead a lay person, iudex, was chosen to adjudicate. Precedents were not reported, so any case law that developed was disguised and almost unrecognised. Each case was to be decided afresh from the laws of the state, which mirrors the (theoretical) unimportance of judges' decisions for future cases in civil law systems today. During the 6th century AD in the Eastern Roman Empire, the Emperor Justinian codified and consolidated the laws that had existed in Rome, so that what remained was one-twentieth of the mass of legal texts from before. This became known as the Corpus Juris Civilis. As one legal historian wrote, "Justinian consciously looked back to the golden age of Roman law and aimed to restore it to the peak it had reached three centuries before." Western Europe, meanwhile, slowly slipped into the Dark Ages, and it was not until the 11th century that scholars in the University of Bologna rediscovered the texts and used them to interpret their own laws. Civil law codifications based closely on Roman law, alongside some influences from religious laws such as Canon law and Islamic law, continued to spread throughout Europe until the Enlightenment; then, in the 19th century, both France, with the Code Civil, and Germany, with the BÃ¼rgerliches Gesetzbuch, modernised their legal codes. Both these codes influenced heavily not only the law systems of the countries in continental Europe (e.g. Greece), but also the Japanese and Korean legal traditions. Today countries that have civil law systems range from Russia and China to most of Central and Latin America.

Common law and equity are systems of law whose special distinction is the doctrine of precedent, or stare decisis (Latin for "to stand by decisions"). Alongside this "judge-made law", common law systems always have governments who pass new laws and statutes. But these are not put into a codified form. Common law comes from England and was inherited by almost every country that once belonged to the British Empire, with the exceptions of Malta, Scotland, the U.S. state of Louisiana and the Canadian province of Quebec. Common law had its beginnings in medieval England, influenced by the Norman conquest of England which introduced legal concepts and institutions from the Norman and Islamic laws. Common law further developed when the English monarchy had been weakened by the enormous cost of fighting for control over large parts of France. King John had been forced by his barons to sign a document limiting his authority to pass laws. This "great charter" or Magna Carta of 1215 also required that the King's entourage of judges hold their courts and judgments at "a certain place" rather than dispensing autocratic justice in unpredictable places about the country. A concentrated and elite group of judges acquired a dominant role in law-making under this system, and compared to its European counterparts the English judiciary became highly centralised. In 1297, for instance, while the highest court in France had fifty-one judges, the English Court of Common Pleas had five. This powerful and tight-knit judiciary gave rise to a rigid and inflexible system of common law. As a result, as time went on, increasing numbers of citizens petitioned the King to override the common law, and on the King's behalf the Lord Chancellor gave judgment to do what was equitable in a case. From the time of Sir Thomas More, the first lawyer to be appointed as Lord Chancellor, a systematic body of equity grew up alongside the rigid common law, and developed its own Court of Chancery. At first, equity was often criticised as erratic, that it "varies like the Chancellor's foot". But over time it developed solid principles, especially under Lord Eldon. In the 19th century the two systems were fused into one another. In developing the common law and equity, academic authors have always played an important part. William Blackstone, from around 1760, was the first scholar to describe and teach it. But merely in describing, scholars who sought explanations and underlying structures slowly changed the way the law actually worked.

Religious law refers to the notion that the word of God is law. Examples include the Jewish Halakha and Islamic Sharia, both of which mean the "path to follow". Christian canon law also survives in some church communities. The implication of religion for law is unalterability, because the word of God cannot be amended or legislated against by judges or governments. However religion never provides a thorough and detailed legal system. For instance, the Quran has some law, and it acts merely as a source of further law through interpretation, Qiyas (reasoning by analogy), Ijma (consensus) and precedent. This is mainly contained in a body of law and jurisprudence known as Sharia and Fiqh respectively, which had a fairly significant influence on the development of common law, as well as some influence on civil law. Another example is the Torah or Old Testament, in the Pentateuch or Five Books of Moses. This contains the basic code of Jewish law, which some Israeli communities choose to use. The Halakha is a code of Jewish law which summarises some of the Talmud's interpretations. Nevertheless, Israeli law allows litigants to use religious laws only if they choose. Canon law is only in use by members of the clergy in the Roman Catholic Church, the Eastern Orthodox Church and the Anglican Communion.

Until the 18th century, Sharia law was practiced throughout the Muslim world in a non-codified form, with the Ottoman Empire's Mecelle code in the 19th century being first attempt at codifying elements of Sharia law. Since the mid-1940s, efforts have been made, in country after country, to bring Sharia law more into line with modern conditions and conceptions. In modern times, Sharia is merely an optional supplement to the civil or common law of most countries, though Saudi Arabia and Iran's whole legal systems source their law on a codified form of Sharia. During the last few decades, one of the fundamental features of the movement of Islamic resurgence has been the call to restore the Sharia, which has generated a vast amount of literature and affected world politics.

Though the legal traditions described have resulted in a number of common traits across jurisdictions, each sovereign entity can have unique aspects. The lists below link to articles on individual jurisdictions, organised by geography.



The history of law is closely connected to the development of civilizations. Ancient Egyptian law, dating as far back as 3000 BCE, had a civil code that was probably broken into twelve books. It was based on the concept of Ma'at, characterised by tradition, rhetorical speech, social equality and impartiality. Around 1760 BCE under King Hammurabi, ancient Babylonian law was codified and put in stone for the public to see in the marketplace; this became known as the Codex Hammurabi. However like Egyptian law, which is pieced together by historians from records of litigation, few sources remain and much has been lost over time. The influence of these earlier laws on later civilisations was small. The Old Testament is probably the oldest body of law still relevant for modern legal systems, dating back to 1280 BCE. It takes the form of moral imperatives, as recommendations for a good society. Ancient Athens, the small Greek city-state, was the first society based on broad inclusion of the citizenry, excluding women and the slave class from about 8th century BCE. Athens had no legal science, and Ancient Greek has no word for "law" as an abstract concept. Yet Ancient Greek law contained major constitutional innovations in the development of democracy.

Roman law was heavily influenced by Greek teachings. It forms the bridge to the modern legal world, over the centuries between the rise and decline of the Roman Empire. Roman law underwent major codification in the Corpus Juris Civilis of Emperor Justinian I. It was lost through the Dark Ages, but rediscovered around the 11th century. MediÃ¦val legal scholars began researching the Roman codes and using their concepts. In mediÃ¦val England, the King's powerful judges began to develop a body of precedent, which became the common law. But also, a Europe-wide Lex Mercatoria was formed, so that merchants could trade using familiar standards, rather than the many splintered types of local law. The Lex Mercatoria, a precursor to modern commercial law, emphasised the freedom of contract and alienability of property. As nationalism grew in the 18th and 19th centuries, Lex Mercatoria was incorporated into countries' local law under new civil codes. The French Napoleonic Code and the German became the most influential. As opposed to English common law, which consists of enormous tomes of case law, codes in small books are easy to export and for judges to apply. However, today there are signs that civil and common law are converging. European Union law is codified in treaties, but develops through the precedent laid down by the European Court of Justice.

Ancient India and China represent distinct traditions of law, and had historically independent schools of legal theory and practice. The Arthashastra, probably compiled around 100 AD (though containing some older material), and the Manusmriti(c. 100-300 AD) were foundational treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and Pluralism, and was cited across Southeast Asia. This Hindu tradition, along with Islamic law, was supplanted by the common law when India became part of the British Empire. Malaysia, Brunei, Singapore and Hong Kong also adopted the common law. The eastern Asia legal tradition reflects a unique blend of secular and religious influences. Japan was the first country to begin modernising its legal system along western lines, by importing bits of the French, but mostly the German Civil Code. This partly reflected Germany's status as a rising power in the late 19th century. Similarly, traditional Chinese law gave way to westernisation towards the final years of the Ch'ing dynasty in the form of six private law codes based mainly on the Japanese model of German law. Today Taiwanese law retains the closest affinity to the codifications from that period, because of the split between Chiang Kai-shek's nationalists, who fled there, and Mao Zedong's communists who won control of the mainland in 1949. The current legal infrastructure in the People's Republic of China was heavily influenced by Soviet Socialist law, which essentially inflates administrative law at the expense of private law rights. Today, however, because of rapid industrialisation China has been reforming, at least in terms of economic (if not social and political) rights. A new contract code in 1999 represented a turn away from administrative domination. Furthermore, after negotiations lasting fifteen years, in 2001 China joined the World Trade Organisation.



The philosophy of law is also known as jurisprudence. Normative jurisprudence is essentially political philosophy and asks "what should law be?". Analytic jurisprudence, on the other hand, is a distinctive field which asks "what is law?". An early famous philosopher of law was John Austin, a student of Jeremy Bentham and first chair of law at the new University of London from 1829. Austin's utilitarian answer was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". This approach was long accepted, especially as an alternative to natural law theory. Natural lawyers, such as Jean-Jacques Rousseau, argue that human law reflects essentially moral and unchangeable laws of nature. Immanuel Kant, for instance, believed a moral imperative requires laws "be chosen as though they should hold as universal laws of nature". Austin and Bentham, following David Hume, thought this conflated what "is" and what "ought to be" the case. They believed in law's positivism, that real law is entirely separate from "morality". Kant was also criticised by Friedrich Nietzsche, who believed that law emanates from The Will to Power and cannot be labelled as "moral" or "immoral". Thus, Nietzsche criticised the principle of equality, and believed that law should be committed to freedom to engage in will to power.

In 1934, the Austrian philosopher Hans Kelsen continued the positivist tradition in his book the Pure Theory of Law. Kelsen believed that though law is separate from morality, it is endowed with "normativity", meaning we ought to obey it. Whilst laws are positive "is" statements (e.g. the fine for reversing on a highway is â‚?00), law tells us what we "should" do (i.e. not drive backwards). So every legal system can be hypothesised to have a basic norm (Grundnorm) telling us we should obey the law. Carl Schmitt, Kelsen's major intellectual opponent, rejected positivism, and the idea of the rule of law, because he did not accept the primacy of abstract normative principles over concrete political positions and decisions.<ref name=">Bielefeldt, Carl Schmitt's Critique of Liberalism, 25â€?6</ref> Therefore, Schmitt advocated a jurisprudence of the exception (state of emergency), which denied that legal norms could encompass of all political experience.Later in the 20th century, H. L. A. Hart attacked Austin for his simplifications and Kelsen for his fictions in The Concept of Law. As the chair of jurisprudence at Oxford University, Hart argued law is a "system of rules". Rules, said Hart, are divided into primary rules (rules of conduct) and secondary rules (rules addressed to officials to administer primary rules). Secondary rules are divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). Two of Hart's students have continued the debate since. Ronald Dworkin was his successor in the Chair of Jurisprudence at Oxford and his greatest critic. In his book Law's Empire, Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. Dworkin argues that law is an "interpretive concept", that requires judges to find the best fitting and most just solution to a legal dispute, given their constitutional traditions. Joseph Raz, on the other hand, has defended the positivist outlook and even criticised Hart's 'soft social thesis' approach in The Authority of Law. Raz argues that law is authority, identifiable purely through social sources, without reference to moral reasoning. Any categorisation of rules beyond their role as authoritative dispute mediation is best left to sociology, rather than jurisprudence.

Economic analysis of law is an approach to legal theory that incorporates and applies the methods and ideas of economics to law. The discipline arose partly out of a critique of trade unions and U.S. antitrust law. The most influential proponents, such as Richard Posner and Oliver Williamson and the so-called Chicago School of economists and lawyers including Milton Friedman and Gary Becker, are generally advocates of deregulation and privatisation, and are hostile to state regulation or what they see as restrictions on the operation of free markets.

The most prominent economic analyst of law is 1991 Nobel Prize winner Ronald Coase. His first major article, The Nature of the Firm (1937), argued that the reason for the existence of firms (companies, partnerships, etc.) is the existence of transaction costs. Rational individuals trade through bilateral contracts on open markets until the costs of transactions mean that using corporations to produce things is more cost-effective. His second major article, The Problem of Social Cost (1960), argued that if we lived in a world without transaction costs, people would bargain with one another to create the same allocation of resources, regardless of the way a court might rule in property disputes. Coase used the example of a nuisance case named Sturges v. Bridgman, where a noisy sweetmaker and a quiet doctor were neighbours and went to court to see who should have to move. Coase said that regardless of whether the judge ruled that the sweetmaker had to stop using his machinery, or that the doctor had to put up with it, they could strike a mutually beneficial bargain about who moves house that reaches the same outcome of resource distribution. Only the existence of transaction costs may prevent this. So the law ought to pre-empt what would happen, and be guided by the most efficient solution. The idea is that law and regulation are not as important or effective at helping people as lawyers and government planners believe. Coase and others like him wanted a change of approach, to put the burden of proof for positive effects on a government that was intervening in the market, by analysing the costs of action.

Sociology of law is a diverse field of study that examines the interaction of law with society. Sociology of law overlaps with jurisprudence, economic analysis of law and more specialised subjects such as criminology. The institutions of law and the social construction of legal issues and systems are relevant areas of inquiry. Initially, legal theorists were suspicious of the discipline. Kelsen attacked one of its founders, Eugen Ehrlich, who wanted to emphasise the difference between positive law, which lawyers learn and apply, and other forms of 'law' or social norms that regulate everyday life, generally preventing conflicts from reaching lawyers and courts. Around 1900 Max Weber defined his "scientific" approach to law, identifying the "legal rational form" as a type of domination, not attributable to people but to abstract norms. Legal rationalism was his term for a body of coherent and calculable law which formed a precondition for modern political developments and the modern bureaucratic state and developed in parallel with the growth of capitalism. Another sociologist, Ã‰mile Durkheim, wrote in The Division of Labour in Society that as society becomes more complex, the body of civil law concerned primarily with restitution and compensation grows at the expense of criminal laws and penal sanctions. Other notable early legal sociologists included Hugo Sinzheimer, Theodor Geiger, Georges Gurvitch and Leon PetraÅ¼ycki in Europe, and William Graham Sumner in the U.S.

The main institutions of law in industrialised countries are independent courts, representative parliaments, an accountable executive, the military and police, bureaucratic organisation, the legal profession and civil society itself. John Locke in Two Treatises On Civil Government, and Baron de Montesquieu after him in The Spirit of the Laws, advocated a separation of powers between the institutions that wield political influence, namely the judiciary, legislature and executive. Their principle was that no person should be able to usurp all powers of the state, in contrast to the absolutist theory of Thomas Hobbes' Leviathan. More recently, Max Weber and many others reshaped thinking about the extensions of the state that come under the control of the executive. Modern military, policing and bureaucratic power over ordinary citizens' daily lives pose special problems for accountability that earlier writers like Locke and Montesquieu could not have foreseen. The custom and practice of the legal profession is an important part of people's access to justice, whilst civil society is a term used to refer to the social institutions, communities and partnerships that form law's political basis.

A judiciary is a group of judges who mediate people's disputes and determine the outcome. Most countries have a system of appeals courts, up to a supreme authority. In the U.S.A., this is the Supreme Court; in Australia, the High Court; in the UK, the House of Lords; in Germany, the Bundesverfassungsgericht; in France, the Cour de Cassation.However, for most European countries the European Court of Justice in Luxembourg may overrule national law, where EU law is relevant. The European Court of Human Rights in Strasbourg allows citizens of the Council of Europe member states to bring cases to it concerning human rights issues.

Some countries allow their highest judicial authority to strike down legislation determined to be unconstitutional. For instance, the United States Supreme Court struck down a Texan law forbidding assistance to women in abortion, in Roe v. Wade. The constitution's fourteenth amendment was interpreted to give Americans a right to privacy, hence a woman's right to choose abortion. The judiciary is theoretically bound by the constitution, much as legislative bodies are. In most countries judges may only interpret the constitution and all other laws. But in common law countries, where matters are not constitutional, the judiciary may also create law under the doctrine of precedent. On the other hand, the UK, Finland and New Zealand still assert the ideal of parliamentary sovereignty, whereby the unelected judiciary may not overturn law passed by a democratic legislature.

Prominent examples of legislatures are the Houses of Parliament in London, the Congress in Washington D.C., the Bundestag in Berlin, the Duma in Moscow and the ''AssemblÃ©e nationale'' in Paris. By the principle of representative government people vote for politicians to carry out their wishes. Most countries are bicameral, meaning they have two separately appointed legislative houses, although countries like Israel, Greece, Sweden and China are unicameral. In the 'lower house' politicians are elected to represent smaller constituencies. The 'upper house' is usually elected to represent states in a federal system (as in Australia, Germany or the U.S.A.) or different voting configuration in a unitary system (as in France). In the United Kingdom the upper house is appointed by the government as a house of review. One criticism of bicameral systems with two elected chambers is that the upper and lower houses may simply mirror one another. The traditional justification of bicameralism is that it minimises arbitrariness and injustice in governmental action.

To pass legislation, a majority of Members of Parliament must vote for a bill in each house. Normally there will be several readings and amendments proposed by the different political factions. If a country has an entrenched constitution, a special majority for changes to the constitution will be required, making changes to the law more difficult. A government usually leads the process, which can be formed from Members of Parliament (e.g. the UK or Germany). But in a presidential system, an executive appoints a cabinet to govern from his or her political allies whether or not they are elected (e.g. the U.S.A. or Brazil), and the legislature's role is reduced to either ratification or veto.

The "executive" in a legal system refers to the government's centre of political authority. In most democratic countries, like the UK, Italy, Germany, India and Japan, it is elected into and drawn from the legislature and is often called the cabinet. Alongside this is usually the head of state, who lacks formal political power but symbolically enacts laws. The head of state is sometimes appointed (the BundesprÃ¤sident in Germany), sometimes hereditary (British monarch) and sometimes elected by popular vote (the President of Austria). The other important model is found in countries like France, the U.S. or Russia. Under these presidential systems, the executive branch is separate from the legislature, and is not accountable to it.

The executive's role may vary from country to country. Usually it will initiate or propose the majority of legislation and handle a country's foreign relations. The military and police often fall under executive control, as well as the bureaucracy. Ministers, or secretaries of state of the government head a country's public offices, such as the health department or the Department of Justice. The election of a different executive is therefore capable of revolutionising an entire country's approach to government.

The military and police are sometimes referred to as "the long and strong arm of the law". While military organizations have existed as long as governments themselves, a standing police force is relatively modern. MediÃ¦val England used a system of traveling criminal courts, or assizes, which used show trials and public executions to instill communities with fear and keep them under control. The first modern police were probably those in 17th century Paris, in the court of Louis XIV, although the Paris Prefecture of Police claim they were the world's first uniformed policemen. In 1829, after the French Revolution and Napoleon's dictatorship, a government decree created the first uniformed policemen in Paris and all other French cities, known as sergents de ville ("city sergeants"). In Britain, the Metropolitan Police Act 1829 was passed by Parliament under Home Secretary Sir Robert Peel, founding the London Metropolitan Police.

Sociologist Max Weber famously argued that the state is that which controls the legitimate monopoly of the means of violence. The military and police carry out enforcement at the request of the government or the courts. The term failed state is used where the police and military no longer control security and order and society moves into anarchy, the absence of government.

The word "bureaucracy" derives from the French for "office" (bureau) and Ancient Greek for "power" (kratos). Like the military and police, all of a legal system's government servants and bodies that make up the bureaucracy carry out the wishes of the executive. One of the earliest references to the concept was made by Baron de Grimm, a German author who lived in France. In 1765 he wrote,

"The real spirit of the laws in France is that bureaucracy of which the late Monsieur de Gournay used to complain so greatly; here the offices, clerks, secretaries, inspectors and intendants are not appointed to benefit the public interest, indeed the public interest appears to have been established so that offices might exist."

Cynicism over "officialdom" is still common, and the workings of public servants is typically contrasted to private enterprise motivated by profit. In fact private companies, especially large ones, also have bureaucracies. Negative perceptions of "red tape" aside, public services such as schooling, health care, policing or public transport are a crucial state function making public bureaucratic action the locus of government power. Writing in the early 20th century, Max Weber believed that a definitive feature of a developed state had come to be its bureaucratic support. Weber wrote that the typical characteristics of modern bureaucracy are that officials define its mission, the scope of work is bound by rules, management is composed of career experts, who manage top down, communicating through writing and binding public servants' discretion with rules.

Lawyers give their clients advice about their legal rights and duties, and represent them in court. As European Court of Human Rights has stated, the law should be adequately accessible to everyone and people should be able to foresee how the law affects them. In order to maintain professionalism, the practice of law is typically overseen by either a government or independent regulating body such as a bar association, bar council or law society. An aspiring practitioner must be certified by the regulating body before undertaking his practice. This usually entails a two or three year programme at a university faculty of law or a law school, earning the student a Bachelor of Laws, a Bachelor of Civil Law or a Juris Doctor degree. This course of study is followed by an entrance examination (e.g. admission to the bar). Some countries require a further vocational qualification before a person is permitted to practice law. For those wishing to become a barrister a year's pupillage under the oversight of an experienced barrister. Beyond the requirements for legal practice higher academic degrees may be pursued. Examples include a Master of Laws, a Master of Legal Studies or a Doctor of Laws.

Once accredited, a lawyer will often work in a law firm, in a chambers as a sole practitioner, in a government post or in a private corporation as an internal counsel. In addition a lawyer may become a legal researcher who provides on-demand legal research through a commercial service or through freelance work. Many people trained in law put their skills to use outside the legal field entirely. Significant to the practice of law in the common law tradition is the legal research to determine the current state of the law. This usually entails exploring case-law reports, legal periodicals and legislation. Law practice also involves drafting documents such as court pleadings, persuasive briefs, contracts, or wills and trusts. Negotiation and dispute resolution skills are also important to legal practice, depending on the field.

The term "civil society" dates back to John Locke, who saw civil society as being of people who have "a common established law and judicature to appeal to, with authority to decide controversies between them." German philosopher Georg Wilhelm Friedrich Hegel also distinguished the "state" from "civil society" (Zivilgesellschaft) in Elements of the Philosophy of Right. Hegel believed that civil society and the state were polar opposites, within the scheme of his dialectic theory of history. Civil society is necessarily a source of law, by being the basis from which people form opinions and lobby for what they believe law should be. As Australian barrister and author Geoffrey Robertson QC wrote of international law, 

"one of its primary modern sources is found in the responses of ordinary men and women, and of the non-governmental organizations which many of them support, to the human rights abuses they see on the television screen in their living rooms."

Freedom of speech, freedom of association and many other individual rights allow people to meet together, discuss, criticise and hold to account their governments, from which the basis of a deliberative democracy is formed. The more people are involved with, concerned by and capable of changing how political power is exercised over their lives, the more acceptable and legitimate the law becomes to the people. Developed political parties, debating clubs, trade unions, impartial media, business and charities are all part of a healthy civil society.























Muhammad Ali (born Cassius Marcellus Clay Jr. on January 17, 1942) is a retired American boxer and former three-time World Heavyweight Champion and winner of an Olympic Light-heavyweight gold medal. In 1999, Ali was crowned "Sportsman of the Century" by Sports Illustrated and the BBC. 

Ali was born in Louisville, Kentucky. He was named after his father, Cassius Marcellus Clay Sr., who was named for the 19th century abolitionist and politician Cassius Clay. Ali changed his name after joining the Nation of Islam in 1964 and subsequently converted to Sunni Islam in 1975.

Ali was best known for his fighting style which he described as "Float like a butterfly, sting like a bee". His movement is often described as a dance, beautiful even. Throughout his career Ali made a name for himself with great handspeed, as well as fast feet and taunting tactics. While Ali was renowned for his fast, sharp out-fighting style, he also had a great chin, and displayed a great heart and ability to take a punch in his 1974 fight against George Foreman in Zaire, called the Rumble in the Jungle.

Muhammad Ali was born on January 17 1942. His father, Clay Sr., painted billboards and signs, and his mother, Odessa Grady Clay, was a household domestic. Although Clay Sr. was a Methodist, he allowed Odessa to bring up both Clay boys as Baptists. 

Ali voluntarily dropped out of Louisville Central High in the Spring of 1958 and although school records do not explain the reason for his withdrawal, they do indicate that the poor grades he was making at the time were probably responsible for his decision. However, in September of that same year, Ali re-enrolled in Central High, a local basketball power, and stayed until he graduated, finishing 376 out of a class of 391 seniors in the class of 1960.

Ali was first directed toward boxing by Louisville police officer Joe E. Martin, who encountered the twelve year old Cassius Clay who was fuming over his bicycle being stolen. However, without Martin knowing, he also began training with Fred Stoner at another gym. In this way, he could continue making $4 a week on Tomorrow's Champions, a TV show that Martin hosted, while benefiting from the coaching of the more-experienced Stoner, who continued working with Ali throughout his amateur career.

Ali's last amateur loss was to Kent Green of Chicago, who could say he was the last person to defeat the champion until Ali lost to Joe Frazier in 1971 as a pro. Under Stoner's guidance, Muhammad Ali went on to win six Kentucky Golden Gloves titles, two national Golden Gloves titles, an Amateur Athletic Union National Title, and the Light Heavyweight gold medal in the 1960 Summer Olympics in Rome. Ali's record was 100 wins, with five losses, when he ended his amateur career.

Ali states (in his 1975 autobiography) that he threw his Olympic gold medal into the Ohio River after being refused service at a 'whites-only' restaurant, and fighting with a white gang. Whether this is true is still debated, although he was given a replacement medal during the 1996 Olympics in Atlanta, where he lit the torch to start the games.

After his Olympic triumph, Ali returned to Louisville to begin his professional career. There, on October 29, 1960, he won his first professional fight, a six-round decision over Tunney Hunsaker, who was the police chief of Fayetteville, West Virginia. 

Standing tall, at 6-ft, 3-in (1.91 m), Ali had a highly unorthodox style for a heavyweight boxer. Rather than the normal style of carrying the hands high to defend the face, he instead relied on foot speed and quickness to avoid punches and carried his hands low. 

From 1960 to 1963, the young fighter amassed a record of 19-0, with 15 knockouts. He defeated boxers such as Tony Esperti, Jim Robinson, Donnie Fleeman, Alonzo Johnson, George Logan, Willi Besmanoff, Lamar Clark (who had won his previous 40 bouts by knockout), Doug Jones and Henry Cooper. 

Ali built a reputation by correctly predicting the round in which he would "finish" several opponents, and by boasting before his triumphs. Ali admitted he adopted the latter practice from "Gorgeous" George Wagner, a popular professional wrestling champion in the Los Angeles area who drew thousands of fans. Often referred to as "the man you loved to hate," George could incite the crowd with a few heated remarks, and Ali followed suit. 

Among Ali's victims were Sonny Banks (who knocked him down during the bout), Alejandro Lavorante, and the aged Archie Moore (a boxing legend who had fought over 200 previous fights, and who had been Ali's trainer prior to Angelo Dundee). Ali had considered continuing using Moore as a trainer following the bout, but Moore had insisted that the cocky "Louisville Lip" perform training camp chores such as sweeping and dishwashing. He also considered having his idol, Sugar Ray Robinson, as a manager, but instead hired Dundee. 

Ali first met Dundee when the latter was in Louisville with light heavyweight champ Willie Pastrano. The teenaged Golden Glove winner traveled downtown to the fighter's hotel, called Dundee from the house phone, and was asked up to their room. He took advantage of the opportunity to query Dundee (who was working with, or had, champions Sugar Ramos and Carmen Basilio) about what his fighters ate, how long they slept, how much roadwork (jogging) they did, and how long they sparred. 

Following his bout with Moore, Ali won a disputed 10-round decision over Doug Jones in a matchup that was named "Fight of the Year" for 1963. Ali's next fight was against Henry Cooper, who knocked Ali down with a left hook near the end of the fourth round. The fight was stopped in the fifth due to a deep cut on Cooper's face.

Despite these close calls, Ali became the top contender for Sonny Liston's title. Despite his impressive record, however, he was not widely expected to defeat the champ. The fight was scheduled for February 25, 1964 in Miami, Florida, but was nearly canceled when the promoter, Bill Faversham, heard that Ali had been seen around Miami and in other cities with the controversial Malcolm X. The Nation of Islam, considered a "hate group" by most media and Americans in 1964, was perceived as a gate-killer to a bout where, given Liston's overwhelming status as the favorite to win (7-1 odds), had Ali's colorful persona as its sole appeal. 

Faversham confronted Ali about his association with Malcolm X (who, at the time, was actually under suspension by the Nation as a result of controversial comments made in the wake of President Kennedy's assassination, which he called a case of "the chickens coming home to roost"). While stopping short of admitting he was a member of the Nation, Ali protested the suggested cancellation of the fight. As a compromise, Faversham asked the fighter to delay his announcement about his conversion to Islam until after the fight. The incident is described in the 1975 book The Greatest: My Own Story by Ali (with Richard Durham).

During the weigh-in on the day before the bout, the ever-boastful Ali, who frequently taunted Liston during the buildup by dubbing him "the big ugly bear" (among other things), declared that he would "float like a butterfly and sting like a bee," and, summarizing his strategy for avoiding Liston's assaults, said, "Your hands can't hit what your eyes can't see."

Ali (still known as Cassius Clay until after the bout), however, had a plan for the fight. Misreading Ali's exuberance as nervousness, Liston was typically over-confident and was unprepared for any result other than a quick knockout victory. In the opening rounds, Ali's speed kept him away from Liston's powerful head and body shots, as he used his height advantage to beat Liston to the punch with his own lightning-quick jab.

By the third round, Ali was ahead on points and had opened a cut under Liston's eye. Liston regained some ground in the fourth, as Clay was blinded by a substance in his eyes. It is unconfirmed whether this was something used to close Liston's cuts, or deliberately applied to Liston's gloves for a nefarious purpose; however, Bert Sugar (author, boxing historian and insider) has recalled at least two other Liston fights in which a similar situation occurred, suggesting the possibility that the Liston corner deliberately attempted to cheat. 

Whatever the case, Liston came into the fourth round aggressively looking to put away the challenger. As Ali struggled to recover his vision, he sought to escape Liston's offensive. He was able to keep out of range until his sweat and tears rinsed the substance from his eyes, responding with a flurry of combinations near the end of the fifth round. By the sixth, he was looking for a finish and dominated Liston. Then, Liston shocked the boxing world when he failed to answer the bell for the seventh round, later claiming a shoulder injury as the reason. Muhammad Ali indeed "Shook up the world!" as he had promised.

In the rematch, which was held in May 1965 in relatively-remote Lewiston, Maine, Ali won by knockout in the first round as a result of what came to be called the "phantom punch." Many believe that Liston, possibly as a result of threats from Nation of Islam extremists, or in an attempt to "throw" the fight to pay off debts, just wanted to call it a day and waited to be counted out (see Muhammad Ali versus Sonny Liston). Others, however, discount both scenarios and insist that it was a quick, chopping Ali punch to the side of the head that legitimately felled Liston.

After winning the championship from Liston in 1964, Clay revealed that he was a member of the Nation of Islam (often called the Black Muslims at the time) and the Nation gave Clay the name Cassius X, discarding his surname as a symbol of his ancestors' enslavement, as had been done by other Nation members. On Friday, March 6, 1964, Malcolm X took Clay on a guided tour of the United Nations building (for a second time). Malcolm X announced that Clay would be granted his "X." That same night, Elijah Muhammad recorded a statement over the phone to be played over the radio that Clay would be renamed Muhammad (one who is worthy of praise) Ali (fourth rightly guided caliph). Only a few journalists (most notably Howard Cosell) accepted it at that time. Venerable boxing announcer Don Dunphy addressed the champion by his adopted name, as did British reporters. The adoption of this name symbolized his new identity as a member of the Nation of Islam. 

Clay had discovered the Nation during a Golden Gloves tournament in Chicago in 1959, even writing a high school report on the organization. His school teachers at Louisville Central High were alarmed that a youngster with that much potential expressed interest in the nationalist faith. They dissuaded him from becoming involved. Many sportswriters of the early 1960s reported that it was Ali's brother, Rudy Clay, who converted to Islam first (estimating the date as 1962). Others wrote that Clay had been seen at Muslim rallies two years before he fought Liston. Ali's own version is that he did buy a copy of the "Muhammad Speaks" newspaper from a Muslim in Chicago, and a 45 rpm record by Minister Louis X (later Farrakhan) called "A White Man's Heaven is a Black Man's Hell." 

Aligning himself with the Nation of Islam made him a lightning rod for controversy, turning the outspoken but popular former champion into one of that era's most recognizable and controversial figures. Appearing at rallies with Nation of Islam leader Elijah Muhammad and declaring his allegiance to him at a time when mainstream America viewed them with suspicion â€?if not outright hostility â€?made Ali a target of outrage, as well as suspicion. Ali seemed at times to provoke such reactions, with viewpoints that wavered from support for civil rights to outright support of separatism. For example, Ali once stated, in relation to integration: "We who follow the teachings of Elijah Muhammad don't want to be forced to integrate. Integration is wrong. We don't want to live with the white man; that's all." And in relation to inter-racial marriage: "No intelligent black man or black woman in his or her right black mind wants white boys and white girls coming to their homes to marry their black sons and daughters." Indeed, Ali's religious beliefs at the time included the notion that the white man was "the devil" and that white people were not "righteous." Ali claimed that white people hated black people.

Ali converted from the Nation of Islam sect to mainstream Sunni Islam in 1975. In a 2004 autobiography, written with daughter Hana Yasmeen Ali, Muhammad Ali attributes his conversion to the shift toward Sunni Islam made by W.D. Muhammad after he gained control of the Nation of Islam upon the death of Elijah Muhammad in 1975.

In 1964, Ali failed the U.S. Armed Forces qualifying test because his writing and spelling skills were sub par. However, in early 1966, the tests were revised and Ali was reclassified 1A. He refused to serve in the United States Army during the Vietnam War as a conscientious objector, because "War is against the teachings of the Holy Qur'an. I'm not trying to dodge the draft. We are not supposed to take part in no wars unless declared by Allah or The Messenger. We don't take part in Christian wars or wars of any unbelievers." Ali also famously said in 1966: "I ain't got no quarrel with them Viet Cong ... They never called me nigger."

From his rematch with Liston in May 1965, to his final defense against Zora Folley in March 1967, he successfully defended his title nine times, an active schedule for that period. Ali was scheduled to fight WBA champion Ernie Terrell in a unification bout in Toronto on March 29, but Terrell backed out and Ali won a 15-round decision against substitute opponent George Chuvalo. He then went to England and defeated Henry Cooper and Brian London by stoppage on cuts. Ali's next defense was against German southpaw Karl Mildenberger, the first German to fight for the title since Max Schmeling. In one of the tougher fights of his life, Ali stopped his opponent in round 12. 

Ali returned to the United States in November 1966 to fight Cleveland "Big Cat" Williams in the Houston Astrodome. A year and a half before the fight, Williams had been shot in the stomach at point-blank range by a Texas policeman. As a result, Williams went into the fight missing one kidney and 10 feet of his small intestine, and with a shriveled left leg from nerve damage from the bullet. Ali beat Williams in three rounds. 

On February 6, 1967, Ali returned to a Houston boxing ring to fight Terrell in what became one of the uglier fights in boxing. Terrell had angered Ali by calling him Clay, and the champion vowed to punish him for this insult. During the fight, Ali kept shouting at his opponent, "What's my name, Uncle Tom ... What's my name?" Terrell suffered 15 rounds of brutal punishment, losing 13 rounds on two judges' scorecards, but Ali did not knock him out. Analysts, including several who spoke to ESPN on the sports channel's "Ali Rap" special, speculated that the fight continued only because Ali wanted to thoroughly punish and humiliate Terrell. After the fight, Tex Maule wrote, "It was a wonderful demonstration of boxing skill and a barbarous display of cruelty."

Ali's last fight in his first reign as world heavyweight champion was on March 22, 1967 against the 35-year old Zora Folley who was seen as something of a journeymen fighter coming into this bout. Folley was knocked out in the 7th round.

Appearing for his scheduled induction into the U.S. Armed Forces on April 28, 1967 in Houston, he refused three times to step forward at the call of his name. An officer warned him he was committing a felony punishable by five years in prison and a fine of $10,000. Once more Ali refused to budge when his name was called.

That day, the New York State Athletic Commission suspended his boxing license and stripped him of his title. Other boxing commissions followed suit.

At the trial two months later, the jury, after only 21 minutes of deliberation, found Ali guilty. The judge imposed the maximum sentence. After a court of appeals upheld the conviction, the case went to the U.S. Supreme Court. During this time, people turned against the war, and support for Ali grew. Ali financially supported himself by opening a restaurant chain called "Champburger" and visiting many college universities to give speeches across the country. Joe Frazier, who had become champion during Ali's absence from the ring, often gave financial assistance to Ali during this time.



In 1970, Ali was allowed to fight again. With the help of a state senator, he was granted a license to box in Georgia because it was the only state in America without a boxing commission. In October 1970, he stopped Jerry Quarry on a cut after three rounds. Shortly after the Quarry fight, the New York State Supreme Court ruled that Ali had been unjustly denied a boxing license. Once again able to fight in New York, he fought Oscar Bonavena at Madison Square Garden in December 1970. After a tough 14 rounds, Ali stopped Bonavena in the 15th, paving the way for a title fight against Joe Frazier, who had acquired the title during Ali's absence and was himself undefeated.

Ali and Frazier met in the ring on March 8, 1971, at Madison Square Garden. The fight, known as '"The Fight of the Century," was one of the most eagerly anticipated bouts of all time and remains one of the most famous. It featured two skilled, undefeated fighters, both of whom had legitimate claims to the heavyweight crown. The fight lived up to the hype, and Frazier punctuated his victory by flooring Ali with a hard left hook in the 15th and final round. Frank Sinatra â€?unable to acquire a ringside seat â€?took photos of the match for Life magazine. Legendary boxing announcer Don Dunphy and actor and boxing aficionado Burt Lancaster called the action for the broadcast, which reached millions of people.

Frazier retained the title on a unanimous decision, dealing Ali his first professional loss. However, Ali won a more important victory on June 28, 1971, when the Supreme Court reversed his conviction for refusing induction by unanimous decision in Clay v. United States. 

In 1973, after a string of victories over top heavyweight opposition in a campaign to force a rematch with Frazier, Ali split two bouts with Ken Norton (in the bout that Ali lost to Norton, Ali suffered a broken jaw), before beating Frazier (who had lost the title to George Foreman) on points in their 1974 rematch, to earn another title shot â€?but this time against a seemingly-invincible Foreman.

Ali regained his title on October 30, 1974 by defeating champion George Foreman in their bout in Kinshasa, Zaire. Hyped as "The Rumble In The Jungle," the fight was promoted by Don King. 

Almost no one, not even Ali's long-time supporter Howard Cosell, gave the former champion a chance of winning. Analysts pointed out that Joe Frazier and Ken Norton had given Ali four tough battles in the ring and won two of them, while Foreman had knocked out both of them in the second round. 

During the bout, Ali employed an unexpected strategy. Leading up to the fight he had declared he was going to "dance" and use his speed to keep away from Foreman and outbox him. However, in the first round, Ali headed straight for the champion and began scoring with a right hand lead, clearly surprising Foreman. Ali caught Foreman nine times in the first round with this technique but failed to knock him out. He then decided to take advantage of the young champion's weakness: staying power. Foreman had won 37 of his 40 bouts by knockout, most within three rounds, with Foreman's eight previous bouts not going past the second round. Ali saw an opportunity to outlast Foreman, and capitalized on it. 

In the second round, the challenger retreated to the ropes inviting Foreman to hit him, while counterpunching and verbally taunting the younger man. Ali's plan was to enrage Foreman and absorb his best blows to exhaust him mentally and physically. While Foreman threw wide shots to Ali's body, Ali countered with stinging straight punches to Foreman's head. Foreman threw hundreds of punches in seven rounds, but with decreasing technique and effectiveness. Ali's tactic of leaning on the ropes, covering up, and absorbing body shots was later termed "The Rope-A-Dope." 

By the end of the seventh round, Foreman was exhausted. In the eighth round, Ali dropped Foreman with a combination at center ring and Foreman failed to make the count. Against the odds, Ali had regained the title. Foreman would become champ again at age 45. Muhammad Ali (Foreman's best friend at the time) did not attend the bout. When asked why, he said "I would deviate attention from George. It was his moment, not mine." Many  praised Ali for his thoughtfulness and respect towards Mr. Foreman.

The "Rumble in the Jungle" was the subject of a 1996 Academy Award winning documentary film, When We Were Kings. The match was ranked seventh in the British television program The 100 Greatest Sporting Moments.

Ali would defend his title successfully from March 1975 (a bout against the "Bayonne Bleeder" Chuck Wepner) until his rematch with Leon Spinks in 1978. George Foreman would remain dormant for most of 1975 before resuming his career in 1976. Despite much publicity, a rematch between Ali and Foreman never materialized, and following a 1977 decision loss to Jimmy Young, who did manage to knock him down, Foreman had a religious experience and subsequently stopped fighting without officially announcing a retirement. 

In March 1975, Ali faced Chuck Wepner in a bout that inspired the original Rocky. While it was largely thought that Ali would dominate, Wepner surprised everyone by not only knocking Ali down in the ninth round, but nearly going the distance. Ali eventually stopped Wepner in the fading minutes of the 15th round, but Wepner's display of courage and resilience gave Sylvester Stallone, then an aspiring writer, actor and director, the basis of the plot for the first of the Rocky franchise, which led to five sequels that have endured for 30 years. In May 1975, Ali faced Ron Lyle, who lost by technical knockout in the 11th round after a barrage of punches by Ali. Two months later, in July 1975, Ali won a 15-round decision against Joe Bugner who was criticized by the press for resorting to defensive tactics rather than mounting an attack.

In October 1975, Ali fought Joe Frazier for the third time. The bout was promoted as the Thrilla in Manila by Don King, who had ascended to prominence following the Ali-Foreman fight. The anticipation was enormous for this final clash between two great heavyweights. Ali believed Frazier was "over the hill" by that point, and his overconfidence may have caused him to train less than he could have. Ali's frequent insults, slurs and demeaning poems increased the anticipation and excitement for the fight, but also enraged a determined Frazier. Regarding the fight Ali famously remarked, "It'll be a killa, and a thrilla, and a chilla, when I get the gorilla in Manila." 

The fight lasted 14 grueling rounds in temperatures approaching 100F. Ali won many of the early rounds, but Frazier staged a comeback in the middle rounds. By the late rounds, however, Ali had reasserted control and the fight was stopped when Frazier was unable to answer the bell for the 15th and final round (his eyes were swollen closed). Frazier's trainer, Eddie Futch, refused to allow Frazier to continue. Ali was quoted after the fight as saying "This must be what death feels like" and congratulated Frazier on his gutsy effort.

In early 1976, Ali would go on to face journeymen fighters such as Jean-Pierre Coopman and Richard Dunn (Ali's last knockout of his career), winning easily inside the distance against both. In April 1976, an out-of-shape Ali out pointed the tough, young brawler Jimmy Young, who went on to defeat George Foreman by decision and made Ali appear slow and immobile. 

Ali's next match after Dunn was a June 25th exhibition against the Japanese wrestler Antonio Inoki. Although widely perceived as a publicity stunt, the match would have a long-term detrimental affect on Ali's mobility. Inoki spent much of the fight on the ground trying to damage Aliâ€™s legs, while Ali spent most of the fight dodging the kicks or staying on the ropes. At the end of 15 rounds, the bout was called a draw. Ali's legs, however, were bleeding, leading to an infection. He suffered two blood clots in his legs as well.

Nevertheless, in September, at Yankee Stadium, Ali faced Ken Norton in their third fight, with Ali winning a close 15-round decision. 

In 1977, Ali faced only two opponents, defeating both by decision: the undistinguished Alfredo Evangelista, who gave Ali another 15-round challenge, and the devastating puncher Earnie Shavers, who nearly knocked him out in the second round. Shavers would be Ali's final successful defense of his heavyweight title. Following the fight, Ali's corner doctor, Ferdie Pacheco, left Ali's entourage when it became clear to him that boxing was taking a significant toll on Ali, both physically and mentally. He made his decision when his warnings to Ali to retire went unheeded.

Olympic champion Leon Spinks finally dethroned Ali by decision in February 1978. The fight was criticized by many fans, since Spinks was a relative rookie with only seven professional bouts in his career. However, Ali reclaimed his title for an unprecedented third time in their September 1978 rematch and then retired at age 37. He returned, however, to face new champion Larry Holmes in 1980. Despite Ali's claim that Holmes would be "mine in nine" it soon became clear he had nothing left and was given a sound beating by Holmes. Angelo Dundee refused to let his man come out for the 11th round, in what became Ali's first and only loss by anything other than a decision. Ali's final fight, a loss by unanimous decision after 10 rounds, was to up-and-coming challenger Trevor Berbick in 1981.

Muhammad Ali defeated almost every top heavyweight in his era, which has been called the golden age of heavyweight boxing. Ali was named "Fighter of the Year" by Ring Magazine more times than any other fighter, and was involved in more Ring Magazine "Fight of the Year" bouts than any other fighter. He is an inductee into the International Boxing Hall of Fame and holds wins over seven other Hall of Fame inductees. He is also one of only three boxers to be named "Sportsman of the Year" by Sports Illustrated. He is regarded as one of the best pound for pound boxers in history. He was a masterful self-promoter, and his psychological tactics before, during, and after fights became legendary. It was his athleticism and boxing skill, however, that enabled him to scale the heights and sustain his position for so many years.

In 1978, three years before Ali's permanent retirement, the Board of Aldermen in his hometown of Louisville, Kentucky voted 6â€? to rename Walnut Street to Muhammad Ali Boulevard. This was controversial at the time, as within a week 12 of the 70 street signs were stolen. Earlier that year, a committee of the Jefferson County Public Schools considered renaming Central High School in his honor, but the motion failed to pass. At any rate, in time, Muhammad Ali Boulevardâ€”and Ali himselfâ€”came to be well accepted in his hometown.

He was the recipient of the 1997 Arthur Ashe Courage Award.

In 1984, Ali discovered he had Parkinson's disease, a neurological syndrome characterized by tremors, rigidity of muscles and slowness of speech and movement, following which his motor functions began a slow decline. Although Ali's doctors disagreed about whether his symptoms were caused by boxing and whether or not his condition was degenerative, he was ultimately diagnosed with Pugilistic Parkinson's syndrome. According to the documentary When We Were Kings, when Ali was asked about whether he has any regrets about boxing due to his disability, he responded that if he didn't box he would still be a painter in Louisville, Kentucky.

Despite the disability, he remains a beloved and active public figure. Recently he was voted into Forbes Celebrity 100 coming in at number 13 behind Donald Trump. In 1985, he served as a guest referee at the inaugural WrestleMania event. In 1987 he was selected by the California Bicentennial Foundation for the U.S. Constitution to personify the vitality of the U.S. Constitution and Bill of Rights in various high profile activities. Ali rode on a float at the 1988 Tournament of Roses Parade, launching the U.S. Constitution's 200th birthday commemoration. He also published an oral history,  by Thomas Hauser, in 1991. Ali received a Spirit of America Award calling him the most recognized American in the world. In 1996, he had the honor of lighting the flame at the 1996 Summer Olympics in Atlanta, Georgia. 

He has appeared at the 1998 AFL (Australian Football League) Grand Final, where Anthony Pratt invited him to watch the game. He also greets runners at the start line of the Los Angeles Marathon every year.

In 1999, Ali received a special one-off award from the BBC at its annual BBC Sports Personality of the Year Award ceremony, namely the BBC Sports Personality of the Century Award in which he received more votes than the other four contenders combined. His daughter Laila Ali also became a boxer in 1999, despite her father's earlier comments against female boxing in 1978: "Women are not made to be hit in the breast, and face like that... the body's not made to be punched right here [patting his chest]. Get hit in the breast... hard... and all that."

On September 13, 1999, Ali was named "Kentucky Athlete of the Century" by the Kentucky Athletic Hall of Fame in ceremonies at the Galt House East.

In 2001, a biographical film, entitled Ali, was made, with Will Smith starring as Ali. The film received mixed reviews, with the positives generally attributed to the acting, as Smith and supporting actor Jon Voight earned Academy Award nominations. Prior to making the Ali movie, Will Smith had continually rejected the role of Ali until Muhammad Ali personally requested that he accept the role. According to Smith, the first thing Ali said about the subject to Smith was: "You ain't pretty enough to play me."

On November 17, 2002, Muhammad Ali went to Afghanistan as "U.N. Messenger of Peace". He was in Kabul for a three-day goodwill mission as a special guest of the United Nations.

He received the Presidential Medal of Freedom at a White House ceremony on November 9, 2005, and the "Otto Hahn peace medal in Gold" of the United Nations Association of Germany (DGVN) in Berlin for his work with the US civil rights movement and the United Nations (December 17, 2005).

On November 19, 2005 (Ali's 19th wedding anniversary), the $60 million non-profit Muhammad Ali Center opened in downtown Louisville. In addition to displaying his boxing memorabilia, the center focuses on core themes of peace, social responsibility, respect, and personal growth.

According to the Ali Center website, "Since he retired from boxing, Ali has devoted himself to humanitarian endeavors around the globe. He is a devout Muslim, and travels the world over, lending his name and presence to hunger and poverty relief, supporting education efforts of all kinds, promoting adoption and encouraging people to respect and better understand one another. It is estimated that he has helped to provide more than 22 million meals to feed the hungry. Ali travels, on average, more than 200 days per year."

At the FedEx Orange Bowl on January 2, 2007, Ali was an honorary captain for the Louisville Cardinals wearing their white jersey, number 19. Ali was accompanied by golf legend Arnold Palmer, who was the honorary captain for the Wake Forest Demon Deacons, and Miami Heat star Dwyane Wade.

A youth club in Ali's hometown and a species of rose (Rosa ali) have also been named after him. On June 5, 2007, he received an honorary doctorate of humanities at Princeton University's 260th graduation ceremony. 

Ali lives in Scottsdale, Arizona with his 4th wife, Yolanda 'Lonnie' Ali. They own a house in Berrien Springs, Michigan, which is for sale. On January 9, 2007, they purchased a house in eastern Jefferson County, Kentucky for $1,875,000.

There is no consensus among boxing experts and historians as to who is the greatest heavyweight boxer of all time. Ring Magazine, a prominent boxing magazine, named Muhammad Ali as number 1 in a 1998 ranking of greatest heavyweights from all eras. But in a 1971 article Nat Fleischer, the founder of the Ring who saw every heavyweight champion from Jim Jeffries to Joe Frazier, refused to include Ali in his all-time top ten, saying: "he does not qualify for rating with the greatest heavyweights of all time". Fleischer was writing after Ali's loss to Frazier, several years before his performance against Foreman and rematches with Frazier.

Recently Ali was named the second greatest fighter in boxing history by ESPN.com behind only welterweight and middleweight great Sugar Ray Robinson. In December of 2007, ESPN listed its choice of the greatest heavyweights of all time. Ali was second on this list also behind Joe Louis, despite the fact that the earlier poll placed Ali ahead of Louis.

Muhammad Ali has been married four times and has seven daughters and two sons. Ali met his first wife, cocktail waitress Sonji Roi, approximately one month before they married on August 14, 1964. Roi's objections to certain Muslim customs in regard to dress for women contributed to the breakup of their marriage. They divorced on January 10, 1966. 

On August 17, 1967, Ali (aged 25) married 17-year old Belinda Boyd. After the wedding, she converted to Islam and changed her name to Khalilah Ali, though she was still called Belinda by old friends and family. They had four children: Maryum (b. 1968), Jamillah and Liban (b. 1970), and Muhammad Ali Jr. (b. 1972). 

However, Ali began an affair with a young woman named Veronica Porsche in 1975. By the summer of 1977, Ali's second marriage was over and he had married Veronica. At the time of their marriage, they had a baby girl, Hana, and Veronica was pregnant with their second child. Their second daughter, Laila, was born in December of 1977. By 1986, Ali and Veronica were divorced.

On November 19, 1986, Ali married Yolanda Ali. They had been friends since 1964 in Louisville. Their mothers were close friends, although Lonnie has publicly denied the popular notion that Muhammad Ali was once her babysitter. They have one adopted son, Asaad. 

Ali has two other daughters, Miya and Khaliah, from extramarital relationships.



As a world champion boxer and social activist, Ali has been the subject of numerous books, films and other creative works. He has appeared on the cover of Sports Illustrated magazine on 37 different occasions, second only to Michael Jordan. His autobiography The Greatest: My Own Story, written with Richard Durham, was published in 1975. When We Were Kings, a 1996 documentary about the Rumble in the Jungle, won an Academy Award, and the 2001 biopic Ali garnered an Oscar nomination for Will Smith's portrayal of the lead role.



|-|align="center" colspan=8|56 Wins (37 knockouts, 19 decisions), 5 Losses (4 decisions, 1 retirement), 0 Draws|-| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Res.| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Opponent| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Type| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Rd., Time| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Date| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Location| align="center" style="border-style: none none solid solid; background: #e3e3e3"|Notes|-align=center|Loss|Trevor Berbick|Decision |10 |1981-12-11|align=left| Nassau, Bahamas||-align=center|Loss|Larry Holmes|Corner retirement|10 |1980-10-02|align=left| Las Vegas, NV|align=left||-align=center|Win|Leon Spinks|Decision |15 |1978-09-15|align=left| New Orleans, LA|align=left||-align=center|Loss|Leon Spinks|Decision |15 |1978-02-15|align=left| Las Vegas, NV|align=left||-align=center|Win|Earnie Shavers|Decision |15 |1977-09-29|align=left| New York City, NY|align=left||-align=center|Win|Alfredo Evangelista|Decision |15 |1977-05-16|align=left| Landover, MD|align=left||-align=center|Win|Ken Norton|Decision |15 |1976-09-28|align=left| The Bronx, New York|align=left||-align=center|Win|Richard Dunn|TKO|5 |1976-05-24|align=left| Munich, Germany|align=left||-align=center|Win|Jimmy Young|Decision |15 |1976-04-30|align=left| Landover, MD|align=left||-align=center|Win|Jean-Pierre Coopman|KO|5 |1976-02-20|align=left| San Juan, Puerto Rico|align=left||-align=center|Win|Joe Frazier|TKO|14 , 0:59|1975-10-01|align=left| Quezon City, Philippines|align=left||-align=center|Win|Joe Bugner|Decision |15 |1975-06-30|align=left| Kuala Lumpur, Malaysia|align=left||-align=center|Win|Ron Lyle|TKO|11 |1975-05-16|align=left| Las Vegas, NV|align=left||-align=center|Win|Chuck Wepner|TKO|15 , 2:41|1975-03-24|align=left| Richfield, OH|align=left||-align=center|Win|George Foreman|KO|8 , 2:58|1974-10-30|align=left| Kinshasa, Zaire|align=left||-align=center|Win|Joe Frazier|Decision |12 |1974-01-28|align=left| New York City, NY|align=left||-align=center|Win|Rudi Lubbers|Decision |12 |1973-10-20|align=left| Jakarta, Indonesia||-align=center|Win|Ken Norton|Decision |12 |1973-09-10|align=left| Inglewood, CA|align=left||-align=center|Loss|Ken Norton|Decision |12 |1973-03-31|align=left| San Diego, CA|align=left||-align=center|Win|Joe Bugner|Decision |12 |1973-02-14|align=left| Las Vegas, NV||-align=center|Win|Bob Foster|KO|7 |1972-11-21|align=left| Stateline, NV|align=left||-align=center|Win|Floyd Patterson|TKO|7 |1972-09-20|align=left| New York City, NY|align=left||-align=center|Win|Alvin Lewis|TKO|11 , 1:15|1972-07-19|align=left| Dublin, Ireland||-align=center|Win|Jerry Quarry|TKO|7 , 0:19|1972-06-27|align=left| Las Vegas, NV|align=left||-align=center|Win|George Chuvalo|Decision |12 |1972-05-01|align=left| Vancouver, Canada|align=left||-align=center|Win|Mac Foster|Decision |15 |1972-04-01|align=left| Tokyo, Japan||-align=center|Win|JÃ¼rgen Blin|KO|7 , 2:12|1971-12-26|align=left| Zurich, Switzerland||-align=center|Win|Buster Mathis|Decision |12 |1971-11-17|align=left| Houston, TX|align=left||-align=center|Win|Jimmy Ellis|TKO|12 , 2:10|1971-07-26|align=left| Houston, TX|align=left||-align=center|Loss|Joe Frazier|Decision |15 |1971-03-08|align=left| New York City, NY|align=left||-align=center|Win|Oscar Bonavena|TKO|15 , 2:03|1970-12-07|align=left| New York City, NY|align=left||-align=center|Win|Jerry Quarry|TKO|3 |1970-10-26|align=left| Atlanta, GA||-align=center|Win|Zora Folley|KO|7 , 1:48|1967-03-22|align=left| New York City, NY|align=left||-align=center|Win|Ernie Terrell|Decision |15 |1967-02-06|align=left| Houston, TX|align=left||-align=center|Win|Cleveland Williams|TKO|3 |1966-11-14|align=left| Houston, TX|align=left||-align=center|Win|Karl Mildenberger|TKO|12 |1966-09-10|align=left| Frankfurt, Germany|align=left||-align=center|Win|Brian London|KO|3 |1966-08-06|align=left| London, England|align=left||-align=center|Win|Henry Cooper|TKO|6 , 1:38|1966-05-21|align=left| London, England|align=left||-align=center|Win|George Chuvalo|Decision |15 |1966-03-29|align=left| Toronto, Canada|align=left||-align=center|Win|Floyd Patterson|TKO|12 , 2:18|1965-11-22|align=left| Las Vegas, NV|align=left||-align=center|Win|Sonny Liston|KO|1 , 2:12|1965-05-25|align=left| Lewiston, ME|align=left||-align=center|Win|Sonny Liston|Corner retirement|7 |1964-02-25|align=left| Miami Beach, FL|align=left||-align=center|Win|Henry Cooper|TKO|5 , 2:15|1963-06-18|align=left| London, England||-align=center|Win|Doug Jones|Decision |10 |1963-03-13|align=left| New York City, NY||-align=center|Win|Charley Powell|KO|3, 2:04|1963-01-24|align=left| Pittsburgh, PA||-align=center|Win|Archie Moore|TKO|4 , 1:35|1962-11-15|align=left| Los Angeles, CA||-align=center|Win|Alejandro Lavorante|KO|5 , 1:48|1962-07-20|align=left| Los Angeles, CA||-align=center|Win|Billy Daniels|TKO|7 , 2:21|1962-05-19|align=left| Los Angeles, CA||-align=center|Win|George Logan|TKO|4 , 1:34|1962-04-23|align=left| New York City, NY||-align=center|Win|Don Warner|TKO|4, 0:34|1962-03-28|align=left| Miami Beach, FL||-align=center|Win|Sonny Banks|TKO|4 , 0:26|1962-02-10|align=left| New York City, NY||-align=center|Win|Willi Besmanoff|TKO|7 , 1:55|1961-11-29|align=left| Louisville, KY||-align=center|Win|Alex Miteff|TKO|6 , 1:45|1961-10-07|align=left| Louisville, KY||-align=center|Win|Alonzo Johnson|Decision |10 |1961-07-22|align=left| Louisville, KY||-align=center|Win|Duke Sabedong|Decision |10 |1961-06-26|align=left| Las Vegas, NV||-align=center|Win|LaMar Clark|KO|2 , 1:27|1961-04-19|align=left| Louisville, KY||-align=center|Win|Donnie Fleeman|TKO|7 |1961-02-21|align=left| Miami Beach, FL||-align=center|Win|Jimmy Robinson|KO|1 , 1:34|1961-02-07|align=left| Miami Beach, FL||-align=center|Win|Tony Esperti|TKO|3 , 1:30|1961-01-17|align=left| Miami Beach, FL||-align=center|Win|Herb Siler|KO|4 |1960-12-27|align=left| Miami Beach, FL||-align=center|Win|Tunney Hunsaker|Decision |6 |1960-10-29|align=left| Louisville, KY|

















The Republic of Cape Verde (Portuguese: Cabo Verde, ), also Cape Verdi, is a republic located on an archipelago in the Macaronesia ecoregion of the North Atlantic Ocean, off the western coast of Africa. The previously uninhabited islands were discovered and colonized by the Portuguese in the fifteenth century (though there may have been earlier discoveries), and attained independence in 1975.

Cape Verde is named after Cap Vert (meaning Green Cape) in Senegal, the westernmost point of continental Africa.

Cape Verde was uninhabited when the Portuguese arrived in 1460 and made the islands part of the Portuguese empire. Due to its location off the coast of Africa, Cape Verde became an important watering station, then sugar cane plantation site, and later a major hub of the trans-atlantic slave trade, that would later form the contemporary African Diaspora.

In 1975, Cape Verde achieved independence from Portugal after a long armed struggle in the jungles of Guinea-Bissau. The African Party for the Independence of Guinea-Bissau and Cape Verde (PAIGC) was the main entity responsible for the independence of Cape Verde. Moreover, the people's revolutionary armed forces of Cuba, too, played a role in the Cape Verdean independence armed struggle in Guinea-Bissau. After independence, the PAIGC attempted to unite Cape Verde and Guinea-Bissau into one nation, the PAIGC controlling both governments, but a coup in the latter nation in 1980 ended these plans. As a result, the G, standing for Guinea-Bissau, in PAIGC was dropped. Consequently, PAICV (African Party for the Independence of Cape Verde) was formed. In Cape Verde the PAICV (affiliated with the PAIGC) governed until democratic elections, held in 1991, resulted in a change of government. The Movimento para a Democracia (MPD) won that election. The MPD was re-elected in 1996. The PAICV returned to power in 2001, and was re-elected in 2006.





Politics of Cape Verde takes place in a framework of a parliamentary representative democratic republic, whereby the Prime Minister of Cape Verde is the head of government, and of a pluriform multi-party system. Executive power is held by the government. Legislative power is vested in both the government and the National Assembly. The Judiciary is independent of the executive and the legislature.

Cape Verde is an archipelago off the west coast of Africa at 15.02N, 23.34W. It is formed by 10 main islands and about 8 islets. The main islands are:

Of these, only Santa Luzia and the five islets are uninhabited. Presently it is a natural reserve. All islands are volcanic, but an active volcano only exists on one of the islands, Fogo (see Mount Fogo).

The isolation of Cape Verde about  from the African mainland has resulted in the islands having a large number of endemic species, many of which are endangered by human development. Endemic birds include Alexander's Swift (Apus alexandri), Raso Lark (Alauda razae), Cape Verde Warbler (Acrocephalus brevipennis), and Iago Sparrow (Passer iagoensis), and reptiles include the Cape Verde Giant Gecko (Tarentola gigas).

Charles Darwin gives a vivid description of the geology, climate, zoology and botany of the islands in the first chapter of his book The Voyage of the Beagle.

Cape Verde is in the tropical zone. Average temperatures range from 24 Â°C (75 Â°F) in January and February to 29 Â°C (85 Â°F) in September. The average annual rainfall for Cape Verde is , with September being the wettest month with . Conversely, the months April to July record less than one millimetre of rainfall each. The climate is arid, but Cape Verde's position in the Atlantic contributes to soften the aridity, that otherwise would be the same aridity as that in continental areas.

Cape Verde is divided into 22 municipalities (called concelhos in Portuguese).





Cape Verde is a small nation that lacks resources and has experienced severe droughts. Agriculture is made difficult by lack of rain and is restricted to only four islands for most of the year. Most of the nation's GDP comes from the service industry. Cape Verde's economy has grown since the late 1990s, and it is now considered a country of average development. Cape Verde has significant cooperation with Portugal at every level of the economy, which has lead it to link its currency first to the Portuguese escudo and, in 1999, to the euro. There are no Patent Laws in Cape Verde. 

Former Portuguese prime minister JosÃ© Manuel DurÃ£o Barroso, now (second semester 2004) president of the European Commission, has promised to help integrate Cape Verde within the European Union sphere of influence via greater cooperation with Portugal. In March 2005, former Portuguese president MÃ¡rio Soares launched a petition urging the European Union to start membership talks with Cape Verde.

Cape Verde has been on the list of the United Nations Small Island Developing States, and is considered a Developing country in economic terms.





Most inhabitants of Cape Verde are a genetic blend of Sub-Saharan Africans and Europeans, the Africans having been slaves and hailing mostly from Senegal, Gambia and Guinea-Bissau. Cape Verdeans' European ancestors include Portuguese settlers and exiles, Portuguese Jews who were victims of the Inquisition, and Spanish and Italian seamen who were granted land by the Portuguese Empire. Many foreigners from other parts of the world settled Cape Verde as their permanent country. Most of them were Dutch, French, British, Arabs and Jews (from Lebanon and Morocco), Chinese (especially from Macau), Americans, and Brazilians (including people of Portuguese and African descent) settlers. All of these have been absorbed into the general Cape Verdean population. 

The majority of the population adheres to Christianity, mostly Catholicism which constitutes some 90% of the population (in many areas Catholicism and the indigenous religion are syncretised). The remaining includes a sizeable Protestant community as well as a small number of BahÃ¡'Ã­ and Buddhist and even smaller Muslim groups.

Cape Verde has been steadily developing since its independence, and besides having been promoted to the group of "medium development" countries in 2007, leaving the Least Developed Countries category (which was is only the second time it has happened to a country), is currently the 5th best ranked country in Africa in terms of Human Development Index.



The Cape Verdean diaspora refers to both historical and present emigration from Cape Verde. Today, more Cape Verdeans live abroad than in Cape Verde itself, with significant emigrant Cape Verdean communities in the United States (500,000 Cape Verdeans), Portugal (80,000) and Angola (45,000) have large populations of Cape Verdeans. There are also significant number of Cape Verdeans in SÃ£o TomÃ© and PrÃ­ncipe, Senegal, France, Brazil, Luxembourg and the Netherlands. Cape Verdean populations also settled Spain, Germany, and other CPLP countries such as Guinea-Bissau.

The culture of Cape Verde reflects its mixed African and Portuguese roots. It is well known for its diverse forms of music such as Morna and a wide variety of dances: the soft dance Morna, and its modernized version, passada, the FunanÃ¡ - a sensual mixed Portuguese and African dance, the extreme sensuality of coladeira, and the Batuque dance. These are reflective of the diverse origins of Cape Verde's residents. The term "Criolo" is used to refer to residents as well as the culture of Cape Verde.

Cape Verdean literature is one of the richest of Lusitanian Africa. 

Cape Verde is known internationally for morna, a form of folk music usually sung in the Cape Verdean Creole, accompanied by clarinet, violin, guitar and cavaquinho. The islands also boast funanÃ¡ and batuque music.

CesÃ¡ria Ã‰vora is perhaps the best internationally-known practicioner of morna.

Cape Verde's official language is Portuguese. It is the language of instruction and official acts. However, the Cape Verdean Creole is used colloquially and is the mother tongue of virtually all Cape Verdeans. Cape Verdean Creole or Kriolu is a dialect continuum of a Portuguese-based creole, which varies from island to island. 

There is a substantial body of literature in Creole, especially in the Santiago Creole and the SÃ£o Vicente Creole. Creole has been gaining prestige since the nation's independence from Portugal.

However, the differences between the varied forms of the language within the islands have been a major obstacle in the way of standardization of the language. Some people have advocated the development of two standards: a North (Barlavento) standard, centered on the SÃ£o Vicente Creole, and a South (Sotavento) standard, centered on the Santiago Creole. Manuel Veiga, PhD, a linguist by training, and Minister of Culture of Cape Verde, is the premier proponent of Kriolu's officialization and standardization.





TACV Cabo Verde Airlines is a scheduled and charter, passenger and cargo airline based in Cape Verde. It is the national flag carrier of Cape Verde, operating an inter-island service and flights to Europe, North America, South America and the West African mainland. Its main base is Sal Airport (SID), with a hub at Praia Airport (RAI).



















Gioachino Antonio Rossini  (Pesaro, February 29, 1792 â€?Passy, November 13, 1868) was a popular Italian composer who wrote 39 operas as well as sacred music and chamber music. His best known works include Il barbiere di Siviglia (The Barber of Seville), La Cenerentola and Guillaume Tell (William Tell). 

Gioachino Antonio Rossini was born into a family of musicians in Pesaro, a town on the Adriatic coast of Italy. His father, Giuseppe, was a horn player and inspector of slaughterhouses, his mother, Anna, was a singer and baker's daughter. Rossini's parents began his musical training early, and by the age of six he was playing the triangle in his father's band.

Rossini's father was sympathetic to the French Revolution and welcomed Napoleon's troops when they arrived in Northern Italy. This became a problem when the Austrians restored the old regime in 1796. Rossini's father was sent to prison, and his mother took him to Bologna, earning her living as a leading singer at various theatres of the Romagna region, where she was ultimately joined by her husband. During this time, he was frequently left in the care of his ageing grandmother, who was unable to effectively control the boy.

He remained at Bologna in the care of a pork butcher, while his father played the horn in the orchestras of the theatres at which his wife sang. The boy had three years' instruction in the harpsichord from Prinetti of Novara, who played the scale with two fingers only, combined his profession of a musician with the business of selling liquor, and fell asleep while he stood, so he was a fit subject for ridicule by his critical pupil.

He was taken from Prinetti and apprenticed to a smith. In Angelo Tesei he found a congenial master, and learned to sight-read, to play accompaniments on the pianoforte, and to sing well enough to take solo parts in the church when he was ten years of age. At thirteen he appeared at the theatre of the Commune in PaÃ«râ€™s Camilla â€?his only public appearance as a singer (1805). He was also a capable horn player in the footsteps of his father.

In 1806,at the age of 14, Rossini became a student of the cello under Cavedagni at the Conservatorio of Bologna. In 1807 he was admitted to the counterpoint class of Padre P. S. Mattei. He learned to play the cello with ease, but the pedantic severity of Mattei's views on counterpoint only served to drive the young composer's views toward a freer school of composition. His insight into orchestral resources is generally ascribed not to the strict compositional rules he learned from Mattei, but to knowledge gained independently while scoring the quartets and symphonies of Haydn and Mozart. At Bologna he was known as "il Tedeschino" ("the Little German") on account of his devotion to Mozart.

Through the friendly interposition of the Marquis Cavalli, his first opera, La Cambiale di Matrimonio, was produced at Venice when he was a youth of eighteen. But two years before this he had already received the prize at the Conservatorio of Bologna for his cantata Il pianto d'Armonia sulla morte dâ€™Orfeo. Between 1810 and 1813, at Bologna, Rome, Venice, and Milan, Rossini produced operas of varying success. All memory of these works is eclipsed by the enormous success of his opera Tancredi.

The libretto was an arrangement of Voltaireâ€™s tragedy by A. Rossi. Traces of PaÃ«r and Paisiello were undeniably present in fragments of the music. But any critical feeling on the part of the public was drowned by appreciation of such melodies as "Di tanti palpiti... Mi rivedrai, ti rivedrÃ²," which became so popular that the Italians would sing it in crowds at the law courts until called upon by the judge to desist.

Rossini continued to write operas for Venice and Milan during the next few years, but their reception was tame and in some cases unsatisfactory after the success of Tancredi. In 1815 he retired to his home at Bologna, where Barbaja, the impresario of the Naples theatre, concluded an agreement with him by which he was to take the musical direction of the Teatro San Carlo and the Teatro Del Fondo at Naples, composing for each of them one opera a year. His payment was to be 200 ducats per month; he was also to receive a share of Barbaja's other business, popular gaming-tables, amounting to about 1000 ducats per annum. This was an extraordinarily lucrative arrangement for any professional musician at that time.

Some older composers in Naples, notably Zingarelli and Paisiello, were inclined to intrigue against the success of the youthful composer; but all hostility was made futile by the enthusiasm which greeted the court performance of his Elisabetta, regina d'Inghilterra, in which Isabella Colbran, who subsequently became the composerâ€™s wife, took a leading part. The libretto of this opera by Schmidt was in many of its incidents an anticipation of those presented to the world a few years later in Sir Walter Scottâ€™s Kenilworth. The opera was the first in which Rossini wrote the ornaments of the airs instead of leaving them to the fancy of the singers, and also the first in which the recitativo secco was replaced by a recitative accompanied by a string quartet.

Rossini's most famous opera was produced on February 20, 1816 at the Teatro Valle in Rome. The libretto by Cesare Sterbini, a version of Beaumarchais' infamous stage play Le Barbier de Seville, was the same as that already used by Giovanni Paisiello in his own Barbiere, an opera which had enjoyed European popularity for more than a quarter of a century. Much is made of how fast Rossini's opera was written, scholarship generally agreeing upon two weeks, a miracle by any standard. Later in life, Rossini claimed to have written the opera in only twelve days. When the opera made its debut as Almaviva, Paisielloâ€™s admirers were extremely indignant, sabotaging the production by whistling and shouting during the entire first act. However, not long after the second performance, the opera became so successful that the fame of Paisiello's opera was transferred to Rossini's, to which the title The Barber of Seville passed as an inalienable heritage.

Between 1815 and 1823 Rossini produced 20 operas. Of these Otello formed the climax to his reform of serious opera, and offers a suggestive contrast with the treatment of the same subject at a similar point of artistic development by the composer Giuseppe Verdi. In Rossiniâ€™s time the tragic close was so distasteful to the public of Rome that it was necessary to invent a happy conclusion to Otello.

Conditions of stage production in 1817 are illustrated by Rossiniâ€™s acceptance of the subject of Cinderella for a libretto only on the condition that the supernatural element should be omitted. The opera La Cenerentola was as successful as Barbiere. The absence of a similar precaution in the construction of his MosÃ¨ in Egitto led to disaster in the scene depicting the passage of the Israelites through the Red Sea, when the defects in stage contrivance always raised a laugh, so that the composer was at length compelled to introduce the chorus "Dal tuo stellato Soglio" to divert attention from the dividing waves.

In 1822, four years after the production of this work, Rossini married the coloratura soprano Isabella Colbran. In the same year, he directed his Cenerentola in Vienna, where Zelmira was also performed. After this he returned to Bologna; but an invitation from Prince Metternich to come to Verona and "assist in the general re-establishment of harmony" was too tempting to be refused, and he arrived at the Congress in time for its opening on October 20, 1822. Here he made friends with Chateaubriand and Dorothea Lieven.

In 1823, at the suggestion of the manager of the Kingâ€™s Theatre, London, he came to England, being much fÃªted on his way through Paris. In England he was given a generous welcome, which included an introduction to King George IV and the receipt of Â£7000 after a residence of five months. In 1824 he became musical director of the ThÃ©Ã¢tre italien in Paris at a salary of Â£800 per annum, and when the agreement came to an end he was rewarded with the offices of Chief Composer to the King and Inspector-General of Singing in France, to which was attached the same income. At the age of 32, Rossini was able to go into semi-retirement with essentially financial independence.

The production of his Guillaume Tell in 1829 brought his career as a writer of opera to a close. The libretto was by Ã‰tienne Jouy and Hippolyte Bis, but their version was revised by Armand Marrast. The music is remarkable for its freedom from the conventions discovered and utilized by Rossini in his earlier works, and marks a transitional stage in the history of opera.Though a very good opera, it is rarely heard uncut today, as the original score runs more than four hours in performance.

In 1829 he returned to Bologna. His mother had died in 1827, and he was anxious to be with his father. Arrangements for his subsequent return to Paris on a new agreement were upset by the abdication of Charles X and the July Revolution of 1830. Rossini, who had been considering the subject of Faust for a new opera, returned, however, to Paris in the November of that year.

Six movements of his Stabat Mater were written in 1832 and the rest in 1839, the year of his father's death. The success of the work bears comparison with his achievements in opera; but his comparative silence during the period from 1832 to his death in 1868 makes his biography appear almost like the narrative of two lives â€?the life of swift triumph, and the long life of seclusion, of which biographers give us pictures in stories of the composer's cynical wit, his speculations in fish culture, his mask of humility and indifference.

His first wife died in 1845, and on August 16, 1846 he married Olympe PÃ©lissier, who had sat for Vernet for his picture of Judith and Holofernes. Political disturbances compelled Rossini to leave Bologna in 1848. After living for a time in Florence he settled in Paris in 1855, where his house was a centre of artistic society. He died at his country house at Passy on Friday November 13, 1868 and was buried in PÃ¨re Lachaise Cemetery, Paris, France. In 1887 his remains were moved to the Basilica di Santa Croce di Firenze, in Florence, where they now rest.

He was a foreign associate of the Institute, grand officer of the Legion of Honour, and the recipient of innumerable orders.

In his compositions Rossini plagiarized even more freely from himself than from other musicians, and few of his operas are without such admixtures frankly introduced in the form of arias or overtures. For example, in Il Barbiere there is an aria for the Count (often omitted) 'Cessa di piu resistere', which Rossini used (with minor changes) in Le Nozze di Teti e di Peleo and in La Cenerentola (the cabaletta for Angelina's Rondo is almost unchanged).

A characteristic mannerism in his orchestral scoring, a long, steady build of sound, creating "tempests in teapots by beginning in a whisper and rising to a flashing, glittering storm"  earned him the nickname of "Signor Crescendo".

















The Apostle Peter, also known as Saint Peter (from the Greek Petros, meaning "rock"), Shimon "Keipha" Ben-Yonah/Bar-Yonah, Simon Peter, Cephas and Keipha (Keipha and Cephas also mean rock)â€”original name Shimon or Simeon () ( (Acts )â€”was one of the Twelve Apostles whom Jesus chose as his original disciples. His life is prominently featured in the New Testament Gospels and the Acts of the Apostles. Peter was a Galilean fisherman assigned a leadership role by Jesus (; ). Many within the early Church, such as St. Irenaeus, assert his primacy among the apostles.

Christian Churches, Roman Catholic Church, Eastern Orthodox, Oriental Orthodox and Anglican Communion, consider Simon Peter a saint and associate him with the foundation of the Church in Rome, even if they differ on the significance of this and of the Pope in present-day Christianity.

Some who recognize his office as Bishop of Antioch and, later, as Bishop of Rome or Pope, hold that his episcopacy held a primacy only of honor, as a first among equals. Some propose that his primacy was not intended to pass to his successors. 

The Roman Martyrology assigns 29 June as the feast day of both Peter and Paul, without thereby declaring that to be the day of their deaths. St. Augustine of Hippo says in his Sermon 295: "One day is assigned for the celebration of the martyrdom of the two apostles. But those two were one. Although their martyrdom occurred on different days, they were one." The Annuario Pontificio gives the year of Peter's death as A.D. 64 or A.D. 67. Some scholars believe that he died on October 13 A.D. 64. It is traditionally believed that the Roman authorities sentenced him to death by crucifixion. According to a tradition recorded or perhaps initiated in the apocryphal Acts of Peter, he was crucified head down. Tradition also locates his burial place where the Basilica of Saint Peter was later built, directly beneath the Basilica's high altar. In art, he is often depicted holding the keys to the kingdom of heaven (interpreted by Roman Catholics as the sign of his primacy over the Church), a reference to Matthew .

Peter's life story relies on the New Testament, since there are few other first-century accounts of his life and death.

According to the Gospel of John, Peter was born in Bethsaida (). His father's name is given as 'Jonah' (, ), although some manuscripts of John give his father's name as John. The synoptic gospels all recount how Peter's mother-in-law was healed by Jesus at their home in Capernaum (; ; ) which, coupled with , implies that Peter was married.

According to the synoptic gospels, before becoming a disciple of Jesus, Simon (that is, Peter whose name was in fact originally Simon) was a fisherman along with his brother Andrew. The Gospel of John also depicts Peter fishing, but only after the resurrection in the story of the Catch of 153 fish.



Matthew and Mark report that while fishing in the Lake of Gennesaret, Simon and his brother Andrew were called by Jesus to be his followers, with the words, "Follow me, and I will make you fishers of men" (; ). In Luke's account Simon is the owner of a boat that Jesus uses to preach to the multitudes who were pressing on him at the shore of Lake Gennesaret (). Jesus then amazes Simon and his companions James and John (Andrew is not mentioned) by telling them to lower their nets, whereupon they catch a huge number of fish. Immediately after this, they follow him ().The Gospel of John gives a slightly different account (). Andrew, we are told, was originally a disciple of John the Baptist. Along with one other disciple, Andrew heard John the Baptist describe Jesus as the "Lamb of God," whereupon he followed Jesus. He then went and fetched his brother Simon, said, "We have found the Messiah," and brought him to Jesus. Jesus then gave Simon the name "Cephas," meaning 'rock', in Aramaic.'Petros', a masculine form of the feminine 'petra' (rock) is the Greek equivalent of this. It had not previously been used as a name, but in the Greek-speaking world it became a popular Christian name after the tradition of Peter's prominence in the early Christian church had been established.

Peter is always mentioned first in the lists of the Twelve. He is also frequently mentioned in the Gospels as forming with James the Elder and John a special group within the Twelve Apostles, present at incidents to which the others were not party, such as at the Transfiguration of Jesus.

Peter is also often depicted in the Gospels as spokesman of all the apostles, and as one to whom Jesus gave special authority. In contrast, Jewish Christians are said to have argued that James the Just was the leader of the group. Some argue James was the Patriarch of Jerusalem, and that this position at times gave him privilege in some (but not all) situations.

According to John, Peter initially refused to allow Jesus to wash his feet. When Jesus responded "If I wash thee not, thou hast no part with me," Peter replied "Lord, not my feet only, but also my hands and my head" ().

According to the Gospel of Matthew, Peter (alone out of all the disciples) was able to walk on water after seeing Jesus do the same thing, but he later fell in because he lost faith. Jesus caught him and scolded him for losing faith. (Matthew 14:22â€?2). (Mark and John also mention Jesus walking on water, but do not mention Peter doing so).

According to John, Peter cut off the ear of a servant of the high priest with a sword at the time of the arrest of Jesus. () John names the servant as Malchus. The synoptic gospels also mention this incident, but do not specifically identify Peter as the swordsman or Malchus as the victim. According to Matthew, Luke and John, Jesus rebuked this act of violence, Luke adding the detail that Jesus touched the ear and healed it.

All four canonical gospels recount that, during the Last Supper, Jesus foretold that Peter would deny association with him three times that same night. In Matthew's account, this is reported as:

Jesus said unto him, "Verily I say unto thee, That this night, before the  crow, thou shalt deny me thrice."

and that Peter did in fact do so, while Jesus was on trial before the high priest. The three Synoptics describe the three denials as follows:

The Gospel of John places the second denial while Peter was still warming himself at the fire, and gives as the occasion of the third denial a claim by someone to have seen him in the garden of Gethsemane when Jesus was arrested.Since Peter does not reappear in Matthew's gospel after his denial of Jesus, a small but notable number of scholars have suggested the theory that Matthew might have viewed Peter as an apostate, and was actually criticising Peter and the groups that looked to him as founder. In the Gospel of Luke, Jesus' prediction of Peter's denial is coupled with a prediction that all the apostles ("you," plural) would be "sifted like wheat," but that it would be Peter's task ("you," singular), when he had turned again, to strengthen his brethren.

In John's gospel, Peter is the first person to enter the empty tomb, although the women and the beloved disciple see it before him (). In Luke's account, the women's report of the empty tomb is dismissed by the apostles and Peter is the only one who goes to check for himself. After seeing the graveclothes he goes home, apparently without informing the other disciples ().



Paul's First Epistle to the Corinthians contains a list of resurrection appearances of Jesus, the first of which is an appearance to "Cephas" (Peter). An appearance to "Simon" is also reported in .In the final chapter of the Gospel of John, Peter, in one of the resurrection appearances of Jesus, three times affirmed his love for Jesus, balancing his threefold denial, and Jesus reconfirmed Peter's position (). Almost all Christians consider the final chapter of the Gospel of John to be canonical, though some scholars hypothesize that it was added later to bolster Peter's status.

In the early Greek versions of this exchange between the risen Jesus and Peter, Jesus asks whether Peter loves him unconditionally (á¼€Î³Î¬Ï€Î¿Ï‚). Peter responds that he considers Jesus a friend (Ï†Î¯Î»Î¿Ï‚). The third time, Jesus asks whether Peter considers Jesus a friend (Ï†Î¯Î»Î¿Ï‚), and Peter responds that he considers Jesus a friend (Ï†Î¯Î»Î¿Ï‚). One interpretation of this is that Peter actually denies Jesus two more time (i.e., denies Jesus the unconditional love (á¼€Î³Î¬Ï€Î¿Ï‚) that Jesus requests); Jesus "comes down" to Peter's level with the final request of mere friendship (Ï†Î¯Î»Î¿Ï‚). 



The author of the Acts of the Apostles portrays Peter as an extremely important figure within the early Christian community, with Peter delivering a significant open-air sermon during Pentecost. According to the same book, Peter took the lead in selecting a replacement for Judas Iscariot (). He was twice arraigned, with John, before the Sanhedrin and directly defied them (, ). He undertook a missionary journey to Lydda, Joppa and Caesarea (), becoming instrumental in the decision to evangelise the Gentiles (). He was present at the Council of Jerusalem, where Paul further argued the case for accepting Gentiles into the Christian community without circumcision.

About halfway through, the Acts of the Apostles turns its attention away from Peter and to the activities of Paul, and the Bible is fairly silent on what occurred to Peter afterwards. A fleeting mention of Peter being in Antioch is made in the Epistle to the Galatians () where Paul confronted him, and historians have furnished other evidence of Peter's sojourn in Antioch. Subsequent tradition held that Peter had been the first Patriarch of Antioch. Some scholars also interpret Paul's brief mention of Peter in 1 Corinthians as evidence that Peter had visited Corinth ().  may imply that he wrote that epistle in Babylon, Egypt, Rome or Jerusalem.

Verses 18-19 in the last chapter of the Gospel of John have been interpreted as referring to Peter's martyrdom by crucifixion, though without reference to its location: "'â€¦when you are old you will stretch out your hands, and another will gird you and take you where you do not want to go.' Jesus said this to indicate the kind of death by which Peter would glorify God" ().

Two sayings are attributed to Peter in the Gospel of Thomas. In the first, Peter compares Jesus to a "just messenger." In the second, Peter asks Jesus to "make Mary leave us, for females don't deserve life," although the verse containing the second is regarded as a dubious, later addition by most scholars. 

In the Apocalypse of Peter, Peter holds a dialogue with Jesus about the parable of the fig tree and the fate of sinners.

In the Gospel of Mary, Peter appears to be jealous of "Mary" (probably Mary Magdalene). He says to the other disciples "Did He really speak privately with a woman and not openly to us? Are we to turn about and all listen to her? Did He prefer her to us?" In reply to this, Levi says "Peter, you have always been hot tempered."

Other noncanonical texts that attribute sayings to Peter include the Secret Book of James and the Acts of Peter.

In the Fayyum Fragment Jesus predicts that Peter will deny him three times before the cock crows at dawn in an account similar to that of the canonical gospels, especially the Gospel of Mark.

The fragmentary Gospel of Peter, attributed to Peter, contains an account of the death of Jesus differing significantly from the canonical gospels. It contains little information about Peter himself, except that after the discovery of the empty tomb, "I, Simon Peter, and Andrew my brother, took our fishing nets and went to the sea".

The early writings indicated in the following paragraphs witness to the tradition that Peter, probably at the time of the Great Fire of Rome of the year 64, for which the Emperor Nero blamed the Christians, met martyrdom in Rome. 

Clement of Rome, in his Letter to the Corinthians (Chapter 5), written c. 80-98, speaks of Peter's martyrdom in the following terms: "Let us take the noble examples of our own generation. Through jealousy and envy the greatest and most just pillars of the Church were persecuted, and came even unto deathâ€?Peter, through unjust envy, endured not one or two but many labours, and at last, having delivered his testimony, departed unto the place of glory due to him."

Saint Ignatius of Antioch, in his Letter to the Romans (Ch. 4) of c. 105-110, tells the Roman Christians: "I do not command you, as Peter and Paul did."

Dionysius of Corinth wrote: "You [Pope Soter] have also, by your very admonition, brought together the planting that was made by Peter and Paul at Rome and at Corinth; for both of them alike planted in our Corinth and taught us; and both alike, teaching similarly in Italy, suffered martyrdom at the same time" (Letter to Pope Soter [A.D. 170], in Eusebius, History of the Church 2:25:8).

St. Irenaeus of Lyon (a disciple of St. Polycarp of Smyrna, who was himself a disciple of the Apostle St. John, which puts Irenaeus not far from the authentic teachings of the Apostles) in c. 175-185 wrote in Against Heresies (Book III, Chapter III, paragraphs 2â€?):

Since, however, it would be too long to enumerate in such a volume as this the succession of all the churches, we shall confound all those who, in whatever manner, whether through self-satisfaction or vainglory, or through blindness and wicked opinion, assemble other than where it is proper, by pointing out here the successions of the bishops of the greatest and most ancient church known to all, founded and organized at Rome by the two most glorious apostles, Peter and Paul, that church which has the tradition and the faith which comes down to us after having been announced to men by the apostles. With that church, because of its superior origin, all the churches must agree, that is, all the faithful in the whole world, and it is in her that the faithful everywhere have maintained the apostolic tradition.''

The blessed apostles, then, having founded and built up the Church, committed into the hands of Linus the office of the episcopate. Of this Linus, Paul makes mention in the Epistles to Timothy. To him succeeded Anacletus; and after him, in the third place from the apostles, Clement was allotted the bishopric. This man, as he had seen the blessed apostles, and had been conversant with them, might be said to have the preaching of the apostles still echoing [in his ears], and their traditions before his eyes. Nor was he alone [in this], for there were many still remaining who had received instructions from the apostles. In the time of this Clement, no small dissension having occurred among the brethren at Corinth, the Church in Rome dispatched a most powerful letter to the Corinthians, exhorting them to peace, renewing their faith, and declaring the tradition which it had lately received from the apostlesâ€?'

Tertullian also writes: "But if you are near Italy, you have Rome, where authority is at hand for us too. What a happy church that is, on which the apostles poured out their whole doctrine with their blood; where Peter had a passion like that of the Lord, where Paul was crowned with the death of John [the Baptist, by being beheaded]" 

Traditions originating in or recorded in the apocryphal Acts of Peter, say that the Romans crucified Peter upside down at his request, due to his wishing not to be equated with Jesus. Acts of Peter is also thought to be the source for the tradition about the famous phrase "Quo Vadis" (Where are you going?), a question that, according to this tradition, Peter, fleeing Rome to avoid execution, asked a vision of Jesus, and to which Jesus responded that he was "going to Rome, to be crucified again," causing Peter to decide to return to the city and accept martyrdom. This story is commemorated in an Annibale Carracci painting. The Church of Quo Vadis, near the Catacombs of Saint Callistus, contains a stone in which Jesus' footprints from this event are supposedly preserved, though this was actually apparently an ex-voto from a pilgrim, and indeed a copy of the original, housed in the Basilica of St Sebastian.

The ancient historian Josephus describes how Roman soldiers would amuse themselves by crucifying criminals in different positions, and it is likely that this would have been known to the author of the Acts of Peter. The position attributed to Peter's crucifixion is thus plausible, either as having happened historically or as being an invention by the author of the Acts of Peter. Death, after crucifixion head down, is unlikely to be caused by suffocation, the usual cause of death in ordinary crucifixion.

A medieval misconception was that the Mamertine Prison in Rome is the place where Peter was imprisoned before his execution.

In 1950, human bones were found buried underneath the altar of St. Peter's Basilica. The bones have been claimed by many to have been that of Peter. These claims were later contradicted by a 1953 excavation of what some believe to be Peter's Tomb in Jerusalem. This discovery seems to clarify Paul's confrontation in Antioch (ca 51 AD) with "Cephas" (Galatians 2:1â€?), as being Peter. Also there is an apocryphal text entitled "Martyrdom of Paul," in which Peter is absent from Paul's death at Rome, stating Paul's only companions to be Luke and Titus (2 Timothy, Paul says "only Luke is with me.") The Vatican contends that the bones found in 1950 are St. Peter's.

Late legends said that Peter had a daughter, who was sometimes identified with the virgin martyr Petronilla. This is at least possible, since  indicates that Cephas (Peter) travelled with his wife. At one point () Peter refers to Mark as his son, although this is generally considered to not be literal.

In Catholic tradition, Peter's leadership role among the Apostles, referred to above lies at the root of the leadership role of the pope among the bishops of the Church. The pope is seen as the successor of Peter as bishop of Rome by all the ancient Christian Churches. Protestants question this belief on the grounds of lack of contemporary evidence, and though the quotations given above from writers like Clement, Ignatius and Tertullian show that by the end of the first century the tradition that Peter went to Rome and was martyred there was already well established, there is no mention of Peter founding the church in Rome or holding office there. The first Epistle of Peter ends with "The church that is in Babylon, chosen together with you, salutes you, and so does my son, Mark." (1 Pet 5:13). Though the word "Babylon" refers literally to a city in Mesopotamia, it could be used cryptically to indicate Rome, as some argue the term is used in Revelation ; ; , and in the works of various Jewish seers. "Babylon" could also simply be a reference to the present age, so the reference to a specific place is not conclusive. 

In reference to Peter's occupation before becoming an Apostle, the popes wear the Fisherman's Ring, which bears an image of the saint casting his nets from a fishing boat. The keys used as a symbol of the Pope's authority refer to the "keys of the kingdom of Heaven" promised to Peter (). The terminology of this "commission" of Peter is unmistakably parallel to the commissioning of Eliakim ben Hilkiah in  and .

Peter is therefore often depicted in both Western and Eastern Christian art holding a key or a set of keys.

In the same passage of the Gospel of Matthew, Jesus tells Peter: "You are Peter, and on this rock I will build my Church." In the original Greek the word translated as "Peter" is Î Î­Ï„ÏÎ¿Ï‚ (Petros) and that translated as "rock" is Ï€Î­Ï„ÏÎ± (petra), two words that, while not identical, give an impression of a play on words. Furthermore, since Jesus presumably spoke to Peter in their native Aramaic language, he would have used kepha in both instances. The Peshitta Text and the Old Syriac text use the word "kepha" for both "Peter" and "rock" in Matthew 16:18. John 1:42 says Jesus called Simon "Cephas", as does Paul in some letters. The traditional Catholic interpretation has therefore been that Jesus told Peter (Rock) that he would build his Church on this Peter (Rock). Many Protestants agree with this interpretation, but dispute the doctrine of Apostolic Succession, thus questioning the authority of the popes without questioning the authority of Peter himself.

Some Protestant scholars disagree with this interpretation on the basis of the difference between the Greek words. In classical Attic Greek petros generally meant "pebble," while petra meant "boulder" or "cliff." Accordingly, taking Peter's name to mean "pebble," they argue that the "rock" in question cannot have been Peter, but something else, either Jesus himself, or the faith in Jesus that Peter had just professed. In appealing to the doctrine of Biblical inerrancy, these scholars claim that speculation regarding the original language/word choice of the event recorded in Matthew is irrelevant because the account in Greek is without error, and thus there must be significance to the different words chosen by the author.Counter-arguments are presented not only by Catholic apologists like Karl Keating but also by scholars of other Christian churches, such as the Evangelical Christian D. A. Carson in The Expositor's Bible Commentary (Grand Rapids: Zondervan, 1984). They point out that the Gospel of Matthew was written, not in the classical Attic form of Greek, but in the Hellenistic Koine dialect, in which there is no distinction in meaning between petros and petra. Moreover, even in Attic Greek, in which the regular meaning of petros was a smallish "stone," there are instances of its use to refer to larger rocks, as in Sophocles, Oedipus at Colonus v. 1595, where petros refers to a boulder used as a landmark, obviously something more than a pebble. In any case, a petros/petra distinction is irrelevant considering the Aramaic language in which the phrase might well have been spoken. In Greek, of any period, the feminine noun petra could not be used as the given name of a male, which may explain the use of Petros as the Greek word with which to translate Aramaic Kepha.

When, in the early fourth century, the Emperor Constantine I decided to honour Peter with a large basilica, the precise location of Peter's burial was so firmly fixed in the belief of the Christians of Rome that the building had to be erected on a site that involved considerable difficulties, both physical (excavating the slope of the Vatican Hill, while the great church could much more easily have been built on level ground only slightly to the south) and moral and legal (demolishing a cemetery). The focal point of St. Peter's Basilica, both in its original form and in its later complete reconstruction, is the altar placed over what is held to be the exact place where Peter was buried.



The Catholic Encyclopedia states that from very early times in the history of the Christian community in Rome, January 18 was the date on which celebrated the memory of the day St. Peter held his first service with Christians in Rome . This feast was suppressed as part of the simplification of the calendar in the 1960s.

Seventh-day Adventists contend that the idea of Peter being the first Pope is based on a misinterpretation. (see the discussions above about the words "petros" and "petra" in Attic and Koine Greek, and as a translation from the Aramaic). 

The Seventh-day Adventists argue in addition that Peter was in need of a firm foundation to gain a sense of stability, as Peter was noted for his great zeal, but instability:

They also argue that the statement by Peter: "Thou art the Christ, the Son of the living God"  is the foundation of the Christian faith: not Peter, but the testimony that Peter gave.

They also argue that Peter's acts are recorded in all of the gospels, and the book of Acts, and his writings were included in the bible, and are used by Christians today. In this sense Peter was used in the building of the Lord's church, as a small stone (petros) would be used.

They also argue that the idea of making a single man the whole foundation of the church would go against the principle taught in  although in  Jesus clearly tells "The Beloved Disciple" to feed and tend his sheep, and the ability to loose and bind is given to every disciple of Christ. ()

The New Apostolic Church, who believes in the re-established Apostle ministry, sees Peter as the first Chief Apostle.

The Church of Jesus Christ of Latter-day Saints (LDS Church or "Mormons") along with other sects of the Latter Day Saint movement believe that Peter was the first leader of the early Christian church, but reject papal succession. In interpreting Matt. 16: 13â€?9 the church has stated, "The words then addressed to him, 'Thou art Peter, and upon this rock I will build my church,' have been made the foundation of the papal claims. But it is the Godhead of Christ, which Peter had just confessed, that is the true keystone of the Church." Latter-day saints believe that as part of the restoration, Peter, James and John came from heaven and conferred the keys of the Melchizedek Priesthood upon Joseph Smith and Oliver Cowdery in 1829, near Harmony Township, Susquehanna County, Pennsylvania.

In the Cuban SanterÃ­a and Palo Mayombe, he has been syncretized with OgÃºn.

The New Testament includes two letters (epistles) ascribed to Peter. Both demonstrate a high quality of cultured and urban Greek, at odds with the linguistic skill that would ordinarily be expected of an Aramaic-speaking fisherman, who would have learned Greek as a second or third language. However, the author of the first epistle explicitly claims to be using a secretary (see below), and this explanation would allow for discrepancies in style without entailing a different source. The textual features of these two epistles are such that a majority of scholars doubt that they were written by the same hand. This means at the most that Peter could not have authored both, or at the least that he used a different secretary for each letter. Some scholars argue that theological differences imply different sources, and point to the lack of references to 2 Peter among the early Church Fathers.

Of the two epistles, the first epistle is considered the earlier. A number of scholars have argued that the textual discrepancies with what would be expected of the biblical Peter are due to it having been written with the help of a secretary or as an amanuensis. Indeed in the first epistle the use of a secretary is clearly described: "By Silvanus, a faithful brother unto you, as I suppose, I have written briefly, exhorting, and testifying that this is the true grace of God wherein ye stand" (1 Peter 5:15). Thus, in regards to at least the first epistle, the claims that Peter would have written Greek poorly seem irrelevant. The references to persecution of Christians, which only began under Nero, cause most scholars to date the text to at least 80, which would require Peter to have survived to an age that was, at that time, extremely old, and almost never reached, particularly by common fishermen. However, the Roman historian Tacitus and the biographer Suetonius both record that Nero's persecution of Christians began immediately after the fire that burned Rome in 64. Such a date, which is in accord with Christian tradition, especially Eusebius (History book 2, 24.1), would not have Peter at an improbable age upon his death. On the other hand, many scholars consider this in reference to the persecution of Christians in Asia Minor during the reign of the emperor Domitian (81-96).

The Second Epistle of Peter,on the other hand, appears to have been copied, in part, from the Epistle of Jude, and some modern scholars date its composition as late as c. 150. Some scholars argue the opposite, that the Epistle of Jude copied 2 Peter, while others contend an early date for Jude and thus observe that an early date is not incompatible with the text. Many scholars have noted the similarities between the apocryphal second pseudo-Epistle of Clement (2nd century) and 2 Peter. Second Peter may be earlier than 150, there are a few possible references to it that date back to the first century or early second century, eg 1 Clement written in c 96 AD, and the later church historian Eusebius claimed that Origen had made reference to the epistle before 250. Even in early times there was controversy over its authorship, and 2 Peter was often not included in the Biblical Canon; it was only in the 4th century that it gained a firm foothold in the New Testament, in a series of synods. In the east the Syriac Church still did not admit it into the canon until the 6th century.

Traditionally, the Gospel of Mark was said to have been written by a person named John Mark, and that this person was an assistant to Peter, hence its content was traditionally seen as the closest to Peter's viewpoint. According to Eusebius's Ecclesiastical History, Papias recorded this belief from John the Presbyter: 

Also Irenaeus wrote about this tradition: 

Based on these quotes, and on the Christian tradition, the information in Mark's Gospel about St. Peter would be based on eyewitness material. It should be observed, however, that some scholars (for differing reasons) dispute the attribution of the Gospel of Mark to its traditional author. The gospel itself is anonymous, and the above passages are the oldest surviving written testimony to its authorship.

According to Jewish folklore (Toledot Yeshu narrative), St. Peter (Shimeon Kepha Ha-Tzadik) has a pristine reputation as a greatly learned and holy man who according to the directions of his sage to bring about the end of one hundred years of strife in Israel, established the Sunday Sabbath for God-Fearers (converted from among Gnostic heretics known as The Watchers) instead of Saturday, Noel (as a new year feast but not as Christmas) instead of Hanukkah, the Feast of the Cross instead of Rosh Hashana, Firstfruits instead of Pesach, remembering The Feast of The Jews  instead of Sukkot, and the Ascension for them instead of Shavuot. R. Judah ben Samuel of Regensburg, who led Germany's 12th-century Chasidei Ashkenaz, considered him to be a Tzaddik (a Jewish saint or spiritual Master among Hasidim) (Sefer Hasidim). The Tosaphist Rabbeinu Tam wrote that he was "a devout and learned Jew who dedicated his life to guiding gentiles along the proper path". Tam also passed on the traditions that St Peter was the author of the Sabbath and feast-day Nishmat prayer, which has no other traditional author, and also that he authored a prayer for Yom Kippur in order to prove his commitment to Judaism despite his work amongst Gentiles (R.J.D. Eisenstein). Legends about Peter and his activities are also mentioned in other medieval works, such as the Mahzor Vitri.

There are also a number of other apocryphal writings that have been either attributed to or written about St. Peter. They were from antiquity regarded as pseudepigrapha. These include:

Over the years "St. Peter" has evolved into a stock character that is now widely used in jokes, cartoons, comedies, dramas, and plays. Such caricatures almost all play upon Peter's role as the "keeper of the keys of the kingdom of heaven" in Matthew 16:19 , on the basis of which he is often depicted as an elderly, bearded man who sits at the pearly gates that serve as heaven's main entrance, and acting as a sort of hotel-style doorman / bouncer who personally interviews prospective entrants into Heaven, often from behind a desk.St. Peter is depicted as a rabbit in the South Park episode Fantastic Easter Special.

In Roman Catholic religious doctrine and tradition, Saint Peter is the patron saint of the following categories 





















Saladin, properly known as Salah al-DÄ«n Yusuf ibn Ayyub (, , ), (c. 1138 â€?March 4, 1193), Sultan of Egypt and Syria, was a 12th-century Kurdish Muslim political and military leader from Tikrit, Iraq. At the height of his power the Ayyubid dynasty, which he founded, ruled over Egypt, Syria, Iraq, Hejaz, and Yemen. He is best-known for leading the Muslim armies during the Crusades and recapturing Jerusalem.

Saladin was born Yusuf Salah ad-Din Ayyub in 1138 CE to a Kurdish family in Tikrit, Iraq. His father, Najm ad-Din Ayyub was banned from Tikrit and moved to Mosul where he met an Arab leader, Zengy, who was currently leading Muslim forces against the Crusaders in Edessa. Zengy made Najm ad-Din the commander of his fortress in Baalbek and after Zengy died in 1146, Nur ad-Din replaced him. Saladin received his name from Nur ad-Din and was sent to Damascus to finish his education. 



After an initial military education under the command of his uncle, Shirkuh (Nur ad-Din's lieutenant who was representing him on campaigns against a faction of the Fatimid Caliphate of Egypt in the 1160s), Saladin eventually succeeded the defeated Fatimid faction and his uncle as vizier in 1169. There, he inherited a difficult role defending Egypt against the incursions of the Crusader Kingdom of Jerusalem under Amalric I (see Crusader invasion of Egypt). His position was tenuous at first; He was not expected to last long in Egypt where there had been many changes of government in previous years due to a long line of child caliphs fought over by competing viziers. With a Sunni Syrian base he had little control over the Egyptian army, which had been dominated by Shias since the rise of the Fatimids, and which was led in the name of the now otherwise powerless caliph al-Adid.

When the caliph died, in September 1171, Saladin had the ulema pronounced the name of Al-Mustadi, the Sunni and, more importantly, Abbassid caliph in Baghdad, at sermon before Friday prayers; authority simply deposed the old line. Saladin ruled Egypt, but officially as the representative of the Turkish Seljuk ruler Nur ad-Din, who himself conventionally recognized the Abbassid caliph.

Saladin revitalized the economy of Egypt, reorganized the military forces and, following his father's advice, stayed away from any conflicts with Nur ad-Din, his formal lord, after he had become the real ruler of Egypt. He waited until Nur ad-Din's death before starting serious military actions: at first against smaller Muslim states, then directing them against the Crusaders.

With Nur ad-Din's death (1174), he assumed the title of Sultan in Egypt founding the Ayyubid dynasty and restoring Sunnism in Egypt. He extended his territory westwards in the Maghreb, and when his uncle was sent up the Nile to pacify some resistance of the former Fatimid supporters, he continued on down the Red Sea to conquer Yemen. He is also regarded as a Waliullah, a person religiously close to God in the Sunni Muslim tradition.

On two occasions, in 1170 and 1172, Saladin retreated from an invasion of the Kingdom of Jerusalem. These had been launched by Nur ad-Din, and Saladin hoped that the Crusader kingdom would remain intact, as a buffer state between Egypt and Syria, until Saladin could gain control of Syria as well. Nur ad-Din and Saladin were headed towards open war on these counts when Nur ad-Din died in 1174. Nur ad-Din's heir, as-Salih Ismail al-Malik, was a mere boy, in the hands of court eunuchs, and died in 1181.

Immediately after Nur ad-Din's death, Saladin marched on Damascus and was welcomed into the city. He reinforced his legitimacy there in the time-honored way, by marrying Nur ad-Din's widow Ismat ad-Din Khatun. Aleppo and Mosul, on the other hand, the two other largest cities that Nur ad-Din had ruled, were never taken, but Saladin managed to impose his influence and authority on them in 1176 and 1186 respectively. While he was occupied in besieging Aleppo, on May 22, 1176, the shadowy Ismaili assassin group, the Hashshashin, attempted to murder him. They made two attempts on his life, the second time coming close enough to inflict wounds.

While Saladin was consolidating his power in Syria, he usually left the Crusader kingdom alone, although he was generally victorious whenever he did meet the Crusaders in battle. One exception was the Battle of Montgisard on November 25, 1177. He was defeated by the combined forces of Baldwin IV of Jerusalem, Raynald of Chatillon and the Knights Templar. Only one tenth of his army made it back to Egypt. 

A truce was declared between Saladin and the Crusader States in 1178. Saladin spent the subsequent year recovering from his defeat and rebuilding his army, renewing his attacks in 1179 when he defeated the Crusaders at the Battle of Jacob's Ford. Crusader counter-attacks provoked further responses by Saladin. Raynald of Chatillon, in particular, harassed Muslim trading and pilgrimage routes with a fleet on the Red Sea, a water route that Saladin needed to keep open. In response, Saladin built a fleet of 30 galleys to attack Beirut in 1182. Raynald threatened to attack the holy cities of Mecca and Medina. In retaliation, Saladin besieged Kerak, Raynald's fortress in Oultrejordain, in 1183 and 1184. Raynald responded by looting a caravan of pilgrims on the Hajj in 1185. According to the later thirteenth-century Old French Continuation of William of Tyre, Raynald captured Saladin's sister in a raid on a caravan, although this claim is not attested in contemporary sources, Muslim or Frankish. In fact, Raynald had attacked a preceding caravan, and Saladin set guards to ensure the safety of his sister and her son, who came to no harm.

In July 1187, Saladin captured most of the Kingdom of Jerusalem. On July 4, 1187, he faced at the Battle of Hattin the combined forces Guy of Lusignan, King Consort of Jerusalem, and Raymond III of Tripoli. In this battle alone the Crusader army was largely annihilated by the motivated army of Saladin in what was a major disaster for the Crusaders and a turning point in the history of the Crusades. Saladin captured Raynald de Chatillon and was personally responsible for his execution. Guy of Lusignan was also captured but his life was spared. Two days after the Battle of Hattin, Saladin ordered the execution of all prisoners of the military orders by beheading. The executions took place as Saladin's secretary himself, Imad ad-Din, from the Ibid, page 138, describes: â€œHe (Saladin) ordered that they should be beheaded, choosing to have them dead rather than in prison. With him was a whole band of scholars and Sufis and a certain number of devout men and ascetics; each begged to be allowed to kill one of them, and drew his sword and rolled back his sleeve. Saladin, his face joyful, was sitting on his dais; the unbelievers showed black despair.â€?The execution of prisoners at Hattin was not the first by Saladin. On August 29, 1179, he captured the castle at Bait al-Ahazon and approximately 700 prisoners were taken and executed.



Saladin had almost captured every Crusader city. Jerusalem capitulated to his forces on October 2, 1187, after a siege. Saladin initially was unwilling to grant terms of quarter to the European occupants of Jerusalem until Balian of Ibelin threatened to kill every Muslim in the city, estimated between 3,000 to 5,000, and to destroy Islamâ€™s holy shrines of the Dome of the Rock and the al-Aqsa Mosque if quarter was not given. Saladin consulted his council and these terms were accepted. Ransom was to be paid for each Frank in the city whether man, woman, or child. Saladin allowed many to leave without having the required amount for ransom for others. According to Imad al-Din, approximately 7,000 men and 8,000 women could not make their ransom and were taken into slavery. 

Tyre, on the coast of modern-day Lebanon was the last major Crusader city that was not captured by Muslim forces. The city was now commanded by Conrad of Montferrat, who strengthened Tyre's defences and withstood two sieges by Saladin. In 1188, at Tortosa, Saladin released Guy of Lusignan and returned him to his wife, Queen Sibylla of Jerusalem. They went first to Tripoli, then to Antioch. In 1189, they sought to reclaim Tyre for their kingdom, but were refused admission by Conrad, who did not recognize Guy as king. Guy then set about besieging Acre. 

Hattin and the fall of Jerusalem prompted the Third Crusade, financed in England by a special "Saladin tithe". Richard I of England led Guy's siege of Acre, conquered the city and executed 3,000 Muslim prisoners including women and children. Saladin retaliated by killing all Franks captured from August 28 - September 10. Beha ad-Din describes a particular grisly scene with two captured Franks during this time period: "Whilst we were there they brought two Franks to the Sultan (Saladin) who had been made prisoners by the advance guard. He had them beheaded on the spot." 

The armies of Saladin engaged in combat with the rival armies of King Richard I of England at the Battle of Arsuf on September 7, 1191 at which Saladin was defeated. Saladin's relationship with Richard was one of chivalrous mutual respect as well as military rivalry; both were celebrated in courtly romances. When Richard was wounded, Saladin offered the services of his personal physician. At Arsuf, when Richard lost his horse, Saladin sent him two replacements. Saladin also sent him fresh fruit with snow, to keep his drinks cold. Richard had suggested to Saladin that his sister could marry Saladin's brother - and Jerusalem could be their wedding gift.

The two came to an agreement over Jerusalem in the Treaty of Ramla in 1192, whereby the city would remain in Muslim hands but would be open to Christian pilgrimages; the treaty reduced the Latin Kingdom to a strip along the coast from Tyre to Jaffa.



Saladin died on March 4, 1193, at Damascus, not long after Richard's departure. When they opened Saladin's treasury they found there was not enough money to pay for his funeral; he had given most of his money away in charity. 

Saladin is buried in a mausoleum in the garden outside the Umayyad Mosque in Damascus, Syria. Emperor Wilhelm II of Germany donated a new marble sarcophagus to the mausoleum. Saladin was, however, not placed in it. Instead the mausoleum, which is open to visitors, now has two sarcophagi: one empty in marble and one in wood containing the body of Saladin.



Despite his fierce struggle against the crusades, Saladin achieved a great reputation in Europe as a chivalrous knight, so much so that there existed by the fourteenth century an epic poem about his exploits, and Dante included him among the virtuous pagan souls in Limbo. Saladin appears in a sympathetic light in Sir Walter Scott's The Talisman (1825). Despite the Crusaders' slaughter when they originally conquered Jerusalem in 1099, Saladin granted amnesty and free passage to all common Catholics and even to the defeated Christian army, as long as they were able to pay the aforementioned ransom (the Greek Orthodox Christians were treated even better, because they often opposed the western Crusaders). An interesting view of Saladin and the world in which he lived is provided by Tariq Ali's novel The Book of Saladin.

Notwithstanding the differences in beliefs, the Muslim Saladin was respected by Christian lords, Richard especially. Richard once praised Saladin as a great prince, saying that he was without doubt the greatest and most powerful leader in the Islamic world. Saladin in turn stated that there was not a more honorable Christian lord than Richard. After the treaty, Saladin and Richard sent each other many gifts as tokens of respect, but never met face to face.

In April 1191, a Frankish woman's three month old baby had been stolen from her camp and had been sold on the market. The Franks urged her to approach Saladin herself with her grievance. After Saladin used his own money to buy the child, "he gave it to the mother and she took it; with tears streaming down her face, and hugged it to her breast. The people were watching her and weeping and I (Ibn Shaddad) was standing amongst them. She suckled it for some time and then Saladin ordered a horse to be fetched for her and she went back to camp."The name Salah ad-Din means "Righteousness of Faith," and through the ages Saladin has been an inspiration for Muslims in many respects. Modern Muslim rulers have sought to capitalize on the reputation of Saladin. A governorate centered around Tikrit and Samarra in modern-day Iraq, Salah ad Din Governorate, is named after him, as is Salahaddin University in Arbil.

Few structures associated with Saladin survive within modern cities. Saladin first fortified the Citadel of Cairo (1175 - 1183), which had been a domed pleasure pavilion with a fine view in more peaceful times. In Syria, even the smallest city is centred on a defensible citadel, and Saladin introduced this essential feature to Egypt.

Among the forts he built was Qalaat Al-Gindi, a mountaintop fortress and caravanserai in the Sinai. The fortress overlooks a large wadi which was the convergence of several caravan routes that linked Egypt and the Middle East. Inside the structure are a number of large vaulted rooms hewn out of rock, including the remains of shops and a water cistern. A notable archaeological site, it was investigated in 1909 by a French team under Jules Barthoux.<ref name="Schreurs_Saladin""></ref>

According to the French writer RenÃ© Grousset, "It is equally true that his generosity, his piety, devoid of fanaticism, that flower of liberality and courtesy which had been the model of our old chroniclers, won him no less popularity in Frankish Syria than in the lands of Islam".

When German Kaiser Wilhelm the Second went to Syria he laid a wreath at the tomb of Saladin in Damascus with the inscription, "knight without fear or blame who often had to teach his opponents the right way to practice chivalry".

Although the Ayyubid dynasty he founded would only outlive him by fifty-seven years, the legacy of Saladin within the Arab World continues to this day. With the rise of Arab nationalism in the Twentieth Century, particularly with regard to the Arab-Israeli conflict, Saladin's heroism and leadership gained a new significance. Saladin's liberation of Palestine from the European Crusaders was taken as the inspiration for the modern-day Arabs' struggle against Zionism. Moreover, the glory and comparative unity of the Arab World under Saladin was seen as the perfect symbol for the new unity sought by Arab nationalists, such as Gamal Abdel Nasser. For this reason, the Eagle of Saladin became the symbol of revolutionary Egypt, and was subsequently adopted by several other Arab states (Iraq, Palestine, and Yemen).

In 1963 an Egyptian movie on Saladin, titled Al Nasser Salah Ad-Din.

In the 2005 film Kingdom of Heaven, Ridley Scott, Saladin is portrayed by Syrian actor Ghassan Massoud.

In the 2007 Swedish film Arn â€?The Knight Templar (Arn â€?Tempelriddaren), Saladin is portrayed by Indian actor and supermodel Milind Soman.









Atheism, as a philosophical view, is the position that either affirms the nonexistence of gods or rejects theism. When defined more broadly, atheism is the absence of belief in deities, alternatively called ''nontheism''. Although atheism is often equated with irreligion, some religious philosophies, such as secular theology and some varieties of Buddhism such as Theravada, either do not include belief in a personal god as a tenet of the religion, or actively teach nontheism.

Many self-described atheists are skeptical of all supernatural beings and cite a lack of empirical evidence for the existence of deities. Others argue for atheism on philosophical, social or historical grounds. Although many self-described atheists tend toward secular philosophies such as humanism and naturalism, there is no one ideology or set of behaviors to which all atheists adhere.

The term atheism originated as a pejorative epithet applied to any person or belief in conflict with established religion. With the spread of freethought, scientific skepticism, and criticism of religion, the term began to gather a more specific meaning and has been increasingly used as a self-description by atheists.



In early Ancient Greek, the adjective  (, from the privative - +  "god") meant "godless". The word began to indicate more-intentional, active godlessness in the 5th century BCE, acquiring definitions of "severing relations with the gods" or "denying the gods, ungodly" instead of the earlier meaning of  () or "impious". Modern translations of classical texts sometimes render  as "atheistic". As an abstract noun, there was also  (), "atheism". Cicero transliterated the Greek word into the Latin . The term found frequent use in the debate between early Christians and Hellenists, with each side attributing it, in the pejorative sense, to the other.

In English, the term atheism was derived from the French  in about 1587. The term atheist (from Fr. ), in the sense of "one who denies or disbelieves the existence of God", predates atheism in English, being first attested in about 1571. Atheist as a label of practical godlessness was used at least as early as 1577. Related words emerged later: deist in 1621, theist in 1662; theism in 1678; and deism in 1682. Deism and theism changed meanings slightly around 1700, due to the influence of atheism; deism was originally used as a synonym for today's theism, but came to denote a separate philosophical doctrine.

Karen Armstrong writes that "During the sixteenth and seventeenth centuries, the word 'atheist' was still reserved exclusively for polemic ... The term 'atheist' was an insult. Nobody would have dreamed of calling himself an atheist." Atheism was first used to describe a self-avowed belief in late 18th-century Europe, specifically denoting disbelief in the monotheistic Abrahamic god. In the 20th century, globalization contributed to the expansion of the term to refer to disbelief in all deities, though it remains common in Western society to describe atheism as simply "disbelief in God". Most recently, there has been a push in certain philosophical circles to redefine atheism as the "absence of belief in deities", rather than as a belief in its own right; this definition has become popular in atheist communities, though its mainstream usage has been limited.

Writers disagree how best to define and classify atheism, contesting what supernatural entities it applies to, whether it is an assertion in its own right or merely the absence of one, and whether it requires a conscious, explicit rejection. A variety of categories have been proposed to try to distinguish the different forms of atheism, most of which treat atheism as "absence of belief in deities" in order to explore the varieties of this nontheism.

Some of the ambiguity and controversy involved in defining atheism arises from difficulty in reaching a consensus for the definitions of words like deity and god. The plurality of wildly different conceptions of god and deities leads to differing ideas regarding atheism's applicability. In contexts where theism is defined as the belief in a singular personal god, for example, people who believe in a variety of other deities may be classified as atheists, including deists and even polytheists. In the 20th century, this view has fallen into disfavor as theism has come to be understood as encompassing belief in any divinity.

With respect to the range of phenomena being rejected, atheism may counter anything from the existence of a god, to the existence of any spiritual, supernatural, or transcendental concepts, such as those of Hinduism and Buddhism.

Definitions of atheism also vary in the degree of consideration a person must put to the idea of gods to be considered an atheist. Minimally, atheism may be seen as the absence of belief in one or more gods. It has been contended that this broad definition includes newborns and other people who have not been exposed to theistic ideas. As far back as 1772, d'Holbach said that "All children are born Atheists; they have no idea of God". Similarly, George H. Smith (1979) suggested that: "The man who is unacquainted with theism is an atheist because he does not believe in a god. This category would also include the child with the conceptual capacity to grasp the issues involved, but who is still unaware of those issues. The fact that this child does not believe in god qualifies him as an atheist." Smith coined the term implicit atheism to refer to "the absence of theistic belief without a conscious rejection of it" and explicit atheism to refer to the more common definition of conscious disbelief.

In Western civilization, the view that children are born atheist is relatively recent. Before the 18th century, the existence of God was so universally accepted in the western world that even the possibility of true atheism was questioned. This is called theistic innatismâ€”the notion that all people believe in God from birth; within this view was the connotation that atheists are simply in denial. There is a position claiming that atheists are quick to believe in God in times of crisis, that atheists make deathbed conversions, or that "there are no atheists in foxholes". Some proponents of this view claim that the anthropological benefit of religion is that religious faith enables humans to endure hardships better (c.f. opium of the people). Some atheists emphasize the fact that there have been examples to the contrary, among them examples of literal "atheists in foxholes".

Philosophers such as Antony Flew and Michael Martin have contrasted strong (positive) atheism with weak (negative) atheism. Strong atheism is the explicit affirmation that gods do not exist. Weak atheism includes all other forms of non-theism. According to this categorization, anyone who is not a theist is either a weak or a strong atheist. The terms weak and strong are relatively recent; however, the equivalent terms negative and positive atheism have been used in the philosophical literature and (in a slightly different sense) in Catholic apologetics. Under this demarcation of atheism, most agnostics qualify as weak atheists.

While agnosticism can be seen as a form of weak atheism, most agnostics see their view as distinct from atheism, which they may consider no more justified than theism, or requires an equal conviction. The supposed unattainability of knowledge for or against the existence of gods is sometimes seen as indication that atheism requires a leap of faith. Common atheist responses to this argument include that unproven religious propositions deserve as much disbelief as all other unproven propositions, and that the unprovability of a god's existence does not imply equal probability of either possibility. Scottish philosopher J. J. C. Smart even argues that "sometimes a person who is really an atheist may describe herself, even passionately, as an agnostic because of unreasonable generalised philosophical scepticism which would preclude us from saying that we know anything whatever, except perhaps the truths of mathematics and formal logic." Consequently, some popular atheist authors such as Richard Dawkins prefer distinguishing theist, agnostic and atheist positions by the probability assigned to the statement "God exists".

[[Image:Supreme Impiety, Atheist and Charlatan - Picta poesis, by BarthÃ©lemy Aneau (1552).jpg|300px|thumb|right|"A child of the mob once asked an astronomer who the father was who brought him into this world. The scholar pointed to the sky, and to an old man sitting, and said: 'That one there is your body's father, and that your soul's.' To which the boy replied: 'WHAT IS ABOVE US IS OF NO CONCERN TO US, and I'm ashamed to be the child of such an aged man!' O WHAT SUPREME impiety, not to want to recognize your father, and not to think God is your maker!"Emblem illustrating practical atheism and its historical association with immorality, titled "Supreme Impiety: Atheist and Charlatan", from Picta poesis, by BarthÃ©lemy Aneau, 1552.]]

The broadest demarcation of atheistic rationale is between practical and theoretical atheism. The different forms of theoretical atheism each derive from a particular rationale or philosophical argument. In contrast, practical atheism requires no specific argument, and can include indifference to and ignorance of the idea of gods.

In practical, or pragmatic, atheism, also known as apatheism, individuals live as if there are no gods and explain natural phenomena without resorting to the divine. The existence of gods is not denied, but may be designated unnecessary or useless; gods neither provide purpose to life, nor influence everyday life, according to this view. A form of practical atheism with implications for the scientific community is methodological naturalismâ€”the "tacit adoption or assumption of philosophical naturalism within scientific method with or without fully accepting or believing it."

Practical atheism can take various forms: 

Historically, practical atheism was considered by some people to be associated with moral failure, willful ignorance and impiety. Those considered practical atheists were said to behave as though God, ethics and social responsibility did not exist; they abandoned duty and embraced hedonism. According to the French Catholic philosopher Ã‰tienne Borne, "Practical atheism is not the denial of the existence of God, but complete godlessness of action; it is a moral evil, implying not the denial of the absolute validity of the moral law but simply rebellion against that law."



Theoretical, or contemplative, atheism explicitly posits arguments against the existence of gods, responding to common theistic arguments such as the argument from design or Pascal's Wager. The theoretical reasons for rejecting gods assume various psychological, sociological, metaphysical, and epistemological forms. 

Epistemological atheism argues that people cannot know God or determine the existence of God. The foundation of epistemological atheism is agnosticism, which takes a variety of forms. In the philosophy of immanence, divinity is inseparable from the world itself, including a person's mind, and each person's consciousness is locked in the subject. According to this form of agnosticism, this limitation in perspective prevents any objective inference from belief in a god to assertions of its existence. The rationalistic agnosticism of Kant and the Enlightenment only accepts knowledge deduced with human rationality; this form of atheism holds that gods are not discernible as a matter of principle, and therefore cannot be known to exist. Skepticism, based on the ideas of Hume, asserts that certainty about anything is impossible, so one can never know the existence of God. The allocation of agnosticism to atheism is disputed; it can also be regarded as an independent, basic world-view.

Other forms of atheistic argumentation that may qualify as epistemological, including logical positivism and ignosticism, assert the meaninglessness or unintelligibility of basic terms such as "God" and statements such as "God is all-powerful". Theological noncognitivism holds that the statement "God exists" does not express a proposition, but is nonsensical or cognitively meaningless. It has been argued both ways as to whether such individuals classify into some form of atheism or agnosticism. Philosophers A. J. Ayer and Theodore M. Drange reject both categories, stating that both camps accept "God exists" as a proposition; they instead place noncognitivism in its own category.

Metaphysical atheism is based on metaphysical monismâ€”the view that reality is homogeneous and indivisible. Absolute metaphysical atheists subscribe to some form of physicalism, hence they explicitly deny the existence of non-physical beings. Relative metaphysical atheists maintain an implicit denial of a particular concept of God based on the incongruity between their individual philosophies and attributes commonly applied to God, such as transcendence, a personal aspect, or unity. Examples of relative metaphysical atheism include pantheism, panentheism, and deism.

[[Image:Epikouros BM 1843.jpg|thumb|left|Epicurus is credited with first expounding the problem of evil. David Hume in his Dialogues concerning Natural Religion (1779) cited Epicurus in stating the argument as a series of questions:"Is [God] willing to prevent evil, but not able? then is he impotent. Is he able, but not willing? then is he malevolent. Is he both able and willing? whence then is evil?"]]

Philosophers such as Ludwig Feuerbach and Sigmund Freud argued that God and other religious beliefs are human inventions, created to fulfill various psychological and emotional wants or needs. This is also the Buddhist view. Karl Marx and Friedrich Engels, influenced by the work of Feuerbach, argued that belief in God and religion are social functions, used by those in power to oppress the working class. According to Mikhail Bakunin, "the idea of God implies the abdication of human reason and justice; it is the most decisive negation of human liberty, and necessarily ends in the enslavement of mankind, in theory and practice." He reversed Voltaire's famous aphorism that if God did not exist, it would be necessary to invent Him, writing instead that "if God really existed, it would be necessary to abolish him."

Logical atheism holds that the various conceptions of gods, such as the personal god of Christianity, are ascribed logically inconsistent qualities. Such atheists present deductive arguments against the existence of God, which assert the incompatibility between certain traits, such as perfection, creator-status, immutability, omniscience, omnipresence, omnipotence, omnibenevolence, transcendence, personhood (a personal being), nonphysicality, justice and mercy.

Theodicean atheists believe that the world as they experience it cannot be reconciled with the qualities commonly ascribed to God and gods by theologians. They argue that an omniscient, omnipotent, and omnibenevolent God is not compatible with a world where there is evil and suffering, and where divine love is hidden from many people. A similar argument is attributed to Siddhartha Gautama, the founder of Buddhism.

Axiological, or constructive, atheism rejects the existence of gods in favor of a "higher absolute", such as humanity. This form of atheism favors humanity as the absolute source of ethics and values, and permits individuals to resolve moral problems without resorting to God. Marx, Nietzsche, Freud, and Sartre all used this argument to convey messages of liberation, full-development, and unfettered happiness.

One of the most common criticisms of atheism has been to the contraryâ€”that denying the existence of a just God leads to moral relativism, leaving one with no moral or ethical foundation, or renders life meaningless and miserable. Blaise Pascal argued this view in 1669.



Although the term atheism originated in 16th-century France, ideas that would be recognized today as atheistic are documented from classical antiquity and the Vedic period.

Atheistic schools are found in Hinduism, which is otherwise a very theistic religion. The thoroughly materialistic and anti-religious philosophical CÄrvÄka School that originated in India around 6th century BCE is probably the most explicitly atheistic school of philosophy in India. This branch of Indian philosophy is classified as a heterodox system and is not considered part of the six orthodox schools of Hinduism, but it is noteworthy as evidence of a materialistic movement within Hinduism. Chatterjee and Datta explain that our understanding of CÄrvÄka philosophy is fragmentary, based largely on criticism of the ideas by other schools, and that it is not a living tradition:

"Though materialism in some form or other has always been present in India, and occasional references are found in the Vedas, the Buddhistic literature, the Epics, as well as in the later philosophical works we do not find any systematic work on materialism, nor any organized school of followers as the other philosophical schools possess. But almost every work of the other schools states, for reputation, the materialistic views. Our knowledge of Indian materialism is chiefly based on these."

Other Indian philosophies generally regarded as atheistic include Classical Samkhya and Purva Mimamsa. The rejection of a personal creator God is also seen in Jainism and Buddhism in India.

Western atheism has its roots in pre-Socratic Greek philosophy, but did not emerge as a distinct world-view until the late Enlightenment. The 5th-century BCE Greek philosopher Diagoras is known as the "first atheist", and strongly criticized religion and mysticism. Critias viewed religion as a human invention used to frighten people into following moral order. Atomists such as Democritus attempted to explain the world in a purely materialistic way, without reference to the spiritual or mystical. Other pre-Socratic philosophers who probably had atheistic views included Prodicus, Protagoras, and Theodorus. The 3rd-century BCE Greek philosopher Strato of Lampsacus also did not believe gods exist.</blockquote>

Socrates was accused of being an atheist for impiety (see Euthyphro dilemma) on the basis that he inspired questioning of the state gods. Although he disputed the accusation that he was a "complete atheist", he was ultimately sentenced to death.

Another atomic materialist, Epicurus, disputed many religious doctrines, including the existence of an afterlife or a personal deity; he considered the soul purely material and mortal. While Epicureanism did not rule out the existence of gods, he believed that if they did exist, they were unconcerned with humanity. 

The Roman poet Lucretius agreed that, if there were gods, they were unconcerned with humanity and unable to affect the natural world. For this reason, he believed humanity should have no fear of the supernatural. In De rerum natura ("On the nature of things"), he expounds his Epicurean views of the cosmos, atoms, the soul, mortality, and religion.

The Roman philosopher Sextus Empiricus held that one should suspend judgment about virtually all beliefsâ€”a form of skepticism known as Pyrrhonismâ€”that nothing was inherently evil, and that ataraxia ("peace of mind") is attainable by withholding one's judgment. His relatively large volume of surviving works had a lasting influence on later philosophers.

The meaning of "atheist" changed over the course of classical antiquity. The early Christians were labeled atheists by non-Christians because of their disbelief in pagan gods. During the Roman Empire, Christians were executed for their rejection of the Roman gods in general and Emperor-worship in particular. When Christianity became the state religion of Rome under Theodosius I in 381, heresy became a punishable offense.

The espousal of atheistic views was rare in Europe during the Early Middle Ages and Middle Ages (see Medieval Inquisition); metaphysics, religion and theology were the dominant interests. There were, however, movements within this period that forwarded heterodox conceptions of the Christian God, including differing views of the nature, transcendence, and knowability of God. Individuals and groups such as Johannes Scotus Eriugena, David of Dinant, Amalric of Bena, and the Brethren of the Free Spirit maintained Christian viewpoints with pantheistic tendencies. Nicholas of Cusa held to a form of fideism he called docta ignorantia ("learned ignorance"), asserting that God is beyond human categorization, and our knowledge of God is limited to conjecture. William of Ockham inspired anti-metaphysical tendencies with his nominalistic limitation of human knowledge to singular objects, and asserted that the divine essence could not be intuitively or rationally apprehended by human intellect. Followers of Ockham, such as John of Mirecourt and Nicholas of Autrecourt furthered this view. The resulting division between faith and reason influenced later theologians such as John Wycliffe, Jan Hus, and Martin Luther.

The Renaissance did much to expand the scope of freethought and skeptical inquiry. Individuals such as Leonardo da Vinci sought experimentation as a means of explanation, and opposed arguments from religious authority. Other critics of religion and the Church during this time included NiccolÃ² Machiavelli, Bonaventure des PÃ©riers, and FranÃ§ois Rabelais.

The Renaissance and Reformation eras witnessed a resurgence in religious fervor, as evidenced by the proliferation of new religious orders, confraternities, and popular devotions in the Catholic world, and the appearance of increasingly austere Protestant sects such as the Calvinists. This era of interconfessional rivalry permitted an even wider scope of theological and philosophical speculation, much of which would later be used to advance a religiously skeptical world-view.

Criticism of Christianity became increasingly frequent in the 17th and 18th centuries, especially in France and England, where there appears to have been a religious malaise, according to contemporary sources. Some Protestant thinkers, such as Thomas Hobbes, espoused a materialist philosophy and skepticism toward supernatural occurrences. In the late 17th century, Deism came to be openly espoused by intellectuals such as John Toland, and practically all the philosophes of 18th-century France and England held to some form of Deism. Despite their ridicule of Christianity, many Deists held atheism in scorn. The first known atheist who threw off the mantle of deism, bluntly denying the existence of gods, was Jean Meslier, a French priest who lived in the early 18th century. He was followed by other openly atheistic thinkers, such as Baron d'Holbach, who appeared in the late 18th century, when expressing disbelief in God became a less dangerous position. David Hume was the most systematic exponent of Enlightenment thought, developing a skeptical epistemology grounded in empiricism, undermining the metaphysical basis of natural theology.



The French Revolution took atheism outside the salons and into the public sphere. Attempts to enforce the Civil Constitution of the Clergy led to anti-clerical violence and the expulsion of many clergy from France. The chaotic political events in revolutionary Paris eventually enabled the more radical Jacobins to seize power in 1793, ushering in the Reign of Terror. At its climax, the more militant atheists attempted to forcibly de-Christianize France, replacing religion with a Cult of Reason. These persecutions ended with the Thermidorian Reaction, but some of the secularizing measures of this period remained a permanent legacy of French politics.

The Napoleonic era institutionalized the secularization of French society, and exported the revolution to northern Italy, in the hopes of creating pliable republics. In the nineteenth century, many atheists and other anti-religious thinkers devoted their efforts to political and social revolution, facilitating the upheavals of 1848, the Risorgimento in Italy, and the growth of an international socialist movement. 

In the latter half of the 19th century, atheism rose to prominence under the influence of rationalistic and freethinking philosophers. Many prominent German philosophers of this era denied the existence of deities and were critical of religion, including Ludwig Feuerbach, Arthur Schopenhauer, Karl Marx, and Friedrich Nietzsche.

Atheism in the 20th century, particularly in the form of practical atheism, advanced in many societies. Atheistic thought found recognition in a wide variety of other, broader philosophies, such as existentialism, Objectivism, secular humanism, nihilism, logical positivism, Marxism, feminism, and the general scientific and rationalist movement.

Logical positivism and scientism paved the way for neopositivism, analytical philosophy, structuralism, and naturalism. Neopositivism and analytical philosophy discarded classical rationalism and metaphysics in favor of strict empiricism and epistemological nominalism. Proponents such as Bertrand Russell emphatically rejected belief in God. In his early work, Ludwig Wittgenstein attempted to separate metaphysical and supernatural language from rational discourse. A. J. Ayer asserted the unverifiability and meaninglessness of religious statements, citing his adherence to the empirical sciences. Relatedly the applied structuralism of LÃ©vi-Strauss sourced religious language to the human subconscious in denying its transcendental meaning. J. N. Findlay and J. J. C. Smart argued that the existence of God is not logically necessary. Naturalists and materialistic monists such as John Dewey considered the natural world to be the basis of everything, denying the existence of God or immortality.

The 20th century also saw the political advancement of atheism, spurred on by interpretation of the works of Marx and Engels. After the 1917 revolution in Russia, increased religious freedom for minority religions lasted for a few years, before the policies of Stalinism turned towards repression of religion. The Soviet Union and other communist states promoted state atheism and opposed religion, often by violent means.Other leaders like E. V. Ramasami Naicker (Periyar), a prominent atheist leader of India, fought against Hinduism and Brahmins for discriminating and dividing people in the name of caste and religion. This was highlighted in 1956 when he made the Hindu god Rama wear a garland made of slippers and made antitheistic statements.

In 1966, ''TIME'' magazine asked "Is God Dead?" in response to the Death of God theological movement, citing the estimation that nearly one in two people in the world lived under an anti-religious power and millions more in Africa, Asia, and South America seemed to lack knowledge of the Christian God. The following year, the Albanian government under Enver Hoxha announced the closure of all religious institutions in the country, declaring Albania the world's first atheist state. These regimes enhanced the negative associations of atheism, especially where anti-communist sentiment was strong in the United States, despite the fact that some prominent atheists were anti-communist. Since the fall of the Berlin Wall, the number of actively anti-religious regimes has reduced considerably. In 2006, Timothy Shah of the Pew Forum noted "a worldwide trend across all major religious groups, in which God-based and faith-based movements in general are experiencing increasing confidence and influence vis-Ã -vis secular movements and ideologies." Gregory S. Paul and Phil Zuckerman consider this a myth and suggest that the actual situation is more complex and nuanced.



It is difficult to quantify the number of atheists in the world. Respondents to religious-belief polls may define "atheism" differently or draw different distinctions between atheism, non-religious beliefs, and non-theistic religious and spiritual beliefs. In addition, people in some regions of the world refrain from reporting themselves as atheists to avoid social stigma, discrimination, and persecution. A 2005 survey published in EncyclopÃ¦dia Britannica finds that the non-religious make up about 11.9% of the world's population, and atheists about 2.3%. This figure does not include those who follow atheistic religions, such as some Buddhists.



A letter published in Nature in 1998 reported a survey suggesting that belief in a personal god or afterlife was at an all-time low among the members of the U.S. National Academy of Science, only 7.0% of whom believed in a personal god as compared with more than 85% of the general U.S. population. In the same year Frank Sulloway of MIT and Michael Shermer of California State University conducted a study which found in their polling sample of "credentialed" U.S. adults (12% had Ph.Ds and 62% were college graduates) 64% believed in God, and there was a correlation indicating that religious conviction diminished with education level.Such an inverse correlation between religiosity and intelligence has been found by 39 studies carried out between 1927 and 2002, according to an article in Mensa Magazine. These findings broadly concur with a 1958 statistical meta-analysis from Professor Michael Argyle of Oxford University. He analyzed seven research studies that had investigated correlation between attitude to religion and measured intelligence among school and college students from the U.S. Although a clear negative correlation was found, the analysis did not identify causality but noted that factors such as authoritarian family background and social class may also have played a part.



Although people who self-identify as atheists are usually assumed to be irreligious, some sects within major religions reject the existence of a personal, creator deity. In recent years, certain religious denominations have accumulated a number of openly atheistic followers, such as atheistic or humanistic Judaism and Christian atheists.

As the strictest sense of positive atheism does not entail any specific beliefs outside of disbelief in God, atheists can hold any number of spiritual beliefs. For the same reason, atheists can hold a wide variety of ethical beliefs, ranging from the moral universalism of humanism, which holds that a moral code should be applied consistently to all humans, to moral nihilism, which holds that morality is meaningless.

Some philosophers, however, have equated atheism with immorality, arguing that morality must be derived from God and cannot exist without a wise creator. Moral precepts such as "murder is wrong" are seen as divine laws, requiring a divine lawmaker and judge. However, many atheists argue that treating morality legalistically involves a false analogy, and that morality does not depend on a lawmaker in the same way that laws do, based on the Euthyphro dilemma, which either renders God unnecessary or morality arbitrary.

Philosophers Susan Neiman and Julian Baggini (among others) assert that behaving ethically only because of divine mandate is not true ethical behavior but merely blind obedience. Baggini argues that atheism is a superior basis for ethics, claiming that a moral basis external to religious imperatives is necessary to evaluate the morality of the imperatives themselvesâ€”to be able to discern, for example, that "thou shalt steal" is immoral even if one's religion instructs itâ€”and that atheists, therefore, have the advantage of being more inclined to make such evaluations.

Atheists such as Sam Harris have argued that Western religions' reliance on divine authority lends itself to authoritarianism and dogmatism. Indeed, religious fundamentalism and extrinsic religion (when religion is held because it serves other, more ultimate interests) have been correlated with authoritarianism, dogmatism, and prejudice. This argument, combined with historical events that are argued to demonstrate the dangers of religion, such as the Crusades, inquisitions, and witch trials, are often used by antireligious atheists to justify their views.











Education encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgment and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). Education means 'to draw out', facilitating realisation of self-potential and latent talents of an individual. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology â€”often more profound than they realizeâ€”though family teaching may function very informally.

Education systems are established to provide education and training, in most cases for children and the young. A curriculum defines what students should know, understand and be able to do as the result of education. A teaching profession delivers teaching which enables learning, and a system of polices, regulations, examinations, structures and funding enables teachers to teach to the best of their abilities. Sometimes education systems can be used to promote doctrines or ideals as well as knowledge, which is known as social engineering. This can lead to political abuse of the system, particularly in totalitarian states and government. 

Primary (or elementary) education consists of the first years of formal, structured education. In general, primary education consists of six or seven years of schooling starting at the age of 5 or 6, although this varies between and sometimes within countries. Globally, around 70% of primary-age children are enrolled in primary education, and this proportion is rising.. Under the Education for All program driven by UNESCO, most countries have committed to achieving universal enrolment in primary education by 2015, and in many countries it is compulsory for children to receive primary education. The division between primary and secondary education is somewhat arbitrary, but it generally occurs at about eleven or twelve years of age. Some education systems have separate middle schools with the transition to the final stage of secondary education taking place at around the age of fourteen. Mostly schools which provide primary education are referred to as primary schools. Primary schools in these countries are often subdivided into infant schools and junior schools.

In most contemporary educational systems of the world, secondary education consists of the second years of formal education that occur during adolescence. It is characterised by transition from the typically compulsory, comprehensive primary education for minors to the optional, selective tertiary, "post-secondary", or "higher" education (e.g., university, vocational school) for adults. Depending on the system, schools for this period or a part of it may be called secondary or high schools, gymnasiums, lyceums, middle schools, colleges, or vocational schools. The exact meaning of any of these varies between the systems. The exact boundary between primary and secondary education varies from country to country and even within them, but is generally around the seventh to the tenth year of schooling. Secondary education occurs mainly during the teenage years. In the United States and Canada primary and secondary education together are sometimes referred to as K-12 education, and in New Zealand Year 1-13 is used. The purpose of secondary education can be to give common knowledge, to prepare for higher education or to train directly in a profession.

Higher education, also called tertiary, third stage or post secondary education, is the non-compulsory educational level following the completion of a school providing a secondary education, such as a high school, secondary school, or gymnasium. Tertiary education is normally taken to include undergraduate and postgraduate education, as well as vocational education and training. Colleges and universities are the main institutions that provide tertiary education. Collectively, these are sometimes known as tertiary institutions.Tertiary education generally results in the receipt of certificates, diplomas, or academic degrees. 

Higher education includes teaching, research and social services activities of universities, and within the realm of teaching, it includes both the undergraduate level (sometimes referred to as tertiary education) and the graduate (or postgraduate) level (sometimes referred to as graduate school). Higher education in that country generally involves work towards a degree-level or foundation degree qualification. In most developed countries a high proportion of the population (up to 50%) now enter higher education at some time in their lives. Higher education is therefore very important to national economies, both as a significant industry in its own right, and as a source of trained and educated personnel for the rest of the economy.

Lifelong, or adult, education has become widespread in many countries. However, education is still seen by many as something aimed at children, and adult education is often branded as adult learning or lifelong learning. Adult education takes on many forms, from formal class-based learning to self-directed learning.

Lending libraries provide inexpensive informal access to books and other self-instructional materials. The rise in computer ownership and internet access has given both adults and children greater access to both formal and informal education.In Scandinavia a unique approach to learning termed folkbildning has long been recognised as contributing to adult education through the use of learning circles.Mode of Education. 1-formal education, 2-informal education , 3-Non formal education. 

Formal Education:- the hierarchically structured, chronologically graded education system, running from primary school through the university and including, in addition to general academic studies, a variety of specialized programs and institutions for full time technical and professional training.

Informal Education:- The truly lifelong process whereby every individual acquires attitude, values, skills and knowledge from daily experience and the educative influences and resources in his or her environment from family and neighbors, from work and play, from the market place the library and the mass media.

Non-Formal Education

any organized educational activity outside the established formal system- whether operating separately or as an important feature of some broader activity that is intended to serve identifiable learning clienteles and learning objectives.

Alternative education, also known as non-traditional education or educational alternative, is a broad term which may be used to refer to all forms of education outside of traditional education (for all age groups and levels of education). This may include both forms of education designed for students with special needs (ranging from teenage pregnancy to intellectual disability) and forms of education designed for a general audience which employ alternative educational philosophies and/or methods. 

Alternatives of the latter type are often the result of education reform and are rooted in various philosophies that are commonly fundamentally different from those of traditional compulsory education. While some have strong political, scholarly, or philosophical orientations, others are more informal associations of teachers and students dissatisfied with certain aspects of traditional education. These alternatives, which include charter schools, alternative schools, independent schools, and home-based learning vary widely, but often emphasize the value of small class size, close relationships between students and teachers, and a sense of community.

In certain places, especially in the United States, the term alternative may largely refer to forms of education catering to "at risk" students, as it is, for example, in this definition drafted by the Massachusetts Department of Education. 

An academic discipline is a branch of knowledge which is formally taught, either at the university, or via some other such method. Functionally, disciplines are usually defined and recognized by the academic journals in which research is published, and by the learned societies to which their practitioners belong. Professors say schooling is 80% psychological, 20% physical effort.

Each discipline usually has several sub-disciplines or branches, and distinguishing lines are often both arbitrary and ambiguous. Examples of broad areas of academic disciplines include the natural sciences, mathematics, computer science, social sciences, humanities and applied sciences.

There has been a great deal of work on learning styles over the last two decades. Dunn and Dunn focused on identifying relevant stimuli that may influence learning and manipulating the school environment, at about the same time as Joseph Renzulli recommended varying teaching strategies. Howard Gardner identified individual talents or aptitudes in his Multiple Intelligences theories. Based on the works of Jung, the Myers-Briggs Type Indicator and Keirsey Temperament Sorter focused on understanding how people's personality affects the way they interact personally, and how this affects the way individuals respond to each other within the learning environment. The work of David Kolb and Anthony Gregorc's Type Delineator follows a similar but more simplified approach.

It is currently fashionable to divide education into different learning "modes". The learning modalities are probably the most common:

It is claimed that, depending on their preferred learning modality, different teaching techniques have different levels of effectiveness. A consequence of this theory is that effective teaching should present a variety of teaching methods which cover all three learning modalities so that different students have equal opportunities to learn in a way that is effective for them.

Teachers need the ability to understand a subject well enough to convey its essence to a new generation of students. The goal is to establish a sound knowledge base on which students will be able to build as they are exposed to different life experiences. The passing of knowledge from generation to generation allows students to grow into useful members of society. Good teachers can translate information, good judgment, experience and wisdom into relevant knowledge that a student can understand and retain. As a profession, teaching has very high levels of Work-Related Stress (WRS) which are listed as amongst the highest of any profession in some countries, such as the United Kingdom. The degree of this problem is becoming increasingly recognized and support systems are put into place.



Technology is an increasingly influential factor in education. Computers and mobile phones are being widely used in developed countries both to complement established education practices and develop new ways of learning such as online education (a type of distance education). This gives students the opportunity to choose what they are interested in learning. The proliferation of computers also means the increase of programming and blogging. Technology offers powerful learning tools that demand new skills and understandings of students, including Multimedia, and provides new ways to engage students, such as Virtual learning environments.Technology is being used more not only in administrative duties in education but also in the instruction of students. The use of technologies such as PowerPoint and interactive whiteboard is capturing the attention of students in the classroom. Technology is also being used in the assessment of students. One example is the Audience Response System (ARS), which allows immediate feedback tests and classroom discussions.

Information and communication technologies (ICTs) are a â€œdiverse set of tools and resources used to communicate, create, disseminate, store, and manage information.â€?These technologies include computers, the Internet, broadcasting technologies (radio and television), and telephony. There is increasing interest in how computers and the Internet can improve education at all levels, in both formal and non-formal settings. Older ICT technologies, such as radio and television, have for over forty years been used for open and distance learning, although print remains the cheapest, most accessible and therefore most dominant delivery mechanism in both developed and developing countries. 

The use of computers and the Internet is still in its infancy in developing countries, if these are used at all, due to limited infrastructure and the attendant high costs of access. Usually, various technologies are used in combination rather than as the sole delivery mechanism. For example, the Kothmale Community Radio Internet uses both radio broadcasts and computer and Internet technologies to facilitate the sharing of information and provide educational opportunities in a rural community in Sri Lanka. The Open University of the United Kingdom (UKOU), established in 1969 as the first educational institution in the world wholly dedicated to open and distance learning, still relies heavily on print-based materials supplemented by radio, television and, in recent years, online programming. Similarly, the Indira Gandhi National Open University in India combines the use of print, recorded audio and video, broadcast radio and television, and audio conferencing technologies. 

The term "computer-assisted learning" (CAL) has been increasingly used to describe the use of technology in teaching.

The history of education according to Dieter Lenzen, president of the Freie UniversitÃ¤t Berlin 1994 "began either millions of years ago or at the end of 1770". Education as a science cannot be separated from the educational traditions that existed before. Education was the natural response of early civilizations to the struggle of surviving and thriving as a culture. Adults trained the young of their society in the knowledge and skills they would need to master and eventually pass on. The evolution of culture, and human beings as a species depended on this practice of transmitting knowledge. In pre-literate societies this was achieved orally and through imitation. Story-telling continued from one generation to the next. Oral language developed into written symbols and letters. The depth and breadth of knowledge that could be preserved and passed soon increased exponentially. When cultures began to extend their knowledge beyond the basic skills of communicating, trading, gathering food, religious practices, etc, formal education, and schooling, eventually followed. Schooling in this sense was already in place in Egypt between 3000 and 500BC.

The philosophy of education is the study of the purpose, nature and ideal content of education. Related topics include knowledge itself, the nature of the knowing mind and the human subject, problems of authority, and the relationship between education and society. At least since Locke's time, the philosophy of education has been linked to theories of developmental psychology and human development.

Fundamental purposes that have been proposed for education include:

A central tenet of education typically includes â€œthe imparting of knowledge.â€?At a very basic level, this purpose ultimately deals with the nature, origin and scope of knowledge. The branch of philosophy that addresses these and related issues is known as epistemology. This area of study often focuses on analyzing the nature and variety of knowledge and how it relates to similar notions such as truth and belief.

While the term, knowledge, is often used to convey this general purpose of education, it can also be viewed as part of a continuum of knowing that ranges from very specific data to the highest levels. Seen in this light, the continuum may be thought to consist of a general hierarchy of overlapping levels of knowing. Students must be able to connect new information to a piece of old information to be better able to learn, understand, and retain information. This continuum may include notions such as data, information, knowledge, wisdom, and realization.

Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Although the terms "educational psychology" and "school psychology" are often used interchangeably, researchers and theorists are likely to be identified as , whereas practitioners in schools or school-related settings are identified as school psychologists. Educational psychology is concerned with the processes of educational attainment in the general population and in sub-populations such as gifted children and those with specific disabilities.

Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. Educational psychology in turn informs a wide range of specialities within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education and classroom management. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks (Lucas, Blazek, & Raley, 2006).

High rates of education are essential for countries to achieve high levels of economic growth.  In theory poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested by rich countries. But economists argue that if the gap in education between a rich and a poor nation is too large, as is the case between the poorest and the richest nations in the world, the transfer of these technologies that drive economic growth becomes difficult, thus the economies of the world's poorest nations stagnate.

The sociology of education is the study of how social institutions and forces affect educational processes and outcomes, and vice versa. By many, education is understood to be a means of overcoming handicaps, achieving greater equality and acquiring wealth and status for all (Sargent 1994). Learners may be motivated by aspirations for progress and betterment. Education is perceived as a place where children can develop according to their unique needs and potentialities. The purpose of education can be to develop every individual to their full potential. The understanding of the goals and means of educational socialization processes differs according to the sociological paradigm used.

In developing countries, the number and seriousness of the problems faced are naturally greater. People are sometimes unaware of the importance of education, and there is economic pressure from those parents who prioritize their children's making money in the short term over any long-term benefits of education. Recent studies on child labor and poverty have suggested that when poor families reach a certain economic threshold where families are able to provide for their basic needs, parents return their children to school. This has been found to be true, once the threshold has been breached, even if the potential economic value of the children's work has increased since their return to school. Teachers are often paid less than other similar professions.

A lack of good universities, and a low acceptance rate for good universities, is evident in countries with a relatively high population density. In some countries, there are uniform, over structured, inflexible centralized programs from a central agency that regulates all aspects of education.

India is now developing technologies that will skip land based phone and internet lines. Instead, India launched EDUSAT, an education satellite that can reach more of the country at a greatly reduced cost. There is also an initiative started by a group out of MIT and supported by several major corporations to develop a $100 laptop. The laptops should be available by late 2006 or 2007. The laptops, sold at cost, will enable developing countries to give their children a digital education, and to close the digital divide across the world.

In Africa, NEPAD has launched an "e-school programme" to provide all 600,000 primary and high schools with computer equipment, learning materials and internet access within 10 years. Private groups, like The Church of Jesus Christ of Latter-day Saints, are working to give more individuals opportunities to receive education in developing countries through such programs as the Perpetual Education Fund. An International Development Agency project called nabuur.com, started with the support of American President Bill Clinton, uses the Internet to allow co-operation by individuals on issues of social development.

Education is becoming increasingly international. Not only are the materials becoming more influenced by the rich international environment, but exchanges among students at all levels are also playing an increasingly important role. In Europe, for example, the Socrates-Erasmus Programme stimulates exchanges across European universities. Also, the Soros Foundation provides many opportunities for students from central Asia and eastern Europe. Some scholars argue that, regardless of whether one system is considered better or worse than another, experiencing a different way of education can often be considered to be the most important, enriching element of an international learning experience.



















An autostereogram is a single-image stereogram (SIS), designed to trick the human brain into perceiving a three-dimensional (3D) scene in a two-dimensional image. In order to perceive 3D shapes in these autostereograms, the brain must overcome the normally automatic coordination between focusing and convergence.

The simplest type of autostereogram consists of horizontally repeating patterns and is known as a wallpaper autostereogram. When viewed with proper convergence, the repeating patterns appear to float in the air above the background. The Magic Eye series of books features another type of autostereogram called a random dot autostereogram. In this type of autostereogram, every pixel in the image is computed from a pattern strip and a depth map. Usually, a hidden 3D scene emerges when the image is viewed with proper viewing technique.

There are two ways an autostereogram can be viewed: wall-eyed and cross-eyed. Most autostereograms are designed to be viewed in only one way, which is usually wall-eyed. Wall-eyed viewing requires that the two eyes adopt a relatively parallel angle, while cross-eyed viewing requires a relatively convergent angle.

In 1838, the British scientist Charles Wheatstone published an explanation of binocular vision (binocular depth perception) which had led him to make stereoscopic drawings and to construct a stereoscope based on a combination of mirrors to allow a person to see 3D images from two 2D pictures (stereograms).

Between 1849 and 1850, David Brewster, a Scottish scientist, improved the Wheatstone stereoscope by using lenses instead of mirrors, thus reducing the size of the contraption. Brewster noticed that staring at repeated patterns in wallpapers could trick the brain into matching pairs of them and thus causing the brain to perceive a virtual plane behind the walls. This is the basis of wallpaper-style autostereograms (also known as single-image stereograms).

In 1959, Bela Julesz, a vision scientist, psychologist, and MacArthur Fellow, discovered the random dot stereogram while working at Bell Laboratories on recognizing camouflaged objects from aerial pictures taken by spy planes. At the time, many vision scientists still thought that depth perception occurred in the eye itself, whereas now it is known to be a complex neurological process. Julesz used a computer to create a stereo pair of random-dot images which, when viewed under a stereoscope, caused the brain to see 3D shapes. This proved that depth perception is a neurological process.

In 1979, Christopher Tyler of Smith-Kettlewell Institute, a student of Julesz and a visual psychophysicist, combined the theories behind single-image wallpaper stereograms and random-dot stereograms to create the first random-dot autostereogram (also known as single-image random-dot stereogram) which allowed the brain to see 3D shapes from a single 2D image without the aid of optical equipment.



Stereopsis, or stereo vision, is the visual blending of two similar but not identical images into one, with resulting visual perception of solidity and depth. In the human brain, stereopsis results from a complex set of mechanisms that form a three-dimensional impression by matching each point (or set of points) in one eye's view with the equivalent point (or set of points) in the other eye's view. It therefore assesses the points' positions in the otherwise inscrutable z-axis (depth).

When the brain is presented with a repeating pattern like wallpaper, it has difficulty matching the two eyes' views accurately. By looking at a horizontally repeating pattern, but converging the two eyes at a point behind the pattern, it is possible to trick the brain into matching one element of the pattern, as seen by the left eye, with another (similar looking) element, beside the first, as seen by the right eye. This gives the illusion of a plane bearing the same pattern but located behind the real wall. The distance at which this plane lies behind the wall depends only on the spacing between identical elements.

Autostereograms use this dependence of depth on spacing to create three-dimensional images. If, over some area of the picture, the pattern is repeated at smaller distances, that area will appear closer than the background plane. If the distance of repeats is longer over some area, then that area will appear more distant (like a hole in the plane). 



People who have never been able to perceive 3D shapes hidden within an autostereogram find it hard to understand remarks such as, "the 3D image will just pop out of the background, after you stare at the picture long enough", or "the 3D objects will just emerge from the background". It helps to illustrate how 3D images "emerge" from the background from a second viewer's perspective. If the virtual 3D objects reconstructed by the autostereogram viewer's brain were real objects, a second viewer observing the scene from the side would see these objects floating in the air above the background image.

The 3D effects in the example autostereogram are created by repeating the tiger rider icons every 140 pixels on the background plane, the shark rider icons every 130 pixels on the second plane, and the tiger icons every 120 pixels on the highest plane. The closer a set of icons are packed horizontally, the higher they are lifted from the background plane. This repeat distance is referred to as the depth or z-axis value of a particular pattern in the autostereogram. The depth value is also known as Z-buffer value.

The brain is capable of almost instantly matching hundreds of patterns repeated at different intervals in order to recreate correct depth information for each pattern. An autostereogram may contain some 50 tigers of varying size, repeated at different intervals against a complex, repeated background. Yet, despite the apparent chaotic arrangement of patterns, the brain is able to place every tiger icon at its proper depth.

Autostereograms where patterns in a particular row are repeated horizontally with the same spacing can be read either cross-eyed or wall-eyed. In such autostereograms, both types of reading will produce similar depth interpretation, with the exception that the cross-eyed reading reverses the depth (images that once popped out are now pushed in). 

However, icons in a row do not need to be arranged at identical intervals. An autostereogram with varying intervals between icons across a row presents these icons at different depth planes to the viewer. The depth for each icon is computed from the distance between it and its neighbor at the left. These types of autostereograms are designed to be read in only one way, either cross-eyed or wall-eyed. All autostereograms in this article are encoded for wall-eyed viewing, unless specifically marked otherwise. An autostereogram encoded for wall-eyed viewing will produce incoherent 3D patterns when viewed cross-eyed. Most Magic Eye pictures are also designed for wall-eyed viewing.

The following wall-eyed autostereogram encodes 3 planes across the x-axis. The background plane is on the left side of the picture. The highest plane is shown on the right side of the picture. There is a narrow middle plane in the middle of the x-axis. Starting with a background plane where icons are spaced at 140 pixels, one can raise a particular icon by shifting it a certain number of pixels to the left. For instance, the middle plane is created by shifting an icon 10 pixels to the left, effectively creating a spacing consisting of 130 pixels. The brain does not rely on intelligible icons which represent objects or concepts. In this autostereogram, patterns become smaller and smaller down the y-axis, until they look like random dots. The brain is still able to match these random dot patterns.

The distance relationship between any pixel and its counterpart in the equivalent pattern to the left can be expressed in a depth map. A depth map is simply a grayscale image which represents the distance between a pixel and its left counterpart using a grayscale value between black and white. By convention, the closer the distance is, the brighter the color becomes. 

Using this convention, a grayscale depth map for the above autostereogram can be created with black, gray and white representing shifts of 0 pixels, 10 pixels and 20 pixels, respectively. A depth map is the key to creation of random-dot autostereograms.

A software program can take a depth map and an accompanying pattern image to produce an autostereogram. The program tiles the pattern image horizontally to cover an area whose size is identical to the depth map. Conceptually, at every pixel in the output image, the program looks up the grayscale value of the equivalent pixel in the depth map image, and uses this value to determine the amount of horizontal shift required for the pixel.

One way to accomplish this is to make the program scan every line in the output image pixel-by-pixel from left to right. It seeds the first series of pixels in a row from the pattern image. Then it consults the depth map to retrieve appropriate shift values for subsequent pixels. For every pixel, it subtracts the shift from the width of the pattern image to arrive at a repeat interval. It uses this repeat interval to look up the color of the counterpart pixel to the left and uses its color as the new pixel's own color.

Unlike the simple depth planes created by simple wallpaper autostereograms, subtle changes in spacing specified by the depth map can create the illusion of smooth gradients in distance. This is possible because the grayscale depth map allows individual pixels to be placed on one of 2n depth planes, where n is the number of bits used by each pixel in the depth map. In practice, the total number of depth planes is determined by the number of pixels used for the width of the pattern image. Each grayscale value must be translated into pixel space in order to shift pixels in the final autostereogram. As a result, the number of depth planes must be smaller than the pattern width.



The fine-tuned gradient requires a pattern image more complex than standard repeating-pattern wallpaper, so typically a pattern consisting of repeated random dots is used. When the autostereogram is viewed with proper viewing technique, a hidden 3D scene emerges. Autostereograms of this form are known as Random Dot Autostereograms.

Smooth gradients can also be achieved with an intelligible pattern, assuming that the pattern is complex enough and does not have big, horizontal, monotonic patches. A big area painted with monotonic color without change in hue and brightness does not lend itself to pixel shifting, as the result of the horizontal shift is identical to the original patch. The following depth map of a shark with smooth gradient produces a perfectly readable autostereogram, even though the 2D image contains small monotonic areas; the brain is able to recognize these small gaps and fill in the blanks. While intelligible, repeated patterns are used instead of random dots, this type of autostereogram is still known by many as a Random Dot Autostereogram, because it is created using the same process.





When a series of autostereograms are shown one after another, in the same way moving pictures are shown, the brain perceives an animated autostereogram. If all autostereograms in the animation are produced using the same background pattern, it is often possible to see faint outlines of parts of the moving 3D object in the 2D autostereogram image without wall-eyed viewing; the constantly shifting pixels of the moving object can be clearly distinguished from the static background plane. To eliminate this side effect, animated autostereograms often use shifting background in order to disguise the moving parts.

When a regular repeating pattern is viewed on a CRT monitor as if it were a wallpaper autostereogram, it is usually possible to see depth ripples. This can also be seen in the background to a static, random-dot autostereogram. These are caused by the sideways shifts in the image due to small changes in the deflection sensitivity (linearity) of the line scan, which then become interpreted as depth. This effect is especially apparent at the left hand edge of the screen where the scan speed is still settling after the flyback phase. This effect is absent from a TFT LCD.



Much advice exists about seeing the intended three-dimensional image in an autostereogram. While some people can simply see the 3D image in an autostereogram, others must learn to train their eyes to decouple eye convergence from lens focusing. 

Not every person can see the 3D illusion in autostereograms. Because autostereograms are constructed based on stereo vision, persons with a variety of visual impairments, even those affecting only one eye, are unable to see the three-dimensional images.

People with amblyopia (also known as lazy eye) are unable to see the three-dimensional images. Children with poor or dysfunctional eyesight during a critical period in childhood may grow up stereoblind, as their brains are not stimulated by stereo images during the critical period. If such vision problem is not corrected in the early childhood, the damage becomes permanent and the adult will never be able to see autostereograms. It is estimated that some 1% to 5% of the population is affected by amblyopia.

Depth perception results from many monocular and binocular visual clues. For objects relatively close to the eyes, binocular vision plays an important role in depth perception. Binocular vision allows the brain to create a single Cyclopean image and to attach a depth level to each point in the Cyclopean image.

The brain uses coordinate shift (also known as parallax) of matched objects to identify depth of these objects. The depth level of each point in the combined image can be represented by a grayscale pixel on a 2D image, for the benefit of the reader. The closer a point appears to the brain, the brighter it is painted. Thus, the way the brain perceives depth using binocular vision can be captured by a depth map (Cyclopean image) painted based on coordinate shift.





The eye operates like a photographic camera. It has an adjustable iris which can open (or close) to allow more (or less) light to enter the eye. As with any camera except pinhole cameras, it needs to focus light rays entering through the iris (aperture in a camera) so that they focus on a single point on the retina in order to produce a sharp image. The eye achieves this goal by adjusting a lens behind the cornea to refract light appropriately. 

When a person stares at an object, the two eyeballs rotate sideways to point to the object, so that the object appears at the center of the image formed on each eye's retina. In order to look at a nearby object, the two eyeballs rotate towards each other so that their eyesight can converge on the object. This is referred to as cross-eyed viewing. To see a faraway object, the two eyeballs diverge to become almost parallel to each other. This is known as wall-eyed viewing, where the convergence angle is much smaller than that in a cross-eyed viewing.

Stereo-vision based on parallax allows the brain to calculate depths of objects relative to the point of convergence. It is the convergence angle that gives the brain the absolute reference depth value for the point of convergence from which absolute depths of all other objects can be inferred.



The eyes normally focus and converge at the same distance in a process known as accommodative convergence. That is, when looking at a faraway object, the brain automatically flattens the lenses and rotates the two eyeballs for wall-eyed viewing. It is possible to train the brain to decouple these two operations. This decoupling has no useful purpose in everyday life, because it prevents the brain from interpreting objects in a coherent manner. To see a man-made picture such as an autostereogram where patterns are repeated horizontally, however, decoupling of focusing from convergence is crucial.

By focusing the lenses on a nearby autostereogram where patterns are repeated and by converging the eyeballs at a distant point behind the autostereogram image, one can trick the brain into seeing 3D images. If the patterns received by the two eyes are similar enough, the brain will consider these two patterns a match and treat them as coming from the same imaginary object. This type of visualization is known as wall-eyed viewing, because the eyeballs adopt a wall-eyed convergence on a distant plane, even though the autostereogram image is actually closer to the eyes. Because the two eyeballs converge on a plane farther away, the perceived location of the imaginary object is behind the autostereogram. The imaginary object also appears bigger than the patterns on the autostereogram because of foreshortening.

The following autostereogram shows 3 rows of repeated patterns. Each pattern is repeated at a different interval to place it on a different depth plane. The two non-repeating lines can be used to verify correct wall-eyed viewing. When the autostereogram is correctly interpreted by the brain using wall-eyed viewing, and one stares at the dolphin in the middle of the visual field, the brain should see two sets of flickering lines, as a result of binocular rivalry.



While there are 6 dolphin patterns in the autostereogram, the brain should see 7 "apparent" dolphins on the plane of the autostereogram. This is a side effect of the pairing of similar patterns by the brain. There are 5 pairs of dolphin patterns in this image. This allows the brain to create 5 apparent dolphins. The leftmost pattern and the rightmost pattern by themselves have no partner, but the brain tries to assimilate these two patterns onto the established depth plane of adjacent dolphins despite binocular rivalry. As a result, there are 7 apparent dolphins, with the leftmost and the rightmost ones appearing with a slight flicker, not dissimilar to the two sets of flickering lines observed when one stares at the 4th apparent dolphin.

Because of foreshortening, the difference in convergence needed to see repeated patterns on different planes causes the brain to attribute different sizes to patterns with identical 2D sizes. In the autostereogram of 3 rows of cubes, while all cubes have the same physical 2D dimensions, the ones on the top row appear bigger, because they are perceived as farther away than the cubes on the second and third rows.

As with a photographic camera, it is easier to make the eye focus on an object when there is intense ambient light. With intense lighting, the eye can constrict the iris, yet allow enough light to reach the retina. The more the eye resembles a pinhole camera, the less it depends on focusing through the lens. In other words, the degree of decoupling between focusing and convergence needed to visualize an autostereogram is reduced. This places less strain on the brain. Therefore, it may be easier for first-time autostereogram viewers to "see" their first 3D images if they attempt this feat with bright lighting.

Vergence control is important in being able to see 3D images. Thus it may help to concentrate on converging/diverging the two eyes to shift images that reach the two eyes, instead of trying to see a clear, focused image. Although the lens adjusts reflexively in order to produce clear, focused images, voluntary control over this process is possible. The viewer alternates instead between converging and diverging the two eyes, in the process seeing "double images" typically seen when one is drunk or otherwise intoxicated. Eventually the brain will successfully match a pair of patterns reported by the two eyes and lock onto this particular degree of convergence. The brain will also adjust eye lenses to get a clear image of the matched pair. Once this is done, the images around the matched patterns quickly become clear as the brain matches additional patterns using roughly the same degree of convergence.





When one moves one's attention from a depth plane to another (for instance, from the top row of the chessboard to the bottom row), the two eyes need to adjust their convergence to match the new repeating interval of patterns. If the level of change in convergence is too high during this shift, sometimes the brain can lose the hard-earned decoupling between focusing and convergence. For a first-time viewer, therefore, it may be easier to see the autostereogram, if the two eyes rehearse the convergence exercise on an autostereogram where the depth of patterns across a particular row remains constant. 

In a random dot autostereogram, the 3D image is usually shown in the middle of the autostereogram against a background depth plane (see the shark autostereogram). It may help to establish proper convergence first by staring at either the top or the bottom of the autostereogram, where patterns are usually repeated at a constant interval. Once the brain locks onto the background depth plane, it has a reference convergence degree from which it can then match patterns at different depth levels in the middle of the image.

The majority of autostereograms, including those in this article, are designed for divergent (wall-eyed) viewing. One way to help the brain concentrate on divergence instead of focusing is to hold the picture in front of the face, with the nose touching the picture. With the picture so close to their eyes, most people cannot focus on the picture. The brain may give up trying to move eye muscles in order to get a clear picture. If one slowly pulls back the picture away from the face, while refraining from focusing or rotating eyes, at some point the brain will lock onto a pair of patterns when the distance between them match the current convergence degree of the two eyeballs.

Another way is to stare at an object behind the picture in an attempt to establish proper divergence, while keeping part of the eyesight fixed on the picture to convince the brain to focus on the picture. A modified method has the viewer stare at her reflection on the shiny surface of the picture, which the brain perceives as being located twice as far away as the picture itself. This may help persuade the brain to adopt the required divergence while focusing on the nearby picture.

For crossed-eyed autostereograms, a different approach needs to be taken. The viewer may hold one finger between his eyes and move it slowly towards the picture, maintaining his focus on the finger at all times, until he is correctly focused on the spot between him and the picture that will allow him to view the illusion.









Sir Winston Leonard Spencer Churchill, KG, OM, CH, TD, FRS, PC (Can) (30 November 1874 â€?24 January 1965) was a British politician known chiefly for his leadership of Great Britain during World War II. He served as Prime Minister of the United Kingdom from 1940 to 1945 and again from 1951 to 1955. A noted statesman and orator, Churchill was also known as an officer in the British Army, a historical writer, and an artist. 

During his army career Churchill saw combat on the Northwest Frontier, in the Sudan and during the Second Boer War, during which he also gained fame and notoriety, as a war correspondent. He also served in the British Army on the Western Front and commanded the 6th Battalion of the Royal Scots Fusiliers. At the forefront of the political scene for almost sixty years, he held many political and cabinet positions. Before the First World War, he served as President of the Board of Trade and Home Secretary during the Liberal governments. In the First World War he served as First Lord of the Admiralty, Minister of Munitions, Secretary of State for War and Secretary of State for Air and during the interwar years, he served as Chancellor of the Exchequer. 

After the outbreak of the Second World War, Churchill was appointed First Lord of the Admiralty. Following the resignation of Neville Chamberlain on 10 May 1940, he became Prime Minister of the United Kingdom and led Britain to victory against the Axis powers. His speeches were a great inspiration to the embattled Allied forces. After losing the 1945 election, he became the leader of the opposition. In 1951, he again became Prime Minister before finally retiring in 1955. Upon his death the Queen granted him the honour of a state funeral, which saw one of the largest assemblies of statesmen in the world.

 A descendant of the famous Spencer family, Winston Leonard Spencer Churchill, like his father, used the surname Churchill in public life. His ancestor George Spencer had changed his surname to Spencer-Churchill in 1817 when he became Duke of Marlborough, to highlight his descent from John Churchill, 1st Duke of Marlborough. Winston's father, Lord Randolph Churchill, the third son of John Spencer-Churchill, 7th Duke of Marlborough, was a politician, while his mother, Lady Randolph Churchill (nÃ©e Jennie Jerome) was the daughter of American millionaire Leonard Jerome. Churchill was born two months premature in a bedroom in Blenheim Palace in Woodstock, Oxfordshire on 30 November 1874. He arrived eight months after his parents' hasty marriage, and had one brother, John Strange Spencer-Churchill.

Churchill had an independent and rebellious nature and generally did poorly in school, for which he was punished. He entered Harrow School on 17 April 1888, where his military career began. Within weeks of his arrival, he had joined the Harrow Rifle Corps. He earned high marks in English and history; he was also the school's fencing champion. He was rarely visited by his mother (then known as Lady Randolph), but wrote letters begging her to either come to the school or to allow him to come home. He also had a very distant relationship with his father and once remarked that they barely spoke to each other. Due to his lack of parental contact he became very close to his nanny, Elizabeth Anne Everest, whom he used to call "Woomany".

Churchill described himself as having a "speech impediment", which he consistently worked to overcome. After many years, he finally stated, "My impediment is no hindrance." Although the Stuttering Foundation of America has claimed that he stammered, the Churchill Centre has concluded that he lisped. His impediment may also have been cluttering, which would fit more with his lack of attention to unimportant details and his very secure ego. Weiss suggests that he may have "excelled because of, rather than in spite of, his cluttering."

Churchill met his future wife, Clementine Hozier, in a ball at the Crewe House, home of the Earl of Crewe and his wife, Margaret Primrose (daughter of Archibald Primrose, 5th Earl of Rosebery), in 1904. In 1908 they met again at a dinner party hosted by Lady St Helier. Churchill found himself seated alongside Hozier at the dinner party, and they soon began their lifelong romance. On August 10 1908, he proposed to Hozier in a house party in Blenheim, in a small Temple of Diana.  On September 12, 1908, they were married in Church of St. Margaret, Westminster. The church was packed; the Bishop of St Asaph conducted the service. In March 1909, the couple moved to a house in 33 Eccleston Square. On July 11 1909, in London, their first child, Diana, was born. After the pregnancy, Clementine moved to Sussex to recover, while Diana stayed in London with her nanny. On May 28 1911, their second child, Randolph, was born in 33 Eccleston Square. After the start of World War I, on October 7 1914, their third child, Sarah, was born in the Admiralty House. The birth was marked with anxiety for Clementine, as Winston had been sent to Antwerp by the Cabinet to "stiffen the resistance of the beleaguered city" prior to the birth. Four days after the official end of World War I, Clementine gave birth to her fourth child, Marigold Frances Churchill, on November 15, 1918. In the early months of August, the Churchills' children were entrusted to a French nursery governess in Kent named Mlle Rose. Clementine, meanwhile, travelled to Eaton Hall to play tennis with Hugh Grosvenor, 2nd Duke of Westminster and his family. While still under the care of Mlle Rose, Marigold had a cold, but was reported to have recovered from the illness. As the illness progressed with hardly any notice, it turned into septicaemia. Following advice from a landlady, Rose sent for Clementine. However the illness turned fatal on August 23 1921, and Marigold was buried in the Kensal Green Cemetery three days later. On September 15 1922, the Churchills' last child, Mary, was born. Later that month, the Churchills bought Chartwell, a house that would be Winston's home until his death in 1965.

After Churchill left Harrow in 1893, he applied to attend the Royal Military Academy, Sandhurst. However it took three attempts before he passed the admittance exam. Once there, he graduated eighth out of a class of 150 in December 1894. He was then commissioned as a Second Lieutenant in the 4th Queen's Own Hussars on 20 February 1895. In 1941, he received the honour of Colonel of the Hussars. He was accused of buggering other students while at Sandhurst, and filed a libel case against the accuser; the accuser withdrew the charges and settled with Churchill for a sum of Â£400.

Churchill's pay as a second lieutenant in the 4th Hussars was Â£300. However he believed that he needed at least Â£500 to support a style of life in keeping with other officers of the regiment. According to biographer Roy Jenkins, this is why he took an interest in war correspondence. When he finished training he asked to be posted to areas of action in which, against all etiquette, he earned additional income as a roving war correspondent for the London newspapers.

Lord Deedes explained to a gathering of the Royal Historical Society in 2001 why Churchill went to the front line: "He was with Grenadier Guards, who were dry [without alcohol] at battalion headquarters. They very much liked tea and condensed milk, which had no great appeal to Winston, but alcohol was permitted in the front line, in the trenches. So he suggested to the colonel that he really ought to see more of the war and get into the front line. This was highly commended by the colonel, who thought it was a very good thing to do."

In 1895, Churchill travelled to Cuba to observe the Spanish fight the Cuban guerrillas; he had obtained a commission to write about the conflict from the Daily Graphic. To his delight, he came under fire for the first time on his twenty-first birthday. He had fond memories of Cuba as a "...large, rich, beautiful island..." He soon received word that his nanny, Mrs Everest, was dying; he then returned to England and stayed with her for a week until she died. He wrote in his journal "She was my favourite friend." In My Early Life he wrote: "She had been my dearest and most intimate friend during the whole of the twenty years I had lived." In early October 1896, he was transferred to Bombay, India. He was considered one of the best polo players in his regiment and led his team to many prestigious tournament victories.

About this time Churchill read William Winwood Reade's Martyrdom of Man, a classic of Victorian atheism, which completed his loss of faith in Christianity and left him with a sombre vision of a godless universe in which humanity was destined, nevertheless, to progress through the conflict between the more advanced and the more backward races. When he was posted to India, and began to read avidly to make up for lost time, he was profoundly impressed by Darwinism. He lost whatever religious faith he may have had through reading Edward Gibbon, he said and took a particular dislike, for some reason, to the Catholic Church, as well as Christian missions. He became, in his own words, "a materialist to the tips of my fingers," and he fervently upheld the worldview that human life is a struggle for existence, with the outcome the survival of the fittest. This philosophy of life and history he expressed in his one novel, Savrola. 

In 1897, Churchill attempted to travel to both report and, if necessary, fight in the Greco-Turkish War, but this conflict effectively ended before he could arrive. Later, while preparing for a leave in England, he heard that three brigades of the British Army were going to fight against a Pashtun tribe and he asked his superior officer if he could join the fight. He fought under the command of General Jeffery, who was the commander of the second brigade operating in Malakand, in what is now Pakistan. Jeffery sent him with fifteen scouts to explore the Mamund Valley; while on reconnaissance, they encountered an enemy tribe, dismounted from their horses and opened fire. After an hour of shooting, their reinforcements, the 35th Sikhs arrived, and the fire gradually ceased and the brigade and the Sikhs marched on. Hundreds of tribesmen then ambushed them and opened fire, forcing them to retreat. As they were retreating four men were carrying an injured officer but the fierceness of the fight forced them to leave him behind. The man who was left behind was slashed to death before Churchillâ€™s eyes; afterwards he wrote of the killer, "I forgot everything else at this moment except a desire to kill this man". However the Sikhs' numbers were being depleted so the next commanding officer told Churchill to get the rest of the men and boys to safety. 

Before he left he asked for a note so he would not be charged with desertion. He received the note, quickly signed, and headed up the hill and alerted the other brigade, whereupon they then engaged the army. The fighting in the region dragged on for another two weeks before the dead could be recovered. He wrote in his journal: "Whether it was worth it I cannot tell." An account of the Siege of Malakand was published in December 1900 as The Story of the Malakand Field Force. He received Â£600 for his account. During the campaign, he also wrote articles for the newspapers The Pioneer and The Daily Telegraph. His account of the battle was one of his first published stories, for which he received Â£5 per column from The Daily Telegraph.

Churchill was transferred to Egypt in 1898 where he visited Luxor before joining an attachment of the 21st Lancers serving in the Sudan under the command of General Herbert Kitchener. During his time he encountered two future military officers, whom he would later work with, during the First World War: Douglas Haig, then a captain and John Jellicoe, then a gunboat lieutenant. While in the Sudan, he participated in what has been described as the last meaningful British cavalry charge at the Battle of Omdurman in September 1898. He also worked as a war correspondent for the Morning Post. By October 1898, he had returned to Britain and begun his two-volume work; The River War, an account of the reconquest of the Sudan published the following year.

Churchill stood for parliament in 1899 as a Conservative candidate in Oldham in a by-election, which he lost, coming third in the contest for two seats.

After Churchill's failure to win the election in Oldham, he went to South Africa in 1899 to report on the Second Boer War. On 12 October 1899, the war between Britain and the Boer Republics broke out in South Africa. He was captured and held in a POW camp in Pretoria. He escaped from the prison camp and travelled almost 300 miles (480 km) to Portuguese LourenÃ§o Marques in Delagoa Bay, with the assistance of an English mine manager. His escape made him a minor national hero for a time in Britain, though instead of returning home, he rejoined General Redvers Buller's army on its march to relieve the British at the Siege of Ladysmith and take Pretoria. This time, although continuing as a war correspondent, he gained a commission in the South African Light Horse Regiment. He was among the first British troops into Ladysmith and Pretoria. In fact, he and the Duke of Marlborough, his cousin, were able to get ahead of the rest of the troops in Pretoria, where they demanded and received the surrender of 52 Boer prison camp guards.

In 1900, Churchill returned to England on the RMS ''Dunottar Castle'', the same ship on which he set sail for South Africa eight months earlier, and published books on the Second Boer War including London to Ladysmith via Pretoria and Ian Hamilton's March, which he then marked by a small tour of the United States



 A year later he won the seat following the 1900 general election. Churchill again stood for Oldham in the 1900 general election. He won, but before taking his seat in the Houses of Parliament, he embarked on a speaking tour throughout Britain and the United States, raising Â£10,000 for himself. In Parliament, he became associated with a faction of the Conservative Party led by Lord Hugh Cecil, the Hughligans. During his first parliamentary session, he opposed the government's military expenditure and Joseph Chamberlain, who proposed extensive tariffs intended to protect Britain's economic dominance. His own constituency effectively deselected him, although he continued to sit for Oldham until the next general election. After the Whitsun recess in 1904 he crossed the floor to sit as a member of the Liberal Party. As a Liberal, he continued to campaign for free trade. When the Liberals took office, with Henry Campbell-Bannerman as Prime Minister, in December 1905, Churchill became Under-Secretary of State for the Colonies dealing mainly with South Africa after the Boer War. 

From 1903 until 1905, Churchill was also engaged in writing Lord Randolph Churchill, a two-volume biography of his father which was published in 1906 and received much critical acclaim. However, filial devotion caused him to soften some of his father's less attractive aspects. He won the seat of Manchester North West in the 1906 general election. When Campbell-Bannerman was succeeded by Herbert Henry Asquith in 1908, Churchill was promoted to the Cabinet as President of the Board of Trade. Under the law at the time, a newly appointed Cabinet Minister was obliged to seek re-election at a by-election; Churchill lost his seat but was soon back as the member for Dundee constituency. As President of the Board of Trade he joined newly appointed Chancellor Lloyd George in opposing First Lord of the Admiralty, Reginald McKenna's proposed huge expenditure for the construction of Navy dreadnought warships, and in supporting the Liberal reforms.. In 1908 he introduced the Trade Boards Bill setting up the first minimum wages in Britain,  In 1909 he set up Labour Exchanges to help unemployed people find work.  He helped draft the first unemployment pension legislation, the National Insurance Act of 1911.

Churchill also assisted in passing the People's Budget.becoming the President of the Budget League, an organisation set up in response to the opposition's "Budget Protest League".The budget included the introduction of new taxes on the wealthy to allow for the creation of new social welfare programmes. After the budget bill was sent to the Commons in 1909 and passed, it went to the House of Lords, where it was vetoed. The Liberals then fought and won two general elections in January and December of 1910 to gain a mandate for their reforms. The budget was then passed following the Parliament Act of 1911 for which he also campaigned. In 1910, he was promoted to Home Secretary. His term was controversial, after his responses to the Siege of Sidney Street and the dispute at the Cambrian Colliery and the suffragettes.

In 1910, a number of coal miners in the Rhondda Valley began what has come to be known as the Tonypandy Riot. The Chief Constable of Glamorgan requested troops be sent in to help police quell the rioting. Churchill, learning that the troops were already traveling, allowed them to go as far as Swindon and Cardiff but blocked their deployment. On 9 November, the Times criticized this decision. In spite of this, the rumour persists that Churchill had ordered troops to attack, and his reputation in Wales and in Labour circles never recovered.

In early January 1911 Churchill made a controversial visit to the Siege of Sidney Street in London. There is some uncertainty as to whether he attempted to give operational commands. A biographer, Roy Jenkins, comments that the reason he went was because "he could not resist going to see the fun himself" and that he did not issue commands. His role and presence attracted much criticism. After an inquest, Arthur Balfour remarked, "He [Churchill] and a photographer were both risking valuable lives. I understand what the photographer was doing but what was the Right Honourable gentleman doing?" Churchill's proposed solution to the suffragette issue was a referendum on the issue but this found no favour with Herbert Henry Asquith and women's suffrage remained unresolved until after the First World War.

In 1911, Churchill was transferred to the office of the First Lord of the Admiralty, a post he held into World War I. He gave impetus to several reform efforts, including development of naval aviation (he undertook flying lessons himself),, the construction of new and larger warships, the development of tanks, and the switch from coal to oil in the Royal Navy, .

On 5 October 1914 Churchill went to Antwerp which the Belgian government proposed to evacuate. The Royal Marine Brigade was there and at Churchillâ€™s urgings the 1st and 2nd Naval Brigades were also committed. Antwerp fell on 10th October with the loss of 2500 men. At the time he was attacked for squandering resources. It is more likely that his actions prolonged the resistance by a week (Belgium had proposed surrendering Antwerp on 3rd October) and that this time saved Calais and Dunkirk.

Churchill was involved with the development of the tank, which was financed from naval research funds.. He then headed the Landships Committee which was responsible for creating the first tank corps and, although a decade later development of the battle tank would be seen as a tactical victory, at the time it was seen as misappropriation of funds. In 1915 he was one of the political and military engineers of the disastrous Gallipoli landings on the Dardanelles during World War I. He took much of the blame for the fiasco, and when Prime Minister Asquith formed an all-party coalition government, the Conservatives demanded his demotion as the price for entry. 

For several months Churchill served in the sinecure of Chancellor of the Duchy of Lancaster. However on 15 November 1915 he resigned from the government, feeling his energies were not being used. and, though remaining an MP, served for several months on the Western Front commanding the 6th Battalion of the Royal Scots Fusiliers, under the rank of Colonel.  In March, 1916 Churchill returned to England after he had become restless in France and wished to speak again in the House of Commons. In July 1917, Churchill was appointed Minister of Munitions, and in January 1919, Secretary of State for War and Secretary of State for Air. He was the main architect of the Ten Year Rule, a principle that allows the Treasury to dominate and control strategic, foreign and financial policies under the assumption that "there would be no great European war for the next five or ten years". 

A major preoccupation of his tenure in the War Office was the Allied intervention in the Russian Civil War. Churchill was a staunch advocate of foreign intervention, declaring that Bolshevism must be "strangled in its cradle". He secured, from a divided and loosely organised Cabinet, intensification and prolongation of the British involvement beyond the wishes of any major group in Parliament or the nation â€?and in the face of the bitter hostility of Labour. In 1920, after the last British forces had been withdrawn, Churchill was instrumental in having arms sent to the Poles when they invaded Ukraine. He became Secretary of State for the Colonies in 1921 and was a signatory of the Anglo-Irish Treaty of 1921, which established the Irish Free State. Churchill was involved in the length negotiations of the treaty and to protect British maritime interests, he engineered part of the Irish Free State agreement to include three Treaty Portsâ€”Queenstown (Cobh), Berehaven and Lough Swillyâ€”which could be used as Atlantic bases by the Royal Navy. Under the terms of the Anglo-Irish Trade Agreement the bases were returned to the newly constituted Ã‰ire in 1938.

In September the Conservative Party withdrew from the Coalition government after a meeting of backbenchers dissatisfied with the handling of the Chanak Crisis This precipitated the October 1922 General Election was looming. The Liberal Party continued to be beset by internal division which affected Churchill's ability when campaigning. He came only fourth in the poll for Dundee losing to the prohibitionist Edwin Scrymgeour. Churchill later quipped that he left Dundee "without an office, without a seat, without a party and without an appendix". Churchill stood for the Liberals again in the 1923 general election, losing in Leicester and then as an independent, first in a by-election in the Westminster Abbey constituency and then successfully in the General Election of 1924, for Epping. The following year, he formally rejoined the Conservative Party, commenting wryly that "Anyone can rat [betray], but it takes a certain ingenuity to re-rat."

Churchill was appointed Chancellor of the Exchequer in 1924 under Stanley Baldwin and oversaw Britain's disastrous return to the Gold Standard, which resulted in deflation, unemployment, and the miners' strike that led to the General Strike of 1926.. His decision, announced in the 1924 Budget, came after long consultation with various economists including John Maynard Keynes, the Permanent Secretary to the Treasury, Sir Otto Niemeyer and the board of the Bank of England. This decision prompted Keynes to write The Economic Consequences of Mr. Churchill, arguing that the return to the gold standard at the pre-war parity in 1925 (Â£1=$4.86) would lead to a world depression. However, the decision was generally popular and seen as 'sound economics' although it was opposed by Lord Beaverbrook and the Federation of British Industries.

Churchill later regarded this as the greatest mistake of his life. However in discussions at the time with former Chancellor McKenna, Churchill acknowledged that the return to the gold standard and the resulting 'dear money' policy was economically bad. In those discussions he maintained the policy as fundamentally political - a return to the pre-war conditions in which he believed. In his speech on the Bill he said "I will tell you what it [the return to the Gold Standard] will shackle us to. It will shackle us to reality."

The return to the pre-war exchange rate and to the Gold Standard depressed industries. The most affected was the coal industry. Already suffering from declining output as shipping switched to oil, as basic British industries like cotton came under more competition in export markets, the return to the pre-war exchange was estimated to add up to 10% in costs to the industry. In July 1925 a Commission of Inquiry reported generally favouring the miners, rather than the mine owners' position. Baldwin, with Churchill's support proposed a subsidy to the industry while a Royal Commission prepared a further report.

That Commission solved nothing and the miners dispute led to the General Strike of 1926, Churchill was reported to have suggested that machine guns be used on the striking miners. Churchill edited the Government's newspaper, the British Gazette, and, during the dispute, he argued that "either the country will break the General Strike, or the General Strike will break the country" and claimed that the fascism of Benito Mussolini had "rendered a service to the whole world," showing, as it had, "a way to combat subversive forces" â€?that is, he considered the regime to be a bulwark against the perceived threat of Communist revolution. At one point, Churchill went as far as to call Mussolini the "Roman geniusâ€?the greatest lawgiver among men."

Later economists, as well as people at the time, also criticised Churchill's budget measures. These were seen as assisting the generally prosperous rentier banking and salaried classes (to which Churchill and his associates generally belonged) at the expense of manufacturers and exporters which were known then to be suffering from imports and from competition in traditional export markets, and as paring the Armed Forces too heavily 

The Conservative government was defeated in the 1929 General Election. Churchill did not seek election to the Conservative Business Committee, the official leadership of the Conservative MPs. Over the next two years, Churchill became estranged from the Conservative leadership over the issues of protective tariffs and Indian Home Rule and by his political views and by his friendships with press barons, financiers and people whose characters were seen as dubious. When Ramsay MacDonald formed the National Government in 1931, Churchill was not invited to join the Cabinet. He was at the low point in his career, in a period known as "the wilderness years". 

He spent much of the next few years concentrating on his writing, including  â€?a biography of his ancestor John Churchill, 1st Duke of Marlborough â€?and A History of the English Speaking Peoples (though the latter was not published until well after World War II). Great Contemporaries and many newspaper articles and collections of speeches He was one of the best paid writers of his time. His political views, set forth in his 1930 Romanes Election and published as Parliamentary Government and the Economic Problem (republished in 1932 in his collection of essays "Thoughts and Adventures") involved abandoning universal suffrage, a return to a property franchise, proportional representation for the major cities and an economic 'sub parliament'.

 During the first half of the 1930s, Churchill was outspoken in his opposition to granting Dominion status to India. He was one of the founders of the India Defence League, a group dedicated to the preservation of British power in India. In speeches and press articles in this period he forecast widespread British unemployment and civil strife in India should independence be granted. The Viceroy Lord Irwin who had been appointed by the prior Conservative Government engaged in the Round Table Conference in early 1931 and then announced the Government's policy that India should be granted Dominion Status. In this the Government was supported by the Liberal Party and, officially at least, by the Conservative Party. Churchill denounced the Round Table Conference. 

At a meeting of the West Essex Conservative Association specially convened so Churchill could explain his position he said, "It is alarming and also nauseating to see Mr Gandhi, a seditious Middle-Temple lawyer, now posing as a fakir of a type well-known in the East, striding half-naked up the steps of the Vice-regal palace...to parley on equal terms with the representative of the King-Emperor." He called the Indian Congress leaders "Brahmins who mouth and patter principles of Western Liberalism."

There were two incidents which damaged Churchill's reputation greatly within the Conservative Party in the period. Both were taken as attacks on the Conservative front bench. The first was his speech on the eve of the St George by-election in April 1931. In a secure Conservative seat, the official Conservative candidate Duff Cooper was opposed by an independent Conservative. The independent was supported by Lord Rothermere, Lord Beaverbrook and their respective newspapers. Although arranged before the by election was set,  Churchill's speech was seen as supporting the independent candidate and as a part of the Press Baron's campaign against Baldwin. Baldwin's position was strengthened when Duff Cooper won and when the civil disobedience campaign in India ceased with the Gandhi-Irwin Pact. The second issue was a claim that Sir Samuel Hoare and Lord Derby had pressured the Manchester Chamber of Commerce to change evidence it had given to the Joint Select Committee considering the Government of India Bill in June 193 and in doing so had breached Parliamentary privilege. He had the matter referred to the House of Commons Privilege Committee which after investigations, in which Churchill gave evidence reported to the House that there had been no breach. The report was debated on 13 June. Churchill was unable to find a single supporter in the House and the debate ended without a division. 

Churchill permanently broke with Stanley Baldwin over Indian independence and never held any office while Baldwin was Prime Minister. Later Churchill was to selectively quote Baldwin to give the impression that Baldwin put party before country by not pursuing a rearmament policy for fear of losing the 1935 election.  This canard had been first put forward in the first edition of Guilty Men but in subsequent editions (including those before Churchill wrote the Gathering Storm) had been corrected. Some historians see his basic attitude to India as being set out in his book My Early Life (1930).. Historians also dispute his motives in maintaining his opposition. Some see him as trying to destabilise the National Government. Some also draw a parallel between Churchill's attitudes to India and those towards the Nazis. 

Beginning in 1932 when he opposed those who advocated giving Germany the right to military parity with France, Churchill spoke often of the dangers of Germany's rearmament. He later, particularly in The Gathering Storm, tried to portray himself as being for a time, a lone voice calling on Britain to strengthen itself to counter the belligerence of Germany. However Lord Lloyd was the first to so agitate. Churchill's attitude toward the fascist dictators was ambiguous. In 1931 he warned against the League of Nations opposing the Japanese in Manchuria "I hope we shall try in England to understand the position of Japan, an ancient state.... On the one side they have the dark menace of Soviet Russia. On the other the chaos of China, four or five provinces of which are being tortured under Communist rule". In contemporary newspaper articles he referred to the Spanish Republican government as a Communist front, and Franco's army as the "Anti-red movement". He supported the Hoare-Laval Pact and continued up until 1937 to praise Benito Mussolini. 

In his 1937 book Great Contemporaries, Churchill wrote: "One may dislike Hitler's system and yet admire his patriotic achievements. If our country were defeated, I hope we should find a champion as admirable (as Hitler) to restore our courage and lead us back to our place among the nations". Speaking in the House of Commons, in 1937, he said "I will not pretend that, if I had to choose between communism and Nazism, I would choose communism". In the same work, Churchill expressed a hope that despite Hitler's apparent dictatorial tendencies, he would use his power to rebuild Germany into a worthy member of the world community. Churchill's first major speech on defence on 7 February 1934 stressed the need to rebuild the Royal Air Force and to create a Ministry of Defence; his second, on 13 July urged a renewed role for the League of Nations. These three topics remained his themes until early 1936. In 1935 he was one of the founding members of Focus which brought together people of differing political backgrounds and occupations who were united in seeking 'the defence of freedom and peace'. Focus led to the formation of the much wider Arms and the Covenant Movement in 1936.

Churchill was holidaying in Spain when the Germans reoccupied the Rhineland in February 1936, and returned to a divided Englandâ€”Labour opposition was adamant in opposing sanctions and the National Government was divided between advocates of economic sanctions and those who said that even these would lead to a humiliating backdown by Britain as France would not support any intervention. Churchill's speech on 9 March was measured and praised by Neville Chamberlain as constructive. But within weeks Churchill was passed over for the post of Minister for Co-ordination of Defence in favour of the Attorney General Sir Thomas Inskip.. Alan Taylor called this; 'An appointment rightly described as the most extraordinary since Caligula made his horse a consul.' In June 1936 Churchill organised a deputation of senior Conservatives who shared his concern to see Baldwin, Chamberlain and Halifax. He had tried to have delegates from the other two parties and later wrote "If the leaders of the Labour and Liberal oppositions had come with us there might have been a political situation so intense as to enforce remedial action". As it was the meeting achieved little, Baldwin arguing that the Government was doing all it could given the anti-war feeling of the electorate.



In June 1936 Walter Monckton told Churchill that the rumours that King Edward VIII intended to marry Mrs Wallis Simpson were true. Churchill then advised against the marriage and said he regarded Mrs Simpson's existing marriage as a 'safeguard'. In November he declined Lord Salisbury's invitation to be part of a delegation of senior Conservative backbenchers who met with Baldwin to discuss the matter. On 25 November he, Attlee and Sinclair met with Baldwin and were told officially of the King's intention and asked whether they would form an administration if Baldwin and the National Government resigned should the King not take the Ministry's advice. Both Attlee and Sinclair said they would not take office if invited to do so. Churchill's reply was that his attitude was a little different but he would support the government.

The Abdication crisis became public, coming to head in the first fortnight of December 1936. At this time Churchill publicly gave his support to the King. The first public meeting of the Arms and the Covenant Movement was on 3rd December. Churchill was a major speaker and later wrote that in replying to the Vote of Thanks he made a declaration 'on the spur of the moment' asking for delay before any decision was made by either the King or his Cabinet. Later that night Churchill saw the draft of the King's proposed wireless broadcast and spoke with Beaverbrook and the King's solicitor about it. On 4 December he met with the King and again urged delay in any decision about abdication. On 5th December he issued a lengthy statement implying that the Ministry was applying unconstitutional pressure on the King to force him to make a hasty decision. On 7th December he tried to address the Commons to plead for delay. He was shouted down. Seemingly staggered by the unanimous hostility of all Members he left.

Churchill's reputation in Parliament and England as a whole was badly damaged. Some such as Alistair Cooke saw him as trying to build a King's Party. Others like Harold Macmillan were dismayed by the damage Churchill's support for the King had done to the Arms and the Covenant Movement. Churchill himself later wrote "I was myself smitten in public opinion that it was the almost universal view that my political life was ended." Historians are divided about Churchill's motives in his support for Edward VIII. Some such as A J P Taylor see it as being an attempt to 'overthrow the government of feeble men'. Others such as Rhode James see Churchill's motives as entirely honourable and disinterested, that he felt deeply for the King.

Churchill later sought to portray himself as an isolated voice warning of the need to rearm against Germany. While it is true that he had little following in the House of Commons during much of the 1930s he was given considerable privileges by the Government. The â€œChurchill groupâ€?in the later half of the decade consisted only of himself, Duncan Sandys and Brendan Bracken. It was isolated from the other main factions within the Conservative Party pressing for faster rearmament and a stronger foreign policy. In some senses the â€˜exileâ€?was more apparent then real. Churchill continued to be consulted on many matters by the Government or seen as an alternative leader . 

Even during the time Churchill was campaigning against Indian independence, he received official and otherwise secret information. From 1932, Churchillâ€™s neighbour, Major Desmond Morton with Ramsay MacDonald's approval, gave Churchill information on German air power. From 1930 onwards Morton headed a department of the Committee of Imperial Defence charged with researching the defence preparedness of other nations. Lord Swinton as Secretary of State for Air, and with Baldwinâ€™s approval, in 1934 gave Churchill access to official and otherwise secret information. Swinton did so, knowing Churchill would remain a critic of the government but believing that an informed critic was better then one relying on rumour and hearsay. Churchill was a fierce critic of Neville Chamberlain's appeasement of Adolf Hitler and in a speech to the House of Commons, he bluntly and prophetically stated, "You were given the choice between war and dishonour. You chose dishonour, and you will have war."

After the outbreak of the World War II Churchill was appointed First Lord of the Admiralty and a member of the War Cabinet, just as he was in the first part of World War I. The Navy, according to myth, sent out the signal: "Winston is back." In this job, he proved to be one of the highest-profile ministers during the so-called "Phony War", when the only noticeable action was at sea. Churchill advocated the pre-emptive occupation of the neutral Norwegian iron-ore port of Narvik and the iron mines in Kiruna, Sweden, early in the war. However, Chamberlain and the rest of the War Cabinet disagreed, and the operation was delayed until the successful German invasion of Norway.

On 10 May 1940, hours before the German invasion of France by a lightning advance through the Low Countries, it became clear that, following failure in Norway, the country had no confidence in Chamberlain's prosecution of the war and so Chamberlain resigned. The commonly accepted version of events states that Lord Halifax turned down the post of Prime Minister because he believed he could not govern effectively as a member of the House of Lords instead of the House of Commons. Although traditionally, the Prime Minister does not advise the King on the former's successor, Chamberlain wanted someone who would command the support of all three major parties in the House of Commons. A meeting between Chamberlain, Halifax, Churchill and David Margesson, the government Chief Whip, led to the recommendation of Churchill, and, as a constitutional monarch, George VI asked Churchill to be Prime Minister and to form an all-party government. Churchill's first act was to write to Chamberlain to thank him for his support.

Churchill's greatest achievement was his refusal to capitulate when defeat seemed imminent, and he remained a strong opponent of any negotiations with Germany throughout the war. Few others in the Cabinet had this degree of resolve. Although there was an element of British public and political sentiment favouring negotiated peace with a clearly ascendant Germany, among them the Foreign Secretary Lord Halifax, Churchill nonetheless refused to consider an armistice with Hitler's Germany. Churchill's skillful use of rhetoric hardened public opinion against a peaceful resolution and prepared the British for a long war. Coining the general term for the upcoming battle, Churchill stated in his "finest hour" speech to the House of Commons on 18 June 1940, "I expect that the Battle of Britain is about to begin." By refusing an armistice with Germany, Churchill kept resistance alive in the British Empire and created the basis for the later Allied counter-attacks of 1942-45, with Britain serving as a platform for the supply of Soviet Union and the liberation of Western Europe.

In response to previous criticisms that there had been no clear single minister in charge of the prosecution of the war, Churchill created and took the additional position of Minister of Defence. He immediately put his friend and confidant, the industrialist and newspaper baron Lord Beaverbrook, in charge of aircraft production. It was Beaverbrook's business acumen that allowed Britain to quickly gear up aircraft production and engineering that eventually made the difference in the war.

Churchill's speeches were a great inspiration to the embattled British. His first speech as Prime Minister was the famous "I have nothing to offer but blood, toil, tears, and sweat". He followed that closely with two other equally famous ones, given just before the Battle of Britain. One included the words:

we shall fight in France, we shall fight on the seas and oceans, we shall fight with growing confidence and growing strength in the air, we shall defend our island, whatever the cost may be, we shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender.The other:Let us therefore brace ourselves to our duties, and so bear ourselves, that if the British Empire and its Commonwealth last for a thousand years, men will still say, 'This was their finest hour'.At the height of the Battle of Britain, his bracing survey of the situation included the memorable line "Never in the field of human conflict was so much owed by so many to so few", which engendered the enduring nickname "The Few" for the Allied fighter pilots who won it. One of his most memorable war speeches came on 10 November 1942 at the Lord Mayor's Luncheon at Mansion House in London, in response to the Allied victory at the Second Battle of El Alamein. Churchill stated: 

This is not the end. It is not even the beginning of the end. But it is, perhaps, the end of the beginning.

Without having much in the way of sustenance or good news to offer the British people, he took a political risk in deliberately choosing to emphasise the dangers instead. 

"Rhetorical power," wrote Churchill, "is neither wholly bestowed, nor wholly acquired, but cultivated." Not all were impressed by his oratory. Robert Menzies, who was the Prime Minister of Australia, said during World War II of Churchill: "His real tyrant is the glittering phrase so attractive to his mind that awkward facts have to give way." Another associate wrote: "He is . . . the slave of the words which his mind forms about ideas. . . . And he can convince himself of almost every truth if it is once allowed thus to start on its wild career through his rhetorical machinery."

Churchill's good relationship with Franklin D. Roosevelt secured vital food, oil and munitions via the North Atlantic shipping routes. It was for this reason that Churchill was relieved when Roosevelt was re-elected in 1940. Upon re-election, Roosevelt immediately set about implementing a new method of providing military hardware and shipping to Britain without the need for monetary payment. Put simply, Roosevelt persuaded Congress that repayment for this immensely costly service would take the form of defending the USA; and so Lend-lease was born. Churchill had 12 strategic conferences with Roosevelt which covered the Atlantic Charter, Europe first strategy, the Declaration by the United Nations and other war policies. After Pearl Harbor was attacked, Churchill's first thought in anticipation of U.S. help was, "We have won the war!" On 26 December 1941 Churchill addressed a joint meeting of the U.S. Congress, asking of Germany and Japan, "What kind of people do they think we are?" Churchill initiated the Special Operations Executive (SOE) under Hugh Dalton's Ministry of Economic Warfare, which established, conducted and fostered covert, subversive and partisan operations in occupied territories with notable success; and also the Commandos which established the pattern for most of the world's current Special Forces. The Russians referred to him as the "British Bulldog".

Churchill's health was fragile, as shown by a mild heart attack he suffered in December 1941 at the White House and also in December 1943 when he contracted pneumonia. Despite this, he travelled over  throughout the war to meet other national leaders. For security, he usually travelled using the alias Colonel Warden.Churchill was party to treaties that would redraw post-World War II European and Asian boundaries. These were discussed as early as 1943. Proposals for European boundaries and settlements were officially agreed to by Harry S Truman, Churchill, and Stalin at Potsdam. At the second Quebec Conference in 1944 he drafted and, together with U.S. President Franklin D. Roosevelt, signed a toned-down version of the original Morgenthau Plan, in which they pledged to convert Germany after its unconditional surrender "into a country primarily agricultural and pastoral in its character." Churchill's strong relationship with Harry Truman was also of great significance to both countries. While he clearly regretted the loss of his close friend and counterpart Roosevelt, Churchill was enormously supportive of Truman in his first days in office, calling him, "the type of leader the world needs when it needs him most."

When Hitler invaded the Soviet Union, Winston Churchill, a vehement anti-Communist, famously stated "If Hitler were to invade Hell, I should find occasion to make a favourable reference to the Devil," regarding his policy toward Stalin. Soon, British supplies and tanks were flowing to help the Soviet Union.The settlement concerning the borders of Poland, that is, the boundary between Poland and the Soviet Union and between Germany and Poland, was viewed as a betrayal in Poland during the post-war years, as it was established against the views of the Polish government in exile. It was Winston Churchill, who tried to motivate MikoÅ‚ajczyk, who was Prime Minister of the Polish government in exile, to accept Stalin's wishes, but MikoÅ‚ajczyk refused. Churchill was convinced that the only way to alleviate tensions between the two populations was the transfer of people, to match the national borders. 

As he expounded in the House of Commons on 15 December 1944, "Expulsion is the method which, insofar as we have been able to see, will be the most satisfactory and lasting. There will be no mixture of populations to cause endless trouble... A clean sweep will be made. I am not alarmed by these transferences, which are more possible in modern conditions." However the resulting expulsions of Germans was carried out by the Soviet Union in a way which resulted in much hardship and, according to a 1966 report by the West German Ministry of Refugees and Displaced Persons, the death of over 2,100,000. Churchill opposed the effective annexation of Poland by the Soviet Union and wrote bitterly about it in his books, but he was unable to prevent it at the conferences. 

During October 1944, he and Eden were in Moscow to meet with the Russian leadership. At this point, Russian forces were beginning to advance into various eastern European countries. Churchill held the view that until everything was formally and properly worked out at the Yalta conference, there had to be a temporary, war-time, working agreement with regard to who would run what. The most significant of these meetings were held on October 9 1944 in the Kremlin between Churchill and Stalin. During the meeting, Poland and the Balkan problems were discussed. Churchill recounted his speech to Stalin on the day: 

Stalin agreed to this Percentages Agreement, ticking a piece of paper as he heard the translation. In 1958, five years after the recount of this meeting was published (in The Second World War), authorities of the Soviet denied that Stalin accepted the "imperialistproposal".

Between February 13 and February 15 1945, British and the U.S. bombers attacked the German city of Dresden, which was crowded with German wounded and refugees. Because of the cultural importance of the city, and of the number of civilian casualties close to the end of the war, this remains one of the most controversial Western Allied actions of the war. Following the bombing Churchill stated in a top secret telegram: 

On reflection, under pressure from the Chiefs of Staff and in response to the views expressed by Sir Charles Portal (Chief of the Air Staff,) and Arthur Harris (AOC-in-C of Bomber Command,) among others, Churchill withdrew his memo and issued a new one. This final version of the memo completed on April 1 1945, stated:



Ultimately, responsibility for the British part of the attack lay with Churchill, which is why he has been criticised for allowing the bombings to happen. The German revisionist historian JÃ¶rg Friedrich, claims that "Winston Churchill's decision to [area] bomb a shattered Germany between January and May 1945 was a war crime" and writing in 2006 the philosopher A. C. Grayling questioned the whole strategic bombing campaign by the RAF presenting the argument that although it was not a war crime it was a moral crime and undermines the Allies contention that they fought a just war. On the other hand, it has also been asserted that Churchill's involvement in the bombing of Dresden was based on the strategic and tactical aspects of winning the war. The destruction of Dresden, while horrific, was designed to expedite the defeat of Germany. As the historian Max Hastings said in an article subtitled, "the Allied Bombing of Dresden": "I believe it is wrong to describe strategic bombing as a war crime, for this might be held to suggest some moral equivalence with the deeds of the Nazis. Bombing represented a sincere, albeit mistaken, attempt to bring about Germany's military defeat." Furthermore British historian, Frederick Taylor asserts that "All sides bombed each other's cities during the war. Half a million Soviet citizens, for example, died from German bombing during the invasion and occupation of Russia. That's roughly equivalent to the number of German citizens who died from Allied raids. But the Allied bombing campaign was attached to military operations and ceased as soon as military operations ceased."

In June 1944 the Allied Forces invaded Normandy and pushed the Nazi forces back into Germany on a broad front over the coming year. After being attacked on three fronts by the Allies, Germany was soon defeated. On May 7, 1945 at the SHAEF headquarters in Rheims the Allies accepted Germany's surrender. On the same day in a BBC news flash John Snagge announced that May 8 would be Victory in Europe Day. On Victory in Europe day Churchill broadcast to the nation that Germany had surrendered and that a final cease fire on all fronts in Europe would come into effect at one minute past midnight that night. Afterwards Churchill told a huge crowd in Whitehall: "This is your victory." The people shouted: "No, it is yours", and Churchill then conducted them in the singing of Land of Hope and Glory. In the evening he made another broadcast to the nation asserting the defeat of Japan in the coming months. The Japanese later surrendered on August 15, 1945.

Although Churchill's role in World War II had generated him much support from the British population, he was defeated in the 1945 election. Many reasons for this have been given, key among them being that a desire for post-war reform was widespread amongst the population and that the man who had led Britain in war was not seen as the man to lead the nation in peace.

For five years he was to serve as the Leader of the Opposition. During these years Churchill continued to have an impact on world affairs. In 1946 he gave his Iron Curtain speech which spoke of the USSR and the creation of the Eastern Bloc. He declared: 

From Stettin in the Baltic to Trieste in the Adriatic, an Iron Curtain has descended across the continent. Behind that line lie all the capitals of the ancient states of Central and Eastern Europe. Warsaw, Berlin, Prague, Vienna, Budapest, Belgrade, Bucharest and Sofia, all these famous cities and the populations around them lie in what I must call the Soviet sphere.

Churchill also argued strongly for British independence from the European Coal and Steel Community (which he saw as a Franco-German project). He saw Britain's place as separate from the continent, much more in-line with the countries of the Commonwealth and the Empire and with the United States, the so-called Anglosphere.</p>

After Labour's defeat in the General Election of 1951, Churchill again became Prime Minister. His third governmentâ€”after the wartime national government and the brief caretaker government of 1945 â€?lasted until his resignation in 1955. His domestic priorities in his last government were overshadowed by a series of foreign policy crises, which were partly the result of the continued decline of British military and imperial prestige and power. Being a strong proponent of Britain as an international power, Churchill would often meet such moments with direct action. One example was his dispatch of British troops to Kenya to deal with the Mau Mau rebellion. Trying to retain what he could of the Empire, he once stated that, "I will not preside over a dismemberment." 

This was followed by events which became known as the Malayan Emergency. In Malaya, a rebellion against British rule had been in progress since 1948. Once again, Churchill's government inherited a crisis, and Churchill chose to use direct military action against those in rebellion while attempting to build an alliance with those who were not. While the rebellion was slowly being defeated, it was equally clear that colonial rule from Britain was no longer plausible. 

Churchill also devoted much of his time in office to Anglo-American relations and although Churchill did not get on well with President Dwight D. Eisenhower, Churchill attempted to maintain the special relationship with the United States. He made four official transatlantic visits to America during his second term as Prime-Minister.

In June 1953, when he was 78, Churchill suffered a stroke at 10 Downing Street. News of this was kept from the public and from Parliament, who were told that Churchill was suffering from exhaustion. He went to his country home, Chartwell, to recuperate from the effects of the stroke which had affected his speech and ability to walk. He returned to public life in October to make a speech at a Conservative Party conference at Margate. However aware that he was slowing down both physically and mentally, Churchill retired as Prime Minister in 1955 and was succeeded by Anthony Eden. Over the coming years Churchill spent less time in parliament until he stood down at the 1964 General Election. Churchill spent most of his retirement at Chartwell and at his home in Hyde Park Gate, in London. As his mental and physical faculties decayed, he began to lose the battle he had fought for so long against the "black dog" of depression. In 1963, U.S. President John F. Kennedy, acting under authorisation granted by an Act of Congress, proclaimed him an Honorary Citizen of the United States, but he was unable to attend the White House ceremony. On 15 January 1965, Churchill suffered a severe stroke that left him gravely ill. He died at his home nine days later, at age 90, on the morning of Sunday 24 January 1965, coincidentally 70 years to the day after his father's death.

By decree of the Queen, his body lay in state for three days and a state funeral service was held at St Paul's Cathedral. This was the first state funeral for a non-royal family member since 1914, and no other of its kind has been held since. As his coffin passed down the Thames on the Havengore, dockers lowered their crane jibs in a salute. The Royal Artillery fired a 19-gun salute (as head of government), and the RAF staged a fly-by of sixteen English Electric Lightning fighters. The funeral also saw the largest assemblage of statesmen in the world until the 2005 funeral of Pope John Paul II. In the fields along the route, and at the stations through which the train passed, thousands stood in silence to pay their last respects. At Churchill's request, he was buried in the family plot at St Martin's Church, Bladon, near Woodstock, not far from his birthplace at Blenheim Palace.

Winston Churchill was also an accomplished artist and took great pleasure in painting, especially after his resignation as First Lord of the Admiralty in 1915. He found a haven in art to overcome the spells of depression or as he termed them 'Black Dog' which he suffered throughout his life, as William Rees-Mogg has stated, "In his own life, he had to suffer the "black dog" of depression. In his landscapes and still lives there is no sign of depression". He is best known for his impressionist scenes of landscape, many of which were painted whilst on holiday in the South of France or Morocco. He continued his hobby throughout his life and painted dozens of paintings, many of which are on show in the studio at Chartwell.

Aside from his work as a politician, Churchill was a prolific writer, writing over 25 stories, biographies and histories. Two of his largest undertakings included his Nobel prize winning six-volume history on The Second World War and A History of the English-Speaking Peoples; a four-volume history covering the period from Caesar's invasions of Britain (55 BC) to the beginning of the First World War (1914).

Aside from receiving the great honour of a state funeral, Churchill also received numerous awards and honours, including being made the first Honorary Citizen of the United States. Churchill received the Nobel Prize in Literature for his numerous published words, especially his six edition set The Second World War. In a 2002 BBC poll of the "100 Greatest Britons", he was proclaimed "The Greatest of Them All" based on approximately a million votes from BBC viewers. Churchill was also rated as one of the most influential leaders in history by ''Time'' magazine.





|-|-





 

||-||}Lyon, also known as Lyons in English (Liyon in Franco-ProvenÃ§al, while former names include Lugdunum in Latin)   in French, is a city in east central France. It is the third largest French city, the first being Paris and the second Marseilles. It is a major centre of business, situated between Paris and Marseilles, and has a reputation as the French capital of gastronomy and a significant role in the history of cinema.

Together with its suburbs and satellite towns, Lyon forms the second largest metropolitan area in France after Paris, with 1,783,400 inhabitants at the 2007 estimate, and approximately the 20th to 25th largest metropolitan area of Western Europe. It's urban area (RÃ©gion Urbaine de Lyon), represents half of the RhÃ´ne-Alpes rÃ©gion population with 2,9 million inhabitants.

Lyon is the prÃ©fecture (capital) of the RhÃ´ne dÃ©partement, and also the capital of the RhÃ´ne-Alpes rÃ©gion.

Lyon is known as the silk capital of the world and is known for its silk and textiles and is a centre for fashion. 

Lyon is also the international headquarters of Interpol and EuroNews.

Lyon was founded as a Roman colony in 43 BCE by Munatius Plancus, a lieutenant of Caesar, on the site of a Gaulish hill-fort settlement called Lug[o]dunonâ€”from the Celtic sun god Lugus ('Light', cognate with Old Irish Lugh, Modern Irish LÃº) and dÃºnon (hill-fort). Lyon was first named Lugdunum meaning the "hill of lights" or "the hill of crows". Lug was equated by the Romans to Mercury.

The three parts of Gaul mentioned by Caesar met at Lyon. Agrippa recognized that Lugdunum's position on the natural highway from northern to south-eastern France made it a natural communications hub, and he made Lyon the starting point of the principal Roman roads throughout Gaul. It then became the capital of Gaul, partly thanks to its fortunate site at the convergence of two navigable rivers, and quickly became the main city of Gaul. Two emperors were born in this city: Claudius and Caracalla. Today the archbishop of Lyon is still referred to as "le primat des Gaules".

The Christians in Lyon were persecuted for their religious views under the reigns of the Roman emperors Marcus Aurelius and Septimus Severus. These included saints such as Blandina (Blandine), Pothinus (Pothin) , and Epipodius (Ã‰pipode), among others.

The great Christian bishop of Lyon in the 2nd century was the Easterner Irenaeus.Burgundian refugees from the destruction of Worms by Huns in 437 were resettled by the military commander of the west, AÃ«tius, at Lugdunum, which was formally the capital of the new Burgundian kingdom by 461.

In 843, by the Treaty of Verdun, Lyon, with the country beyond the SaÃ´ne, went to Lothair I, and later became a part of the Kingdom of Arles. Lyon only came under French control in the fourteenth century.

Fernand Braudel remarked, "Historians of Lyon are not sufficiently aware of the bi-polarity between Paris and Lyon, which is a constant structure in French development" from the late Middle Ages to the Industrial Revolution (Braudel 1984 p. 327). The fairs in Lyon, the invention of Italian merchants, made it the economic countinghouse of France in the late 15th century. When international banking moved to Genoa, then Amsterdam, Lyon simply became the banking centre of France; its new Bourse (treasury), built in 1749, still resembled a public bazaar where accounts were settled in the open air. During the Renaissance, the city developed with the silk trade, especially with Italy; the Italian influence on Lyon's architecture can still be seen. Thanks to the silk trade, Lyon became an important industrial town during the 19th century.

Lyon was a scene of mass violence against Huguenots in the St. Bartholomew's Day Massacres in 1572.

The silk workers of Lyon, known as canuts, staged two major uprisings: in 1831 and 1834. The 1831 uprising saw one of the first recorded uses of the black flag as an emblem of protest.

Lyon was a centre for the occupying German forces and also a stronghold of resistance during World War II, and the city is now home to a resistance museum. (See also Klaus Barbie.) The traboules, or secret passages, through the houses enabled the local people to escape Gestapo raids.

Lyon's geography is dominated by the RhÃ´ne and SaÃ´ne rivers which converge to the south of the historic city centre forming a sort of peninsula or "presqu'Ã®le"; two large hills, one to the west and one to the north of the historic city centre; and a large plain which sprawls eastward from the historic city centre.

To the west is FourviÃ¨re, known as "the hill that prays", the location for the highly decorated Notre-Dame de FourviÃ¨re basilica, several convents, the palace of the Archbishop, the Tour mÃ©tallique (a highly visible TV tower, replicating the last stage of the Eiffel Tower) and a funicular.

To the north is the Croix-Rousse, "the hill that works", traditionally home to many small silk workshops, an industry for which the city was renowned.

The original medieval city (Vieux Lyon) was built on the west bank of the SaÃ´ne river at the foot of the FourviÃ¨re hill, west of the presqu'Ã®le. (This area, along with portions of the presqu'ile and much of the Croix-Rousse are recognized as a UNESCO World Heritage Site, see below.)

On the peninsula (presqu'Ã®le) between the rivers RhÃ´ne and SaÃ´ne is located the third largest public square in France, and one of the largest in Europe, the Place Bellecour. Specifically, it is the largest clear square (i.e., without any patches of greenery, trees or any other kind of obstacles) in Europe. The broad, pedestrian-only Rue de la RÃ©publique leads north from Place Bellecour.

East of the RhÃ´ne from the presqu'Ã®le is a large area of flat ground upon which sits much of modern Lyon and most of the city's population.Situated in this area is the urban centre of Part-Dieu which clusters the former CrÃ©dit Lyonnais Tower (central France's only skyscraper), the Part-Dieu shopping centre, and Lyon's main rail terminal, Lyon Part-Dieu.

North of this district is the relatively wealthy 6th arrondissement, which is home to the Parc de la TÃªte d'Or, one of Europe's largest urban parks, the prestigious LycÃ©e du Parc to the south of the park, and Interpol's headquarters on the park's western edge.



Lyon is the capital of the RhÃ´ne-Alpes rÃ©gion, the prÃ©fecture of the RhÃ´ne dÃ©partement, and the capital of 14 cantons, covering 1 commune, and with a total population of 488 300 (2007).



Like Paris and Marseille, Lyon is divided into a number of municipal arrondissements (sometimes translated into English as boroughs), each of which is identified by a number and has its own council and town hall.

Five arrondissements were originally created in 1852, when three neighbouring communes (La Croix-Rousse, La GuillotiÃ¨re, and Vaise) were annexed by Lyon.

Between 1867 and 1959, the 3rd arrondissement (which originally covered the whole of the Left Bank of the RhÃ´ne) was split three times, creating a new arrondissement in each case.

Then, in 1963, the commune of Saint-Rambert-l'ÃŽle-Barbe was annexed to Lyon's 5th arrondissement. A year later, in 1964, the 5th was split to create Lyon's 9th â€?and, to date, final â€?arrondissement.

Within each arrondissement, there are a number of recognisable "quartiers" or neighbourhoods:

||-||-||-||-||-||}

The Saint-Jean and the Croix-Rousse areas, which are noted for their narrow passageways (traboules) that pass through buildings and link the streets either side, were designated UNESCO World Heritage Sites in 1998.

These are the main sights of Lyon.

There are several international schools in Lyon, including:

Lyon is home to Ligue 1 football team Olympique Lyonnais, commonly known as "Lyon" or "OL". The team has enjoyed unprecedented success recently, winning the last six national titles and establishing themselves as France's premier football club. The captain of the side, Juninho Pernambucano is one of several Brazilians at Lyon, and he has received many awards while leading his team to unrivalled success. The team competes in the prestigious UEFA Champions' League and currently plays at the impressive Stade de Gerland, where the Danone Nation's Cup is held every year. The team is set to move to a new stadium in DÃ©cines-Charpieu (in the eastern suburbs) in 2010. Lyon also has a rugby union team, Lyon OU, currently playing in division 2, Rugby Pro D2. In addition, Lyon has a rugby league side: Lyon Villeurbanne RhÃ´ne XIII, or LVR XIII, play in the French rugby league championship. The club's current home ground is Stade Georges Lyvet in Villeurbanne. Finally, Villeurbanne also has a renowned basketball team, ASVEL, who play at the Astroballe arena in Cusset.

Saint-ExupÃ©ry International Airport is located 20 km to the east of Lyon, and serves as a base for regional and international flights.

Lyon is connected to the north (Lille, Paris, Brussels) and the south (Marseille, Montpellier) by the TGV. It was the first city to be connected by the TGV c. 1981.

Lyon has two major railway stations: Lyon Part-Dieu, which was built to accommodate the TGV and has become the principal railway station for extra-regional trains; and Lyon Perrache, which is an older station that now primarily serves regional rail services. In practice, many trains, including TGVs, serve both stations. Smaller railway stations include Gorge de Loup, Vaise, Venissieux and St-Paul.

Lyon Saint-ExupÃ©ry International Airport is also directly connected to the TGV network with its own station Gare de Lyon Saint-ExupÃ©ry.

The City is at the heart of a dense road network and is the meeting point of several motorways: A6 (to Paris), A7 (to Marseille), A42 (to Geneve), A43 (Grenoble). The city is now bypassed by the A46.

Lyon is served by the Eurolines intercity coach organisation. Its Lyon terminal is located at the city's Perrache railway station.



The TCL (French: Transports en Commun Lyonnais), Lyon's public transport system, consisting of metros, buses and trams, serves 62 communes of the Lyon agglomeration. The metro system has 4 lines, 39 stations and runs with a frequency of up to a metro every 2 minutes. The bus system consists of normal buses, trolleybuses and coaches for areas outside the centre, but which operate on the same ticketing scheme. There are three tram lines since December 2006: T1 from Montrochet in the south to IUT-Fessine in the north, Tram T2 from Perrache station in the southwest to Saint-Priest in the southeast, and Tram T3 from Part-Dieu to Meyzieu.

The public transit system is complemented by VÃ©lo'v, a bicycle network providing a low cost and convenient bicycle hire service where bicycles can be hired and dropped off at any of several hundred stations throughout the city.



Lyon's sister cities are:











An airship or dirigible is a lighter than air (buoyant) aircraft that can be steered and propelled through the air using rudders and propellers. Unlike other aerodynamic aircraft such as fixed-wing aircraft (airplanes) and helicopters, which produce lift by moving a wing or airfoil through the air, aerostatic aircraft, such as airships and hot air balloons, stay aloft by filling a large cavity, such as a balloon, with a lighter than air gas.

The main types of airship are Non-rigid airships (or blimps), semi-rigid airships and rigid airships. Blimps are small airships without internal skeletons. Semi-rigid airships are slightly larger and have some form of internal support such as a fixed keel. Rigid airships with a full skeleton, such as the massive Zeppelin transoceanic models, are now a thing of the past.

Airships were the first aircraft to make controlled, powered flight. They were widely used before the 1940s. Their use decreased over time as their capabilities were surpassed by those of airplanes. Their decline furthered with a series of high-profile accidents, including the 1937 burning of the hydrogen-filled Hindenburg near Lakehurst, New Jersey. Airships are still used today in certain niche applications, such as advertising and as a camera platform for sporting events.

In many countries, airships are also known as dirigibles from the French (dirigir to direct plus -ible), meaning "directable" or steerable. The first airships were called dirigible balloons. Over time, the word balloon was dropped from the phrase. In the modern usage, balloon refers to buoyant aircraft that generally rely on wind currents for movement, though vertical movement can be controlled in both.

The term zeppelin is a genericised trademark that originally referred to airships manufactured by the Zeppelin Company. Their crafts' names were usually prefixed with the word Luftschiff, German for "airship".

In modern common usage, the terms zeppelin, dirigible and airship are used interchangeably for any type of rigid airship, with the terms blimp or airship alone used to describe non-rigid airships. Although the blimp also qualifies as a "dirigible", the term is seldom used with blimps. In modern technical usage, airship is the term used for all aircraft of this type, with zeppelin referring only to aircraft of that manufacture, and blimp referring only to non-rigid airships.

The term airship is sometimes informally used to mean any machine capable of atmospheric flight. 

There is often some confusion around the term aerostat with regard to airships. This confusion arises because aerostat has two different meanings. One meaning of aerostat refers to all craft that remain aloft using buoyancy. In this sense, airships are a type of aerostat. The other, more narrow and technical meaning of aerostat refers only to tethered or moored balloons. In this second technical sense, airships are distinct from aerostats. This airship/aerostat confusion is often exacerbated by the fact that both airships and aerostats have roughly similar shapes and comparable tail fin configurations, although only airships have motors.



Any gas that is lighter than air can be used to create buoyant lift, however many such gases are either toxic, flammable, corrosive, or a combination of these, limiting their use in airships. Historically, hydrogen and helium have been used in large airships. A calculation based on the gas densities shows that hydrogen provides only 8% more lift than helium. Coal gas was used by several early pioneers as a cheaper more easily available substitute for hydrogen.

After the discovery of helium in the late 1890s, and development of processes to produce the gas commercially, helium was the preferred lifting gas. However until the 1950s, the United States was the sole producer of helium, and because the U.S. had embargoed exports of helium to Germany for strategic purposes since the 1920s, German airships were filled with hydrogen. The Hindenburg, for example, was originally designed to be filled with helium, but its unavailability forced the airship's operators to use hydrogen, with infamous results.

Ships called thermal airships utilize heated air, in a fashion similar to hot air balloons, as their lifting gas.



In 1784 Jean-Pierre Blanchard fitted a hand-powered propeller to a balloon, the first recorded means of propulsion carried aloft. In 1785, he crossed the English Channel with a balloon equipped with flapping wings for propulsion, and a bird-like tail for steerage.

The first person to make an engine-powered flight was Henri Giffard who, in 1852, flew 27Â km (17Â miles) in a steam-powered airship. Airships would develop considerably over the next two decades: In 1863, Dr. Solomon Andrews devised the first fully steerable airship, although it had no motor. In 1872, the French naval architect Dupuy de Lome launched a large limited navigable balloon, which was driven by a large propeller and the power of eight people. It was developed during the Franco-Prussian war, as an improvement to the balloons used for communications between Paris and the countryside during the Siege of Paris by German forces, but was only completed after the end of the war. Charles F. Ritchel made a public demonstration flight in 1878 of his hand-powered one-man rigid airship and went on to build and sell five of his aircraft. Paul Haenlein flew an airship with an internal combustion engine running on the coal gas used to inflate the envelope over Vienna, the first use of such an engine to power an aircraft in 1872.

In the 1880s a Serb named Ogneslav Kostovic Stepanovic also designed and built an airship. However the craftwas destroyed by fire before it flew. In 1883, the first electric-powered flight was made by Gaston Tissandier who fitted a  Siemens electric motor to an airship. The first fully controllable free-flight was made in a French Army airship, La France, by Charles Renard and Arthur Constantin Krebs in 1884 . The  long, 66,000 cubic foot (1,900 mÂ³) airship covered 8Â km (5Â miles) in 23Â minutes with the aid of an  electric motor.

In 1888-97, Dr. Frederich WÃ¶lfert built three Daimler Motor Company-built petrol engine powered airships, the last of which caught fire in flight and killed both occupants.

In 1896, a rigid airship created by Croatian engineer David Schwarz made its first flight at Tempelhof field in Berlin. After Schwarz's death, his wife, Melanie Schwarz, was paid 15,000 Marks by Count Ferdinand von Zeppelin for information about the airship.

In 1901, Alberto Santos-Dumont, in his airship "Number 6", a small blimp, won the Deutsch de la Meurthe prize of 100,000 francs for flying from the Parc Saint Cloud to the Eiffel Tower and back in under thirty minutes. Many inventors were inspired by Santos-Dumont's small airships and a veritable airship craze began world-wide. Many airship pioneers, such as the American Thomas Scott Baldwin financed their activities through passenger flights and public demonstration flights. Others, such as Walter Wellman and Melvin Vaniman set their sights on loftier goals, attempting two polar flights in 1907 and 1909, and two trans-atlantic flights in 1910 and 1912.

The "Golden Age of Airships" began in July 1900 with the launch of the Luftschiff Zeppelin LZ1. This led to the most successful airships of all time: The Zeppelins. These were named after Count von Zeppelin who began experimenting with rigid airship designs in the 1890s leading to the badly flawed LZ1 (1900) and the more successful LZ2 (1906). At the beginning of World War I the Zeppelin airships had a framework composed of triangular lattice girders, covered with fabric and containing separate gas cells. Multi-plane, later cruciform, tail fins were used for control and stability, and two engine/crew cars hung beneath the hull driving propellers attached to the sides of the frame by means of long drive shafts. Additionally there was a passenger compartment (later a bomb bay) located halfway between the two cars.

The prospect of airships as bombers had been recognised in Europe well before the airships were up to the task. H. G. Wells The War in the Air (1908) described the obliteration of entire fleets and cities by airship attack. On 5 March 1912, Italian forces became the first to use dirigibles for a military purpose during reconnaissance west of Tripoli behind Turkish lines. It was World War I, however, that marked the airship's real debut as a weapon.

Albert Caquot designed an Observation Balloon for the French army in 1914. The Type R Observation balloon was used by all the allied forces, including the British and United States Armies, at the end of the World War. In 1919, Japan equipped the Imperial Army with several "Caquot dirigeables".

The Germans, French and Italians all operated airships in scouting and tactical bombing roles early in the war, and all learned that the airship was too vulnerable for operations over the front. The decision to end operations in direct support of armies was made by all in 1917. Count Zeppelin and others in the German military believed they had found the ideal weapon with which to counteract British Naval superiority and strike at Britain itself. More realistic airship advocates believed the Zeppelin was a valuable long range scout/attack craft for naval operations. Raids began by the end of 1914, reached a first peak in 1915, and then were discontinued in August 1918. Zeppelins proved to be terrifying but inaccurate weapons. Navigation, target selection and bomb-aiming proved to be difficult under the best of conditions. The darkness, high altitudes and clouds that were frequently encountered by zeppelin missions reduced accuracy even further. The physical damage done by the zeppelins over the course of the war was trivial, and the deaths that they caused (though visible) amounted to a few hundred at most. The zeppelins also proved to be vulnerable to attack by aircraft and antiaircraft guns, especially those armed with incendiary bullets. Several were shot down in flames by British defenders, and others crashed 'en route'. In retrospect, advocates of the naval scouting role of the airship proved to be correct, and the land bombing campaign proved to be disastrous in terms of morale, men and material. Many pioneers of the German airship service died in what was the first strategic bombing campaign in history. Countermeasures by the British were sound detection, equipment, search lights and anti-aircraft artillery, and starting in 1915 night fighters. One method used early in the war when short range meant the airships had to fly from forward bases, and when only Zeppelin production facilities were in Friedrichshafen, was bombing of airship sheds by the British Royal Naval Air Service. Late in the war, the development of the aircraft carrier led to the first successful carrier air strike in history. The morning of 19 July 1918, seven Sopwith 2F.1 Camels were launched from HMS Furious and struck the airship base at Tondern, destroying the Zeppelins L 54 and L 60.Before the World War, the British Army was interested in blimps for scouting purposes. The Royal Navy recognizing the potential threat that scouting Zeppelins might pose, decided in 1908 to produce an example of rigid airship so that the threat might be evaluated in practice instead of theory. The Royal Navy was to continue development of rigid airships until the end of the war. The British Army abandoned airship development in favor of airplanes by the start of the war, but the Royal Navy had recognised the need for small airships to counteract the submarine and mine threat in coastal waters. Beginning in February 1915, began to deploy the SS (Sea Scout) class of blimp. These had a small envelope of 60-70,000 cu feet and at first used standard single engined planes (BE2c, Maurice Farman, Armstrong FK) shorn of wing and tail surfaces as control cars, an economy measure. Eventually more advanced blimps with purpose built cars, such as the C (Coastal), C* (Coastal Star), NS (North Sea), SSP (Sea Scout Pusher), SSZ (Sea Scout Zero), SSE (Sea Scout Experimental) and SST (Sea Scout Twin) classes were developed. The NS class, after initial teething problems proved to be the largest and finest airships in British service. They had a gas capacity of 360,000 cu feet, a crew of 10 and an endurance of 24Â hours. Six  bombs were carried, as well as 3-5 machine guns. British blimps were used for scouting, mine clearance, and submarine attack duties. During the war, the British operated 226 airships, mostly non-rigid, most of which were of indigenous construction, though some non-rigid airships operated were purchased from France and even Germany (before the war). Of that number several were sold to Russia, France, the US and Italy. Britain, in turn, purchased one M-type semi-rigid from Italy whose delivery was delayed until 1918. Nine rigid airships had been completed by the armistice, although several more were in an advanced state of completion by the war's end. The large number of trained crews, low attrition rate and constant experimentation in handling techniques meant that at the war's end Britain was the world leader in non-rigid airship technology.

Both France and Italy continued airships throughout the war. France preferred non-rigid types while Italy operated 49 semi-rigid airships in both the scouting and bombing roles.

Airplanes had essentially replaced airships as bombers by the end of the war, and Germany's remaining zeppelins were scuttled by their crews, scrapped or handed over to the Allied powers as spoils of war. The British rigid airship program, meanwhile, had been largely a reaction to the potential threat of the German one and was largely, though not entirely, based on imitations of the German ships.

Airships were operated in a number of nations between the two world wars. The major operators of rigid airships were Britain, the United States and Germany, and a few were operated by Italy and France. Italy, the Soviet Union, United States and Japan operated semi-rigid airships, while blimps were operated in many nations.

The British R33 and R34 were near-identical copies of the German L 33, which crashed virtually intact in Yorkshire on September 24 1916. Despite being almost three years out of date by the time they were launched in 1919, they were two of the most successful in British service. The creation of the Royal Air Force (RAF) in early 1918 created a hybrid British airship program. The RAF was uninterested in airships and the Admiralty was, so a deal was made where the Admiralty would design any future military airships while the RAF would handle manpower, facilities and operations. 

After the armistice, the airship program was rapidly wound down, and rigid airship operations were curtailed. On July 2 1919 R34 began the first double crossing of the Atlantic by an aircraft. It landed at Mineola, Long Island on July 6, 1919 after 108Â hours in the air. The return crossing commenced on July 8 because of concerns about mooring the ship in the open, and took 75Â hours. Impressed, British leaders began to contemplate a fleet of airships to link Britain to its far-flung colonies. But post-war economic conditions led to scrapping most airships and dispersion of trained personnel, until starting construction of the R-100 and R-101 in 1929. The major consequence of Britain's interest in establishing airship service to the empire was the effort to use the Allies' seizure of German airships and airship sheds to avoid competition from Germany. The US Navy contracted to buy the British built R-38, but before that airship was turned over to the US, it was lost to structural failure due to both improper design and operation.

The first American-built rigid dirigible was USS ''Shenandoah'', christened on August 20 in Lakehurst, New Jersey. It flew in 1923, while the Los Angeles was under construction. It was the first ship to be inflated with the noble gas helium, which was still so rare that the Shenandoah contained most of the world's reserves. When the Los Angeles was delivered, the two airships had to share the limited supply of Helium, and thus alternated operating and overhauls.

The United States Navy purchased what became the USS ''Los Angeles'' and paid with "war reparations" money, owed according to the Versailles Treaty, thus saving The Zeppelin works. The success of the Los Angeles encouraged the US Navy to invest in its own, larger airships. The USS Los Angeles flew successfully for 8Â years.

Meanwhile Germany was building the Graf Zeppelin, the largest airship that could be built in the company's existing shed, and intended to stimulate interest in passenger airships. The Graf Zeppelin burned blau gas, similar to propane, stored in large gas bags below the hydrogen cells, as fuel. Since its density was similar to that of air, it avoided the weight change when fuel was used, and thus the need to valve hydrogen. The "Graf" was a great success and compiled an impressive safety record. For example it flew over one million miles (including the first circumnavigation of the globe by air) without a single passenger injury.

The US Navy developed the idea of using airships as "flying aircraft carriers." There were two airships, the world's largest at the time, to test the principleâ€”the USS ''Akron'' and USS ''Macon''. Each carried four fighters in their "hanger", and could carry a fifth on the "trapeze." The "Flying Aircraft Carrier" had mixed results. By the time the Navy started to develop a sound doctrine for using the ZRS-type airships, the last of the two built, USS Macon, was lost. The seaplane had become more mature, and was considered a better investment.

Eventually the US Navy lost all three American-built rigid airships to accidents. USS Shenandoah on a poorly planned publicity flight flew into a severe thunderstorm over Noble County, Ohio on 3 September 1925. It broke into pieces, killing 14 of her crew. USS Akron was caught in a severe storm and flown into the surface of the sea off the shore of New Jersey on April 3 1933. It carried no life boats and few life vests, so 73 of her 76-men crew died from drowning or hypothermia. USS Macon was lost after suffering a structural failure off the shore of Point Sur, California on 12 February 1935. The failure caused a loss of gas, which was made much worse when the aircraft was driven over pressure height causing it to lose too much helium to maintain flight. Only 2 of her 83-man crew died in the crash thanks to the inclusion of life jackets and inflatable rafts after the Akron disaster.

In Britain during the 1920s, Sir Denistoun Burney suggested a plan for air service throughout the Empire by airships (the Burney Scheme). Following the election of Ramsey MacDonald, the Burney scheme was transformed into a government-controlled program which contracted for two airships, one to be developed by the Airship Guarantee Company, the other by the Royal Airship Works. The two designs were radically different. The "capitalist" ship, the R100, was conservative, while the "socialist" ship, the R101, was wildly innovative. Construction was delayed, and the airships did not fly until 1929. Neither airship was capable of the service intended, though the R100 did complete a proving flight to Canada and back in 1930.

In October 1930 there were rushed preparations to fly the R-101, which had not been adequately tested and had serious deficiencies, on a flight to India carrying the Air Minister of the MacDonald government, Christopher Birdwell, Lord Thompson for an important Imperial conference. An air worthiness certificate was issued at the last moment. The R101 left on the flight on 5 October but hours later crashed in France killing 48 of the 54 people aboard. Because of the bad publicity surrounding the crash, the Air Ministry grounded the competing R100 in 1930 and sold it for scrap in 1931, ending the era of British rigid airships.

By the mid-1930s only Germany still pursued the airship. The Zeppelin company continued to operate the Graf Zeppelin on passenger service between Germany and Brazil. Even with the small Graf Zeppelin, the operation was almost profitable. In the mid-1930s work started to build an airship designed specifically to operate a passenger service across the Atlantic. The ''Hindenburg'' (LZ 129) completed a very successful 1936 season carrying passengers between Lakehurst, New Jersey and Germany. But 1937 started with the most spectacular and widely remembered airship accident. Approaching the mooring mast minutes before landing on 6 May 1937, the Hindenburg burst into flames and crashed. Of the 97 people aboard, 36 died: 13 passengers, 22 aircrew, and one American ground-crewman. The disaster happened before a large crowd, was filmed and a radio news reporter was cutting a recording of his coverage of the arrival. This was a disaster which theater goers could see and hear the next day. On that same next day, the Graf Zeppelin landed at the end of its flight from Brazil, ending intercontinental passenger airship travel.

Hindenburg's sister ship, the ''Graf Zeppelin II'', could not fly without helium which the United States refused to sell. The Graf Zeppelin flew some test flights and conducted electronic espionage until 1939 when it was grounded due to the start of the war. The last two Zeppelins were scrapped in 1940.

Development of airships continued only in the United States, and in a small way, the Soviet Union.

While Germany determined that airships were obsolete for military purposes in the coming war and concentrated on the development of airplanes, the United States pursued a program of military airship construction even though it had not developed a clear military doctrine for airship use. At the Japanese attack on Pearl Harbor on 7 December 1941 that brought the United States into World War II, it had 10 non-rigid airships:

Only K and TC class airships were suitable for combat and they were quickly pressed into service against Japanese and German submarines which were then sinking US shipping within visual range of the US coast. US Navy command, remembering the airship anti-submarine success from World War I, immediately requested new modern anti-submarine airships and on 2 January 1942 formed the ZP-12 patrol unit based in Lakehurst from the 4 K airships. The ZP-32 patrol unit was formed from two TC and two L airships a month later, based at NAS Moffett Field in Sunnyvale, California. An airship training base was created there as well. In December 1941 and the first months of 1942, the Goodyear blimp Resolute was operated as an anti-submarine privateer based out of Los Angeles. As the only US craft to operate under a Letter of Marque since the War of 1812, the Resolute, armed with a rifle and flown by its civilian crew, patrolled the seas for submarines.



In the years 1942â€?4, approximately 1,400 airship pilots and 3,000 support crew members were trained in the military airship crew training program and the airship military personnel grew from 430 to 12,400. The US airships were produced by the Goodyear factory in Akron, Ohio. From 1942 till 1945, 154 airships were built for the US Navy (133 K-class, ten L-class, seven G-class, four M-class) and five L-class for civilian customers (serial number L-4 to L-8).

The primary airship tasks were patrol and convoy escort near the US coastline. They also served as an organisation center for the convoys to direct ship movements, and were used in naval search and rescue operations. Rarer duties of the airships included aerophoto reconnaissance, naval mine-laying and mine-sweeping, parachute unit transport and deployment, cargo and personnel transportation. They were deemed quite successful in their duties with the highest combat readiness factor in the entire US air force (87%).

During the war some 532 ships without airship escort were sunk near the US coast by enemy submarines. Only one ship, the tanker Persephone, of the 89,000 or so in convoys escorted by blimps was sunk by the enemy. Airships engaged submarines with depth charges and, less frequently, with other on-board weapons. They were excellent at driving submarines down, where their limited speed and range prevented them from attacking convoys. The weapons available to airships were so limited that until the advent of the homing torpedo they had little chance of sinking a submarine.

Only one airship was ever destroyed by U-boat: on the night of 18/19 July 1943 a K-class airship (K-74) from ZP-21 division was patrolling the coastline near Florida. Using radar, the airship located a surfaced German submarine. The K-74 made her attack run but the U-boat opened fire first. K-74's depth charges did not release as she crossed the U-boat and the K-74 received serious damage, losing gas pressure and an engine but landing in the water without loss of life. The crew was rescued by patrol boats in the morning, but one crewman, Isadore Stessel, died from a shark attack. The U-Boat, U-134, was slightly damaged and the next day or so was attacked by aircraft sustaining damage that forced it to return to base. It was finally sunk on 24 August 1943 by a British Vickers Wellington near Vigo, Spain

Fleet Airship Wing One operated from Lakehurst, NJ, Glynco, GA, Weeksville, NC, South Weymouth NAS Massachusetts, Brunswick NAS and Bar Harbor ME, Yarmouth, Nova Scotia, and Argentia, Newfoundland. 

Some US airships saw action in the European war theatre. The ZP-14 unit operating in the Mediterranean area from June 1944 completely denied the use of the Gibraltar Straits to Axis submarines. Airships from the ZP-12 unit took part in the sinking of the last U-Boat before German capitulation, sinking U-881 on 6 May 1945 together with destroyers Atherton and Mobery. 

Other airships patrolled the Caribbean, Fleet Airship Wing Two, Headquartered at NAS Richmond, Florida, covered the Gulf of Mexico from Richmond and Key West, FL, Houma, Louisiana, as well as Hitchcock and Brownsville, Texas. FAW 2 also patrolled the northern Caribbean from San Julian, the Isle of Pines and Guantanamo Bay, Cuba as well as Vernam Field Jamaica.

Navy blimps of Fleet Airship Wing Five, (ZP-51) operated from bases in Trinidad, British Guiana and Parmaribo, Dutch Guiana. Fleet Airship Wing Four operated along the coast of Brazil. Two squadrons, VP-41 and VP-42 flew from bases at AmapÃ¡, Igarape Assu, Sao Luiz, Fortaleza, Fernando de Noronha, Recife, Maceiro, Ipitanga, Caravellas, Vitoria and the hanger built for the Graf Zeppelin at Santa Cruz.

Fleet Airship Wing Three operated squadrons, ZP-32 from Moffett Field, ZP-31 at NAS Santa Anna, and ZP-33 at Tillamook Oregon. Auxiliary fields were at Del Mar, Lompoc, Watsonville and Eureka, CA, North Bend and Astoria, Oregon, as well as Shelton and Quillayute in Washington.

From 2 January 1942 till the end of war airship operations in the Atlantic, the airships of the Atlantic fleet made 37,554 flights and flew 378,237Â hours. Of the over 70,000 ships in convoys protected by blimps, only one was sunk by a submarine while under blimp escort.

The Soviet Union used a single airship during the war. The W-12, built in 1939, entered service in 1942 for paratrooper training and equipment transport. It made 1432 runs with 300 metric tons of cargo until 1945. On 1 February 1945 the Soviets constructed a second airship, a Pobieda-class (Victory-class) unit (used for mine-sweeping and wreckage clearing in the Black Sea) which crashed on 21 January 1947. Another W-class â€?W-12bis Patriot was commissioned in 1947 and was mostly used for crew training, parades and propaganda.

Although airships are no longer used for passenger transportation, they are still used for other purposes such as advertising and sightseeing. 

In June, 1987, the US Navy awarded a US$168.9Â million contract to Westinghouse Electric and Airship Industries of the UK to demonstrate whether a blimp could be used as an airborne platform to detect the threat of sea-skimming missiles, such as the Exocet.

In recent years, the Zeppelin company has reentered the airship business. Their new model, designated the Zeppelin NT made its maiden flight on September 18, 1997. There are currently three NT aircraft flying. One was sold to a Japanese company, and was planned to be flown to Japan in the summer of 2004. But due to delays getting permission from the Russian government, the company decided to transport the airship to Japan by ship. 

Blimps are used for advertising and as TV camera platforms at major sporting events. The most iconic of these are the Goodyear blimps. Goodyear operates 3 blimps in the United States, and the Lightship group operates up to 19 advertising blimps around the world. Airship Management Services owns and operates 3 Skyship 600 blimps. Two operate as advertising and security ships in the North America and the Caribbean. 

Skycruise Switzerland AG owns and operates 2 Skyship 600 blimps. One operates regularly over Switzerland used on sightseeing tours. 

The Switzerland-based Skyship 600 has also played other roles over the years. For example, it was flown over Athens during the 2004 Summer Olympics as a security measure. In November 2006, it carried advertising calling it "The Spirit of Dubai" as it began a publicity tour from London to Dubai, UAE on behalf of The Palm Islands, the world's largest man-made islands created as a residential complex.

Los Angeles-based Worldwide Aeros Corp. produces FAA Type Certified Aeros 40D Sky Dragon airships.

In May 2006, US Navy began to fly airships again after a hiatus of nearly 44Â years. The program uses a single American Blimp Company A-170 non-rigid airship. Operations focus on crew training and research, and the platform integrator is Northrop Grumman. The program is directed by the Naval Air Systems Command and is being carried out at NAES Lakehurst, the original center of US Navy lighter-than-air operations in previous decades.

In November 2006, the US Army bought an A380+ airship from American Blimp Corporation through a Systems level contract with Northrop Grumman and Booz Allen Hamilton. The airship will start flight tests in late 2007 with a primary goal of carrying  of payload to an altitude of  under remote control and autonomous waypoint navigation. The program will also demonstrate carrying  of payload to  The platform could be used for Multi-Intelligence collections. Northrop Grumman (formerly Westinghouse) has responsibility for the overall program.



Several companies, such as Cameron Balloons in Bristol, United Kingdom, build hot-air airships. These combine the structures of both hot-air balloons and small airships. The envelope is the normal 'cigar' shape, complete with tail fins, but is inflated with hot air (as in a balloon) to provide the lifting force, instead of helium. A small gondola, carrying the pilot (and sometimes between 1 and 3 passengers), a small engine and the burners to provide the hot air is suspended below the envelope, below an opening through which the burners protrude. 

Hot-air airships typically cost less to buy and maintain than modern helium-based blimps, and can be quickly deflated after flights. This makes them easy to carry in trailers or trucks and inexpensive to store. They are usually very slow moving, with a typical top speed of 15-20Â mph. They are mainly used for advertising, but at least one has been used in rainforests for wildlife observation, as they can be easily transported to remote areas.

Remote controlled (RC) airships, a type of Unmanned Aerial System (UAS), are sometimes used for commercial purposes such as advertising and aerial video and photography as well as recreational purposes. They are particularly common as an advertising mechanism at indoor stadiums. While RC airships are sometimes flown outdoors, doing so for commercial purposes is illegal in the US In particular, Docket FAA-2006-25714 states that: "The FAA recognizes that people and companies other than modelers might be flying UAS with the mistaken understanding that they are legally operating under the authority of AC 91-57. AC 91-57 only applies to modelers, and thus specifically excludes its use by persons or companies for business purposes."

Hybrid designs such as the Heli-Stat airship/helicopter, the Aereon aerostatic/aerodynamic craft, and the Cyclocrane (a hybrid aerostatic/rotorcraft), have struggled to take flight. The Cyclocrane was also interesting in that the airship's envelope rotated along its longitudinal axis.

CL160 was a very large semi-rigid airship to be built in Germany by the start-up Cargolifter, but funding ran out in 2002 after a massive hangar was built. The hangar, built just outside Berlin, has since been converted into a resort called "Tropical Islands".

In 2005, a short-lived project of the US Defense Advanced Research Projects Agency (DARPA) was WALRUS HULA which explored the potential for using airships as long-distance, heavy lift craft. The primary goal of the research program was to determine the feasibility of building an airship capable of carrying 500 short tons (450Â metric tons) of payload a distance of 12,000Â miles (20,000Â km) and land on an unimproved location without the use of external ballast or ground equipment (such as masts). In 2005, two contractors, Lockheed-Martin and US Aeros Airships were each awarded approximately $3Â million to do feasibility studies of designs for WALRUS. In late March 2006, DARPA announced the termination of work on WALRUS after completion of the current Phase I contracts.

The US government is funding two major projects in the high altitude arena. The Composite Hull High Altitude Powered Platform (CHHAPP) is sponsored by US Army Space and Missile Defense Command. This aircraft is also sometimes called HiSentinel High-Altitude Airship. This prototype ship made a 5-hour test flight in September 2005. The second project, the high-altitude airship (HAA), is sponsored by DARPA. In 2005, DARPA awarded a contract for nearly $150Â million to Lockheed-Martin for prototype development. First flight of the HAA is planned for 2008.

Many companies are working on high-altitude airships. Techsphere is developing a high-altitude version of their spherically shaped airships. JP Aerospace has discussed its long-range plans that include not only high altitude communications and sensor applications but also an "orbital airship" capable of lifting cargo into low Earth orbit with a marginal transportation cost of $1 per short ton per mile of altitude.

On January 31 2006 Lockheed-Martin made the first flight of their secretly built hybrid airship designated the P-791 at the company's flight test facility on the Palmdale Air Force Plant 42. The design is very similar in to the SkyCat, unsuccessfully promoted for many years by the now financially troubled British company Advanced Technology Group. Although Lockheed Martin is developing a design for the DARPA WALRUS HULA project, it claimed that the P-791 is unrelated to WALRUS. Nonetheless, the design represents an approach that may well be applicable to WALRUS. Some believe that Lockheed-Martin had used the secret P-791 program as a way to get a "head-start" on the other WALRUS competitor, US Aeros Airships.

A privately funded effort to build a heavy-lift aerostatic/aerodynamic hybrid craft, called the Dynalifter, is being carried out by Ohio Airships. Test flights are to begin in Spring 2006.

The research and development company for airship technologies, 21st century Airships Inc., has developed a spherical-shaped airship, and airships for high altitude, environmental research, surveillance and military applications, heavy lift and sightseeing. Its airships have set numerous world records.

In Russia, AUGUR-RosAerosystems Group is manufacturing non-rigid multi-functional airships for up to 10 passengers, as well as patrol airships including the Au-12 and Au-30. They are also working on developmental programs for heavy-lift cargo models and high-altitude stratospheric ships. One of AUGUR-RosAeroSystems manufactured Au-30 airships will take part in the expedition to the North Pole challenged by famous French explorer Jean-Louis Etienne for Arctic ice pack measurements in April, 2008.

Several proposals have been made for the use of airships in the robotic exploration of those planets (and one moon, Titan) which have atmosphere thick thick enough to provide buoyancy. Some of these applications are discussed under Aerobots.

The proposed Aeroscraft is Aeros Corporation's continuation of the now canceled WALRUS project. The Aeroscraft is not an airship or hybrid airship; it is a new type of buoyancy assisted air vehicle. Unlike any other aircraft the Aeroscraft generates lift through a combination of aerodynamics, thrust vectoring and gas buoyancy generation and management, and for much of the time will fly heavier than air.

There is a case for the airship or zeppelin as a medium to long distance air 'cruise ship' using helium as a lifting agent. Airship passengers could have spacious decks inside the hull to give ample room for sitting, sleeping and recreation. There would be ample room for restaurants and similar facilities. The potential exists for a market in more leisurely journeys, such as cruises over scenic terrain.

The advantage of airships over airplanes is that static lift sufficient for flight is generated by the lifting gas and requires no engine power. This was an immense advantage before the middle of WW I and remained an advantage for long distance, or long duration operations until WW II. Modern concepts for high altitude airships include photovoltaic cells to reduce the need to land to refuel, thus they can remain in the air until consumables expire.

The disadvantages are that the drag on an airship rises as the square of its speed, while the power required to propel it increases as the cube of the speed. In airplanes, lift and drag increase together with speed, so that for a given lift the drag is effectually constant at any speed, and so the power required only increases linearly with speed until close to the speed of sound. Given the large flat plate area and wetted surface of the airship, a practical limit is reached somewhere between 80 and . So the airship is not trusted with an important position by speed, but by durability as surveillance-gathering platform or other airborne early warning mission. In these cases, speed is not a critical need.

The altitude an airship can fly at largely depends on how much lifting gas it can lose due to expansion before stasis is reached. The ultimate altitude record for a rigid airship was set in 1917 by the L-55 under the command of Kurt Flemming (who later died in the Hindenburg) when he forced the airship to 24,000Â feet (7,300 m) attempting to cross France after the "Silent Raid" on London. The L-55 lost lift as the descent to lower altitudes over Germany compressed the gas left in the cells, and thus the weight of air displaced. L-55 crashed due to loss of lift. While such waste of gas was necessary for the survival of airships in the later years of WW I, it was impractical for commercial operations, or operations of helium filled military airships. The highest flight made by a hydrogen filled passenger airship was  on the Graf Zeppelin's around the world flight. The practical limits for rigid airships was about , and for pressure airships around .

Modern airships use dynamic helium volume. At sea level altitude, helium only takes up a small part of the hull, while the rest volume is filled with air. As the airship ascends, the helium inflates with reduced outer pressure, and air is pushed out and released from the downward valve. This allows airships to reach any altitude with balanced inner and outer pressure, if the buoyancy is enough. Some civil aerostat could reach  without explosion due to overloaded inner pressure. 

The greatest disadvantage of the airship is size, which is essential to increasing performance. As size increases, the problems of ground handling increase geometrically. As the German Navy transitioned from the "p" class Zeppelins of 1915 (with a volume of over 1.1Â million cubic feet) to the larger "q" class of 1916, the "r" class of 1917, and finally the "w" class of 1918, at almost 2.2Â million cubic feet, ground handling problems reduced the number of days the Zeppelins were able to make patrol flights. This availability declined from 34% in 1915, to 24.3% in 1916 and finally 17.5% in 1918.

So long as the power-to-weight ratios of aircraft engines remained low and specific fuel consumption high, the airship had an edge for long range or duration operations. As those figures changed, the balance shifted rapidly in the airplane's favor. By mid-1917 the airship could no longer survive in a combat situation where the threat was airplanes. By the late 1930s, the airship barely had an advantage over the airplane on intercontinental over-water flights, and that advantage had vanished by the end of WW II.

This is in face-to-face tactical situation, current High Altitude Airship project is planned to survey hundreds of kilometers as their operation radius, often much farther than normal engage range of a military airplane. This provides better early warning, even farther than the Aegis system. The current Aegis system is often based on sea vessel like Ticonderoga Class and Burke Class, which have restricted radio horizon and line of sight. For example, a radar mounted on a 30Â meter high vessel platform has radio horizon at 19.5Â kilometers range, while a radar mounted on an 18000 m altitude HAA has radio horizon at 478.1Â kilometers range. This is significantly important for detecting low-flying cruise missiles or fighter-bombers. 

The blimp remained a viable military system only until the conventional submarine was replaced by the nuclear submarine. Today, airships are used primarily for command, control and communication platform; to establish and maintain reliable and secure connectivity among all forces, provide transparent data across the echelons; precisely locate friendly and enemy forces; detect targets on an extended battlefield at a minimal exposure to enemy forces; real time targeting; navigation assistance; battle management; monitor radio conversations, etc. These are not tough combat missions.

The lift gas, helium, is not merely inert but acts as a fire extinguisher.

Modern airships have a natural buoyancy and special design that offers a virtually zero catastrophic failure mode. The internal hull pressure is maintained at only 1%-2% above surrounding air pressure, the vehicle is highly tolerant to physical damage or to attack by small arms fire or missiles.

While on long-haul flights weather patterns would be flown to avoid bad weather, the hull's mass largely dampens the effect of turbulence â€?just as a large tanker rides through rough seas.

An airship is usually a poor lightning target, as it is constructed mainly from composite materials. If it is struck, built-in protection devices minimise the risk to the vehicle and its cargo.

A series of structural vulnerability tests were done by the UK Defence Evaluation and Research Agency DERA on a Skyship 600, an earlier airship built by the Munk team to a similar pressure-stabilised design. Several hundred high-velocity bullets were fired through the hull, and even two hours later the vehicle would have been able to return to base. The airship is virtually impervious to automatic rifle and mortar fire: ordnance passes through the envelope without causing critical helium loss. In all instances of light armament fire evaluated under both test and live conditions, the vehicle was able to complete its mission and return to base.















Niue ( in English) is an island nation located in the South Pacific Ocean. It is commonly known as the "Rock of Polynesia." Natives of the island call it "the Rock." Although self-governing, Niue is in free association with New Zealand, meaning that the Sovereign in Right of New Zealand is also Niue's head of state. Most diplomatic relations are conducted by New Zealand on Niue's behalf. Niue is located 2,400 kilometres northeast of New Zealand in a triangle between Tonga, Samoa, and the Cook Islands. The Niuean language and the English language are both taught in schools and used in day-to-day business and communications. The people are predominantly Polynesian.

The first European to sight Niue was Captain James Cook in 1774. Cook made three attempts to land on the island but was refused permission to do so by the Polynesian inhabitants. He named the island "Savage Island" because, legend has it, the natives that "greeted" him were painted in what appeared to Cook and his crew to be blood. However, the substance on their teeth was that of the red banana and not blood.

For the next couple of centuries the island remained known as Savage Island, until its original name Niu Ä“ (coconut behold) regained use. Yet its official name is still NiuÄ“ fekai (wild NiuÄ“).

The next notable European visitors were from the London Missionary Society and arrived in 1846 on the "Messenger of Peace". After many years of trying to land a European missionary on Niue, a Niuean named Nukai Peniamina was taken away and trained as a Pastor at the Malua Theological College in Samoa. Peniamina returned as a missionary with the help of Toimata Fakafitifonua. He was finally allowed to land in Uluvehi Mutalau after a number of attempts in other villages had failed. The Chiefs of Mutalau village allowed Peniamina to land and assigned over 60 warriors to protect him day and night at the fort in Fupiu. Christianity was first taught to the Mutalau people before it was spread to all the villages on Niue; originally, other major villages opposed the introduction of Christianity and had sought to kill Peniamina. The people from the village of Hakupu, although the last village to receive Christianity, came and asked for a "word of god"; hence their village was renamed "Ha Kupu Atua" meaning "any word of god", or "Hakupu" for short.

Niue was a British protectorate for a time, but the UK's involvement ended in 1901 when New Zealand annexed the island. Independence in the form of self-government was granted by the New Zealand parliament with the 1974 constitution. Robert Rex, CMG OBE (who was ethnically part European, part native) was appointed the country's first Premier, a position he continued to hold through re-election until his death 18 years later. Rex became the first Niuean to receive knighthood in 1984.

In January 2004, Niue was hit by Cyclone Heta, which killed two people and caused extensive damage to the entire island, as well as wiping out most of the south of the capital, Alofi.

The Niue Constitution Act vests executive authority in Her Majesty the Queen in Right of New Zealand and the Governor-General of New Zealand. The Niue Constitution specifies that in everyday practice, sovereignty is exercised by the Cabinet of the Premier of Niue and three other ministers. The premier and ministers are members of the Niue Legislative Assembly, the nation's parliament.

The assembly consists of twenty democratically elected members, fourteen of whom are elected by the electors of each village constituency. The remaining six are elected by all registered voters in all constituencies. Electors must be New Zealand citizens, resident for at least three months, and candidates must have been electors, and resident for twelve months. The Speaker is elected by the assembly and is the first official to be elected in the first sitting of the Legislative Assembly following an election. The new Speaker calls for nominations for the Premier; the candidate with the most votes from the twenty members is elected. The Premier then selects three other members to form the Cabinet of Ministers, the executive arm of government. The other two organs of government, following the Westminster model, are the Legislative Assembly and the Judiciary. Terms before new elections last three years.

All Members of Parliament, past or present, are entitled to State Funerals. State Funerals may also be given as well to any distinguished individual offered the honour by the Premier and his Cabinet.

Niue is a 269Â kmÂ² island located in the southern Pacific Ocean, east of Tonga. The geographic coordinates of Niue are .

There are three geographically outlying coral reefs within the Exclusive Economic Zone that do not have any land area:

Niue is one of the world's largest coral islands. The terrain of Niue consists of steep limestone cliffs along the coast with a central plateau rising to about 60 metres above sea level. A coral reef surrounds the island, with the only major break in the reef being in the central western coast, close to the capital, Alofi. A notable feature of the island is the number of limestone caves found close to the coast.

The island is roughly oval in shape (with a diameter of about 18 kilometres), with two large bays indenting the western coast (Alofi Bay in the centre and Avatele Bay in the south). Between these is the promontory of Halagigie Point. A small peninsula, TePÄ Point (or Blowhole Point), is located close to the settlement of Avatele in the southwest. Most of the island's population resides close to the west coast, around the capital, and in the northwest.

The island has a tropical climate, with most rainfall occurring between November and April.

Some of the soils on the island are geochemically very unusual. They are extremely highly weathered tropical soils, with high levels of iron and aluminium oxides (oxisol), but as established by the research of New Zealand scientists starting with Sir Ernest Marsden, they contain surprisingly high levels of natural radioactivity. There is almost no uranium, but the radionucleides Th-230 and Pa-231 head the decay chains. This is the same distribution of elements as found naturally on very deep seabeds, but the geochemical evidence suggests that in the case of Niue the origin is extreme weathering of coral and brief sea submergence 120,000 years ago. A process, "endothermal upwelling" in which mild natural volcanic heat entrains deep seawater up through the porous coral may also contribute.

No adverse health effects from the radioactivity have been demonstrated and calculations show that level of radioactivity would probably be much too low to be detected in the population.

These unusual soils are very rich in phosphate, but it is not accessible to plants, being in the very insoluble form of iron phosphate, or crandallite.

It is thought that rather similar radioactive soils may exist on Lifou and Mare (island) near New Caledonia, and Rennell in the Solomon Islands, but no other locations are known.

Niue has been self-governing, in free association with New Zealand, since 1974. Niue is fully responsible for its internal affairs. Having no military or the resources to maintain a global diplomatic network, New Zealand retains responsibility for the foreign affairs and defence of Niue, but these obligations are only exercised at the request of the government of Niue. The island mainly interacts with the world through its diplomatic mission in Wellington, New Zealand. 

Niue is also a member of the South Pacific Forum and a number of regional and international agencies. It is not a member of the United Nations, but is a state party to the United Nations Convention on the Law of the Sea, the United Nations Framework Convention on Climate Change and the Ottawa Treaty.

Niue established diplomatic relations with the People's Republic of China on December 12, 2007.

Niue's economy is rather small, with a GDP of around $7.6 million estimated in 2000. Most economic activity revolves around the Government, as the Government was traditionally in charge of organising and managing the affairs of the new country since 1974. However, since the economy of Niue has reached a stage where state regulation may now give way to the private sector in Niue's development, there is an ongoing effort to develop the private sector. The Niue Chamber of Commerce is the body representing some of the private businesses on Niue. Following Cyclone Heta the Government made a major commitment towards rehabilitating and developing the private sector in Niue. The Government allocated $1 million for the private sector, which was spent on helping businesses devastated by the cyclone, and on the construction of the Fonuakula Industrial Park. This industrial park is now completed and some businesses are already operating from it. The Fonuakula Industrial Park is managed by the Niue Business Centre, a quasi-governmental organisation providing advisory services to the businesses on Niue.

Most Niuean families grow their own food crops for subsistence and some are sold at the Niue Makete in Alofi while some are exported to their families in New Zealand. The Niuean taro is known in Samoa as Niue taro and in international markets as pink taro. Niue also exports taro to the New Zealand market. The Niue taro is a natural variety and is very resistant to pests.The Niue Government and the Reef Group from New Zealand started two joint ventures in 2003 and 2004 involving the development of the fisheries and noni (Morinda citrifolia, a small tree with edible fruit) in Niue. The Niue Fish Processors, Ltd. (NFP) is a joint venture company processing fresh fish, mainly tuna (yellow fin, big eye and albacore), for export to the overseas markets. NFP operates out of their state-of-the-art fish plant in Amanau Alofi South which was completed and opened in October 2004, where they have facilities for freezing fish, blast freezers and ice towers for producing ice. The fish plant is self-sufficient: they have their own power generators. Currently NFP use desalinated water for their operations. At the moment there are four fishing boats catching fish with more boats expected to join the fleet soon. Niue is greatly concerned with the sustainability of the industry, limiting the number of boats fishing in Niue waters to less than ten at any time. The Niue noni joint venture operates out of the Vaiea farm, which used to be a Government livestock farm and was later used as a quarantine station for alpacas airlifted from Peru to Australia, a scheme which has now ceased. The company planted the biggest noni plantation in the southern hemisphere, consisting of over thirty thousand plants. This may be the first time noni has been commercially cultivated in an open field, because noni usually grows in the wild on Niue. There is also a factory at the farm for extracting the juice of the noni which is exported to New Zealand for bottling.

In August 2005, an Australian mining company, Yamarna Goldfields, suggested that Niue might have the world's largest deposit of uranium. By early September, these hopes were seen as overoptimistic, and in late October the company cancelled its plans to mine, announcing that exploration drilling had identified nothing of commercial value. The Australian Securities and Investments Commission filed charges in January 2007 against two directors of the company, now called Mining Projects Group Ltd, alleging that their conduct was deceptive and they engaged in insider trading. There is an Australian company that had been issued a mineral prospecting license in the early 1970s which is still very active in doing research and collecting data on potential mineral deposits on Niue.

Remittances from Niuean expatriates used to be one of the major sources of foreign exchange in the 1970s and early 1980s. The continuous migration of Niueans to New Zealand, however, has shifted most members of nuclear and extended families to New Zealand, removing the need to send remittances back home. In the late 1990s PFTAC conducted studies on the Niue balance of payments, which confirms that Niueans are receiving little remittances but are sending more monies overseas, mainly for paying for imported goods and for the education of Niuean students sent to study in New Zealand. 

Foreign aid, principally from New Zealand, has been the island's principal source of income. Tourism has been identified as one of the three priority economic sectors (the other two are Fisheries and Agriculture) for economic development in Niue. In 2006 estimated visitor expenditure reached $1.6million making Tourism a major export industry for Niue. Niue will continue to receive direct support from the Government and overseas donor agencies. Air New Zealand is the sole airline serving Niue, flying to Niue once a week. It took over after Polynesian Airlines stopped flying in November 2005. There is currently a tourism development strategy to increase the number of rooms available to overseas tourists at a sustainable level. Niue is also trying to attract foreign investors to invest in the tourism industry of Niue by offering import and company tax concessions as incentives. The number of tourists visiting Niue is increasing, climbing from 1939 in 2000 to 1446 in 2001, 2084 in 2002, 2706 in 2003, 2550 in 2004, and 2793 in 2005. The main purpose of their visits in 2005 were: holiday (1236), business (664), visiting friends and relatives (591) and other reasons (302). In 2005 tourists came from the following countries: Australia (304), New Zealand (1529), the South Pacific (296), Other Pacific (99), USA (136), Canada (45), UK (99), Germany (31), France (37), Other European countries (128), Japan (8) and other Asian countries (36).

Government expenses consistently exceed revenue to a substantial degree, with aid from New Zealand subsidizing public service payrolls. The government also generates some revenue, mainly from income tax, import tax and the lease of phone lines. The government briefly flirted with the creation of "offshore banking", but, under pressure from the US Treasury, agreed to end its support for schemes designed to minimize tax in countries like New Zealand. Niue now provides an automated Companies Registration (www.companies.gov.nu), which is administered by the New Zealand Ministry of Economic Development.

Niue has licensed the .nu top-level domain on the Internet to a private company .NU Domain, but the company and the Government of Niue now dispute the amount and type of compensation that Niue receives from the licensor.

Niue's economy suffered from the devastating tropical Cyclone Heta of 2004.

Niue uses the New Zealand dollar.



Rugby union is a popular sport in Niue.











Angkor Wat (or Angkor Vat) (Khmer: áž¢áž„áŸ’áž‚ážšážœážáŸ’áž?, a World Heritage Site, is a temple at Angkor, Cambodia, built for King Suryavarman II in the early 12th century as his state temple and capital city. As the best-preserved temple at the site, it is the only one to have remained a significant religious centre since its foundationâ€”first Hindu, dedicated to Vishnu, then Buddhist. The temple is the epitome of the high classical style of Khmer architecture. It has become a symbol of Cambodia, appearing on its national flag, and it is the country's prime attraction for visitors.

Angkor Wat combines two basic plans of Khmer temple architecture: the temple mountain and the later galleried temple. It is designed to represent Mount Meru, home of the gods in Hindu mythology: within a moat and an outer wall 3.6 km (2.2 miles) long are three rectangular galleries, each raised above the next. At the centre of the temple stands a quincunx of towers. Unlike most Angkorian temples, Angkor Wat is oriented to the west; scholars are divided as to the significance of this. The temple is admired for the grandeur and harmony of the architecture, its extensive bas-reliefs and for the numerous devatas (guardian spirits) adorning its walls.

The modern name, Angkor Wat, in use by the 16th century, means "City Temple": Angkor is a vernacular form of the word nokor which comes from the Sanskrit word nagara (capital), while wat is the Khmer word for temple. Prior to this time the temple was known as Preah Pisnulok, after the posthumous title of its founder, Suryavarman II.

Angkor Wat lies 5.5 km north of the modern town of Siem Reap, and a short distance south and slightly east of the previous capital, which was centred on the Baphuon. It is in an area of Cambodia where there are an important group of ancient structures. It is the southernmost of Angkor's main sites.

The initial design and construction of the temple took place in the first half of the 12th century, during the reign of Suryavarman II (ruled 1113â€“c. 1150), Dedicated to Vishnu, it was built as the king's state temple and capital city. As neither the foundation stela nor any contemporary inscriptions referring to the temple have been found, its original name is unknown, but it may have been known as Vrah Vishnulok after the presiding deity. Work seems to have ended on the king's death, leaving some of the bas-relief decoration unfinished. In 1177 Angkor was sacked by the Chams, the traditional enemies of the Khmer. Thereafter the empire was restored by a new king, Jayavarman VII, who established a new capital and state temple (Angkor Thom and the Bayon respectively) a few kilometres to the north.

In the 14th or 15th century the temple was converted to Theravada Buddhist use, which continues to the present day. Angkor Wat is unusual among the Angkor temples in that although it was somewhat neglected after the 16th century it was never completely abandoned, its preservation being due in part to the fact that its moat also provided some protection from encroachment by the jungle.

One of the first Western visitors to the temple was Antonio da Magdalena, a Portuguese monk who visited in 1586 and said that it "is of such extraordinary construction that it is not possible to describe it with a pen, particularly since it is like no other building in the world. It has towers and decoration and all the refinements which the human genius can conceive of". However, the temple was popularised in the West only in the mid-19th century on the publication of Henri Mouhot's travel notes. The French explorer wrote of it:"One of these templesâ€”a rival to that of Solomon, and erected by some ancient Michelangeloâ€”might take an honourable place beside our most beautiful buildings. It is grander than anything left to us by Greece or Rome, and presents a sad contrast to the state of barbarism in which the nation is now plunged."

Mouhot, like other early Western visitors, was unable to believe that the Khmers could have built the temple, and mistakenly dated it to around the same era as Rome. The true history of Angkor Wat was pieced together only from stylistic and epigraphic evidence accumulated during the subsequent clearing and restoration work carried out across the whole Angkor site.Angkor Wat required considerable restoration in the 20th century, mainly the removal of accumulated earth and vegetation. Work was interrupted by the civil war and Khmer Rouge control of the country during the 1970s and 1980s, but relatively little damage was done during this period other than the theft and destruction of mostly post-Angkorian statues.

The temple has become a symbol of Cambodia, and is a source of great national pride. A depiction of Angkor Wat has been a part of every Cambodian national flag since the introduction of the first version circa 1863â€”the only building to appear on any national flag. In January 2003 riots erupted in Phnom Penh when a false rumour circulated that a Thai soap opera actress had claimed that Angkor Wat belonged to Thailand.

Angkor Wat, located at , is a unique combination of the temple mountain, the standard design for the empire's state temples, the later plan of concentric galleries, and influences from Orissa and the Chola of Tamil Nadu, India. The temple is a representation of Mount Meru, the home of the gods: the central quincunx of towers symbolises the five peaks of the mountain, and the walls and moat the surrounding mountain ranges and ocean. Access to the upper areas of the temple was progressively more exclusive, with the laity being admitted only to the lowest level.

Unlike most Khmer temples, Angkor Wat is oriented to the west rather than the east. This has led many (including Glaize and George CoedÃ¨s) to conclude that Suryavarman intended it to serve as his funerary temple. Further evidence for this view is provided by the bas-reliefs, which proceed in a counter-clockwise directionâ€”prasavya in Hindu terminologyâ€”as this is the reverse of the normal order. Rituals take place in reverse order during Brahminic funeral services. The archaeologist Charles Higham also describes a container which may have been a funerary jar which was recovered from the central tower. Freeman and Jacques, however, note that several other temples of Angkor depart from the typical eastern orientation, and suggest that Angkor Wat's alignment was due to its dedication to Vishnu, who was associated with the west.

A further interpretation of Angkor Wat has been proposed by Eleanor Mannikka. Drawing on the temple's alignment and dimensions, and on the content and arrangement of the bas-reliefs, she argues that these indicate a claimed new era of peace under king Suryavarman II: "as the measurements of solar and lunar time cycles were built into the sacred space of Angkor Wat, this divine mandate to rule was anchored to consecrated chambers and corridors meant to perpetuate the king's power and to honor and placate the deities manifest in the heavens above." Mannikka's suggestions have been received with a mixture of interest and scepticism in academic circles. She distances herself from the speculations of others, such as Graham Hancock, that Angkor Wat is part of a representation of the constellation Draco.



Angkor Wat is the prime example of the classical style of Khmer architectureâ€”the Angkor Wat styleâ€”to which it has given its name. By the 12th century Khmer architects had become skilled and confident in the use of sandstone (rather than brick or laterite) as the main building material. Most of the visible areas are of sandstone blocks, while laterite was used for the outer wall and for hidden structural parts. The binding agent used to join the blocks is yet to be identified, although natural resins or slaked lime have been suggested.

Angkor Wat has drawn praise above all for the harmony of its design, which has been compared to the architecture of ancient Greece or Rome. According to Maurice Glaize, a mid-20th-century conservator of Angkor, the temple "attains a classic perfection by the restrained monumentality of its finely balanced elements and the precise arrangement of its proportions. It is a work of power, unity and style."

Architecturally, the elements characteristic of the style include: the ogival, redented towers shaped like lotus buds; half-galleries to broaden passageways; axial galleries connecting enclosures; and the cruciform terraces which appear along the main axis of the temple. Typical decorative elements are devatas (or apsaras), bas-reliefs, and on pediments extensive garlands and narrative scenes. The statuary of Angkor Wat is considered conservative, being more static and less graceful than earlier work. Other elements of the design have been destroyed by looting and the passage of time, including gilded stucco on the towers, gilding on some figures on the bas-reliefs, and wooden ceiling panels and doors.

The Angkor Wat style was followed by that of the Bayon period, in which quality was often sacrificed to quantity. Other temples in the style are Banteay SamrÃ©, Thommanon, Chao Say Tevoda and the early temples of Preah Pithu at Angkor; outside Angkor, Beng Mealea and parts of Phanom Rung and Phimai.



The outer wall, 1025 by 802 m and 4.5 m high, is surrounded by a 30 m apron of open ground and a moat 190 m wide. Access to the temple is by an earth bank to the east and a sandstone causeway to the west; the latter, the main entrance, is a later addition, possibly replacing a wooden bridge. There are gopuras at each of the cardinal points; the western is much the largest and has three ruined towers. Glaize notes that this gopura both hides and echoes the form of the temple proper. Under the southern tower is a statue of Vishnu, known as Ta Reach, which may originally have occupied the temple's central shrine. Galleries run between the towers and as far as two further entrances on either side of the gopura often referred to as "elephant gates", as they are large enough to admit those animals. These galleries have square pillars on the outer (west) side and a closed wall on the inner (east) side. The ceiling between the pillars is decorated with lotus rosettes; the west face of the wall with dancing figures; and the east face of the wall with balustered windows, dancing male figures on prancing animals, and devatas, including (south of the entrance) the only one in the temple to be showing her teeth.

The outer wall encloses a space of 820,000 square metres (203 acres), which besides the temple proper was originally occupied by the city and, to the north of the temple, the royal palace. Like all secular buildings of Angkor, these were built of perishable materials rather than of stone, so nothing remains of them except the outlines of some of the streets. Most of the area is now covered by forest. A 350 m causeway connects the western gopura to the temple proper, with naga balustrades and six sets of steps leading down to the city on either side. Each side also features a library with entrances at each cardinal point, in front of the third set of stairs from the entrance, and a pond between the library and the temple itself. The ponds are later additions to the design, as is the cruciform terrace guarded by lions connecting the causeway to the central structure.

The temple stands on a terrace raised higher than the city. It is made of three rectangular galleries rising to a central tower, each level higher than the last. Mannikka interprets these galleries as being dedicated to the king, Brahma, the moon, and Vishnu. Each gallery has a gopura at each of the points, and the two inner galleries each have towers at their corners, forming a quincunx with the central tower. Because the temple faces west, the features are all set back towards the east, leaving more space to be filled in each enclosure and gallery on the west side; for the same reason the west-facing steps are shallower than those on the other sides.The outer gallery measures 187 by 215 m, with pavilions rather than towers at the corners. The gallery is open to the outside of the temple, with columned half-galleries extending and buttressing the structure. Connecting the outer gallery to the second enclosure on the west side is a cruciform cloister called Preah Poan (the "Hall of a Thousand Buddhas"). Buddha images were left in the cloister by pilgrims over the centuries, although most have now been removed. This area has many inscriptions relating the good deeds of pilgrims, most written in Khmer but others in Burmese and Japanese. The four small courtyards marked out by the cloister may originally have been filled with water. North and south of the cloister are libraries.

Beyond, the second and inner galleries are connected to each other and to two flanking libraries by another cruciform terrace, again a later addition. From the second level upwards, devatas abound on the walls, singly or in groups of up to four. The second-level enclosure is 100 by 115 m, and may originally have been flooded to represent the ocean around Mount Meru. Three sets of steps on each side lead up to the corner towers and gopuras of the inner gallery. The very steep stairways represent the difficulty of ascending to the kingdom of the gods. This inner gallery, called the Bakan, is a 60 m square with axial galleries connecting each gopura with the central shrine, and subsidiary shrines located below the corner towers. The roofings of the galleries are decorated with the motif of the body of a snake ending in the heads of lions or garudas. Carved lintels and pediments decorate the entrances to the galleries and to the shrines.The tower above the central shrine rises 43 m to a height of 65 m above the ground; unlike those of previous temple mountains, the central tower is raised above the surrounding four. The shrine itself, originally occupied by a statue of Vishnu and open on each side, was walled in when the temple was converted to Theravada Buddhism, the new walls featuring standing Buddhas. In 1934, the conservator George TrouvÃ© excavated the pit beneath the central shrine: filled with sand and water it had already been robbed of its treasure, but he did find a sacred foundation deposit of gold leaf two metres above ground level.

Integrated with the architecture of the building, and one of the causes for its fame is Angkor Wat's extensive decoration, which predominantly takes the form of bas-relief friezes.The inner walls of the outer gallery bear a series of large-scale scenes mainly depicting episodes from the Hindu epics the Ramayana and the Mahabharata. Higham has called these, "the greatest known linear arrangement of stone carving". From the north-west corner anti-clockwise, the western gallery shows the Battle of Lanka (from the Ramayana, in which Rama defeats Ravana) and the Battle of Kurukshetra (from the Mahabharata, showing the mutual annihilation of the Kaurava and Pandava clans). On the southern gallery follow the only historical scene, a procession of Suryavarman II, then the 32 hells and 37 heavens of Hindu mythology.Glaize writes of;"...those unfortunate souls who are to be thrown down to hell to suffer a refined cruelty which, at times, seems to be a little disproportionate to the severity of the crimes committed. So it is that people who have damaged others' property have their bones broken, that the glutton is cleaved in two, that rice thieves are afflicted with enormous bellies of hot iron, that those who picked the flowers in the garden of Shiva have their heads pierced with nails, and thieves are exposed to cold discomfort."On the eastern gallery is one of the most celebrated scenes, the Churning of the Sea of Milk, showing 92 asuras and 88 devas using the serpent Vasuki to churn the sea under Vishnu's direction (Mannikka counts only 91 asuras, and explains the asymmetrical numbers as representing the number of days from the winter solstice to the spring equinox, and from the equinox to the summer solstice). It is followed by Vishnu defeating asuras (a 16th-century addition). The northern gallery shows Krishna's victory over Bana (where according to Glaize, "The workmanship is at its worst") and a battle between the Hindu gods and asuras. The north-west and south-west corner pavilions both feature much smaller-scale scenes, some unidentified but most from the Ramayana or the life of Krishna.

Since the 1990s, Angkor Wat has seen a resumption of conservation efforts and a massive increase in tourism. The temple is part of the Angkor World Heritage Site, established in 1992, which has provided some funding and has encouraged the Cambodian government to protect the site. The German Apsara Conservation Project (GACP) is working to protect the devatas and other bas-reliefs which decorate the temple from damage. The organisation's survey found that around 20% of the devatas were in very poor condition, mainly because of natural erosion and deterioration of the stone but in part also due to earlier restoration efforts. Other work involves the repair of collapsed sections of the structure, and prevention of further collapse: the west facade of the upper level, for example, has been buttressed by scaffolding since 2002, while a Japanese team completed restoration of the north library of the outer enclosure in 2005.

Angkor Wat has become a major tourist destination: attendance figures for the temple are not published, but in 2004 the country received just over a million international arrivals, of whom according to the Ministry of Tourism 57% planned to visit the temple. The influx of tourists has so far caused relatively little damage, other than some graffiti; ropes and wooden steps have been introduced to protect the bas-reliefs and floors, respectively. Tourism has also provided some additional funds for maintenanceâ€”approximately 28% of ticket revenues across the whole Angkor site is spent on the templesâ€”although most work is carried out by foreign government-sponsored teams rather than by the Cambodian authorities.













A bridge is a structure built to span a gorge, valley, road, railroad track, river, body of water, or any other physical obstacle. Designs of bridges will vary depending on the function of the bridge and the nature of the terrain where the bridge is to be constructed.

The first bridges were made by nature â€?as simple as a log fallen across a stream. The first bridges made by humans were probably spans of wooden logs or planks and eventually stones, using a simple support and crossbeam arrangement. Most of these early bridges could not support heavy weights or withstand strong currents. It was these inadequacies which led to the development of better bridges.

The ancient Romans built arch bridges and aqueducts that could stand in conditions that would damage or destroy earlier designs. Some of them still stand today. An example is the AlcÃ¡ntara Bridge, built over the river Tagus, in Spain. Most earlier bridges would have been swept away by the strong current. The Romans also used cement, which reduced the variation of strength found in natural stone. One type of cement, called pozzolana, consisted of water, lime, sand, and volcanic rock. Brick and mortar bridges were built after the Roman era, as the technology for cement was lost then later rediscovered.

Although large Chinese bridges of wooden construction existed at the time of the Warring States, the oldest surviving stone bridge in China is the Zhaozhou Bridge, built from 595 to 605 AD during the Sui Dynasty. This bridge is also historically significant as it is the world's oldest open-spandrel stone segmental arch bridge. European segmental arch bridges date back to at least the AlconÃ©tar Bridge (approximately 2nd century AD), while the enormous Roman era Trajan's Bridge (105 AD) featured open-spandrel segmental arches in wooden construction. 

Rope bridges, a simple type of suspension bridge, were used by the Inca civilization in the Andes mountains of South America, just prior to European colonization in the 1500s.

During the 18th century there were many innovations in the design of timber bridges by Hans Ulrich, Johannes Grubenmann, and others. The first book on bridge engineering was written by Hubert Gautier in 1716. 

With the Industrial Revolution in the 19th century, truss systems of wrought iron were developed for larger bridges, but iron did not have the tensile strength to support large loads. With the advent of steel, which has a high tensile strength, much larger bridges were built, many using the ideas of Gustave Eiffel.

The Oxford English Dictionary traces the origin of the word bridge to an Old English word brycg, of the same meaning, derived from a hypothetical Proto-Germanic root brugjÅ. There are cognates in other Germanic languages (for instance BrÃ¼cke in German, brug in Dutch, brÃºgv in Faroese or bro in Danish, Norwegian and Swedish).

Another theory suggests that "bridge" comes from Turkish "kÃ¶prÃ¼" (lit. bridge). It is highly possible that Turkish lent this word to Eastern European languages and then, in time, it arrived in English. "KÃ¶prÃ¼" itself is derived from "kÃ¶prÃ¼k (kÃ¶pÃ¼k)" which literally means "foam".

The word for the Pope, pontiff, comes from the Latin word pontifex meaning "bridge builder" or simply "builder". The word "Pope" however comes from "papa" meaning "father".

There are six main types of bridges: beam bridges, cantilever bridges, arch bridges, suspension bridges, cable-stayed bridges and truss bridges.

A bridge is designed for trains, pedestrian or road traffic, a pipeline or waterway for water transport or barge traffic. An aqueduct is a bridge that carries water, resembling a viaduct, which is a bridge that connects points of equal height.A Road-rail bridge carries both road and rail traffic. 

Bridges are subject to unplanned uses as well. The areas underneath some bridges have become makeshift shelters and homes to homeless people, and the undersides of bridges all around the world are spots of prevalent graffiti. Some bridges attract people attempting suicide, and become known as a suicide bridges.

To create a beautiful image, some bridges are built much taller than necessary. This type, often found in east-Asian style gardens, is called a Moon bridge, evoking a rising full moon. Other garden bridges may cross only a dry bed of stream washed pebbles, intended only to convey an impression of a stream. Often in palaces a bridge will be built over an artificial waterway as symbolic of a passage to an important place or state of mind. A set of five bridges cross a sinuous waterway in an important courtyard of the Forbidden City in Beijing, the People's Republic of China. The central bridge was reserved exclusively for the use of the Emperor, Empress, and their attendants.

 Bridges may be classified by how the four forces of tension, compression, bending and shear are distributed through their structure. Most bridges will employ all of the principal forces to some degree, but only a few will predominate. The separation of forces may be quite clear. In a suspension or cable-stayed span, the elements in tension are distinct in shape and placement. In other cases the forces may be distributed among a large number of members, as in a truss, or not clearly discernible to a casual observer as in a box beam. Bridges can also be classified by their lineage, which is shown as the vertical axis on the diagram to the right.

A bridge's structural efficiency may be considered to be the ratio of load carried to bridge mass, given a specific set of material types. In one common challenge students are divided into groups and given a quantity of wood sticks, a distance to span, and glue, and then asked to construct a bridge that will be tested to destruction by the progressive addition of load at the center of the span. The bridge taking the greatest load is by this test the most structurally efficient. A more refined measure for this exercise is to weigh the completed bridge rather than measure against a fixed quantity of materials provided and determine the multiple of this weight that the bridge can carry, a test that emphasizes economy of materials and efficient glue joints (see ''balsa wood bridge'').

A bridge's economic efficiency will be site and traffic dependent, the ratio of savings by having a bridge (instead of, for example, a ferry, or a longer road route) compared to its cost. The lifetime cost is composed of materials, labor, machinery, engineering, cost of money, insurance, maintenance, refurbishment, and ultimately, demolition and associated disposal, recycling, and replacement, less the value of scrap and reuse of components. Bridges employing only compression are relatively inefficient structurally, but may be highly cost efficient where suitable materials are available near the site and the cost of labor is low. For medium spans, trusses or box beams are usually most economical, while in some cases, the appearance of the bridge may be more important than its cost efficiency. The longest spans usually require suspension bridges.

















Life is a condition that distinguishes organisms from inorganic objects, i.e. non-life, and dead organisms, being manifested by growth through metabolism, reproduction, and the power of adaptation to environment through changes originating internally. A physical characteristic of life is that it feeds on negative entropy. In more detail, according to physicists such as John Bernal, Erwin SchrÃ¶dinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena which are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form (see: entropy and life). 

A diverse array of living organisms can be found in the biosphere on Earth. Properties common to these organismsâ€”plants, animals, fungi, protists, archaea and bacteriaâ€”are a carbon- and water-based cellular form with complex organization and heritable genetic information. They undergo metabolism, possess a capacity to grow, respond to stimuli, reproduce and, through natural selection, adapt to their environment in successive generations.

An entity with the above properties is considered to be a living organism, that is an organism that is alive hence can be called a life form. However, not every definition of life considers all of these properties to be essential. For example, the capacity for descent with modification is often taken as the only essential property of life. This definition notably includes viruses, which do not qualify under narrower definitions as they are acellular and do not metabolize. Broader definitions of life may also include theoretical non-carbon-based life and other alternative biology. Some forms of artificial life, however, especially wet artificial life, might alternatively be classified as real life.

There is no universal definition of life; there are a variety of definitions proposed by different scientists. To define life in unequivocal terms is still a challenge for scientists.

Conventional definition: Often scientists say that life is a characteristic of organisms that exhibit the following phenomena:



However, others cite several limitations of this definition. Thus, many members of several species do not reproduce, possibly because they belong to specialized sterile castes (such as ant workers), these are still considered forms of life. One could say that the property of life is inherited; hence, sterile or hybrid organisms such as mules, ligers, and eunuchs are alive although they are not capable of self-reproduction. However, (a) The species as a whole does reproduce, (b) There are no cases of species where 100% of the individuals reproduce, and (c) specialized non-reproducing individuals of the species may still partially propagate their DNA or other master pattern through mechanisms such as kin selection. 

Viruses and aberrant prion proteins are often considered replicators rather than forms of life, a distinction warranted because they cannot reproduce without very specialized substrates such as host cells or proteins, respectively. Also, the Rickettsia and Chlamydia are examples of bacteria that cannot independently fulfill many vital biochemical processes, and depend on entry, growth, and replication within the cytoplasm of eukaryotic host cells. However, most forms of life rely on foods produced by other species, or at least the specific chemistry of Earth's environment. 

Still others contest such definitions of life on philosophical grounds. They offer the following as examples of life: viruses which reproduce; storms or flames which "burn"; certain computer software programs which are programmed to mutate and evolve; future software programs which may evince (even high-order) behavior; machines which can move; and some forms of proto-life consisting of metabolizing cells without the ability to reproduce. Still, most scientists would not call such phenomena expressive of life. Generally all seven characteristics are required for a population to be considered a life form.

The systemic definition of life is that living things are self-organizing and autopoietic (self-producing). These objects are not to be confused with dissipative structures (e.g. fire). 

Variations of this definition include Stuart Kauffman's definition of life as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle.

Proposed definitions of life include: 

The above definition includes worker caste ants, viruses and mules while precluding flames. It also explains why bees can be alive and yet commit suicide in defending their hive. They are only individual instances of the living system that comprises all life forms on planet Earth (which is the only living system known to mankind).

Although it cannot be pinpointed exactly, evidence suggests that life on Earth has existed for about 3.7 billion years.

There is no truly "standard" model for the origin of life, but most currently accepted scientific models build in one way or another on the following discoveries, which are listed roughly in order of postulated emergence:

There are many different hypotheses regarding the path that might have been taken from simple organic molecules to protocells and metabolism. Many models fall into the "genes-first" category or the "metabolism-first" category, but a recent trend is the emergence of hybrid models that do not fit into either of these categories.

Traditionally, people have divided organisms into the classes of plants and animals, based mainly on their ability of movement. The first known attempt to classify organisms, as per personal observations, was conducted by the Greek philosopher Aristotle.

He classified all living organisms known at that time as either a plant or an animal. Aristotle distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively. He divided the blooded animals into five groups: viviparous quadrupeds (mammals), birds, oviparous quadrupeds (reptiles and amphibians), fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which also included the spiders, scorpions, and centipedes, in addition to what we now define as insects), shelled animals (such as most molluscs and echinoderms) and "zoophytes". Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time, and remained the ultimate authority for many centuries after his death. His observations on the anatomy of octopus, cuttlefish, crustaceans, and many other marine invertebrates are remarkably accurate, and could only have been made from first-hand experience with dissection.

The exploration of parts of the New World produced large numbers of new plants and animals that needed descriptions and classification. The old systems made it difficult to study and locate all these new specimens within a collection and often the same plants or animals were given different names because the number of specimens were too large to memorize. A system was needed that could group these specimens together so they could be found, the binomial system was developed based on morphology with groups having similar appearances. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced, which, directed first to familiar kinds, was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification.

Carolus Linnaeus is best known for his introduction of the method still used to formulate the scientific name of every species. Before Linnaeus, long many-worded names (composed of a generic name and a differentia specifica) had been used, but as these names gave a description of the species, they were not fixed. In his Philosophia Botanica (1751) Linnaeus took every effort to improve the composition and reduce the length of the many-worded names by abolishing unnecessary rhetorics, introducing new descriptive terms and defining their meaning with an unprecedented precision. In the late 1740s Linnaeus began to use a parallel system of naming species with nomina trivialia. Nomen triviale, a trivial name, was a single- or two-word epithet placed on the margin of the page next to the many-worded "scientific" name. The only rules Linnaeus applied to them was that the trivial names should be short, unique within a given genus, and that they should not be changed. Linnaeus consistently applied nomina trivialia to the species of plants in Species Plantarum (1st edn. 1753) and to the species of animals in the 10th edition of Systema Naturae (1758). By consistently using these specific epithets, Linnaeus separated nomenclature from taxonomy. Even though the parallel use of nomina trivialia and many-worded descriptive names continued until late in the eighteenth century, it was gradually replaced by the practice of using shorter proper names combined of the generic name and the trivial name of the species. In the nineteenth century, this new practice was codified in the first Rules and Laws of Nomenclature, and the 1st edn. of Species Plantarum and the 10th edn. of Systema Naturae were chosen as starting points for the Botanical and Zoological Nomenclature respectively. This convention for naming species is referred to as binomial nomenclature. Today, nomenclature is regulated by Nomenclature Codes, which allows names divided into ranks; separately for botany and for zoology. Whereas Linnaeus classified for ease of identification, it is now generally accepted that classification should reflect the Darwinian principle of common descent.

The Fungi have long been a problematic group in the biological classification: Originally, they were treated as plants. For a short period Linnaeus had placed them in the taxon Vermes in Animalia because he was misinformed: the hyphae were said to have been worms. He later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. As it turned out, the fungi are more closely related to animals than to plants.

As new discoveries enabled us to study cells and microorganisms, new groups of life where revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in Protozoa as animals and Protophyta/Thallophyta as plants, but were united by Haeckel in his kingdom Protista, later the group of prokaryotes were split of in the kingdom Monera, eventually this kingdom would be divided in two separate groups, the Bacteria and the Archaea, leading to the six-kingdom system and eventually to the three-domain system. The 'remaining' protists would later be divided into smaller groups in clades in relation to more complex organisms. Thomas Cavalier-Smith, who has published extensively on the classification of protists, has recently proposed that the Neomura, the clade which groups together the Archaea and Eukarya, would have evolved from Bacteria, more precisely from Actinobacteria.

As microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, sometimes these are considered to be alive and are treated in the domain of non-cellular life named Acytota or Aphanobionta.

And thus all the primary taxonomical ranks were established: Domain, Kingdom, Phylum, Class, Order, Family, Genus, Species

Since the 1960s a trend called cladistics has emerged, arranging taxa in an evolutionary or phylogenetic tree. If a taxon includes all the descendants of some ancestral form, it is called monophyletic, as opposed to paraphyletic, groups based on traits which have evolved separately and where the most recent common ancestor is not included are called polyphyletic.

A new formal code of nomenclature, the PhyloCode, to be renamed "International Code of Phylogenetic Nomenclature" (ICPN), is currently under development, intended to deal with clades, which do not have set ranks, unlike conventional Linnaean taxonomy. It is unclear, should this be implemented, how the different codes will coexist.





Earth is the only planet in the universe known to harbour life. The Drake equation has been used to estimate the probability of life elsewhere, but scientists disagree on many of the values of variables in this equation (although strictly speaking Drake equation estimates relate the number of extraterrestrial civilizations in our galaxy with which we might come in contact - not probability of life elsewhere). Depending on those values, the equation may either suggest that life arises frequently or infrequently. Drake himself estimated the number of civilizations in our galaxy with which we might expect to be able to communicate at any given time as equal to one.

Relating to the origin of life on Earth, panspermia and exogenesis are theories proposing that life originated elsewhere in the universe and was subsequently transferred to Earth perhaps via meteorites, comets or cosmic dust. However those theories do not help explain the origin of this extraterrestrial life.













Genetics, a discipline of biology, is the science of heredity and variation in living organisms. Knowledge of the inheritance of characteristics has been implicitly used since prehistoric times for improving crop plants and animals through selective breeding. However, the modern science of genetics, which seeks to understand the mechanisms of inheritance, only began with the work of Gregor Mendel in the mid-1800s. Although he did not know the physical basis for heredity, Mendel observed that inheritance is fundamentally a discrete process with specific traits that are inherited in an independent manner â€?these basic units of inheritance are now called genes.

Following the rediscovery of Mendel's observations in the early 1900s, research in 1910s yielded the first physical understanding of inheritance â€?that genes are arranged linearly along large cellular structures called chromosomes. By the 1950s it was understood that the core of a chromosome was a long molecule called DNA and genes existed as linear sections within the molecule. A single strand of DNA is a chain of four types of nucleotides; hereditary information is contained within the sequence of these nucleotides. Solved by Watson and Crick in 1953, DNA's three-dimensional structure is a double-stranded helix, with the nucleotides on each strand complementary to each other. Each strand acts as a template for synthesis of a new partner strand, providing the physical mechanism for the inheritance of information.

The sequence of nucleotides in DNA is used to produce specific sequences of amino acids, creating proteins â€?a correspondence known as the "genetic code". This sequence of amino acids in a protein determines how it folds into a three-dimensional structure, this structure is in turn responsible for the protein's function. Proteins are responsible for almost all functional roles in the cell. A change to DNA sequence can change a protein's structure and behavior, and this can have dramatic consequences in the cell and on the organism as a whole.

Although genetics plays a large role in determining the appearance and behavior of organisms, it is the interaction of genetics with the environment an organism experiences that determines the ultimate outcome. For example, while genes play a role in determining a person's height, the nutrition and health that person experiences in childhood also have a large effect.





Although the science of genetics has its origins in the work of Gregor Mendel in the mid-1800s, various theories of inheritance preceded Mendel. These theories generally assumed that there existed an inheritance of acquired characteristics: the belief that individuals inherit traits that have been strengthened in their parents. Today, the theory is commonly associated with Jean-Baptiste Lamarck, who used this pattern of inheritance to explain the evolution of various traits within species (these changes are now understood to be the product of natural selection rather than a product of soft inheritance).

The modern science of genetics traces its roots to the observations made by Gregor Johann Mendel, a German-Czech Augustinian monk and scientist who made detailed studies of the nature of inheritance in plants. In his paper "Versuche Ã¼ber Pflanzenhybriden" ("Experiments on Plant Hybridization"), presented in 1865 to the Brunn Natural History Society, Gregor Mendel traced the inheritance patterns of certain traits in pea plants and showed that they could be described mathematically. Although not all features show these patterns of Mendelian inheritance, his work suggested the utility of the application of statistics to the study of inheritance.

The significance of Mendel's observations was not understood until early in the twentieth century, after his death, when his research was re-discovered by other scientists working on similar problems. The word "genetics" itself was coined in 1905 by William Bateson, a significant proponent of Mendel's work, in a letter to Adam Sedgwick. (The adjective "genetic", derived from the Greek word "genno" Î³ÎµÎ½Î½ÏŽ: to give birth, predates the noun and dates back to the 1830's; it was first used in the biological sense in 1859 by Charles Darwin in the The Origin of Species.) Bateson publicly promoted and popularized usage of word "genetics" to describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization in London, England, in 1906.



In the decades following rediscovery and popularization of Mendel's work, numerous experiments sought to elucidate the molecular basis of DNA. In 1910 Thomas Hunt Morgan argued that genes reside on chromosomes, based on observations of a sex-linked white eye mutation in fruit flies. In 1913 his student Alfred Sturtevant used the phenomenon of genetic linkage and the associated recombination rates to demonstrate and map the linear arrangement of genes upon the chromosome.



Although chromosomes were known to contain genes, chromosomes were composed of both protein and DNA â€?it was unknown which was critical for heredity or how the process occurred. In 1928, Frederick Griffith published his discovery of the phenomenon of transformation (see Griffith's experiment); sixteen years later, in 1944, Oswald Theodore Avery, Colin McLeod and Maclyn McCarty used this phenomenon to isolate and identify the molecule responsible for transformation as DNA. The Hershey-Chase experiment in 1952 identified DNA (rather than protein) as the genetic material of viruses, further evidence that DNA was the molecule responsible for inheritance.

James D. Watson and Francis Crick resolved the structure of DNA in 1953, using the X-ray crystallography work of Rosalind Franklin that indicated the molecule had a helical structure. Their double-helix model paired a sequence of nucleotides with a "complement" on the other strand. This structure not only provided a physical explanation for information contained within the order of the nucleotides, but also a physical mechanism for duplication through separation of strands and the reconstruction of a partner strand based on the nucleotide pairings. Although the structure explained the process of inheritance, it was still unknown how DNA influenced the behavior of cells. In the following years many scientists sought to understand how DNA controls the process of protein production within ribosomes, eventually discovering the transcription of DNA into messenger RNA and uncovering the genetic code which links the nucleotide sequence of messenger RNA to the amino acid sequence of protein.

With this molecular understanding of DNA, an explosion of research based on this understanding of the molecular nature of DNA became possible. The development of chain-termination DNA sequencing in 1977 enabled the determination of nucleotide sequences on DNA, and the PCR method developed by Kary Banks Mullis in 1983 allowed the isolation and amplification of arbitrary segments of DNA. These and other techniques, through the pooled efforts of the Human Genome Project and parallel private effort by Celera Genomics, culminated in the sequencing of the human genome in 2001.



At its most fundamental level, inheritance in organisms occurs by means of discrete traits, called "genes". This property was first observed by Gregor Mendel, who studied the segregation of heritable traits in pea plants. In his experiments studying the trait for flower color, Mendel observed that the flowers of each pea plant were either purple or white â€?and never an intermediate between the two colors. These different, discrete versions of the same gene are called "alleles".

In the case of pea plants, each organism has two alleles of each gene, and the plants inherit one allele from each parent. Many organisms, including humans, have this pattern of inheritance. Organisms with two copies of the same allele are called "homozygous", while organisms with two different alleles are "heterozygous".

The set of alleles for a given organism is called its genotype, while the observable trait the organism has is called its "phenotype". When organisms are heterozygous, often one allele is called "dominant" as its qualities "dominate" the phenotype of the organism, while the other allele is called "recessive" as its qualities "recede" and are not observed. Dominant alleles are often abbreviated with a capital letter, while recessive alleles are given a lowercase version of the same letter. Some alleles do not have complete dominance and instead have incomplete dominance by expressing an intermediate phenotype, or codominance by expressing both alleles at once.

When a pair of organisms reproduce sexually, their offspring randomly inherit one of the two alleles from each parent. The outcome of these crosses can be visualized by use of a Punnett square. These observations of discrete inheritance and the segregation of alleles are collectively known as "Mendel's first law" or the "Law of Segregation".



Organisms have thousands of genes, and in sexually reproducing organisms assortment of these genes are generally independent of each other. This means that the inheritance of an allele for yellow or green pea color is unrelated to the inheritance of alleles for white or purple flowers. This phenomenon, known as "Mendel's second law" or the "Law of independent assortment", means that the alleles of different genes get shuffled between parents to form offspring with many different combinations. (Some genes do not assort independently, demonstrating genetic linkage, a topic discussed later in this article.)

Often different genes can interact in a way that influences the same trait. In the blue-eyed Mary, for example, there exists a gene with alleles that determine the color of flowers: blue or magenta. Another gene, however, controls whether the flowers have color at all: color or white. When a plant has two copies of this white allele, its flowers are white â€?regardless of whether the first gene has blue or magenta alleles. This interaction between genes is called "epistasis", with the second gene epistatic to the first.

 Chapter 4 (Gene Interaction): Gene interaction and modified dihybrid ratios

Many traits are not discrete features (eg. purple or white flowers) but are instead continuous features (eg. human height and skin color). These "complex traits" are the product of interactions of many genes. The influence of these genes is mediated, to varying degrees, by the environment an organism has experienced. The degree to which an organism's genes contribute to a complex trait is called "heritability". Measurement of the heritability of a trait is relative, though â€?in a more variable environment, the environment has a bigger influence on the total variation of the trait. For example, human height is a complex trait with a heritability of 89% in the United States. In Nigeria, however, where people experience a more variable access to good nutrition and health care, height has a heritability of only 62%.



The molecular basis for genes is deoxyribonucleic acid (DNA). DNA is composed of a chain of nucleotides, of which there are four types: adenine (A), cytosine (C), guanine (G), and thymine (T). Genetic information exists in the sequence of these nucleotides, and genes exist as stretches of sequence along the DNA chain. Viruses are the only exception to this rule â€?sometimes viruses use the very similar molecule RNA instead of DNA as their genetic material. The full set of all hereditary material in an organism is called its "genome".

DNA normally exists as a double-stranded molecule, coiled into the shape of a double-helix. Each nucleotide in DNA preferentially pairs with its partner nucleotide on the opposite strand: A pairs with T, and C pairs with G. Thus, in its two-stranded form, each strand effectively contains all necessary information, redundant with its partner strand. This structure of DNA is the physical basis for inheritance: DNA replication duplicates the genetic information by splitting the strands and using each strand as a template for synthesis of a new partner strand.

Genes are arranged linearly along the long chains of DNA sequence, called chromosomes. In bacteria, each cell has a single circular chromosome, while eukaryotic organisms (which includes plants and animals) have their DNA arranged in multiple linear chromosomes. These DNA strands are often extremely long; the largest human chromosome, for example, is about 247 million base pairs in length. The DNA of a chromosome is associated with structural proteins which organize, compact, and control access to the DNA, forming a material called chromatin; in eukaryotes chromatin is usually composed of nucleosomes, repeating units of DNA wound around a core of histone proteins. The full set of hereditary material in an organism (usually the combined DNA sequences of all chromosomes) is called the "genome".

While haploid organisms have only one copy of each chromosome, most animals and many plants are diploid, containing two copies of each chromosome and thus two copies of every gene. The two alleles for a gene are located on identical loci of sister chromatids, each allele inherited from a different parent.

An exception exists in the sex chromosomes, specialized chromosomes many animals have evolved that play a role in determining the sex of the organism. In humans and other mammals the Y chromosome has very few genes and triggers the development of male sexual characteristics, while the X chromosome is similar to the other chromosomes and contains many genes unrelated to sex determination. Females have two copies of the X chromosome, but males have one Y and only one X chromosome â€?this difference in X chromosome copy numbers leads to the unusual inheritance patterns of sex linked disorders.



When cells divide, their full genome is copied and each daughter cell inherits one copy. This is a simplest form of reproduction and is the basis for asexual reproduction. Asexual reproduction can also occur in multicellular organisms, always producing offspring which each inherit a full copy of a single parental genome. When asexual reproduction occurs, the child organisms are "clones" as they contain the same genetic material as the parent.

Eukaryotic organisms often use sexual reproduction to generate offspring which contain a mixture of genetic material inherited from two different parents. The process of sexual reproduction generally alternates between forms which contain single copies of the genome (haploid) and double copies (diploid). Haploid cells fuse and combine genetic material to create a diploid cell with paired chromosomes. Diploid organisms form haploids by dividing, without replicating their DNA, to create daughter cells that randomly inherit one of each pair of chromosomes. Most animals and many plants are diploid for the majority of their lifespan, with the haploid form reduced to single cell gametes.

Although they do not use the haploid/diploid method of sexual reproduction, bacteria have many methods of acquiring new genetic information. Some bacteria can undergo conjugation, transferring a small circular piece of DNA to another bacterium. Bacteria can also take up raw DNA fragments found in the environment and integrate them into their genome, a phenomenon known as transformation. This processes result in horizontal gene transfer, transmitting fragments of genetic information between organisms that would otherwise be unrelated.





The diploid nature of chromosomes allows for genes on different chromosomes to assort independently during sexual reproduction, recombining to form new combinations of genes. Genes on the same chromosome would theoretically never recombine, however, were it not for the process of chromosomal crossover. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes. This process of chromosomal crossover generally occurs during meiosis, a series of cell divisions that creates haploid germ cells which later combine with other germ cells to form child organisms.

The probability of chromosomal crossover occurring between two given points on the chromosome is related to the distance between them. For an arbitrarily long distance, the probability of crossover is high enough that the inheritance of the genes is effectively uncorrelated. For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage â€?alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linear linkage map that roughly describes the arrangement of the genes along the chromosome.





Genes generally express their functional effect through the production of proteins, which are complex molecules responsible for most functions in the cell. Proteins are chains of amino acids, and the DNA sequence of a gene (through an RNA intermediate) is used to produce a specific protein sequence. Each group of three nucleotides in the sequence, called a codon, corresponds to one of the twenty possible amino acids in protein â€?this correspondence is called the genetic code. The flow of information is unidirectional: information is transferred from nucleotide sequences into the amino acid sequence of proteins, but never from protein back into the sequence of DNA â€?a phenomenon Francis Crick called the central dogma of molecular biology.

The specific sequence of amino acids results in a unique three-dimensional structure for that protein, and the three-dimensional structures of protein are related to their function. Some are simple structural molecules, like the fibers formed by the protein collagen. Proteins can bind to other proteins and simple molecules, sometimes acting as enzymes by facilitating chemical reactions within the bound molecules (without changing the structure of the protein itself). Protein structure is dynamic; the protein hemoglobin bends into slightly different forms as it facilitates the capture, transport, and release of oxygen molecules within mammalian blood.

A single nucleotide difference within DNA can cause a single change in the amino acid sequence of a protein. Because protein structures are the result of their amino acid sequences, some changes can dramatically change the properties of a protein by destabilizing the structure or changing the surface of the protein in a way that changes its interaction with other proteins and molecules. For example, sickle-cell anemia is a human genetic disease that results from a single base difference within the coding region for the Î²-globin section of hemoglobin, causing a single amino acid change that changes hemoglobin's physical properties. Sickle-cell versions of hemoglobin stick to themselves, stacking to form fibers that distort the shape of red blood cells carrying the protein. These sickle-shaped cells no longer flow smoothly through blood vessels, having a tendency to clot or degrade, causing the medical problems associated with the disease.

Although genes contain all the information an organism uses to function, the environment plays an important role in determining the ultimate phenotype â€?a dichotomy often referred to as "nature vs. nurture". The phenotype of an organism depends on the interaction of genetics with the environment. One example of this is the case of temperature-sensitive mutations. Often, a single amino acid change within the sequence of a protein does not change its behavior and interactions with other molecules, but it does destabilize the structure. In a high temperature environment, where molecules are moving more quickly and hitting each other, this results in the protein losing its structure and failing to function. In a low temperature environment, however, the protein's structure is stable and functions normally. This sort of mutation is visible in the coat coloration of Siamese cats, where a mutation in an enzyme responsible for pigment production causes it to destabilize and lose function at high temperatures. The protein remains functional in areas of skin that are colder â€?legs, ears, tail, and face â€?and so the cat has dark fur at its extremities.

Environment also plays a dramatic role in effects of the human genetic disease phenylketonuria. The mutation that causes phenylketonuria disrupts the ability of the body to break down the amino acid phenylalanine, causing toxic build-up of an intermediate molecule that, in turn, causes severe symptoms of progressive mental retardation and seizures. If someone with the phenylketonuria mutation is kept on a strict diet that avoids this amino acid, however, they remain normal and healthy.



The genome of a given organism contains thousands of genes, but not all these genes need to be active at any given moment. A gene is "expressed" when it is being transcribed into mRNA (and translated into protein), and there exist many cellular methods of controlling the expression of genes such that proteins are produced only when needed by the cell. Transcription factors are regulatory proteins that bind to the start of genes, either promoting or inhibiting the transcription of the gene. Within the genome of Escherichia coli bacteria, for example, there exists a series of genes necessary for the synthesis of the amino acid tryptophan. However, when tryptophan is already available to the cell, these genes for tryptophan synthesis are no longer needed. The presence of tryptophan directly affects the activity of the genes â€?the tryptophan molecules bind to the tryptophan repressor (a transcription factor), changing the repressor's structure such that the repressor is "active" and binding to the genes. The tryptophan repressor blocks the transcription and prevents expression of the genes, thus creating negative feedback regulation of the tryptophan synthesis process.

Differences in gene expression are especially clear within multicellular organisms, where cells all contain the same genome but have very different structures and behaviors due to the expression of different sets of genes. All the cells in a multicellular organism derive from a single cell, differentiating into different cell types in response to external and intercellular signals and gradually establishing different patterns of gene expression to create different behaviors. No single gene is responsible for the development of structures within multicellular organisms, these patterns arise from the complex interactions between many cells.

Within eukaryotes there exist structural features of chromatin which influence the transcription of genes, often in the form of modifications to DNA and chromatin that are stably inherited by daughter cells. These features are called "epigenetic" because they exist "on top" of the DNA sequence and retain inheritance from one cell generation to the next. Because of epigenetic features, different cell types grown within the same medium can retain very different properties. Although epigenetic features are generally dynamic over the development of organisms, some, like the phenomenon of paramutation, have multigenerational inheritance and exist as rare exceptions to the general rule of DNA as the basis for inheritance.



During the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can have an impact on the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low â€?1 error in every 10-100 million bases â€?due to the "proofreading" ability of DNA polymerases. (Without proofreading error rates are a thousand-fold higher; because many viruses rely on DNA and RNA polymerases that lack proofreading ability they experience higher mutation rates.) Processes which increase the rate of changes in DNA are called "mutagenic": mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, while UV radiation induces mutations by causing damage to the DNA structure. Chemical damage to DNA occurs naturally as well, and cells use DNA repair mechanisms to repair mismatches and breaks in DNA -- nevertheless, the repair sometimes fails to return the DNA to its original sequence.

In organisms which use chromosomal crossover to exchange DNA and shuffle genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment, which makes some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequence -- duplications, inversions or deletions of entire regions, or the accidental exchanging of whole parts between different chromosomes (called "translocation").



Mutations produce organisms with different genotypes, and those differences can result in different phenotypes. Many genetic mutations, called "neutral mutations", have a negligible effect on an organism's phenotype, health, and reproductive fitness. Mutations which do have an effect are often deleterious, but occasionally mutations arise which are beneficial in the current environmental context of the organism.

Population genetics research studies the distributions of these genetic differences within populations and how the distributions change over time. Changes in the frequency of an allele in a population can be influenced by natural selection, where a given allele's higher rate of survival and reproduction causes it to become more frequent in the population over time. Genetic drift can also occur, where chance events lead to random changes in allele frequency.

Over many generations, the genomes of organisms can change, resulting in the phenomenon of evolution. Mutations and the selection for beneficial mutations can cause a species to evolve into forms that better survive their environment, a process called adaptation. New species are formed through the process of speciation, a process often caused by geographical separations that allow different populations to genetically diverge.

As sequences diverge and change during the process of evolution, these differences between sequences can be used as a molecular clock to calculate the evolutionary distance between them. Genetic comparisons are generally considered the most accurate method of characterizing the relatedness between species, an improvement over the sometimes deceptive comparison of phenotypic characteristics. The evolutionary distances between species can be combined to form evolutionary trees. These trees are commonly considered the most accurate representation of relatedness, although the transfer of genetic material between unrelated species (known as "horizontal gene transfer" and most common in bacteria) cannot be represented by the tree.



Although geneticists originally studied inheritance in a wide range of organisms, researchers began to specialize in studying the genetics of a particular subset of organisms. The fact that significant research already existed for a given organism would encourage new researchers to choose it for further study, and so eventually a few "model organisms" became the basis for most genetics research. Common research topics in model organism genetics include the study of gene regulation and the involvement of genes in development and cancer.

Organisms were chosen, in part, for convenience â€?short generation times and facile genetic manipulation made some organisms popular genetics research tools. Widely used model organisms include the gut bacterium Escherichia coli, the plant Arabidopsis thaliana, baker's yeast (Saccharomyces cerevisiae), the nematode Caenorhabditis elegans, the common fruit fly (Drosophila melanogaster), and the common house mouse (Mus musculus).

Medical genetics research seeks to find and study the genetic causes of human diseases. When searching for an unknown gene that may be involved in a disease, researchers commonly use genetics linkage and genetic pedigree charts to find the location on the genome associated with the disease. At the population level, researchers take advantage of Mendelian randomization to look for locations in the genome that are associated with diseases, a technique especially useful for multigenic traits not clearly defined by a single gene. Once a candidate gene is found, further research is often done on the same gene (called an orthologous gene) in model organisms.

Although many inherited diseases are subjects of genetic research, cancer is also considered a genetic disease. The process of cancer development in the body is a combination of events. Mutations occasionally occur within cells in the body as they divide â€?while these mutations will not be inherited by any offspring, they can affect the behavior of cells, sometimes causing them to grow and divide more frequently. There are biological mechanisms that attempt to stop this process â€?signals are given to inappropriately dividing cells that should trigger cell death, but sometimes additional mutations occur that cause cells to ignore these messages. An internal process of natural selection occurs within the body and eventually mutations accumulate within cells to promote their own growth, creating a cancerous tumor that grows and invades various tissues of the body.



A variety of techniques exist for manipulating DNA in the laboratory. Restriction enzymes are a commonly used enzyme that cuts DNA at specific sequences, producing predictable fragments of DNA. The use of ligation enzymes allows these fragments to be stitched back together, and by ligating fragments of DNA together from different sources, researchers can create recombinant DNA. Often associated with genetically modified organisms, recombinant DNA is commonly used in the context of plasmids â€?short circular DNA fragments with a few genes on them. By inserting plasmids into bacteria and growing those bacteria on plates of agar (to isolate clones of bacteria cells), researchers can clonally amplify the inserted fragment of DNA (a process known as molecular cloning). (Cloning can also refer to the creation of clonal organisms, through various techniques.)

DNA can also be amplified using a procedure called the polymerase chain reaction (PCR). By using specific short sequences of DNA, PCR can exponentially amplify a targeted region of DNA, isolating and amplifying a selected section of DNA. Because it can amplify from extremely small amounts of DNA, PCR is also often used to detect the presence of specific DNA sequences.

One of the most fundamental technologies developed to study genetics, DNA sequencing allows researchers to determine the sequence of nucleotides in DNA fragments. Developed in 1977 by Frederick Sanger and coworkers, chain-termination sequencing is now routinely used to sequence DNA fragments. With this technology, researchers have been able to study the molecular sequences associated with many human diseases. As sequencing has become less expensive and with the aid of computational tools, researchers have sequenced the genomes of many organisms by stitching together the sequences of many different fragments (a process called "genome assembly"). These technologies were used to sequence the human genome, leading to the completion of the Human Genome Project in 2003.

The large amount of sequences available has created the field of "genomics," research which uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield of bioinformatics, which uses computational approaches to analyze large sets of biological data.











The Franks or Frankish people ( or gens Francorum) were West Germanic tribes first identified in the 3rd century as an ethnic group living north and east of the Lower Rhine. Under the Merovingian dynasty, they founded one of the Germanic monarchies which replaced the Western Roman Empire from the 5th century. The Frankish state consolidated its hold over large parts of western Europe by the end of the eighth century and the Carolingian Empire and its successor states were Frankish. The Salian political elite were one of the most active forces in spreading Christianity over western Europe. 

Contemporary definitions of the ethnicity of the Franks vary by period and point of view. It is often unclear whether people referred to as Franks referred to themselves as such. Within Francia, the Franks were initially a distinct group with their own culture. 

From the third to fifth centuries some Franks raided Roman territory while other Franks joined the Roman troops. Only the Salians formed a kingdom on Roman soil that was acknowledged by the Romans after 357. In the climate of the collapse of imperial authority in the West, the Frankish tribes were united under the Merovingians and conquered all of Gaul save Septimania in the 6th century. 

The mythology of the Franks was probably a form of Germanic polytheism, later adapted and supplanted in the wake of their incursion into the Roman Empire. Like many Germanic peoples, the Franks concocted an origins story to explain their connection with peoples of classical history. In the case of the Franks, these peoples were the Sicambri and the Trojans. An anonymous work of 727 called Liber Historiae Francorum states that following the fall of Troy, 12,000 Trojans led by chiefs Priam and Antenor moved to the Tanais (Don) river, settled in Pannonia near the Sea of Azov and founded a city called "Sicambria". In just two generations (Priam and his son Marcomer) from the fall of Troy (by modern scholars dated in the late Bronze Age) they arrive in the late fourth century at the Rhine. An earlier variation of this story can be read in Fredegar. In Fredegar's version an early king named Francio serves as namegiver for the Franks, just as Romulus has lent his name to Rome.

The Franks enter history around 260 due to an invasion across the Rhine into the Roman Empire. They are first mentioned on the Tabula Peutingeriana as the Chamavi qui et Pranci (probably an error for Franci, "Chamavi, who are Franks"). Over the next century other Frankish tribes besides the Chamavi surface in the records. The major primary sources include Panegyrici Latini, Ammianus Marcellinus, Claudian, Zosimus, Sidonius Apollinaris and Gregory of Tours. As early as 357 a Frankish king from the Salians enters Roman soil to stay.

Modern scholars of the Migration Period are in agreement that the Frankish identity emerged at the first half of the third century out of various earlier, smaller groups, including the Salii, Sicambri, Chamavi, Bructeri, Chatti, and Chattuarii, who inhabited the lower Rhine valley and lands immediately to its east. This was a social development. 

The Salian Franks invaded the Roman Empire and were accepted as Foederati by Julian the apostate in 358. By the end of the fifth century, the Salian Franks extended their footprint on Roman soil to a territory including the Netherlands south of the Rhine, Belgium and Northern France in which they received other peoples, mainly of the Frankish ethnicity. They gave rise to the Merovingian dynasty in the 5th century.

Franks appear in Roman texts as both allies and enemies (laeti or dediticii). Around 250, one group of Franks, taking advantage of a weakened Roman Empire, penetrated as far as Tarragona in present-day Spain, plaguing this region for about a decade before Roman forces subdued them and expelled them from Roman territory. About forty years later, the Franks had the region of the Scheldt river (present day west Flanders and southwest Netherlands) under control , and were raiding the Channel, disrupting transportation to Britain. Roman forces pacified the region, but did not expel the Franks , who continued to be feared as pirates along the shores at least till the time of Julian the Apostate (358), when Salian Franks were granted to settle as foederati in Toxandria, according to Ammianus Marcellinus.

The first Frankish chief to make himself "King of the Franks" (rex Francorum) was Clovis I in 509. He had conquered the Kingdom of Soissons of the Roman general Syagrius and expelled the Visigoths from southern Gaul at the Battle of VouillÃ©, thus establishing Frankish hegemony over most of Gaul (excluding Burgundy, Provence, and Brittany), which he left to his successors, the Merovingians, to conquer.

Clovis divided his realm between his four sons in a manner which would become familiar, as his sons and grandsons in turn divided their kingdoms between their sons. Clovis' sons united to defeat Burgundy in 534, but internecine feuding came to the fore during the reigns of the brothers Sigebert I and Chilperic I and their sons and grandsons, largely fueled by the rivalry of the queens Fredegunda and Brunhilda. This period saw the emergence of three distinct regna (realms or subkingdoms): Austrasia, Neustria, and Burgundy. Each region developed in its own way and often sought to exert influence over the others. The rising star of the Arnulfing clan of Austrasia meant that the centre of political gravity in the kingdom gradually shifted eastwards from Paris and Tours to the Rhineland. 

The Frankish realm was united again in 613 by Chlothar II, son of Chilperic. Chlothar granted the Edict of Paris to the nobles in an effort to cut down on corruption and unite his vast realm under his authority. After the militarily successful reign of his son and successor Dagobert I, royal authority rapidly declined under a series of kings traditionally known as rois fainÃ©ants. By 687, after the Battle of Tertry, the chronicler could say that the mayor of the palace, formerly the king's chief household official, "reigned." Finally, in 751, with the approval of the papacy and the nobility, the mayor Pepin the Short deposed the last Meroving, Childeric III, and had himself crowned, inaugurating a new dynasty, the Carolingians.

The unification of most of what is now western and central Europe under one chief ruler provided a fertile ground for the continuation of what is known as the Carolingian Renaissance. Despite the almost constant internecine warfare that beset the Carolingian Empire, the extension of Frankish rule and Roman Christianity over such a large area ensured a fundamental unity throughout the Empire. Each part of the Carolingian Empire developed differently; Frankish government and culture depended very much upon individual rulers and their aims. Those aims shifted as easily as the changing political alliances within the Frankish leading families. However, those families, the Carolingians included, all shared the same basic beliefs and ideas of government. These ideas and beliefs had their roots in a background that drew from both Roman and Germanic tradition, a tradition that began before the Carolingian ascent and continued to some extent even after the deaths of Louis the Pious and his sons.

In general Germanic peoples on the borders are known to have served in the Roman army since the days of Julius Caesar. The tribes at the Rhine delta that later became Franks are no exception to that general rule. Despite the fact that from the 3rd century onward large quantities of Germanic peoples served in the Roman army, others kept on invading and raiding Roman soil. This caused confrontations between Franks and their neighbours on Roman soil as the Batavi and Menapii. When Roman administration collapsed in Gaul in 260 due to a joint invasion of Franks and Alamanni, The Germanic Batavian Postumus was forced to usurp power to restore order. From that moment on Germanic soldiers in the Roman army, most notably Franks, were visibly promoted from the ranks. A few decades later the Menapian Carausius (born in Batavia) created a Batavian-British rumpstate on Roman soil that was supported by Frankish soldiers and pirates. In the mid of the 4th century Frankish soldiers like Magnentius, Silvanus and Arbitio held a dominating position in the Roman army. From description of Ammianus Marcellinus it becomes clear that both Frankish and Alamannic armies were organised like Romans and fought comparably.

After the invasion of Chlodio the Roman armies at the Rhine-border became a Frankish "franchise", and Franks were known to levy Roman-like troops that were supported by a Roman-like armour-industry. This lasted at least till the days of Procopius, when the Roman Empire was gone for more than a century, because this historian reported that the former Rhine-army was still in operation and that legions kept on using the same standard and insignia as their forefathers during Roman time.

Militarily, the Franks under the Merovingians melded Germanic custom with Roman organisation and several important innovations. Before the conquest of Gaul, the Franks fought primarily as a tribe unless they were part of a Roman military unit fighting in conjunction with other regiments. 

The primary sources for Frankish military custom and armament are Ammianus Marcellinus, Agathias, and Procopius, the latter two Eastern Roman historians writing about Frankish intervention in the Gothic War. 

Writing of 539, Procopius says:At this time the Franks, hearing that both the Goths and Romans had suffered severely by the war . . . forgetting for the moment their oaths and treaties . . . (for this nation in matters of trust is the most treacherous in the world), they straightway gathered to the number of one hundred thousand under the leadership of Theudebert and marched into Italy: they had a small body of cavalry about their leader, and these were the only ones armed with spears, while all the rest were foot soldiers having neither bows nor spears, but each man carried a sword and shield and one axe. Now the iron head of this weapon was thick and exceedingly sharp on both sides, while the wooden handles was very short. And they are accustomed always to throw these axes at one signal in the first charge and thus to shatters the shields of the enemy and kill the men.

His contemporary, Agathias, says:The military equipment of this people [the Franks] is very simple. . . . They do not know the use of the coat of mail or greaves and the majority leave the head uncovered, only a few wear the helmet. They have their chests bare and backs naked to the loins, they cover their thighs with either leather or linen. They do not serve on horseback except in very rare cases. Fighting on foot is both habitual and a national custom and they are proficient in this. At the hip they wear a sword and on the left side their shield is attached. They have neither bows nor slings, no missile weapons except the double edged axe and the angon which they use most often. The angons are spears which are neither very short nor very long they can be used, if necessary, for throwing like a javelin, and also in hand to hand combat.

While the above quotations have been used as a statement of the military practices of the Frankish armies in the sixth century and have even been extrapolated to the entire period preceding Charles Martel's reforms (early â€?mid eighth century), post-Second World War historiography has emphasised the inherited Roman characteristics of the Frankish military from the date of the beginning of the conquest of Gaul. The Byzantine authors present several contradictions and difficulties. Procopius denies the Franks the use of the spear while Agathias makes it one of their primary weapons. They agree that the Franks were primarily infantrymen, threw axes, and carried a sword and shield. Both writers also contradict the authority of Gallic authors of the same general time period (Sidonius Apollinaris and Gregory of Tours) and the archaeological evidence. Scramasaxes and arrowheads are numerous in Frankish graves even though the Byzantine historians do not assign them to the Franks. 

The evidence of Gregory and of the Lex Salica implies that the early Franks were a cavalry people. In fact, some modern historians have hypothesised that the Franks possessed so numerous a body of horses that they could use them to plough fields and thus were agriculturally technologically advanced over their neighbours. Perhaps the Byzantine writers considered the Frankish horse to be insignificant relative to the Greek cavalry, which is probably accurate.

The Frankish military establishment incorporated much of the pre-existing Roman institutions in Gaul, especially during and after the conquests of Clovis I in the late fifth and early sixth centuries. Frankish military strategy revolved around the holding and taking of fortified centres (castra) and in general these centres were held by garrisons of milites or laeti, that is, former Roman soldiers. Throughout Gaul the descendants of Roman soldiers continued to wear their uniforms and perform their ceremonial duties. 

Immediately beneath the Frankish king in the military hierarchy were the leudes or sworn followers of the king. They could be Gallo-Romans or Franks, laymen or clergy. Some historians have gone to the length of relating their oath-making to the later development of feudalism. The king also had an elite bodyguard called the truste (trustis). Members of the truste, antrustiones, often served in centannae, garrison settlements of Franks (or others) established for military and police purposes throughout the realm. The actual day-to-day bodyguard of the king was made up of pueri who were probably antrustiones. All high-ranking men had pueri (bodyguards). 

The Frankish military was not composed solely of Franks and Gallo-Romans, but also contained Saxons, Alans, Taifals, and Alemanni. After the conquest of Burgungy (534) the well-organised military institutions of that kingdom were integrated into the Frankish realm. Chief among these was the standing army under the command of the Patrician of Burgundy. 

In the late sixth century, during the wars instigated by Fredegund and Brunhilda, the Merovingian monarchs introduced a new element into their militaries: the local levy. A levy consisted in all the able-bodied men of a district who at the call had to report for military service. The local levy applied only to a city and its environs. Initially only in certain cities in western Gaul, in Neustria and Aquitaine, did the kings possess the right or power call up the levy. The commanders of the local levies were always different from the commanders of the urban garrisons. Often the former were commanded by the counts of the districts. A much rarer occurrence was the general levy, which applied to the entire kingdom and included peasants (pauperes and inferiores). General levies could also be made within the still-pagan trans-Rhenish stem duchies at the bequest of a monarch. The Saxons, Alemanni, and Thuringii all had the levy and it could be depended upon by the Frankish monarchs until the mid-seventh century, when the stem dukes began to sever their ties to the monarchy. Radulf of Thuringia called up the levy for a war against Sigebert III in 640. 

Soon the local levy spread to Austrasia and the less Romanised regions of Gaul. On an intermediate level, the kings began calling up territorial levies from the regions of Austrasia (which did not have major cities of Roman origin). However, all the forms of the levy gradually disappeared in the course of the seventh century after the reign of Dagobert I. Under the so-called rois fainÃ©ants, the levies disappeared by mid-century in Austrasia and later in Burgundy and Neustria. Only in Aquitaine, which was fast becoming independent of the central Frankish monarchy, did complex military institutions persist into the eighth century. In the final half of the seventh century and first half of the eight in Merovingian Gaul the chief military actors became the lay and ecclesiastical magnates with their bands of armed followers called retainers. The other aspects of the Merovingian military, mostly Roman in origin or innovations of powerful kings, disappeared from the scene by the eighth century.

The equipment of the Merovingian armies was as varied as the composition. Magnates were known to provide their retainers with coats of mail, helmets, shields, lances, swords, bows and arrows, and war horses. The magnates private armies resembled in armament those of the Gallo-Roman potentiatores of the late Empire. The descendants of Roman soldiers continued to use their service weapons. There was a strong element of Alanic cavalry settled in Armorica which influenced the fighting style of the Bretons down into the twelfth century. Local urban levies could be reasonably well-armed and even mounted, but the more general levies were composed of pauperes and inferiores who were mostly farmers by trade and carried into battle whatever weapons they had at hand, often tools or farming implements which made them of militarily ineffective and thus rarely called upon. The peoples east of the Rhine â€?Franks, Saxons, and even Wends â€?who were sometimes called upon to serve wore less and more rudimentary armour and carried more primitive weaponry, including spears and axes. Few of these men were mounted and they were not affected very much by Roman traditions and technologies. 

Merovingian strategy was wound up in the militarised nature of the entire society. The Franks, to a good deal unlike their Germanic neighbours in this respect, were disposed to call annual meetings in March (the so-called Marchfeld, because assemblies so large had to meet in open fields) whereat the nobles in the presence of the king determined the military target or targets for the coming season of campaigning. In their civil wars with one another, the Merovingian kings concentrated on the holding of fortified places and cities (castra) and siege warfare was a primary aspect in all their endeavours. Siege engines of Roman type were used extensively and the greatest emphasis on tactics was tied to sieges. In offensive wars waged against external foes, the objective was typically the acquisition of booty or the enforcement of tribute. Only in the lands beyond the Rhine did the Merovingians seek to extend their political control over their neighbours. 

Tactically, the Merovingians borrowed heavily from the Romans, especially regarding siege warfare. However, they were not bereft of innovation and there seems to be little remnant of tribal custom in their battle tactics, which were highly flexible and designed to meet the specific circumstances under which battle was being given. Subterfuge, as a tactic, was endlessly employed. Cavalry formed a large segment of the Merovingian military, but mounted troops readily dismounted when appropriate to fight on foot with the infantry. The Merovingians were capable of raising naval forces when necessary. The most significant naval campaign was waged against the Danes by Theuderic I in 515 and involved ocean-worthy ships. More regular was the use of rivercraft on the Loire, Rhone, and Rhine.



A well known weapon of the Franks is the "scramas", a javelin that is better known under the Latin word francisca. Historian Ammianus Marcellinus shows us that the Franks used this weapon in the same way late Roman troops used their javelins.

The ethnonym Franc has sometimes been traced to Francisca (Latin) *frankon (Old English franca), meaning "javelin" This would compare to the seax (knife) after which the Saxons were named or the halberd (battle-axe) after which the Lombards may have been named. The throwing axe of the Franks is known as the francisca but, conversely, the weapon may have been named after the tribe. A. C. Murray says, "The etymology of Franci is uncertain ('the fierce ones' is the favourite explanation), but the name is undoubtedly of Germanic origin."

The language spoken by the early Franks is known as Old Frankish and is only attested in a few words in the Lex Salica and in personal names, and is mostly reconstructed from Old Low Franconian and loanwords in Old French and Latin. It evolved eventually into Old Low Franconian and then into Old Dutch in the Low Countries, while in what is now Germany the Eastern Franconian dialects were slowly replaced from the 14th century by High German. In what became France, from the 8th century Frankish was replaced by Old French south of the language border. From the 10th century the language border slowly retreated north to the current border between French and the Germanic languages Dutch and German. 

There is no surviving work of literature in the Frankish language and perhaps no such works ever existed. Latin was the written language of Gaul before and during the Frankish period. Of the Gallic works which survive, there are a few chronicles, many hagiographies and saints' lives, and a small corpus of poems.

The word Frank has the meaning of "free" (e.g. English , , ) This arose because, after the conquest of Gaul, only Franks were free of taxation.

Echoes of Frankish paganism arise in the primary sources, but their meaning is not always clear. Modern scholars vary wildly about their interpretation, but it is very likely that Frankish paganism shared most of its characteristics with the other varieties of Germanic paganism. 

It was highly ritualistic and many daily activities centred around the multiple deities, chiefest of which may have been the Quinotaur, a water-god from whom the Merovingians were reputed to have derived their ancestry. Most of the pagan gods were associated with local cult centres and their sacred character and power were associated with specific regions, outside of which they were neither worshipped nor feared. Most of the gods were "worldly", possessing form and having concrete relation to earthly objects, in contradistinction to the transcedent God of Christianity.

Archaeologically, Frankish paganism has been observed in the burial site of Childeric I, where the king's body was found covered in a cloth decorated with numerous bees or flies. The symbolism of these insects is unknown. 

Some Franks converted early to Christ, like the usurper Silvanus in the 4th century. In 496, Clovis I, who had married a Burgundian Catholic named Clotilda, was baptised into the Catholic faith by Saint Remi. According to Gregory of Tours, over 3000 of his soldiers were baptised alongside him. This event had an immense impact on the history of Europe, for at the time the Franks were the only major Germanic tribe not ruled by a predominantly Arian aristocracy (their contemporary rivals, the Ostrogoths, Visigoths, and Lombards, were of the Arian persuasion), and this led to a naturally amicable relationship between the Catholic Church and the Franks.

The Frankish church of the Merovingians was shaped by a number of internal and external forces: it had to come to terms with an established Gallo-Roman Christian hierarchy entrenched in a culturally resistant aristocracy; it had to Christianize pagan Frankish sensibilities and control their external expression; it had to provide a theological basis for Merovingian forms of kingship, which were deeply rooted in pagan Germanic tradition; it had to accommodate Irish and Anglo-Saxon missionary activities on the one hand and papal requirements on the other. The Carolingian reformation of monastic life and teaching and church-state relations can be seen both as the culmination of the Frankish church and a transformation of it. 

The increasing personal wealth of the Merovingian elite allowed the endowment of many monasteries, such as those of the Irish missionary Saint Columbanus. The fifth, sixth and seventh centuries saw two major waves of hermitism in the Frankish world, a movement which was eventually reorganised by legislation requiring that all monks and hermits follow the Rule of St Benedict.

The period of Frankish rule saw the gradual replacement, always pushed for by Rome, of the Gallican rite of the Gallo-Roman church with the Roman rite; this does not seem to have stirred passions outside the clergy. 

The Church seems to have had a somewhat uneasy relationship with the Merovingian kings, whose claim to rule depended on a mystique of royal descent that the Church had not yet come to terms with, and who tended to revert to the polygamy of their pagan ancestors. When the mayors took over, the Church was supportive, and an Emperor crowned by the Pope was much more to their liking.

Early Frankish art and architecture belong to that phase of European art called Migration Period art, and have left very few remains. The later period is called Carolingian art, or, especially in architecture, the Pre-Romanesque. 

Very little is preserved in the way of Frankish architecture of the Merovingian period. The works of Gregory of Tours praise the churches of his day, which mostly seem to have been timber-built, with larger examples using the basilica plan, but the most completely surviving example of Merovingian architecture is a baptistery dedicated to Saint John in Poitiers. It is a small building with three apses, now much rebuilt, essentially continuing Gallo-Roman style. In the South of France a number of small baptistries have survived, as separate baptistries fell permanently out of fashion in later periods, so they were not updated as the main churches have been. 

What is preserved of the visual and plastic arts largely consists of archaeological finds of jewellery (such as brooches), weapons (such as swords with decorative hilts), and apparel (such as capes and sandals) found in grave sites, such as the famous grave of the queen Aregund, discovered in 1959, or the Treasure of Gourdon, deposited soon after 524. Not many illuminated manuscripts survive from the Merovingian period, though the few that do, like the Gelasian Sacramentary, contain a great deal of zoomorphic representations. Compared to the similar hybrid works of Insular art from the British Isles, Frankish works in all these media show more continuing use of late Antique style and motifs, and a lesser degree of skill and sophistication in design and manufacture. The numbers surviving are so small, however, that the best quality of work may not be represented.

The work of the main centres of the Carolingian Renaissance represents a great transformation from that of the earlier period, and has survived in far greater quantity. The visual and literary arts were lavishly funded and encouraged by Charlemange, using imported artists where necessary, and Carolingingian developments were in many areas decisive for the future course of Western art. 

The main surviving monument of Carolingian architecture is the Palatine Chapel in Aachen, which is an impressive and confident adaptation of San Vitale, Ravenna, from where some of the pillars were brought. Many other important buildings can be largely reconstructed, such as the monasteries of Centula or St Gall, or the old Cologne Cathedral, now rebuilt. These were now large structures and complexes with a distinctive and sophisticated style, including an emphasis on the vertical and the frequent use of towers. 

Carolingian illuminated manuscripts and ivory plaques survive in reasonable numbers, and now approach those of Constantinople in quality, as was certainly the intention.

Like other Germanic peoples, the legal precedents of the Franks were originally housed only in the memory of designated specialists, rachimburgs, parallel to Scandinavian lawspeakers. By the time codes began to be written down in the sixth century, there persisted two basic legal subdivisions within the Frankish nation: Salian Franks were subject to Salic law, Ripuarian Franks to Ripuarian law. Gallo-Romans south of the Loire River and the clergy remained subject to tradiational Roman law. Germanic law was overwhelmingly concerned with private law, which protects individuals, over public law, which protects the interest of the state. According to Michel Rouche, "Frankish judges devoted as much care to a case involving the theft of a dog as Roman judges did to cases involving the fiscal responsibility of curiales, or municipal councilors."



Because the Frankish kingdom dominated Western Europe for centuries, terms derived from "Frank" were used by many in Eastern Europe, the Middle East, and beyond as a synonym for Roman Christians (e.g., al-Faranj in Arabic, farangi in Persian, Frenk in Turkish, Feringhi in Hindustani, and Frangos in Greek). See also Thai à¸à¸£à¸±à¹ˆà¸?farang. During the crusades, which were at first led mostly by nobles from northern France who claimed descent from Charlemagne, both Muslims and Christians used these terms as ethnonyms to describe the Crusaders. This usage is often followed by modern historians, who call Western Europeans in the eastern Mediterranean "Franks" regardless of their country of origin. Compare with Rhomaios, RÃ»mi ("Roman"), used for Orthodox Christians. Catholics on various islands in Greece are still referred to as Î¦ÏÎ±Î³ÎºÎ¿Î¹, "Frangoi" (Franks). Examples include the naming of a Catholic from the Island of Syros as "Frangosyrianos" (Î¦ÏÎ±Î³ÎºÎ¿ÏƒÏ…ÏÎ¹Î±Î½Î¿Ï‚). The term Frangistan was used by Muslims to refer to the land where the Crusaders came from, i.e. Christian Europe.

The Carolingian elite produced Feudalism. This social structure, or parts of it, went on to influence much of Western Europe from the Middle Ages onwards. Also, the Franks and their leaders became an important part of the legendary history of Western Europe. Because of this, many European rulers and writers used the idea of a Frankish legacy as justification for political claims or for political and social theories. In the twentieth century, Franks and Frankish leaders became common political symbols for European unity.















Turkish (  ) is a language spoken by 95â€?10 million people worldwide, making it the most commonly spoken of the Turkic languages. Its speakers are located predominantly in Turkey, with smaller ranges in North Cyprus, Bulgaria, and other parts of Eastern Europe. Turkish is also spoken by several million immigrants in Western Europe, particularly in Germany.

The roots of the language can be traced to Siberia and Mongolia (Inner Asia), with the first written records dating back nearly 1,200 years. To the west, the influence of Ottoman Turkishâ€”the immediate precursor of today's Turkishâ€”spread as the Ottoman Empire expanded. In 1928, as one of AtatÃ¼rk's Reforms in the early years of the new Turkish Republic, the Ottoman script was replaced with a phonetic variant of the Latin alphabet. Concurrently, the newly-founded Turkish Language Association initiated a drive to reform the language by removing Persian and Arabic loanwords in favor of native variants and coinages from Turkic roots.

The distinctive characteristics of Turkish are vowel harmony and extensive agglutination. The basic word order of Turkish is Subject Object Verb. Turkish has a T-V distinction: second-person plural forms can be used for individuals as a sign of respect. Turkish also has no noun classes or grammatical gender.





Turkish is a member of the Turkish, or Western, subgroup of the Oghuz languages, which includes Gagauz and Azeri. The Oghuz languages form the Southwestern subgroup of the Turkic languages, a language family comprising some 30 living languages spoken across Eastern Europe, Central Asia. and Siberia. Some linguists believe the Turkic languages to be a part of a larger Altaic language family. About 40% of Turkic language speakers are Turkish speakers. The characteristic features of Turkish, such as vowel harmony, agglutination, and lack of grammatical gender, are universal within the Turkic family and the Altaic languages. There is a high degree of mutual intelligibility between Turkish and the other Oghuz languages, including Azeri, Turkmen, Qashqai, and Gagauz.



The earliest known Turkic inscriptions reside in modern Mongolia. The Bugut inscriptions written in the Sogdian alphabet during the First GÃ¶ktÃ¼rk Khanate are dated to the second half of the 6th century. 

The two monumental Orkhon inscriptions, erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khan and dating back to some time between 732 and 735, constitute another important early record. After the discovery and excavation of these monuments and associated stone slabs by Russian archaeologists in the wider area surrounding the Orkhon Valley between 1889 and 1893, it became established that the language on the inscriptions was the Old Turkic language written using the Orkhon script, which has also been referred to as "Turkic runes" or "runiform" due to an external similarity to the Germanic runic alphabets. 



The Turkish language is not directly descended from the Old Turkic of the Orkhon inscriptions. It is, rather, a member of the Oghuz branch.With the Turkic expansion during Early Middle Ages (c. 6thâ€?1th centuries), the Seljuqs of the Oghuz Turks, in particular, brought their language, Oghuz Turkicâ€”the direct ancestor of today's Turkish languageâ€”into Anatolia during the 11th century. Also during the 11th century, an early linguist of the Turkic languages, KaÅŸgarlÄ± Mahmud from the Kara-Khanid Khanate, published the first comprehensive Turkic language dictionary and map of the geographical distribution of Turkic speakers in the Compendium of the Turkic Dialects (Ottoman Turkish: DivÃ¢nÃ¼ LÃ¼gati't-TÃ¼rk).

Following the adoption of Islam c. 950 by the Kara-Khanid Khanate and the Seljuq Turks, who are both regarded as the cultural ancestors of the Ottomans, the administrative language of these states acquired a large collection of loanwords from Arabic and Persian. Turkish literature during the Ottoman period, particularly Ottoman Divan poetry, was heavily influenced by Persian, including the adoption of poetic meters and a great quantity of imported words. The literary and official language during the Ottoman Empire (c. 1299â€?922) was a mixture of Turkish, Persian, and Arabic that differed considerably from the period's everyday spoken Turkish, and is termed Ottoman Turkish.

After the foundation of the Republic of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932 under the patronage of Mustafa Kemal AtatÃ¼rk, with the aim of conducting research on Turkish. It featured a number of Armenian linguists, the foremost of whom, Genocide-survivor Hagop Martaian (a.k.a. Agop Dilachar), codified the Turkish alphabet. One of the tasks of the newly-established association was to initiate a language reform to replace loanwords of Arabic and Persian origin with Turkish equivalents. By banning the usage of imported words in the press, the association succeeded in removing several hundred foreign words from the language. While most of the words introduced to the language by the TDK were newly derived from Turkic roots, it also opted for reviving Old Turkish words which had not been used for centuries.

Due to this sudden change in the language, older and younger people in Turkey started to differ in their vocabularies. While the generations born before the 1940s tend to use the older terms of Arabic or Persian origin, the younger generations favor new expressions. It is particularly ironic that AtatÃ¼rk himself, in his monumental speech to the new Parliament in 1927, used a style of Ottoman diction which today sounds so alien that it has had to be "translated" three times into modern Turkish: first in 1963, again in 1986, and most recently in 1995. There is also a political dimension to the language debate, with conservative groups tending to use more archaic words in the press or everyday language.

The past few decades have seen the continuing work of the TDK to coin new Turkish words to express new concepts and technologies as they enter the language, mostly from English. Many of these new words, particularly information technology terms, have received widespread acceptance. However, the TDK is occasionally criticized for coining words which sound contrived and artificial. Some earlier changesâ€”such as bÃ¶lem to replace fÄ±rka, "political party"â€”also failed to meet with popular approval (in fact, fÄ±rka has been replaced by the French loanword parti). Some words restored from Old Turkic have taken on specialized meanings; for example betik (originally meaning "book") is now used to mean "script" in computer science.

Many of the words derived by TDK coexist with their older counterparts. This usually happens when a loanword changes its original meaning. For instance, dert, derived from the Persian dard (Ø¯Ø±Ø¯ "pain"), means "problem" or "trouble" in Turkish; whereas the native Turkish word aÄŸrÄ± is used for physical pain. Sometimes the loanword has a slightly different meaning from the native Turkish word, giving rise to a situation similar to the coexistence of Germanic and Romance words in English (see List of Germanic and Latinate equivalents). Among some of the old words that were replaced are terms in geometry, cardinal directions, some months' names, and many nouns and adjectives. Some examples of modern Turkish words and the old loanwords are:





Turkish is natively spoken by the Turkish people in Turkey and by the Turkish diaspora in some 30 other countries. In particular, Turkish-speaking minorities exist in countries that formerly (in whole or part) belonged to the Ottoman Empire, such as Bulgaria, Cyprus, Greece (primarily in Western Thrace), the Republic of Macedonia, Romania, and Serbia. More than two million Turkish speakers live in Germany, and there are significant Turkish-speaking communities in France, the Netherlands, Austria, Belgium, Switzerland, and the United Kingdom. Due to the cultural assimilation of Turkish immigrants in host countries, not all ethnic Turkish immigrants speak the language with native fluency.

The number of native speakers in Turkey is about 60â€?7 million, corresponding to about 90â€?3 percent of the population, and 65â€?3 million native speakers exist worldwide. Turkish is spoken as a first or second language by almost all of Turkey's residents, with Kurdish making up most of the remainder (about 3,950,000 as estimated in 1980). However, even most linguistic minorities in Turkey are bilingual, speaking Turkish as a second language to levels of native fluency. 

Turkish is the official language of Turkey and partially recognized state of Northern Cyprus. It is one of the official languages of Cyprus. It also has official (but not primary) status in the Prizren District of Kosovo and several municipalities of the Republic of Macedonia, depending on the concentration of Turkish-speaking local population.

In Turkey, the regulatory body for Turkish is the Turkish Language Association (TÃ¼rk Dil Kurumu or TDK), which was founded in 1932 under the name TÃ¼rk Dili Tetkik Cemiyeti ("Society for Research on the Turkish Language"). The Turkish Language Association was influenced by the ideology of linguistic purism: indeed one of its primary tasks was the replacement of loanwords and foreign grammatical constructions with equivalents of Turkish origin. These changes, together with the adoption of the new Turkish alphabet in 1928, shaped the modern Turkish language spoken today. TDK became an independent body in 1951, with the lifting of the requirement that it should be presided over by the Minister of Education. This status continued until August 1983, when it was again made into a governmental body in the constitution of 1982, following the military coup d'Ã©tat of 1980.



Istanbul Turkish is established as the official standard language of Turkey. Dialectal variation persists, in spite of the levelling influence of the standard used in mass media and the Turkish education system since the 1930s. Academically, researchers from Turkey often refer to Turkish dialects as aÄŸÄ±z or ÅŸive, leading to an ambiguity with the linguistic concept of accent, which is also covered with these same words. Projects investigating Turkish dialects are being carried out by several universities, as well as a dedicated work group of the Turkish Language Association. Work is currently in progress for the compilation and publication of their research as a comprehensive dialect atlas of the Turkish language.

The standard dialect of the Turkish language is Ä°stanbul. Rumelice is spoken by immigrants from Rumelia, and includes the distinct dialects of Deliorman, Dinler, and Adakale, which are influenced by the theoretized Balkan linguistic union. KÄ±brÄ±s is the name for Cypriot Turkish and is spoken by the Turkish Cypriots. Edirne is the dialect of Edirne. Ege is spoken in the Aegean region, with its usage extending to Antalya. The nomadic YÃ¶rÃ¼k tribes of the Mediterranean Region and the Balkan peninsula also have their own dialect of Turkish.

GÃ¼neydoÄŸu is spoken in the southeast, to the east of Mersin. DoÄŸu, a dialect in Eastern Anatolia, has a dialect continuum with Azeri, particularly with Karapapak dialects in some areas. The Central Anatolia region speaks Orta Anadolu. Karadeniz, spoken in the Eastern Black Sea Region and represented primarily by the Trabzon dialect, exhibits substratum influence from Greek in phonology and syntax. Kastamonu is spoken in Kastamonu and its surrounding areas. The HemÅŸinli dialect, known as HemÅŸince, is spoken by the western group of Hamshenis around Rize, influenced by Armenian. KaramanlÄ±ca is spoken in Greece, where it is also named KÎ±ÏÎ±Î¼Î±Î½Î»Î®Î´Î¹ÎºÎ± (Karamanlidika). It is the literary standard for Karamanlides.



The phoneme  (usually referred to as yumuÅŸak g ("soft g")), ÄŸ in Turkish orthography, actually represents a rather weak front-velar or palatal approximant between front vowels. It never occurs at the beginning of a word, but always follows a vowel. When word-final or preceding another consonant, it lengthens the preceding vowel. 

In native Turkic words, the sounds , , and  are in complementary distribution with , , and ; the former set occurrs adjacent to front vowels and the latter adjacent to back vowels. The distribution of these phonemes is often unpredictable, however, in foreign borrowings and proper nouns. In such words, , , and  often occur with back vowels: some examples are given below.

When a vowel is added to nouns ending with postvocalic <k>, the <k> becomes <ÄŸ> by consonant alternation. A similar alternation applies to certain loan-words ending in <p> and <t>, which become <b> and <d>, respectively, with the addition of a vowel.

The vowels of the Turkish language are, in their alphabetical order, a, e, ''Ä±'', ''i'', o, Ã¶, u, and Ã¼. Undotted <Ä±> is the close back unrounded vowel . There are no diphthongs in Turkish; when two vowels come together, which occurs rarely and only with loanwords, each vowel retains its individual sound. However, a slight diphthong can occur when two vowels surround a yumuÅŸak g. For example, the word soÄŸuk ("cold") can be pronounced /soÊŠk/ (resembling the English soak) by some speakers.



The Turkish vowel system can be considered as being two-dimensional, where vowels are characterised by two features: front/back and rounded/unrounded. Vowel harmony is the principle by which a native Turkish word incorporates either exclusively back vowels (a, Ä±, o, and u) or exclusively front vowels (e, i, Ã¶, and Ã¼). The pattern of vowels is shown in the table below.

Grammatical affixes have "a chameleon-like quality", and obey one of the following patterns of vowel harmony:

The following examples, based on the copula -dir4 ("[it] is"), illustrate the principles of vowel harmony in practice: TÃ¼rkiye'dir ("it is Turkey"), kapÄ±dÄ±r ("it is the door"), but gÃ¼ndÃ¼r ("it is the day"), paltodur ("it is the coat").

There are some exceptions to the rules of vowel harmony. In compound words, the vowels need not harmonize between the constituent words of the compound. Forms like bu+gÃ¼n ("today") or baÅŸ+kent ("capital") are permissible. In addition, vowel harmony does not apply in loanwords and some invariant affixes, such as -yor (present tense) and -bil- (potential). Some loanwords do, however, exhibit partial or even complete vowel harmony (e.g. mÃ¼mkÃ¼n "possible" < Arabic mumkin; and dÃ¼rbÃ¼n "binoculars" < Persian dÅ«rbÄ«n). There are also a few native Turkish words that do not follow the rule, such as anne ("mother"). In such words, suffixes harmonize with the final vowel: thus annedir ("she is a mother"). Many loanwords from Arabic and French, however, take front-vowel suffixes after final back vowels: for example halsiz < hal + -siz4 "listless", meÃ§huldÃ¼r < meÃ§hul + -dir4 "it is unknown", harfler < harf + -lerÂ² "(alphabetical) letters" (instead of the expected *halsÄ±z, *meÃ§huldur and *harflar). 

The road sign in the photograph above illustrates several of these features:

Stress is usually on the last syllable. Exceptions include some suffix combinations and loanwords, particularly fromItalian and Greek, as well as many proper names. While such loanwords are usually stressed on the penultimate syllable ( lokanta "restaurant" or  iskele "quay"), the stress of proper names is less predictable ( Ä°stanbul,  Ankara).



Turkish is an agglutinative language and frequently uses affixes, or endings. One word can have many affixes and these can also be used to create new words, such as creating a verb from a noun, or a noun from a verbal root (see the section on Word formation). Most affixes indicate the grammatical function of the word.The only native prefixes are alliterative intensifying syllables used with adjectives or adverbs: for example sÄ±m'sÄ±cak ("boiling hot" < sÄ±cak) and mas'mavi ("bright blue" < mavi).

The extensive use of affixes can give rise to long words. It is jokingly said that the longest Turkish word is Ã‡ekoslovakyalÄ±laÅŸtÄ±ramadÄ±klarÄ±mÄ±zdanmÄ±ÅŸsÄ±nÄ±z, meaning "You are said to be one of those that we couldn't manage to convert to a Czechoslovak". This example is of course contrived; but long words do frequently occur in normal Turkish, as in this heading of a newspaper obituary column: BayramlaÅŸamadÄ±klarÄ±mÄ±z (Bayram [festival]-Recipr-Impot-Partic-Plur-PossPl1; "Those of our number with whom we cannot exchange the season's greetings"). 

There is no definite article in Turkish, but definiteness of the object is implied when the accusative ending is used (see below). Turkish nouns decline by taking case-endings, as in Latin. There are six noun cases in Turkish, with all the endings following vowel harmony (shown in the table using the shorthand superscript notation. The plural marker -lerÂ² immediately follows the noun before any case or other affixes (e.g. kÃ¶ylerin "of the villages").

The accusative case marker is used only for definite objects; compare aÄŸaÃ§ gÃ¶rdÃ¼k "we saw a tree" with aÄŸacÄ± gÃ¶rdÃ¼k "we saw the tree". The plural marker -lerÂ² is not used when a class or category is meant: aÄŸaÃ§ gÃ¶rdÃ¼k can equally well mean "we saw trees [as we walked through the forest]"â€”as opposed to aÄŸaÃ§larÄ± gÃ¶rdÃ¼k "we saw the trees [in question]".

The declension of aÄŸaÃ§ illustrates two important features of Turkish phonology: consonant assimilation in suffixes (aÄŸaÃ§tan, aÄŸaÃ§ta) and voicing of final consonants before vowels (aÄŸacÄ±n, aÄŸaca, aÄŸacÄ±).

Additionally, nouns can take suffixes that assign person: for example -imiz4, "our". With the addition of the copula (for example -im4, "I am") complete sentences can be formed. The interrogative particle mi4 immediately follows the word being questioned: kÃ¶ye mi? "[going] to the village?", aÄŸaÃ§ mÄ±? "[is it a] tree?".

The Turkish personal pronouns in the nominative case are ben (1s), sen (2s), o (3s), biz (1pl), siz (2pl, or formal/polite 2s), and onlar (3pl). They are declined regularly with some exceptions: benim (1s gen.); bizim (1pl gen.); bana (1s dat.); sana (2s dat.); and the oblique forms of o use the root on. All other pronouns (reflexive kendi and so on) are declined regularly.

Turkish adjectives are not declined. However most adjectives can also be used as nouns, in which case they are declined: e.g. gÃ¼zel ("beautiful") â†?gÃ¼zeller ("(the) beautiful ones / people"). Used attributively, adjectives precede the nouns they modify. The adjectives var ("existent") and yok ("non-existent") are used in many cases where English would use "there is" or "have", e.g. sÃ¼t yok ("there is no milk", lit. "(the) milk (is) non-existent"); the construction "noun 1-GEN noun 2-POSS var/yok" can be translated "noun 1 has/doesn't have noun 2"; imparatorun elbisesi yok "the emperor has no clothes" ("(the) emperor-of clothes-his non-existent"); kedimin ayakkabÄ±larÄ± yoktu ("my cat had no shoes", lit. "cat-my-of shoe-plur.-its non-existent-past tense").

Turkish verbs indicate person. They can be made negative, potential ("can"), or impotential ("cannot"). Furthermore, Turkish verbs show tense (present, past, inferential, future, and aorist), mood (conditional, imperative, necessitative, and optative), and aspect. Negation is expressed by the infix -meÂ²- immediately following the stem.

All Turkish verbs are conjugated in the same way, except for the irregular and defective verb i-, the Turkish copula, which can be used in compound forms (the shortened form is called an enclitic): GelememiÅŸti = GelememiÅŸ idi = GelememiÅŸ + i- + -di

Turkish has several participles, including present (with the ending -enÂ²), future (-ecekÂ²), past (-miÅŸ4), and aorist (-erÂ² or -ir4). These forms can function as either adjectives or nouns: oynamayan Ã§ocuklar "children who do not play", oynamayanlar "those who do not play"; okur yazar "reader-writer = literate", okur yazarlar "literates". 

The most important function of participles is to form modifying phrases equivalent to the relative clauses found in most European languages. The participles used in these constructions are the future (-ecekÂ²) and an older form (-dik4), which covers both present and past meanings. The use of these "personal" or "relative" participles is illustrated in the following table, in which the examples are presented according to the grammatical case which would be seen in the equivalent English relative clause.

Word order in simple Turkish sentences is generally Subject Object Verb, as in Korean and Latin, but unlike English. In more complex sentences, the basic rule is that the qualifier precedes the qualified: this principle includes, as an important special case, the participial modifiers discussed above. The definite precedes the indefinite: thus Ã§ocuÄŸa hikÃ¢yeyi anlattÄ± "she told the child the story", but hikÃ¢yeyi bir Ã§ocuÄŸa anlattÄ± "she told the story to a child".

It is possible to alter the word order to stress the importance of a certain word or phrase. The main rule is that the word before the verb has the stress without exception. For example, if one wants to say "Hakan went to school" with a stress on the word "school" (okul, the indirect object) it would be "Hakan okula gitti". If the stress is to be placed on "Hakan" (the subject), it would be "Okula Hakan gitti" which means "it's Hakan who went to school".





The 2005 edition of GÃ¼ncel TÃ¼rkÃ§e SÃ¶zlÃ¼k, the official dictionary of the Turkish language published by Turkish Language Association, contains 104,481 entries, of which about 14% are of foreign origin. Among the most significant foreign contributors to Turkish vocabulary are Arabic, French, Persian, Italian, English, and Greek. 

Turkish extensively uses agglutination to form new words from nouns and verbal stems. The majority of Turkish words originate from the application of derivative suffixes to a relatively small set of core vocabulary. 

An example set of words derived from a substantive root:

Another example, starting from a verbal root:

New words are also frequently formed by compounding two existing words into a new one, as in German. A few examples of compound words are given below:





Turkish is written using a modified version of the Latin alphabet introduced in 1928 by AtatÃ¼rk to replace the Arabic-based Ottoman Turkish alphabet. The Ottoman alphabet marked only three different vowelsâ€”long Ä, Å« and Ä«â€”and included several redundant consonants, such as variants of z (which were distinguished in Arabic but not in Turkish). The omission of short vowels in the Arabic script made it particularly unsuitable for Turkish, which has eight vowels.

The reform of the script was an important step in the cultural reforms of the period. The task of preparing the new alphabet and selecting the necessary modifications for sounds specific to Turkish was entrusted to a Language Commission composed of prominent linguists, academics, and writers. The introduction of the new Turkish alphabet was supported by public education centers opened throughout the country, cooperation with publishing companies, and encouragement by AtatÃ¼rk himself, who toured the country teaching the new letters to the public.As a result, there was a dramatic increase in literacy from its original Third World levels. 

Latin was applied to the Turkish language for educational purposes even before the 20th century reform. Instances include a 1635 Latin-Albanian dictionary Frang Bardhi, who also incorporated several saying in the Turkish language, as an appendix to his work (e.g. â€˜Alma agatsdan irak duschamasâ€?â€?An apple does not fall far from its tree.)

Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonetic, with one letter corresponding to each phoneme. Most of the letters are used approximately as in English, the main exceptions being <c>, which denotes  (<j> being used for the  found in Persian and European loans); and the undotted <Ä±>, representing . As in German, <Ã¶> and <Ã¼> represent  and . The letter <ÄŸ>, in principle, denotes  but has the property of lengthening the preceding vowel and assimilating any subsequent vowel. The letters <ÅŸ> and <Ã§> represent  and , respectively. A circumflex is written over back vowels following <k>, <g>, or <l> when these consonants represent , , and â€”almost exclusively in Arabic and Persian loans.

The specifically Turkish letters and spellings described above are illustrated in this table:

Dostlar Beni HatÄ±rlasÄ±n by AÅŸÄ±k Veysel ÅžatÄ±roÄŸlu (1894â€?973), a minstrel and highly regarded poet in the Turkish folk literature tradition.

Details of the sources cited only by the author's name are given in full in the References section.



Printed sources

On-line sources

Further reading



















Captain Marvel is a fictional comic book superhero, originally published by Fawcett Comics and later by DC Comics. Created in 1939 by artist C. C. Beck and writer Bill Parker, the character first appeared in Whiz Comics #2 (February 1940). With a premise that taps adolescent fantasy, Captain Marvel is the alter ego of Billy Batson, a youth who works as a radio news reporter and was chosen to be a champion of good by the wizard Shazam. Whenever Billy speaks the wizard's name, he is instantly struck by a magic lightning bolt that transforms him into an adult superhero empowered with the abilities of six mythical figures. Several friends and family members, most notably Marvel Family cohorts Mary Marvel and Captain Marvel, Jr., can share Billy's power and become "Marvels" themselves. 

Hailed as "The World's Mightiest Mortal" in his adventures, Captain Marvel was nicknamed "The Big Red Cheese" by archvillain Doctor Sivana, an epithet later adopted by Captain Marvel's fans. Based on sales, Captain Marvel was the most popular superhero of the 1940s, as his Captain Marvel Adventures comic book series sold more copies than Superman and other competing superhero books during the mid-1940s. Captain Marvel was also the first comic book superhero to be adapted to film, in 1941 (The Adventures of Captain Marvel).

Fawcett ceased publishing Captain Marvel-related comics in 1953, due in part to a copyright infringement suit from DC Comics alleging that Captain Marvel was an illegal infringement of Superman. In 1972, DC licensed the Marvel Family characters and returned them to publication, acquiring all rights to the characters by 1991. DC has since integrated Captain Marvel and the Marvel Family into their DC Universe, and have attempted to revive the property several times. However, Captain Marvel has not regained widespread appeal with new generations, although a 1970s Shazam! live-action television series featuring the character was popular.

Because Marvel Comics trademarked their Captain Marvel comic book during the interim between the original Captain Marvel's Fawcett years and DC years, DC Comics is unable to promote and market their Captain Marvel/Marvel Family properties under that name. Since 1972, DC has instead used the trademark Shazam! as the title of their comic books and thus the name under which they market and promote the character. Consequently, Captain Marvel himself is sometimes erroneously referred to as Shazam.



After the success of National Comics' new superhero characters Superman and Batman, Fawcett Publications decided in 1939 to start its own comics division. Fawcett recruited writer Bill Parker to create several hero characters for the first title in their line, tentatively titled Flash Comics. Besides penning stories featuring Ibis the Invincible, Spy Smasher, Golden Arrow, Lance O'Casey, Scoop Smith and Dan Dare for the new book, Parker also wrote a story about a team of six superheroes, each possessing a special power granted to them by a mythological figure. Fawcett Comics' executive director Ralph Daigh decided it would be best to combine the team of six into one hero who would embody all six powers. Parker responded by creating a character he called "Captain Thunder." Staff artist Clarence Charles "C. C." Beck was recruited to design and illustrate Parker's story, rendering it in a direct, somewhat cartoony style that became his trademark.

The first issue of the comic book, printed as both Flash Comics #1 and Thrill Comics #1, had a low-print run in the fall of 1939 as an ashcan copy created for advertising purposes. Shortly after its printing, however, Fawcett found it could not trademark "Captain Thunder," "Flash Comics," or "Thrill Comics," because all three names were already in use. Consequently, the book was renamed Whiz Comics, and Fawcett artist Pete Costanza suggested changing Captain Thunder's name to "Captain Marvelous," which the editors shortened to "Captain Marvel." The word balloons in the story were re-lettered to label the hero of the main story as "Captain Marvel." Whiz Comics #2, dated February 1940, was published in late 1939. Since it was the first of that title to actually be published, the issue is sometimes referred to as Whiz Comics #1, despite the issue number printed on it.

Inspirations for Captain Marvel came from a number of sources. His visual appearance was modeled after that of Fred MacMurray, a popular American actor of the period. C. C. Beck's later versions of the character would resemble other American actors, including Cary Grant and Jack Oakie. Fawcett Publications' founder, Wilford H. Fawcett, was nicknamed "Captain Billy," which inspired the name "Billy Batson" and Marvel's title as well. Fawcett's earliest magazine was titled Captain Billy's Whiz Bang, which inspired the title Whiz Comics. In addition, Fawcett adapted several of the elements that had made Superman, the first popular comic book superhero, popular (super strength and speed, science-fiction stories, a mild mannered reporter alter ego), and incorporated them into Captain Marvel. Fawcett's circulation director Roscoe Kent Fawcett recalled telling the staff, "give me a Superman, only have his other identity be a 10 or 12-year-old boy rather than a man."

As a result, Captain Marvel was given a twelve-year-old boy named Billy Batson as an alter ego. In the origin story printed in Whiz Comics #2, Billy, a homeless newsboy, is lead by a mysterious stranger to a secret subway tunnel. An odd subway car with no visible driver takes them to the lair of the wizard Shazam, who grants Billy the power to become the adult superhero Captain Marvel. In order to transform into Captain Marvel, Billy must speak the wizard's name, an acronym for the six various legendary figures who had agreed to grant aspects of themselves to a willing subject: the wisdom of '''S'''olomon; the strength of '''H'''ercules; the stamina of '''A'''tlas; the power of '''Z'''eus; the courage of '''A'''chilles; and the speed of '''M'''ercury. Speaking the word produces a bolt of magic lightning which transforms Billy into Captain Marvel; speaking the word again reverses the transformation with another bolt of lightning.

Captain Marvel wore a bright red costume, inspired by both military uniforms and ancient Egyptian and Persian costumes as depicted in popular operas, with gold trim and a lightning bolt insignia on the chest. The body suit originally included a buttoned lapel, but was changed to a one-piece skintight suit within a year at the insistence of the editors (the current DC costume of the character has the lapel restored to it). The costume also included a white-collared cape trimmed with gold flower symbols, usually asymmetrically thrown over the left shoulder and held around his neck by a gold cord. The cape came from the ceremonial cape worn by the British nobility, photographs of which appeared in newspapers in the 1930s.

In addition to introducing the main character and his alter ego, Captain Marvel's first adventure in Whiz Comics #2 also introduced his archenemy, the evil Doctor Sivana, and found Billy Batson talking his way into a job as an on-air radio reporter. Captain Marvel was an instant success, with Whiz Comics #2 selling over 500,000 copies. By 1941, he had his own solo series, Captain Marvel Adventures, while continuing to appear in Whiz Comics as well. He also made periodic appearances in other Fawcett books, including Master Comics.

[[Image:Marvel-familt-lt-marvels.jpg|left|thumb|250px|Detail from The Marvel Family #2 (June 1946), cover art by C. C. Beck. From left to right: Captain Marvel; Lt. "Fat" Marvel; Captain Marvel, Jr.; Lt. "Tall" Marvel; Lt. "Hillbilly" Marvel; and Mary Marvel. Uncle Marvel can be seen seated at the piano in the background.]]

Through his adventures, Captain Marvel soon gained a host of enemies. His most frequent foe was Doctor Sivana, a mad scientist who was determined to rule the world, yet was thwarted by Captain Marvel at every turn. Marvel's other villains included Adolf Hitler's champion Captain Nazi, an older Egyptian renegade Marvel called Black Adam, an evil magic-powered brute named Ibac, and an artificially intelligent nuclear-powered robot called Mister Atom. The most notorious Captain Marvel villains, however, were the nefarious Mister Mind and his Monster Society of Evil, which recruited several of Marvel's previous adversaries. The "Monster Society of Evil" story arc ran as a twenty-five chapter serial in Captain Marvel Adventures #22â€?6 (March 1943 â€?May 1945), with Mister Mind eventually revealed to be a highly intelligent yet tiny worm from another planet.

In the early 1940s, Captain Marvel also gained allies in the Marvel Family, a collective of superheroes with powers and/or costumes similar to Captain Marvel's. (By comparison, Superman spin-off character Superboy first appeared in 1944, while Supergirl first appeared in 1959). Whiz Comics #21 (September 1941) marked the debut of the Lieutenant Marvels, the alter egos of three other boys (all also named Billy Batson) who found that, by saying "Shazam!" in unison, they too could become Marvels. In Whiz Comics #25 (December 1941), a friend named Freddy Freeman, mortally wounded by an attack from Captain Nazi, was given the power to become teenage boy superhero Captain Marvel, Jr. A year later in Captain Marvel Adventures #18 (December 1942), Billy and Freddy met Billy's long-lost twin sister Mary Bromfield, who discovered she could, by saying the magic word "Shazam," become teenage superheroine Mary Marvel.

Captain Marvel, Mary Marvel and Captain Marvel, Jr. were featured as a team in a new comic series entitled The Marvel Family. This was published alongside the other Captain Marvel-related titles, which now included Wow Comics featuring Mary, Master Comics featuring Junior, and both Mary Marvel Comics and Captain Marvel, Jr. Comics. Non-super-powered Marvels such as the "lovable con artist" Uncle Marvel and his niece, Freckles Marvel, also sometimes joined the other Marvels on their adventures. A funny animal character, Hoppy the Marvel Bunny, was created in 1942 and later given a spin-off series of his own.

The members of the Marvel Family often teamed up with the other Fawcett superheroes, who included Ibis the Invincible, Bulletman and Bulletgirl, Spy Smasher, Minute-Man, and Mr. Scarlet and Pinky. Among the many artists and writers who worked on the Marvel Family stories alongside C. C. Beck and main writer Otto Binder were Joe Simon and Jack Kirby, Mac Raboy, Pete Costanza, Kurt Schaffenberger, and Marc Swayze.



Through much of the Golden age of comic books, Captain Marvel proved to be the most popular superhero character of the medium with his comics outselling all others, including those featuring Superman. In fact, Captain Marvel Adventures sold fourteen million copies in 1944, and was at one point being published weekly with a circulation of 1.3 million copies an issue (proclaimed on the cover of issue #19 as being the "Largest Circulation of Any Comic Magazine"). Part of the reason for this popularity included the inherent wish-fulfillment appeal of the character to children, as well as the humorous and surreal quality of the stories. Billy Batson typically narrated each Captain Marvel story, speaking directly to his reading audience from his WHIZ radio microphone, relating each story from the perspective of a young boy.

Detective Comics (later known as National Comics Publications, National Periodical Publications, and today known as DC Comics) sued Fawcett Comics for copyright infringement in 1941, alleging that Captain Marvel was based on their character Superman. After seven years of litigation, the National Comics Publications v. Fawcett Publications case went to trials court in 1948. Although the judge presiding over the case decided that Captain Marvel was an infringement, DC was found to be negligent in copyrighting several of their Superman daily newspaper strips, and it was decided that National had abandoned the Superman copyright. As a result, the initial verdict, delivered in 1951, was decided in Fawcett's favor.

National appealed this decision, and Judge Learned Hand declared in 1952 that National's Superman copyright was in fact valid. Judge Hand did not find that the character of Captain Marvel itself was an infringement, but rather that specific stories or super feats could be infringements, and that the truth of this would have to be determined in a re-trial of the case. The judge therefore sent the matter back to the lower court for final determination.

Instead of retrying the case, however, Fawcett decided to settle with National out of court. The National lawsuit was not the only problem Fawcett faced in regards to Captain Marvel. While Captain Marvel Adventures had been the top-selling comic series during World War II, it suffered declining sales every year after 1945 and by 1949 it was selling only half its wartime rate. Fawcett tried to revive the popularity of its assorted Captain Marvel series in the early 1950s by introducing elements of the horror comics trend that gained popularity at the time. Feeling that a decline in the popularity of superhero comics meant that it was no longer worth continuing the fight, Fawcett agreed to never again publish a comic book featuring any of the Captain Marvel-related characters, and to pay National $400,000 in damages. Fawcett shut down its comics division in the autumn of 1953 and laid off its comic-creating staff. Whiz Comics had ended with issue #146 in June 1952, Captain Marvel Adventures was cancelled with #150 (November 1953), and The Marvel Family ended its run with #89 (January 1954).

In the 1950s, a small British publisher, L. Miller and Son, published a number of black and white reprints of American comic books, including the Captain Marvel series. With the outcome of the National v. Fawcett lawsuit, L. Miller and Son found their supply of Captain Marvel material abruptly cut off. They requested the help of a British comic writer, Mick Anglo, who created a thinly disguised version of the superhero called Marvelman. Captain Marvel, Jr. was adapted to create Young Marvelman, while Mary Marvel had her gender changed to create the male Kid Marvelman. The magic word "Shazam!" was replaced with "Kimota", "Atomic" backwards. The new characters took over the numbering of the original Captain Marvel's United Kingdom series with issue number 25.

Marvelman ceased publication in 1963, but was revived in 1982 by writer Alan Moore in the pages of Warrior Magazine. Moore's black and white serialized adventures were reprinted in color by Eclipse Comics under the new title Miracleman beginning in 1985, and continued publication in the United States after Warrior's demise. Within the metatextual storyline of the comic series itself, it was noted that Marvelman's creation was based upon Captain Marvel comics, by both Alan Moore and later Marvelman/Miracleman writer Neil Gaiman.



When superhero comics became popular again in the mid-1960s in what is now called the Silver Age of comics, Fawcett was unable to revive Captain Marvel because in order to settle the lawsuit it had agreed never to publish the character again. Eventually, they licensed the characters to DC Comics in 1972, and DC began planning a revival. Because Marvel Comics had by this time established its own claim to the use of Captain Marvel as a comic book title, DC published their book under the name Shazam! Since then, that title has become so linked to Captain Marvel that many people have taken to identifying the character as "Shazam" instead of his actual name.

The Shazam! comic series began with issue #1, dated February 1973. It contained both new stories and reprints from the 1940s and 1950s. The first story attempted to explain the Marvel Family's absence by stating that they, Dr. Sivana, Sivana's children, and most of the supporting cast had been accidentally trapped in suspended animation for twenty years until finally breaking free.

Dennis O'Neil was the primary writer of the book; his role was later taken over by writers Elliott S! Maggin and E. Nelson Bridwell. C. C. Beck drew stories for the first ten issues of the book before quitting due to creative differences; Bob Oksner, Fawcett alumnus Kurt Schaffenberger, and Don Newton were among the later artists of the title.

With DC's Multiverse concept in effect during this time, it was stated that the revived Marvel Family and related characters lived within the DC Universe on the parallel world of "Earth-S". While the series began with a great deal of fanfare, the book had a lackluster reception. The creators themselves had misgivings; Beck said, "As an illustrator I could, in the old days, make a good story better by bringing it to life with drawings. But I couldn't bring the new [Captain Marvel] stories to life no matter how hard I tried." Shazam! was canceled with issue #35 (June 1978) and relegated to a back-up position in World's Finest Comics (from #253, October-November 1978, to #282, August 1982, skipping only #271 which featured a full-length origin of the Superman-Batman team story) and Adventure Comics (from #491, September 1982, through #498, April 1983; only #491 and #492 featured original stories however, the rest containing Fawcett era reprint stories). With their 1985 limited series Crisis on Infinite Earths, DC fully integrated the characters into the DC Universe.

The first post-Crisis appearance of Captain Marvel was in the 1986 Legends miniseries. In 1987, Captain Marvel appeared as a member of the Justice League in Keith Giffen and J. M. DeMatteis' relaunch of that title. That same year, he was also given his own miniseries titled Shazam! The New Beginning. With this four-issue miniseries, writers Roy and Dann Thomas and artist Tom Mandrake attempted to re-launch the Captain Marvel mythos and bring the wizard Shazam, Dr. Sivana, Uncle Dudley and Black Adam into the modern DC Universe with an altered origin story.

The most notable change that Thomas, Giffen, and DeMatteis introduced into the Captain Marvel mythos was that the personality of young Billy Batson is retained when he transforms into the Captain. The Golden Age comics, on the other hand, tended to treat Captain Marvel and Billy as two separate personalities. This change would remain for most future uses of the character, as justification for his sunny, Golden-Age personality in the darker modern-day comic book world.



DC finally purchased the rights to all of the Fawcett Comics characters in 1991. In 1994, the unpopular revision of the character from the Shazam! The New Beginning was retconned again and given a revised origin in The Power of Shazam!, a painted graphic novel written and illustrated by Jerry Ordway. This story became Captain Marvel's official DC Universe origin story (with his appearances in Legends and Justice League still counting as part of this continuity).

Ordway's story more closely followed Captain Marvel's Fawcett origins, with only slight additions and changes. For example, in this version of the origin, it is Black Adam (in his non-powered form of Theo Adam) who killed Billy Batson's parents. The graphic novel was a critically acclaimed success, leading to a Power of Shazam! ongoing series which ran from 1995 to 1999. That series reintroduced the Marvel Family, and many of their allies and enemies, into the modern-day DC Universe.

Marvel also appeared in Mark Waid and Alex Ross's critically acclaimed miniseries Kingdom Come. Set thirty years in the future, Kingdom Come features a brainwashed Captain Marvel playing a major role in the story as a mind-controlled pawn of an elderly Lex Luthor. In 2000, Captain Marvel starred in an oversized special graphic novel, Shazam! Power of Hope, written by Paul Dini and painted by Alex Ross.

Since the cancellation of the Power of Shazam! title in 1999, the Marvel Family have made appearances in a number of other DC comic books. Black Adam became a main character in Geoff Johns and David S. Goyer's JSA series, which depicted the latest adventures of the Justice Society of America. Captain Marvel also appeared regularly in JSA in 2003 and 2004. He also appeared in Frank Miller's graphic novel , the sequel to Miller's highly-acclaimed graphic novel The Dark Knight Returns.

The Marvel Family played an integral part in DC's 2005/2006 Infinite Crisis crossover, which began DC's efforts to retool the Shazam! franchise. In the Day of Vengeance limited series, which preceded the Infinite Crisis event, the wizard Shazam is killed by the Spectre, and Captain Marvel assumes the wizard's place in the Rock of Eternity. The Marvel Family made a handful of guest appearances in the year-long weekly maxi-series 52, which featured Black Adam as one of its main characters. The Marvel Family also appeared frequently in the 12-issue bimonthly painted limited series Justice by Alex Ross, Jim Krueger, and Doug Braithwaite, published from 2005 to 2007. 

The Trials of Shazam!, a 12-issue limited series also written by Judd Winick and illustrated by Howard Porter (issues one through eight) and Mauro Cascioli (issues nine through twelve), began publication in August 2006. The series redefines the Shazam mythos, the characters and their place in the DC Universe. Trials of Shazam! features Captain Marvel, now with a white costume and long white hair, taking over the role of the wizard Shazam under the name Marvel, while Freddy Freeman attempts to prove himself worthy to the individual gods so that he can become their new champion and herald under the name Shazam. Currently, Mary Marvel is a featured character in DC's sequel to 52, a weekly series entitled Countdown to Final Crisis.

A four-issue Captain Marvel/Superman limited series, Superman/Shazam: First Thunder, was published between September 2005 and March 2006. The miniseries, written by Judd Winick with art by Josh Middleton, depicted the first meeting between the two heroes.

A second Captain Marvel limited series, Shazam! The Monster Society of Evil, written and illustrated by Jeff Smith (creator of Bone), was published in four 48-page installments between February and July 2007. Smith's Shazam! mini-series, in the works since 2003, is a more traditional take on the character, which updates and re-imagines Captain Marvel's origin. According to Smith, the story is in continuity and takes the place of the character's previously established origins as depicted in the The Power of Shazam! graphic novel. However, this has not been confirmed by any secondary sources. 

A new Captain Marvel comic, Billy Batson and the Magic of Shazam!, is set to debut in early 2008 under DC's Johnny DC youth-oriented imprint. Set to follow the lead of Smith's version, it will be written and drawn by Mike Kunkel.

When Billy Batson says the magic word "Shazam" and transformed into Captain Marvel, he was granted the following powers:

In current comics continuity, Marvel has assumed the throne of Shazam at the Rock of Eternity, and now has access to the dead wizard's greatly enhanced magical powers and abilities. However, Marvel is required to remain on the Rock of Eternity, and can only be away from the Rock for twenty-four hours at a time.

In Superman (first series" #276 (June 1974), Superman found himself at odds with Captain Thunder, a superhero displaced from another Earth and another time. Thunder had been tricked by his archenemies in the Monster League of Evil into doing evil, and Thunder therefore was made to do battle with Superman. Captain Thunder, whose name was derived from Captain Marvel's original moniker, was a thinly veiled pastiche of Marvel; down to his similar costume, his young alter ego named "Willie Fawcett", and a magic word ("Thunder!") which was an acronym for seven entities and their respective powers.

At the time of Superman #276, DC had been publishing Shazam! comics for two years, but had kept that universe separate from those of its other publications. The real Captain Marvel would finally meet Superman in Justice League of America #137 two years later.

In the final issue of the maxi-series 52 (#52, May 2, 2007) , a new Multiverse is revealed, originally consisting of 52 identical realities. Among the parallel realities shown is one designated Earth-5. As a result of Marvel Family foe Mister Mind "eating" aspects of this reality, it takes on visual aspects similar to the pre-Crisis Earth-S, including the Marvel Family characters. The names of the characters are not mentioned in the panel in which they appear, but a character visually similar to Captain Marvel appears. Based on comments by 52 co-author Grant Morrison, this alternate universe is not the pre-Crisis Earth-S.



Captain Marvel often fights evil as a member of a superhero team known as the Marvel Family, made up of himself and several other heroes: The wizard Shazam who empowers the team, Captain Marvel's sister Mary Marvel and Marvel's protÃ©gÃ© Captain Marvel, Jr. Before the Crisis on Infinite Earths, the Marvel Family also included part-time members such as Mary's non-powered friend "Uncle" Dudley aka Uncle Marvel, Dudley's non-powered niece Freckles Marvel, a team of proteges (all of whose alter egos are named "Billy Batson") known as the Lieutenant Marvels, and the funny-animal pink rabbit version of Captain Marvel, Hoppy the Marvel Bunny.

Through his adventures, Captain Marvel gained an extensive rogues gallery, the most notable of whom include the evil mad scientist Doctor Sivana (and, pre-Crisis, the Sivana Family), Shazam's corrupted previous champion Black Adam, Adolph Hitler's champion Captain Nazi, and the mind-controlling worm Mister Mind and his Monster Society of Evil. Other Marvel Family foes include the evil robot Mister Atom, the "World's Mightiest Immortal" Oggar, and Ibac and Sabbac, demon-powered supervillains who transform by magic as Captain Marvel does.

The Marvel Family's non-powered allies include Dr. Sivana's good-natured adult offspring Beautia and Magnificus Sivana, Mister "Tawky" Tawny the talking tiger, WHIZ radio president and Billy's employer Sterling Morris, Billy's girlfriend Cissie Sommerly, Billy's school principal Miss Wormwood, and Mary's adoptive parents Nick and Nora Bromfield.



The first filmed adaptation of Captain Marvel was produced in 1941. The Adventures of Captain Marvel, starring Tom Tyler in the title role and Frank Coglan, Jr. as Billy Batson, was a twelve-part film serial produced by Republic Pictures in 1941. Often ranked among the finest examples of the form, its release made Captain Marvel the first superhero to be depicted in film. Whitey Murphy, a supporting character in the serial, found his way into Fawcett's Captain Marvel stories, and elements of the serial's plot were later worked into DC's The Power of Shazam continuity. The Adventures of Captain Marvel (which, ironically, was originally pitched to National Comics as a Superman film serial) predated Fleischer Studios' Superman cartoons by six months. 

Over thirty years later, Filmation produced Shazam!, a live-action television show which ran from 1974 to 1977 on CBS. From 1975 until the end of its run, it aired as one-half of The Shazam!/Isis Hour, featuring Filmation's own The Secrets of Isis as a companion program. The Shazam! TV show was a more indirect approach to the character; it told of Billy Batson/Captain Marvel making road trips, instead of flying across the USA to combat evil. Shazam! starred Michael Gray as Billy Batson, with both Jackson Bostwick (season one) and John Davey (seasons two and three) as Captain Marvel. An adapted version of Isis, the heroine of The Secrets of Isis, was introduced into DC Comics in 2006 as Black Adam's wife in the weekly comic book series 52. 

Shortly after the Shazam! show ended its network run, Captain Marvel, played by Garrett Craig, appeared as a character in a pair of low-budgeted live action comedy specials, produced by Hanna-Barbera Productions under the name Legends of the Superheroes in 1978. The specials also featured Howard Morris as Doctor Sivana, and Ruth Buzzi as Aunt Minerva, marking the first appearance of those characters in film or television. Filmation revisited the character a few years later for an animated Shazam! cartoon, which ran on NBC from 1981 to 1982 as part of the Kid Superpower Hour with Shazam!. The rest of the Marvel Family joined Captain Marvel on his adventures in this series, which were more similar to his comic-book adventures than the 1970s TV show. Dr. Sivana, Mr. Mind, Black Adam, and other familiar Captain Marvel foes appeared as enemies.

Although Captain Marvel did not appear in Hanna-Barbera's long-running Saturday morning cartoon series Super Friends (which featured many of the other DC superheroes), he did appear in some of the merchandise associated with the show.  

Billy Batson has a non-speaking cameo in the episode "Obsession" from the Kids' WB's . Actors portraying Captain Marvel make "cameo" appearances in both a dream-sequence from an episode of The Drew Carey Show, and in the Beastie Boys' music video for "Alive". In 2005, Captain Marvel guest starred in the June 11, 2005 episode of Cartoon Network's animated series Justice League Unlimited. The episode, entitled "Clash", featured Jerry O'Connell as the voice of Captain Marvel, with Shane Haboucha as Billy Batson. A climactic fight sequence between Captain Marvel and Superman pays homage to the Superman/Captain Marvel battle from Mark Waid and Alex Ross' Kingdom Come mini-series.

Captain Marvel has a cameo appearance in the animated film . He is shown during President John F. Kennedy's famous speech.

New Line Cinema began development of a Shazam! live-action feature film in the early 2000s. Formerly based on a screenplay by William Goldman & Bryan Goluboff, the film, to be titled Billy Batson and the Legend of Shazam!, . is now being written by John August.  Peter Segal (The Longest Yard, 50 First Dates) is attached as director, with Michael Uslan set as producer. The film will be distributed by Warner Bros., and is currently expected to be completed for a 2009 release. Although no casting choices have been made for Captain Marvel or Billy Batson, actor and former wrestler Dwayne "The Rock" Johnson has agreed to appear in the film as Black Adam. 

Captain Marvel's adventures have contributed a number of elements to both comic book culture and pop culture in general. The most notable of these is the regular use of Superman and Captain Marvel as adversaries in Modern Age comic book stories. 

The fictional Superman/Captain Marvel rivalry was parodied in "Superduperman," a satirical comic book story by Harvey Kurtzman and Wally Wood in the fourth issue of Mad (April-May, 1953). In the parody, inspired by the Fawcett/DC legal battles, Superduperman, endowed with muscles on muscles, does battle with Captain Marbles, a Captain Marvel caricature. Marbles' magic word is "SHAZOOM", which stands for Strength, Health, Aptitude, Zeal, Oxâ€”power of, Oxâ€”power of another and Money. In contrast to Captain Marvel's perceived innocence and goodness, Marbles was greedy and money-grubbing.

While publishing its Shazam! revival in the 1970s, DC Comics published a story in Superman #276 (June 1974) featuring a battle between the Man of Steel and a thinly disguised version of Captain Marvel called Captain Thunder, a reference to the character's original name. Two years later, Justice League of America #135-137 featured a story arc which featured the heroes of Earth-1, Earth-2, and Earth-S teaming together against their enemies. It was in this story that Superman and Captain Marvel first met, albeit briefly.

Following this Justice League story, DC followed Mad's cue and often pitted Captain Marvel and Superman against each other for any number of reasons, but usually as an inside joke to the characters' long battles in court; they are otherwise staunch allies. Notable Superman/Captain Marvel battles in DC Comics include All-New Collectors' Edition #C-58 (1979), All-Star Squadron #37 (1984), and Superman #102 (1995). The Superman/Captain Marvel battle depicted in Kingdom Come #4 (1996) served as the climax of that miniseries. The "Clash" episode of the DC-based animated TV series Justice League Unlimited, which included Captain Marvel as a guest character, featured a Superman/Captain Marvel fight as its centerpiece.

Although kid superheroes had generally been neglected before Captain Marvel's introduction, kid sidekicks soon became commonplace shortly after Marvel's success: Robin was paired with Batman in May 1940, and Captain America was introduced with sidekick Bucky in March 1941. The idea of a young boy who transformed into a superhero proved popular enough to inspire a number of superheroes who undergo similar transformations, including Marvel Comics' Darkhawk and Ghost Rider, Malibu Comics' Prime, and animated/action figure superheroes such as Hanna-Barbera's Mighty Mightor and Young Samson, Mattel/Filmation's He-Man, Warner Bros. Animation's Freakazoid, and Cartoon Network's Ben 10. Other heroes, including Marvel Comics' Thor and (for a time) Marvel's version of Captain Marvel, undergo similarly magical transformations from a weak human form to a god-empowered form.

The Image Comics character Mighty Man, created by Erik Larsen and appearing primarily in Larsen's series The Savage Dragon, is an obvious homage to Captain Marvel. Similarities run deep, from MM's initial secret identity being a young boy with an alliterative name ("Bobby Berman") to his lightning bolt chest emblem in his earliest version (a later version used an 8-pointed star similar to Marvel Comics' Captain Marvel logo, stressing again the inspiration for the character), to his greatest foe being a mad scientist named "Dr. Nirvana". 

In pop culture, Billy Batson/Captain Marvel's magic word, "Shazam!", became a popular exclamation from the 1940s on, often used in place of an expletive. The most notable user of the word "Shazam!" in this form was Gomer Pyle, a character from the 1960s sitcom The Andy Griffith Show. Foxxy Cleopatra from the 2002 film Austin Powers in Goldmember is also fond of the word. In another 2002 movie, Spider-Man, the character Peter Parker shouts "Shazam!" while trying to control his powers. 

Even more than ten years after the character first disappeared, the superhero was still used for allusions and jokes, in films such as West Side Story, TV shows such as The Monkees, M*A*S*H, and American Dad!, and songs such as "Shazam" (1960) by Duane Eddy. Elvis Presley was a fan of Captain Marvel, Jr. comic books as a child, and later styled his hair to look like Freddy Freeman's and based his stage jumpsuits and TCB lightning logo on Captain Marvel Junior's costume and lightning-bolt insignia.  The Academy of Comic Book Arts named its Shazam Award in honor of the character's mythos. The Beatles mentioned Captain Marvel in their song "The Continuing Story of Bungalow Bill" (1968).











The Abbey Theatre (), also known as the National Theatre of Ireland (), is in Dublin, Ireland. The Abbey first opened its doors to the public on 27 December 1904, and despite losing its original building to a fire in 1951 has remained active to the present day. The Abbey was the first state-subsidised theatre in the English-speaking world; from 1925 onwards it received an annual subsidy from the Irish Free State.

In its early years, the theatre was closely associated with the writers of the Celtic revival, many of whom were involved in its foundation and most of whom had plays staged there. The Abbey served as a nursery for many of the leading Irish playwrights and actors of the 20th century, including William Butler Yeats, Lady Gregory Augusta, Sean O'Casey and John Millington Synge. In addition, through its extensive programme of touring abroad and its high visibility to foreign, particularly North American, audiences, it has become an important part of the Irish tourist industry.

 The Abbey arose from three distinct bases, the first of which was the seminal Irish Literary Theatre. Founded by Lady Gregory, Edward Martyn and William Butler Yeats in 1899â€”with assistance from George Mooreâ€”it had presented plays in the Ancient Concert Rooms and the Gaiety Theatre, which brought critical approval but limited public interest.

The second base involved the work of two Irish brothers, William and Frank Fay. William worked in the 1890s with a touring company in Ireland, Scotland and Wales while Frank was heavily involved in amateur dramatics in Dublin. After William returned, the brothers staged productions in halls around the city and eventually formed W. G. Fay's Irish National Dramatic Company, focused on the development of Irish acting talent. In April 1902, the Fays gave three performances of Ã†'s play Deirdre and Yeats' Cathleen NÃ­ Houlihan in a hall in St Theresa's Hall on Clarendon Street. The performances played to a mainly working-class audience rather than the usual middle-class Dublin theatre-goers. The run was a great success, thanks in part to Maud Gonne, who played the lead in Yeats' play. The company continued at the Ancient Concert Rooms, producing works by Seumas O'Cuisin, Fred Ryan and Yeats.

The third base was the presence in Dublin of Annie Elizabeth Fredericka Horniman. Horniman was a middle-class Englishwoman with previous experience of theatre production, having been involved in the presentation of George Bernard Shaw's Arms and the Man in London in 1894. She came to Dublin in 1903 to act as Yeats' unpaid secretary and to make costumes for a production of his play The King's Threshold. Her money would make the Abbey Theatre a viable reality, and according to the critic Adrian Frazier, would "make the rich feel at home, and the poor - on a first visit - out of place."

 Encouraged by the St Theresa's Hall success, Yeats, Lady Gregory, Ã†, Martyn, and John Millington Synge founded the Irish National Theatre Society in 1903 with funding from Horniman. At first, performances were staged in the Molesworth Hall. When the Hibernian Theatre of Varieties in Lower Abbey Street and an adjacent building in Marlborough Street became available after fire safety authorities closed the Hibernia, Horniman and William Fay agreed to buy and refit the space to meet the society's needs. On 11 May 1904, the society formally accepted Horniman's offer of the use of the building. As Horniman was not normally resident in Ireland, the royal letters patent required were paid for by her but granted in the name of Lady Gregory. William Fay was appointed theatre manager, responsible for training the actors in the newly established repertory company. Yeats' brother Jack was commissioned to paint portraits of all the leading figures in the society for the foyer, while Sarah Purser was hired to design stained glass for the same space.

On 27 December, the curtains went up on opening night. The bill consisted of three one-act plays, On Baile's Strand and Cathleen NÃ­ Houlihan by Yeats, and Spreading the News by Lady Gregory. On the second night, In the Shadow of the Glen by Synge replaced the second Yeats play, and these two bills alternated over a five-night run. Frank Fay, playing CÃºchulainn in On Baile's Strand, was the first actor on the Abbey stage. Although Horniman had designed the costumes, neither she nor Lady Gregory were present. Horniman had, in fact, returned to England, and her main role with the Abbey over the coming years, in addition to providing funding, was to organise publicity and bookings for touring Abbey productions in London and provincial England. 

In 1905, Yeats, Lady Gregory and Synge decided to turn the theatre into a limited liability company, the National Theatre Society Ltd., without properly consulting Horniman. Annoyed by this treatment, she hired Ben Iden Payne, a former Abbey employee, to help run her new repertory company in Manchester.

The new theatre found great popular success, and large crowds attended many of its productions. The Abbey was fortunate in having Synge as a key member as he was then considered one of the foremost English-language dramatists. The theatre staged many plays by eminent or soon-to-be eminent authors, including Yeats, Lady Gregory, Moore, Martyn, Padraic Colum, George Bernard Shaw, Oliver St John Gogarty, F. R. Higgins, Thomas MacDonagh, Lord Dunsany, T. C. Murray, James Cousins and Lennox Robinson. Many of these authors served on the board, and it was during this time that the Abbey gained its reputation as a writers' theatre.

The Abbey's fortunes worsened in January 1907 when the opening of Synge's The Playboy of the Western World resulted in civil disturbance. The troubles (since known as the Playboy Riots) were encouraged, in part, by nationalists who believed the theatre was insufficiently political and who took offence at Synge's use of the word 'shift'â€”known at the time as a symbol representing Kitty O'Shea and adultery and hence seen as a slight on the virtue of Irish womanhood. A significant portion of the crowd rioted, causing the remainder of the play to be acted out in dumbshow. Nationalist anger was further provoked by the theatre's decision to call in the police. Although press opinion soon turned against the rioters and the protests faded, the Abbey was shaken, and Synge's nextâ€”and last completedâ€”play, The Tinker's Wedding (1908), was not staged for fear of further disturbances. That same year, the Fay brothers' association with the theatre ended when they emigrated to the United States and Lennox Robinson took over the Abbey's day-to-day management. 

In 1909, Shaw's The Shewing-Up of Blanco Posnet led to further protests. The subsequent discussion occupied a full issue of the theatre's journal, "The Arrow". Also that year, the proprietors decided to make the Abbey independent of Annie Horniman, who had indicated a preference for this course. Relations with Horniman had been tense, partly because she wished to be involved in choosing which plays were to be performed and when. As a mark of respect for the death of King Edward VII, an understanding existed that Dublin theatres were to close on the night of 7 May 1910. Robinson, however, kept the Abbey open. When Horniman heard of Robinson's decision, she severed her connections with the company. By her own estimate, she had invested Â£10,350â€”worth approximately $1 million in 2007 US dollarsâ€”on the project.

With the loss of Horniman, Synge, and the Fays, the Abbey under Robinson tended to drift, suffering from falling public interest and box office returns. This trend was halted for a time by the emergence of Sean O'Casey as an heir to Synge. O'Casey's career as a dramatist began with The Shadow of a Gunman, staged by the Abbey in 1923. This was followed by Juno and the Paycock in 1924, and The Plough and the Stars in 1926. This last play resulted in riots reminiscent of those that had greeted the Playboy 19 years earlier. Once again, concerned about public reaction, the Abbey rejected O'Casey's next play, and he emigrated to England shortly thereafter.

In 1924, Yeats and Lady Gregory offered the Abbey to the government of the Free State as a gift to the Irish people. Although the offer was refused, the following year Minister of Finance Ernest Blythe arranged an annual government subsidy of Â£850 for the Abbey. This made the company the first state-supported theatre in the English-speaking world. The subsidy allowed the theatre to avoid bankruptcy, but the amount was too small to save it from financial difficulty. 

The Abbey School of Acting and the Abbey School of Ballet were set up that year. The latter was led by Ninette de Valoisâ€”who had provided choreography for a number of Yeats' playsâ€”and ran until 1933.

Around this time additional space was acquired, allowing for a small experimental theatre, the Peacock, to be set up in the ground floor of the main theatre. In 1928, Hilton Edwards and MicheÃ¡l MacLiammoir launched the Gate Theatre, initially using the Peacock to stage works by European and American dramatists. However, the Gate primarily sought work from new Irish playwrights, and despite the new space, the Abbey entered a period of artistic decline. This is illustrated by the story of how one new work was said to have come to the theatre. Denis Johnston, the story goes, submitted his first play, Shadowdance, to the Abbey; however it was rejected by Lady Gregory and returned to the author with â€œThe Old Lady says Noâ€?written across the title page. Johnston decided to re-title the play, and The Old Lady Says 'No'Â  was staged by the Gate in the Peacock in 1928. The veracity of this story has been questioned by academic critics Joseph Ronsley and Christine St. Peter.

The tradition of the Abbey as primarily a writers' theatre survived Yeats' withdrawal from day-to-day involvement. Frank O'Connor sat on the board from 1935 to 1939, served as managing director from 1937, and had two plays staged during this period. However he was alienated from and unable to cope with many of the other board members. O'Connor's past adultery was held against him, and although he fought a formidable battle to retain his position, soon after Yeats died machinations were put in place to remove him. 

During the 1940s and 1950s, the staple fare at the Abbey was comic farce set in the idealised peasant world of Ã‰amon de Valera, which, if it ever existed, no longer was relevant to most Irish citizens. As a result, audience numbers continued to decline. This drift might have been more dramatic but for the appearance of popular actors including F. J. McCormick and dramatists including George Shiels, who could still draw a crowd. Another Abbey tenant was Austin Clarke, whose Dublin Verse Speaking Societyâ€”later the Lyric Theatreâ€”operated out of the Peacock from 1941 to 1944 and the Abbey from 1944 to 1951.

On 17 July 1951, fire destroyed the building, and only the Peacock survived intact. The company leased the old Queen's Theatre in September and continued in residence there until 1966. The Queen's had been home to the Happy Gang, a team of comedians who specialised in popular skits, farces and pantomimes and drew wide audiences. With its continued diet of 'peasant comedies', the new tenants were not far removed from the old. However neither of the two more interesting Irish dramatists to emerge in the 1950s, Brendan Behan and Samuel Beckett, featured in these productions. In February 1961, the ruins of the Abbey were demolished, and plans for rebuilding began with a design by the Irish architect Michael Scott. On 3 September 1963, the President of Ireland, Eamon de Valera, laid the foundation stone for the new theatre. The Abbey reopened on 18 July 1966.

A new building, a new generation of dramatists including such figures as Hugh Leonard, Brian Friel and Tom Murphy, and tourism that included the National Theatre as a key cultural attraction helped revive the theatre. Aiding the revival was the theatre's involvement, beginning in 1957, in the Dublin Theatre Festival. Plays such as Friel's Philadelphia Here I Come! (1964), The Faith Healer (1979) and Dancing at Lughnasa (1990), Murphy's A Whistle In the Dark (1961) and The Gigli Concert (1983) and Leonard's Da (1973) and A Life (1980) helped raise the Abbey's international profile through successful runs in the West End in London, and on Broadway in New York. 

In December 2004, the theatre celebrated its centenary with events that included performances of the original programme by amateur dramatic groups and the professional premiere of Michael West's Dublin By Lamplight, staged by Annie Ryan for The Corn Exchange company at the Project Arts Centre in November 2004. However, despite the centenary, not all was well. Audience numbers were falling; the Peacock was closed for lack of money; the theatre was near bankruptcy, and the staff felt the threat of huge lay-offs. In September a motion of no confidence in Artistic Director Ben Barnes was tabled by two members, playwrights Jimmy Murphy and Ulick O'Connor, of the theatre's advisory council. Barnes, criticised for touring with a play in Australia during deep financial and artistic crisis at home, flew back and survived the motion. The debacle put the Abbey under great public scrutiny. On 12 May 2005, Barnes and Managing Director Brian Jackson resigned after it became known that the theatre's deficit of â‚?.85Â million had been underestimated. The new director, Fiach MacÂ Conghail, who had been due to start in January 2006, took over in May 2005.

On 20 August 2005, the Abbey Theatre Advisory Council approved a plan to dissolve the Abbey's owner, the National Theatre Society, and replaced it with a company limited by guarantee, the Abbey Theatre Limited. After strong debate the program was accepted. Basing its actions on this plan, the Arts Council of Ireland awarded the Abbey â‚?5.7Â million in January 2006 to be spread over three years. The grant represented an approximate 43Â percent increase in the Abbey's revenues and was the largest grant ever awarded by the Arts Council. The new company was established on 1 February 2006, with the announcement of a new Abbey Board chaired by High Court Judge Bryan McMahon. In March 2007, the larger auditorium in the theatre was radically reconfigured by Jean-Guy Lecat as part of a larger upgrading of the theatre. 

More than 20 writers have been under commission by the Abbey since MacÂ Conghail was appointed director in May 2005. A developing trend is for the Abbey to produce new Irish plays commissioned and developed by London's Royal Court theatre; Tom Murphy's Alice Trilogy and Marina Carr's Woman and Scarecrow are examples.

After discussions over many years, the Irish government announced in 2007 that a new theatre building would be procured for the Abbey by way of a public-private partnership contract for design, construction, financing and maintenance. This building will be in Dublin's "Docklands" area and will comprise three auditorium spaces, including a 700-seat main theatre, a 350-seat secondary performance space and a 150-seat studio theatre, along with rehearsal and education facilities, storage, wardrobe, archive and office space, and one or more bars and restaurants and a bookshop. The general and artistic operation of the new theatre will continue to be the responsibility of the Abbey Theatre Amharclann na Mainistreach Ltd.











The Council of Lithuania (, , ), after July 11 1918  The State Council of Lithuania (Lithuanian: Lietuvos ValstybÄ—s Taryba), was convened at the Vilnius Conference that took place between September 18 and 23, 1917. The council was granted the executive authority of the Lithuanian people and was entrusted to establish an independent Lithuanian state. On February 16, 1918, the members of the council signed the Act of Independence of Lithuania, and declared Lithuania an independent state based on democratic principles. The council managed to establish the proclamation of independence despite the presence of German troops in the country until the autumn of 1918. The council continued its efforts until the Constituent Assembly of Lithuania () first met on May 15, 1920.

After the last Partition of the Polish-Lithuanian Commonwealth in 1795, Lithuania had become part of the Russian Empire. During the 19th century, both the Poles and the Lithuanians attempted to restore their independence. They rebelled during the November Uprising in 1830 and the January Uprising in 1863, but the first realistic opportunity came about during the World War I. In 1915, Germany occupied Lithuania as its troops marched towards Russia. After the Russian Revolution in 1917, opportunities for independence opened up. Germany, avoiding direct annexation, tried to find a middle path that would involve some kind of union with Germany. In the light of upcoming peace negotiations with Russia, the Germans agreed to allow the Vilnius Conference, hoping that it would proclaim that the Lithuanian nation wanted to be detached from Russia and wished for a closer relationship with Germany. However, the conference, held between September 18 and 23, 1917, adopted a resolution that an independent Lithuania should be established and that a closer relationship with Germany would depend on whether it recognized the new state. On September 21st, the attendees at the conference elected a 20-member Council of Lithuania to establish this resolution. The German authorities did not allow that resolution to be published, but they did permit the council to proceed. The authorities censored the Council's newspaper, Lietuvos aidas (Echo of Lithuania), preventing the Council from reaching a wider public audience. The Vilnius Conference also resolved that a constituent assembly should be elected by popular vote as soon as possible.

The twenty men who comprised the Council were of different ages (the youngest was 25; the oldest 66), social status, professions, and political affiliations. There were eight lawyers, four priests, three agronomists, two financiers, a doctor, a publisher, and an engineer. Eight of the members were Christian democrats and seven were not affiliated. All except one held degrees in higher education, and all were multilingual, fluent at a minimum in Lithuanian and Russian and often in Polish and German as well. The Council's last surviving member, Aleksandras Stulginskis, died in September 1969.

During the first meeting on September 24, Antanas Smetona was elected as the chairman of the council. The chairman, two vice-chairmen, and two secretaries made up the presidium. The vice-chairs and secretaries would change from time to time, but Smetona retained the chairmanship until 1919 when he was elected the first President of Lithuania. Smetona was succeeded by Stasys Å ilingas as the chairman. He was not among the original twenty members. The first change in membership took place on July 13, 1918, when six new members (Martynas YÄas, Augustinas Voldemaras, Juozas Purickis, Eliziejus Draugelis, Jurgis Alekna and Stasys Å ilingas) were admitted and four (Kairys, VileiÅ¡is, BirÅ¾iÅ¡ka, NarutaviÄius) resigned. By spring of 1919, the council had almost doubled in size.

Soon after the council was elected, major developments took place in Russia. The October Revolution brought the Bolsheviks to power. They signed a truce with Germany on December 2, 1917 and started peace negotiations. Germany needed some documentation of its relationship with Lithuania. In the so-called Berlin Protocol Germany offered to recognize Lithuanian independence if the latter agreed to form a firm and permanent federation with Germany, based on conventions concerning military affairs, transportation, customs, and currency. The council agreed, on condition that Lithuania would decide its own internal affairs and foreign policy. The Germans rejected this proposal. On December 11, the council adopted a resolution agreeing to a "firm and permanent alliance" with Germany on the basis of the four conventions. Only fifteen members voted for this resolution, but all twenty signed it. 

The Germans broke their promise and did not recognize the state and did not invite its delegation to the negotiations of Treaty of Brest-Litovsk. Lithuanians, including those living abroad, disapproved of the December 11 declaration. The declaration, seen as pro-German, was an obstacle in establishing diplomatic relations with England, France and the United States, the enemies of Germany. On January 8, the same day that Woodrow Wilson announced his Fourteen Points, the council proposed amendments to the declaration of December 11 calling for a constituent assembly. The amendments were rejected by the Germans and it was made clear that the Council would serve only advisory functions. The council was torn apart and a few members threatened to leave. On February 16th, the council, temporarily chaired by Jonas BasanaviÄius, decided to re-declare independence, this time mentioning nothing specific about a relationship with Germany. That was left for a constituent assembly to decide. February 16th is now one of Lithuania's two official Independence Days.

The Germans were not satisfied with the new declaration and demanded that the council go back to the December 11 decision. On March 3, Germany and Bolshevik Russia signed the Treaty of Brest-Litovsk. It declared that the Baltic nations were in the German interest zone and that Russia renounced any claims to them. On March 23, Germany recognized independent Lithuania on the basis of the December 11 declaration. However, nothing in essence changed either in Lithuania or in Council's status: any efforts to establish administration were hindered. The form of government, however, was left undecided. Germany, ruled by a kaiser, obviously preferred a monarchy. They proposed a personal union with the Prussian Hohenzollern dynasty. As an alternative, on June 4 the council voted to invite Duke Wilhelm of Urach, Count of WÃ¼rttemberg, to become the monarch of Lithuania. He agreed and was elected King of Lithuania (Mindaugas II) on July 13, 1918. The decision was very controversial and four members of the council left in protest. 

Germany did not recognize the new king and its relationship with the council remained tense. The council was not allowed to determine the borders of Lithuania, establish an embassy in Berlin, or begin forming a stable administrative system. It received small funds to cover its expenses only in September of 1918. The situation changed when the German Revolution started and Germany lost the war in fall of 1918 - it was no longer in a position to dictate terms. On November 2, the council adopted the first provisional constitution. The decision to invite King Mindaugas II was annulled and this helped to reconcile the political factions. The functions of government were entrusted to a 3-member presidium, and Augustinas Voldemaras was invited to form the first Cabinet of Ministers. The first government was formed on November 11, 1918, the day that Germany signed the armistice in CompiÃ¨gne. The council began to organize an army, police, local government, and other institutions. It also expanded to include ethnic minorities (Jews and Belarusians).

As German forces retreated and Bolshevik forces approached Vilnius, on January 2, 1919 the council moved to Kaunas. The Freedom Wars started. On April 4, the second provisional constitution was adopted, creating the office of President of Lithuania. Antanas Smetona, as the chairman of the council, became the first president. The German forces did not leave Lithuania until July 1919. Due to wars and other turmoil, elections to the Constituent Assembly of Lithuania were not held until spring of 1920. The council adjourned on May 15, 1920.











Herbig-Haro objects are small patches of nebulosity associated with newly-born stars, and are formed when gas ejected by young stars collides with clouds of gas and dust nearby at speeds of several hundred kilometres per second. Herbig-Haro objects are ubiquitous in star-forming regions, and several are often seen around a single star, aligned along its rotational axis.



HH objects are transient phenomena, lasting only a few thousand years at most. They can evolve visibly over quite short timescales as they move rapidly away from their parent star into the gas clouds in interstellar space (the interstellar medium or ISM). Hubble Space Telescope observations reveal complex evolution of HH objects over a few years, as parts of them fade while others brighten as they collide with clumpy material in the interstellar medium.

The objects were first observed in the late 19th century by Sherburne Wesley Burnham, but were not recognised as being a distinct type of emission nebula until the 1940s. The first astronomers to study them in detail were George Herbig and Guillermo Haro, after whom they have been named. Herbig and Haro were working independently on studies of star formation when they first analysed Herbig-Haro objects, and recognised that they were a by-product of the star formation process.

The first Herbig-Haro object was observed in the late 19th century by Burnham, when he looked at the star T Tauri with the  refracting telescope at Lick Observatory and noted a small patch of nebulosity nearby. However, it was catalogued merely as an emission nebula, later becoming known as Burnham's Nebula, and was not recognised as a distinct class of object. However, T Tauri was found to be a very young and variable star, and is the prototype of the class of similar objects known as T Tauri stars which have yet to reach a state of hydrostatic equilibrium between gravitational collapse and energy generation through nuclear fusion at their centres.



Fifty years after Burnham's discovery, several similar nebulae were discovered which were so small as to be almost star-like in appearance. Both Haro and Herbig made independent observations of several of these objects during the 1940s. Herbig also looked at Burnham's Nebula and found that it displayed an unusual electromagnetic spectrum, with prominent emission lines of hydrogen, sulphur and oxygen. Haro found that all the objects of this type were invisible in infrared light.

Following their independent discoveries, Herbig and Haro met at an astronomy conference in Tucson, Arizona. Herbig had initially paid little attention to the objects he had discovered, being primarily concerned with the nearby stars, but on hearing Haro's findings he carried out more detailed studies of them. The Soviet astronomer Viktor Ambartsumian gave the objects their name, and based on their occurrence near young stars (a few hundred thousand years old), suggested that they might represent an early stage in the formation of T Tauri stars.

Studies showed that HH objects were highly ionised, and early theorists speculated that they might contain low-luminosity hot stars. However, the absence of infrared radiation from the nebulae meant there could not be stars within them, as these would have emitted abundant infrared light. Later studies suggested that the nebulae might contain protostars, but eventually HH objects came to be understood as material ejected by nearby young stars, and colliding at supersonic speeds with the interstellar medium (ISM), with the resulting shock waves generating visible light .

In the early 1980s, observations revealed for the first time the jet-like nature of most HH objects. This led to the understanding that the material ejected to form HH objects is highly collimated (concentrated into narrow jets). Stars are often surrounded by accretion disks in their first few hundred thousand years of existence, which form as gas falls onto them, and the rapid rotation of the inner parts of these disks leads to the emission of narrow jets of partially ionized plasma perpendicular to the disk, known as polar jets. When these jets collide with the interstellar medium, they give rise to the small patches of bright emission which comprise HH objects .



Emission from HH objects is caused by shock waves when they collide with the interstellar medium, but their motions are complicated. Spectroscopic observations of their doppler shifts indicate velocities of several hundred kilometres per second, but the emission lines in the spectra of HH objects are too weak to have been formed in such high speed collisions. This probably means that some of the material they are colliding with is also moving outwards, although at a slower speed .

The total mass being ejected to form typical HH objects is estimated to be of the order of 1â€?0 Earth-masses, a very small amount of material compared to the mass of the stars themselves . The temperatures observed in HH objects are typically about 8000â€?2,000Â K, similar to those found in other ionized nebulae such as H II regions and planetary nebulae. They tend to be quite dense, with densities ranging from a few thousand to a few tens of thousands of particles per cmÂ³, compared to generally less than 1000/cmÂ³ in H II regions and planetary nebulae . HH objects consist mostly of hydrogen and helium, which account for about 75% and 25% respectively of their mass. Less than 1% of the mass of HH objects is made up of heavier chemical elements, and the abundances of these are generally similar to those measured in nearby young stars .

Near to the source star, about 20â€?0% of the gas in HH objects is ionised, but this proportion decreases at increasing distances. This implies that the material is ionised in the polar jet, and recombines as it moves away from the star, rather than being ionised by later collisions. Shocking at the end of the jet can re-ionise some material, however, giving rise to bright "caps" at the ends of the jets.

Over 400 individual HH objects or groups are now known. They are ubiquitous in star-forming H II regions, and are often found in large groups. They are typically observed near Bok globules (dark nebulae which contain very young stars) and often emanate from them. Frequently, several HH objects are seen near a single energy source, forming a string of objects along the line of the polar axis of the parent star.

The number of known HH objects has increased rapidly over the last few years, but is still thought to be a very small proportion of the total number existing in our galaxy. Estimates suggest that up to 150,000 exist , the vast majority of which are too far away to be resolved with current technological capabilities. Most HH objects lie within 0.5 parsecs of their parent star, with very few found more than 1Â pc away. However, some are seen several parsecs away, perhaps implying that the interstellar medium is not very dense in their vicinity, allowing them to travel further from their source before dispersing. 



Spectroscopic observations of HH objects show that they are moving away from the source stars at speeds of 100 to 1000Â km/s. In recent years, the high optical resolution of Hubble Space Telescope observations has revealed the proper motion of many HH objects in observations spaced several years apart. These observations have also allowed estimates of the distances of some HH objects via the expansion parallax method.

As they move away from the parent star, HH objects evolve significantly, varying in brightness on timescales of a few years. Individual knots within an object may brighten and fade or disappear entirely, while new knots have been seen to appear. As well as changes caused by interactions with the ISM, interactions between jets moving at different speeds within HH objects also cause variations.

The eruption of jets from the parent stars occurs in pulses rather than as a steady stream. The pulses may produce jets of gas moving in the same direction but at different speeds, and interactions between different jets create so-called "working surfaces", where streams of gases collide and generate shock waves.



The stars which are behind the creation of Herbig-Haro objects are all very young stars, the youngest of which are still protostars in the process of forming from the surrounding gases. Astronomers divide these stars into classes 0, I, II and III, according to how much infrared radiation the stars give off . A greater amount of infrared radiation implies a larger amount of cooler material surrounding the star, which indicates that it is still coalescing. The numbering of the classes arises because class 0 objects (the youngest) were not discovered until classes I, II and III had already been defined.

Class 0 objects are only a few thousand years old, so young that they are not yet undergoing nuclear fusion reactions at their centres. Instead, they are powered only by the gravitational potential energy released as material falls onto them . Nuclear fusion has begun in the cores of Class I objects, but gas and dust are still falling onto their surfaces from the surrounding nebula. They are generally still shrouded in dense clouds of dust and gas, which obscure all their visible light and mean that they can only be observed at infrared and radio wavelengths. Infall of gas and dust has largely finished in Class II objects, but they are still surrounded by disks of dust and gas, while class III objects have only trace remnants of their original accretion disk.

Studies have shown that about 80% of the stars giving rise to HH objects are in fact binary or multiple systems (two or more stars orbiting each other), which is a much higher proportion than that found for low mass stars on the main sequence. This may indicate that binary systems are more likely to generate the jets which give rise to HH objects, and evidence suggests that the largest HH outflows might be formed when multiple systems disintegrate. It is thought that most stars form as multiple systems, but that a sizable fraction are disrupted before they reach the main sequence, by gravitational interactions with nearby stars and dense clouds of gas .



Herbig-Haro (HH) objects associated with very young stars or very massive protostars are often hidden from view at optical wavelengths by the cloud of gas and dust from which they form. This surrounding natal material can produce tens or even hundreds of magnitudes of extinction at optical wavelengths. Such deeply embedded objects can only be observed at infrared or radio wavelengths , usually in the light of hot molecular hydrogen or warm carbon monoxide emission. 

In recent years, infrared images have revealed dozens of examples of "infrared HH objects". Most look like bow waves (similar to the waves at the head of a sailing ship), and so are usually referred to as molecular "bow shocks". Like HH objects, these supersonic shocks are driven by collimated jets from the two poles of a protostar. They sweep up or "entrain" the surrounding dense molecular gas to form a continuous flow of material, which is referred to as a Bipolar outflow. Infrared bow shocks travel at hundreds of kilometers per second, heating gas to hundreds or even thousands of degrees. Because they are associated with the youngest stars, where accretion is particularly strong, infrared bow shocks are usually associated with more powerful jets than their optical HH cousins.

The physics of infrared bow shocks can be understood in much the same way as that of HH objects, since these objects are essentially the same - it is only the conditions in the jet and surrounding cloud that are different, causing infrared emission from molecules rather than optical emission from atoms and ions .













Algeria (, Al Jaza'ir , Berber: , Dzayer ), officially the People's Democratic Republic of Algeria, is the second largest country on the African continent and the 11th largest country in the world in terms of total area. It is bordered by Tunisia in the northeast, Libya in the east, Niger in the southeast, Mali and Mauritania in the southwest, a few kilometers of the Western Sahara in the west, Morocco in the northwest, and the Mediterranean Sea in the north.

Algeria is a member of the United Nations, African Union, Arab League, and OPEC. It also contributed towards the creation of the Arab Maghreb Union. Constitutionally, Algeria is defined as an Islamic, Arab, and Amazigh (Berber) country.

Al-jazÄâ€™ir is itself a truncated form of the city's older name jazÄâ€™ir banÄ« mazghannÄ, "the islands of (the tribe) Bani Mazghanna", used by early medieval geographers such as al-Idrisi and Yaqut al-Hamawi.





Algeria has been inhabited by Berbers (or Imazighen) since at least 10,000 BC. After 1000 BC, the Carthaginians began establishing settlements along the coast. The Berbers seized the opportunity offered by the Punic Wars to become independent of Carthage, and Berber kingdoms began to emerge, most notably Numidia. In 200 BC, however, they were once again taken over, this time by the Roman Republic. When the Western Roman Empire collapsed, Berbers became independent again in many areas, while the Vandals took control over other parts, where they remained until expelled by the generals of the Byzantine Emperor, Justinian I. The Byzantine Empire then retained a precarious grip on the east of the country until the coming of the Arabs in the eighth century.

Having converted the Kutama of Kabylie to its cause, the Shia Fatimids overthrew the Rustamids, and conquered Egypt. They left Algeria and Tunisia to their Zirid vassals; when the latter rebelled and adopted Sunnism, the Shia Fatimids sent in the Banu Hilal, a populous Arab tribe, to weaken them. This initiated the Arabization of the region. The Almoravids and Almohads, Berber dynasties from the west founded by religious reformers, brought a period of relative peace and development; however, with the Almohads' collapse, Algeria became a battleground for their three successor states, the Algerian Zayyanids, Tunisian Hafsids, and Moroccan Marinids. In the fifteenth and sixteenth centuries, the Spanish Empire started attacking and subsuming a few Algerian coastal settlements.

Algeria was brought into the Ottoman Empire by Khair ad-Din and his brother Aruj in 1517, and they established Algeria's modern boundaries in the north and made its coast a base for the Ottoman corsairs; their privateering peaked in Algiers in the 1600s. Piracy on American vessels in the Mediterranean resulted in the First (1801â€?805) and Second Barbary War (1815) with the United States. Those piracy acts forced people captured on the boats into slavery; alternatively when the pirates attacked coastal villages in southern and western Europe the inhabitants were forced into slavery.Raids by Barbary pirates on Western Europe did not cease until 1816, when a Royal Navy raid, assisted by six Dutch vessels, destroyed the port of Algiers and its fleet of Barbary ships.Spanish occupation of Algerian ports at this time was a source of concern for the local inhabitants.

On the pretext of a slight to their consul, the French invaded Algiers in 1830. In contrast to Morocco and Tunisia, the conquest of Algeria by the French was long and particularly violent and resulted in the disappearance of about a third of the Algerian population. France was responsible for the extermination of 1.5 million Algerians. According to Olivier Le Cour Grandmaison, the French pursued a policy of extermination against the Algerians.

The French conquest of Algeria was slow due to intense resistance from such as Emir Abdelkader, Ahmed Bey and Fatma N'Soumer. Indeed the conquest was not technically complete until the early 1900s when the last Tuareg were conquered.

Meanwhile, however, the French made Algeria an integral part of France, a status that would end only with the collapse of the Fourth Republic in 1958. Tens of thousands of settlers from France, Spain, Italy, and Malta moved in to farm the Algerian coastal plain and occupy significant parts of Algeria's cities. These settlers benefited from the French government's confiscation of communally held land, and the application of modern agriculture techniques that increased the amount of arable land. Algeria's social fabric suffered during the occupation: literacy plummeted, while land confiscation uprooted much of the population.

Starting from the end of the nineteenth century, people of European descent in Algeria (or natives like Spanish people in Oran), as well as the native Algerian Jews (typically Sephardic in origin), became full French citizens. After Algeria's 1962 independence, they were called Pieds-Noirs. In contrast, the vast majority of Muslim Algerians (even veterans of the French army) received neither French citizenship nor the right to vote.

In 1954, the National Liberation Front (FLN) launched the Algerian War of Independence which was a guerrilla campaign. By the end of the war, newly elected President Charles de Gaulle, understanding that the age of empire was ending, held a plebiscite, offering Algerians three options. This resulted in an overwhelming vote for complete independence from the French Colonial Empire. Over one million people, 10% of the population, then fled the country for France in just a few months in mid-1962. These included most of the 1,025,000 Pieds-Noirs, as well as 81,000 Harkis (pro-French Algerians serving in the French Army).

As feared, there were widespread reprisals against those who remained in Algeria. It is estimated that somewhere between 50,000 and 150,000 Harkis and their dependents were killed by the FLN or by lynch mobs in Algeria, sometimes in circumstances of extreme cruelty.

In foreign policy, Algeria was a member and leader of the Non-Aligned Movement. A dispute with Morocco over the Western Sahara nearly led to war. While Algeria shares much of its history and cultural heritage with neighbouring Morocco, the two countries have had somewhat hostile relations with each other ever since Algeria's independence. This is for two reasons: Morocco's disputed claim to portions of western Algeria (which led to the Sand War in 1963), and Algeria's support for the Polisario Front, an armed group of Sahrawi refugees seeking independence for the Moroccan-ruled Western Sahara, which it hosts within its borders in the city of Tindouf.

Within Algeria, dissent was rarely tolerated, and the state's control over the media and the outlawing of political parties other than the FLN was cemented in the repressive constitution of 1976.

BoumÃ©dienne died in 1978, but the rule of his successor, Chadli Bendjedid, was little more open. The state took on a strongly bureaucratic character and corruption was widespread.

The modernization drive brought considerable demographic changes to Algeria. Village traditions underwent significant change as urbanization increased. New industries emerged, agricultural employment was substantially reduced. Education was extended nationwide, raising the literacy rate from less than 10% to over 60%. There was a dramatic increase in the fertility rate to 7-8 children per mother.

Therefore by 1980, there was a very youthful population and a housing crisis. The new generation struggled to relate to the cultural obsession with the war years and two conflicting protest movements developed: left-wingers, including Berber identity movements; and Islamic 'intÃ©gristes'. Both groups protested against one-party rule but also clashed with each other in universities and on the streets during the 1980s. Mass protests from both camps in Autumn 1988 forced Bendjedid to concede the end of one-party rule. Elections were planned to happen in 1991.In December 1991, the Islamic Salvation Front won the first round of the country's first multi-party elections. The military then intervened and cancelled the second round, forced then-president Bendjedid to resign, and banned all political parties based on religion (including the Islamic Salvation Front). The ensuing conflict engulfed Algeria in the violent Algerian Civil War.

More than 160,000 people were killed between 17 January 1992 and June 2002. Most of the deaths were between militants and government troops, but a great number of civilians were also killed. The question of who was responsible for these deaths was controversial at the time amongst academic observers; many were claimed by the Armed Islamic Group. Though many of these massacres were carried out by Islamic extremists, the Algerian regime itself has used the army and foreign mercenaries to conduct horrific massacres of men, women and children and then blame it upon all Islamic groups within the country in a campaign to discredit them and Islam amongst the wider population.Elections resumed in 1995, and after 1998, the war waned. On 27 April 1999, after a series of short-term leaders representing the military, Abdelaziz Bouteflika, the current president, was elected.

By 2002, the main guerrilla groups had either been destroyed or surrendered, taking advantage of an amnesty program, though sporadic fighting continued in some areas (See Islamic insurgency in Algeria (2002â€“present)).

The issue of Berber language and identity increased in significance, particularly after the extensive Kabyle protests of 2001 and the near-total boycott of local elections in Kabylie. The government responded with concessions including naming of Tamazight (Berber) as a national language and teaching it in schools.

Much of Algeria is now recovering and developing into an emerging economy. The high prices of oil and gas are being used by the new government to improve the country's infrastructure and especially improve industry and agricultural land. Recently, overseas investment in Algeria has increased.

Most of the coastal area is hilly, sometimes even mountainous, and there are a few natural harbours. The area just south of the coast, known as the Tell Atlas, is fertile. Further south is the Atlas mountain range and the Sahara desert. The Ahaggar Mountains (Arabic: Ø¬Ø¨Ø§Ù„ Ù‡Ù‚Ø§Ø±â€?, also known as the Hoggar, are a highland region in central Sahara, southern Algeria. They are located about 1,500Â km (932 miles) south of the capital, Algiers and just west of Tamanghasset. 

Algiers, Oran , Constantine, and Annaba are Algeria's main cities.

Northern Algeria is in the temperate zone and has a mild, Mediterranean climate. It lies within approximately the same latitudes as Southern California and has somewhat similar climatic conditions. Its broken topography, however, provides sharp local contrasts in both prevailing temperatures and incidence of rainfall. Year-to-year variations in climatic conditions are also common.

In the Tell Atlas, temperatures in summer average between 21 and 24 Â°C and in winter drop to 10 to 12 Â°C. Winters are not particularly cold, but the humidity level is high. In eastern Algeria, the average temperatures are somewhat lower, and on the steppes of the High Atlas plateaux, winter temperatures hover only a few degrees above freezing. A prominent feature of the climate in this region is the sirocco, a dusty, choking south wind blowing off the desert, sometimes at gale force. This wind also occasionally reaches into the coastal Tell.



In Algeria, only a relatively small corner of the torrid Sahara lies across the Tropic of Cancer in the torrid zone. In this region even in winter, midday desert temperatures can be very hot. After sunset, however, the clear, dry air permits rapid loss of heat, and the nights are cool to chilly. Enormous daily ranges in temperature are recorded.

Rainfall is fairly abundant along the coastal part of the Tell Atlas, ranging from 400 to 670 mm annually, the amount of precipitation increasing from west to east. Precipitation is heaviest in the northern part of eastern Algeria, where it reaches as much as 1000 mm in some years. Farther inland, the rainfall is less plentiful. Prevailing winds that are easterly and north-easterly in summer change to westerly and northerly in winter and carry with them a general increase in precipitation from September through December, a decrease in the late winter and spring months, and a near absence of rainfall during the summer months. Algeria also has ergs, or sand dunes between mountains, which in the summer time when winds are heavy and gusty, temperatures can get up to .



The head of state is the President of the Republic, who is elected to a five-year term, renewable once. Algeria has universal suffrage at age 18. The President is the head of the Council of Ministers and of the High Security Council. He appoints the Prime Minister who is also the head of government. The Prime Minister appoints the Council of Ministers.

The Algerian parliament is bicameral, consisting of a lower chamber, the National People's Assembly (APN), with 380 members; and an upper chamber, the Council Of Nation, with 144 members. The APN is elected every five years.

Under the 1976 constitution (as modified 1979, and amended in 1988, 1989, and 1996) Algeria is a multi-party state. All parties must be approved by the Ministry of the Interior. To date, Algeria has had more than 40 legal political parties. According to the constitution, no political association may be formed if it is "based on differences in religion, language, race, gender or region."

Tensions between Algeria and Morocco in relation with the Western Sahara conflict, have put great obstacles in the way of tightening the Maghreb Arab Union, nominally established in 1989 but with little practical weight, with its coastal neighbors.

Algeria is currently divided into 48 provinces (wilayas), 553 districts (daÃ¯ras) and 1,541 municipalities (communes, baladiyahs). Each province, district, and municipality is named after its seat, which is mostly also the largest city.

According to the Algerian constitution, a province is a territorial collectivity enjoying some economic freedom. The People's Provincial Assembly is the political entity governing a province, which has also a "president", who is elected by the members of the that assembly. They are in turn elected on universal suffrage every five years. The "Wali" (Prefect or governor) directs each province. This person is chosen by the Algerian President to handle the PPA's decisions.

The administrative divisions have changed several times since independence. When introducing new provinces, the numbers of old provinces are kept, hence the non-alphabetical order. With their official numbers, currently (since 1983) they are: 

The fossil fuels energy sector is the backbone of Algeria's economy, accounting for roughly 60% of budget revenues, 30% of GDP, and over 95% of export earnings. The country ranks fourteenth in petroleum reserves, containing 11.8 billion barrels of proven oil reserves with estimates suggesting that the actual amount is even more. The U.S. Energy Information Administration reported that in 2005, Algeria had 160 trillion cubic feet (Tcf) of proven natural gas reserves, the eighth largest in the world.

Algeriaâ€™s financial and economic indicators improved during the mid-1990s, in part because of policy reforms supported by the International Monetary Fund (IMF) and debt rescheduling from the Paris Club. Algeriaâ€™s finances in 2000 and 2001 benefited from an increase in oil prices and the governmentâ€™s tight fiscal policy, leading to a large increase in the trade surplus, record highs in foreign exchange reserves, and reduction in foreign debt. The government's continued efforts to diversify the economy by attracting foreign and domestic investment outside the energy sector have had little success in reducing high unemployment and improving living standards, however. In 2001, the government signed an Association Treaty with the European Union that will eventually lower tariffs and increase trade. In March 2006, Russia agreed to erase $4.74 billion of Algeria's Soviet-era debt during a visit by President Vladimir Putin to the country, the first by a Russian leader in half a century. In return, president Bouteflika agreed to buy $7.5 billion worth of combat planes, air-defense systems and other arms from Russia, according to the head of Russia's state arms exporter Rosoboronexport.

Algeria also decided in 2006 to pay off its full $8bn (Â£4.3bn) debt to the Paris Club group of rich creditor nations before schedule. This will reduce the Algerian foreign debt to less than $5bn in the end of 2006. The Paris Club said the move reflected Algeria's economic recovery in recent years.

Since Roman times Algeria has been noted for the fertility of its soil. 9.4% of Algerians are employed in the agricultural sector. 

A considerable amount of cotton was grown at the time of the United States' Civil War, but the industry declined afterwards. In the early years of the twentieth century efforts to extend the cultivation of the plant were renewed. A small amount of cotton is also grown in the southern oases. Large quantities of a vegetable that resembles horsehair, an excellent fiber, are made from the leaves of the dwarf palm. The olive (both for its fruit and oil) and tobacco are cultivated with great success. 

More than 7,500,000 acres (30,000Â kmÂ²) are devoted to the cultivation of cereal grains. The Tell is the grain-growing land. During the time of French rule its productivity was increased substantially by the sinking of artesian wells in districts which only required water to make them fertile. Of the crops raised, wheat, barley and oats are the principal cereals. A great variety of vegetables and fruits, especially citrus products, are exported. Algeria also exports figs, dates, esparto grass, and cork. It is the largest oat market in Africa.. 

Algeria is known for Bertolli's olive oil spread, although the spread has an Italian background.

The current population of Algeria is 33,333,216 (July 2007 est.).About 70% of Algerians live in the northern, coastal area; the minority who inhabit the Sahara are mainly concentrated in oases, although some 1.5 million remain nomadic or partly nomadic. Almost 30% of Algerians are under 15. Algeria has the fourth lowest fertility rate in the Greater Middle East after Cyprus, Tunisia, and Turkey.

97% of the population is classified ethnically as Berber/Arab and religiously as Sunni Muslim 97% , the few non-Sunni Muslims are mainly Ibadis 1.3% from the M'Zab valley. (See also Islam in Algeria.) A mostly foreign Roman Catholic community of about 45,000 exists, along with about 350,000 Protestant Christians, and some 500 Jewish. The Jewish community of Algeria, which once constituted 2% of the total population, has substantially decreased due to emigration, mostly to France and Israel. 

Europeans account for less than 1% of the population, inhabitating almost exclusively the largest metropolitan areas. However, during the colonial period there was a large (15.2% in 1962) European population, consisting primarily of French people, in addition to Spaniards in the west of the country, Italians and Maltese in the east, and other Europeans in smaller numbers known as pieds-noirs, concentrated on the coast and forming a majority in cities like BÃ´ne, Oran, Sidi Bel AbbÃ¨s, and Algiers. Almost all of this population left during or immediately after the country's independence from France.Housing and medicine continue to be pressing problems in Algeria. Failing infrastructure and the continued influx of people from rural to urban areas has overtaxed both systems. According to the UNDP, Algeria has one of the world's highest per housing unit occupancy rates for housing, and government officials have publicly stated that the country has an immediate shortfall of 1.5 million housing units.

Women make up 70 percent of Algeriaâ€™s lawyers and 60 percent of its judges. Women dominate medicine. Increasingly, women contribute more to household income than men. Sixty percent of university students are women, university researchers say.

Most Algerians are Berber or Arab, by language or identity, but almost all Algerians are Berber in origin. Today, the Arab-Berber issue is often a case of self-identification or identification through language and culture, rather than a racial or ethnic distinction. The Berber people are divided into several ethnic groups, Kabyle in the mountainous north-central area, Chaoui in the eastern Atlas Mountains, Mozabites in the M'zab valley, and Tuareg in the far south. Small pockets of Black African populations also are in Algeria.Turkish Algerians represent 5% of the population and are living mainly in the big cities. (citation needed)

Education is officially compulsory for children between the ages of 6 and 15. In the year 1997, there was an outstanding amount of teachers and students in primary schools. 

In Algeria there are 10 universities, seven colleges, and five institutes for higher learning. The University of Algiers (founded in 1909), which is located in the capital of Algeria, Algiers has about 267,142 students. The Algeran school system is structured into Basic, General Secondary, and Technical Secondary levels:

Modern Algerian literature, split between Arabic and French, has been strongly influenced by the country's recent history. Famous novelists of the twentieth century include Mohammed Dib, Albert Camus, and Kateb Yacine, while Assia Djebar is widely translated. Among the important novelists of the 1980s were Rachid Mimouni, later vice-president of Amnesty International, and Tahar Djaout, murdered by an Islamist group in 1993 for his secularist views. In philosophy and the humanities, Jacques Derrida, the father of deconstruction, was born in El Biar in Algiers; Malek Bennabi and Frantz Fanon are noted for their thoughts on decolonization; Augustine of Hippo was born in Tagaste (modern-day Souk Ahras); and Ibn Khaldun, though born in Tunis, wrote the Muqaddima while staying in Algeria.Algerian culture has been strongly influenced by Islam, the main religion. The works of the Sanusi family in pre-colonial times, and of Emir Abdelkader and Sheikh Ben Badis in colonial times, are widely noted. The Latin author Apuleius was born in Madaurus (Mdaourouch), in what later became Algeria.

The Algerian musical genre best known abroad is raÃ¯, a pop-flavored, opinionated take on folk music, featuring international stars such as Khaled and Cheb Mami. However, in Algeria itself the older, highly verbal chaabi style remains more popular, with such stars as El Hadj El Anka, Dahmane El Harrachi and El Hachemi Guerouabi, while the tuneful melodies of Kabyle music, exemplified by Idir, Ait Menguellet, or LounÃ¨s Matoub, have a wide audience. For more classical tastes, Andalusi music, brought from Al-Andalus by Morisco refugees, is preserved in many older coastal towns.

In painting, Mohammed Khadda and M'Hamed Issiakhem have been notable in recent years.





Most Algerians speak Algerian Arabic.Arabic is spoken natively in dialectal form ("Darja") by some 83.2% of the population. However in the media and official occasions the spoken language is Standard Arabic.

The Berbers (or Imazighen), who form approximately 45% of the population, largely speak one of the various dialects of Tamazight as opposed to Arabic. But a majority can use the both, Berber and Algerian Arabic. Arabic remains Algeria's only official language, although Tamazight has recently been recognized as a national language alongside it. 

Ethnologue counts eighteen living languages within Algeria, splitting both Arabic and Tamazight into several different languages, as well as including the Korandje language, which is unrelated to Arabic or Tamazight.

The language issue is politically sensitive, particularly for the Berber minority, which has been disadvantaged by state-sanctioned Arabization. Language politics and Arabization have partly been a reaction to the fact that 130 years of French colonization had left both the state bureaucracy and much of the educated upper class completely Francophone, as well as being motivated by the Arab nationalism promoted by successive Algerian governments. 

French is still the most widely studied foreign language, but very rarely spoken as a native language. Since independence, the government has pursued a policy of linguistic Arabization of education and bureaucracy, with some success, although many university courses continue to be taught in French. Recently, schools have started to incorporate French into the curriculum as early as children start to learn Arabic, as many Algerians are fluent in French. French is also used in media and commerce.



The Armed forces of Algeria consist of: 

It is the direct successor of the ArmÃ©e de LibÃ©ration Nationale (ALN), which fought French colonial occupation during the Algerian War of Independence (1954-62).

The People's National Army consists of 127,500 members, with some 100,000 reservists. The army is under the control of the president, who also is minister of National Defense (current president is Abdelaziz Bouteflika). Defense expenditures accounted for some $2.67 billion or 3.5% of GDP. One and a half years of national military service is compulsory for males.

Algeria is a leading military power in North Africa and has its force oriented toward its western (Morocco) and eastern (Libya) borders. Its primary military supplier has been the former Soviet Union, which has sold various types of sophisticated equipment under military trade agreements, and the People's Republic of China. Algeria has attempted, in recent years, to diversify its sources of military material. Military forces are supplemented by a 45,000-member gendarmerie or rural police force under the control of the president and 30,000-member SÃ»retÃ© nationale or Metropolitan police force under the Ministry of the Interior. 

Recently, the Algerian Air Force signed a deal with Russia to purchase 49 MiG-29SMT and 6 MiG-29UBT at an estimated $1.5 Billion. They also agreed to return old airplanes purchased from the Former USSR. Russia is also building 2 636-type diesel submarines for Algeria.

There are several UNESCO World Heritage Sites in Algeria including Al Qal'a of Beni Hammad, the first capital of the Hammadid empire; Tipasa, a Phoenician and later Roman town; and DjÃ©mila and Timgad, both Roman ruins; M'Zab Valley, a limestone valley containing a large urbanized oasis; also the Casbah of Algiers is an important citadel. The only natural World Heritage Sitesis the Tassili n'Ajjer, a mountain range.















A city is an urban settlement with a particularly important status which differentiates it from a town. 

City is primarily used to designate an urban settlement with a large population. However, city may also indicate a special administrative, legal, or historical status. 

In the United States, "city" is primarily a legal term meaning an urban area with a degree of autonomy (i.e. a township), rather than meaning an entire large settlement (metropolitan area). Outside the United States, "city" implies an entire settlement or metropolitan area, although there are notable exceptions, e.g. the City of London. In the United Kingdom, a city is a settlement with a charter ("letters patent") from the Crown, giving that settlement city status.

Present-day cities are products of the industrial revolution and are generally distinguished by land area and population. Large, industrialized cities generally have advanced organizational systems for sanitation, utilities, land distribution, housing, and transportation. In economic terms, a city is simply defined as the absence of physical space between people and firms. This close proximity greatly facilitates interaction between people and firms, benefiting both parties in the process. However, there is debate now whether the age of technology and instantaneous communication with the use of the internet are making cities obsolete. 

A big city, or metropolis, is usually accompanied by a subcity; for example, Aurora, Colorado is a subcity of Denver, Colorado. Such cities also contain large amounts of urban sprawl, creating large amounts of business commuters. Once a city sprawls far enough to reach another city, this region can be deemed a megalopolis, or a cluster of urban areas.

There is currently insufficient evidence to assert what conditions in world history spawned the first true cities. Theorists, however, have offered arguments for what the right conditions might have been and have identified some basic mechanisms that might have been the important driving forces.

The conventional materialist perspective holds that cities first formed after the Neolithic revolution. The Neolithic revolution brought agriculture, which made denser human populations possible, thereby supporting city development . The advent of farming encouraged hunter-gatherers to abandon nomadic lifestyles and to choose to settle near others who lived off of agricultural production. The increased population density encouraged by farming and the increased output of food per unit of land, created conditions that seem more suitable for city-like activities. In his book, â€œCities and Economic Development,â€?Paul Bairoch takes up this position as he provides a seemingly straightforward argument, which makes agricultural activity appear necessary before true cities can form. 

According to Vere Gordon Childe, for a settlement to qualify as a city, it must have enough surplus of raw materials to support trade . Bairoch points out that, due to sparse population densities that would have persisted in pre-Neolithic, hunter-gatherer societies, the amount of land that would be required to produce enough food for subsistence and trade for a large population would make it impossible to control the flow of trade. To illustrate this point, Bairoch offers â€œWestern Europe during the pre-Neolithic, [where] the density must have been less than 0.1 person per square kilometerâ€?  as an example. Using this population density as a base for calculation, and allotting 10% of food towards surplus for trade and assuming that there is no farming taking place among the city dwellers, he calculates that â€œin order to maintain a city with a population of 1,000, and without taking the cost of transportation into account, an area of 100,000 square kilometers would have been required. When the cost of transportation is taken into account, the figure rises to 200,000 square kilometers..." . Bairoch noted that 200,000 square kilometers is roughly the size of Great Britain.

In her book â€œThe Economy of Cities,â€?Jane Jacobs makes the controversial claim that cities saw the birth of agriculture. Jacobs does not lend her theory to any strict definition of a city, but her account suggestively contrasts what could only be thought of as primitive city-like activity to the activity occurring in neighboring hunter-gatherer settlements. 

To argue that cities came first, Jacobs offers a fictitious scenario where a valued natural resource leads to primitive economic activity that eventually creates conditions for the discovery of grain culture. Jacobs calls the imaginary city New Obsidian, where a stock of obsidian is controlled and traded with neighboring hunting groups. Those that do not control the stock demand the obsidian, so hunters travel great distances to barter what they have. Hunters value obsidian because â€œ[o]bsidian makes the sharpest tools to be had" . Hunters arrive with live animals and produce, providing New Obsidian with food imports. When New Obsidians want goods that they do not have access to at their settlement, they take the obsidian as a currency to other settlements for trade. This basic economic activity turns the little city into a sort of â€œdepotâ€?where, in addition to exporting obsidian, a service of obtaining, handling and trading of goods that are brought in from elsewhere are made available for secondary customers. This activity brings more people to the center as jobs are created and goods are being traded. Among the goods traded are seeds of all different sorts and they are stored in unprecedented combinations. In various ways, some accidental, the seeds are sown, and the variation in yields among the different types of seeds are readily observed, more readily than they would in the wild. The seeds that yield the most grain are noticed and trading them begins to occur within the city. Owing to this local dealing, New Obsidians find that their grain yields are the best and for the first time â€œthe selection becomes deliberate and conscious. The choices made now are purposeful, and they are made among various strains of already cultivated crosses, and their crosses, mutants and hybrids . The new way of producing food allows for food surplus and the surplus is offset by the population increase that results from an increase in labor that the new production method has created. The new source of food allows New Obsidian to switch its imports from mostly food, to mostly other materials that neighboring settlements are rich in, but could not barter with before. The craftsman that develop in New Obsidian make good use of the explosion of the new material imports and the work to be done increases rapidly along with the population as neighboring settlements are absorbed by the city activities.

Theorists have identified many possible reasons for why people would have originally decided to come together to form dense populations. In his book â€œCity Economics,â€?Brendan Oâ€™Flaherty asserts â€œCities could persistâ€”as they have for thousands of yearsâ€”only if their advantages offset the disadvantages" . Oâ€™Flaherty illustrates two similar attracting advantages known as increasing returns to scale and economies of scale, which are concepts normally associated with firms, but their applications are seen in more basic economic systems as well. Increasing returns to scale occurs when â€œdoubling all inputs more than doubles the output [and] an activity has economies of scale if doubling output less than doubles costâ€?. To offer an example of these concepts, Oâ€™Flaherty makes use of â€œone of the oldest reasons why cities were built: military protectionâ€?. In this example, the inputs are anything that would be used for protection (i.e.: a wall) and the output is the area protected and everything of value contained in it. Oâ€™Flaherty then asks that we suppose that the area to be protected is square and each hectare inside it has the same value of protection. The advantage is expressed as: .

(1) , where O is the output (area protected) and s stands for the length of a side. This equation shows that output is proportional to the square of the length of a side.

The inputs depend on the length of the perimeter:

(2)	, where I stands for the quantity of inputs. This equation shows that the perimeter is proportional to the length of a side.

So there are increasing returns to scale:

(3)	. This equation (algebraically, combining (1) and (2)) shows that with twice the inputs, you produce quadruple the output.

Also, economies of scale:

(4)	I =4O^1/2. This equation (combining (1) and (2)) shows that the same output requires less input.

â€œCities, then, economize on protection, and so protection against marauding barbarian armies is one reason why people have come together to live in citiesâ€¦â€?.

Similarly, â€œAre Cities Dying?â€?by Edward L. Glaeser, delves into similar reasons for city formation: reduced transport costs for goods, people, and ideas. An interesting piece from Glaeserâ€™s article is his argument about the benefits of proximity. He claims that if you double a city size, workers have a ten percent increase in earnings. Glaeser furthers his argument by logically stating that bigger cities donâ€™t pay more for equal productivity in a smaller city, so it is reasonable then to assume that workers actually become more productive if you move them to a city twice the size than they initially worked in. However, the workers donâ€™t really benefit from the ten percent wage increase because it is recycled back into the higher cost of living in a bigger city.

Older cities appear to be jumbled together, seemingly without a structural plan. This quality is a legacy of earlier unplanned or organic development, and is often perceived by today's tourists to be picturesque. In contrast, cities founded after the advent of the automobile and planned accordingly tend to have expansive boulevards impractical to navigate on foot.

Modern city planning has seen many different schemes for how a city should look. The most commonly seen pattern is the grid, favoured by the Romans, almost a rule in parts of the New World, and used for thousands of years in China. Derry was the first ever planned city in Ireland, begun in 1613, with the walls being completed five years later. The central diamond within a walled city with four gates was thought to be a good design for defence. The grid pattern chosen was widely copied in the colonies of British North America. However, the grid has been around for far longer than the British Empire. The Ancient Greeks often gave their colonies around the Mediterranean a grid plan. One of the best examples is the city of Priene. This city even had its different districts, much like modern city planning today. Fifteen centuries earlier the Indus Valley Civilization was using grids in such cities as Mohenjo-Daro. Also in Medieval times we see a preference for linear planning. Good examples are the cities established in the south of France by various rulers and city expansions in old Dutch and Flemish cities.



Other forms may include a radial structure in which main roads converge on a central point, often the effect of successive growth over long time with concentric traces of town walls and citadels - recently supplemented by ring-roads that take traffic around the edge of a town. Many Dutch cities are structured this way: a central square surrounded by concentric canals. Every city expansion would imply a new circle (canals + town walls). In cities like Amsterdam and Haarlem, and elsewhere, such as in Moscow, this pattern is still clearly visible.

Towns and cities have a long history, although opinions vary on whether any particular ancient settlement can be considered to be a city. A city formed as central places of trade for the benefit of the members living in close proximity to others facilitates interaction of all kinds. These interactions generate both positive and negative externalities between otherâ€™s actions. Benefits include reduced transport costs, exchange of ideas, sharing of natural resources, large local markets, and later in their development, amenities such as running water and sewage disposal. Possible costs would include higher rate of crime, higher mortality rates, higher cost of living, worse pollution, traffic and high commuting times. Cities will grow when the benefits of proximity between people and firms are higher than the cost. The first true towns are sometimes considered to be large settlements where the inhabitants were no longer simply farmers of the surrounding area, but began to take on specialized occupations, and where trade, food storage and power was centralized. In 1950 Gordon Childe attempted to define a historic city with 10 general metrics. These are:

This categorisation is descriptive, and not all ancients cities fit into this well, but it is used as a general touchstone when considering ancient cities.

One characteristic that can be used to distinguish a small city from a large town is organized government. A town accomplishes common goals through informal agreements between neighbors or the leadership of a chief. A city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to feed the government workers. The governments may be based on heredity, religion, military power, work projects (such as canal building), food distribution, land ownership, agriculture, commerce, manufacturing, finance, or a combination of those. Societies that live in cities are often called civilizations. A city can also be defined as an absence of physical space between people and firms.

Early cities developed in a number of regions of the ancient world. Mesopotamia can claim the earliest cities, particularly Eridu, Uruk, and Ur. Although it has sometimes been claimed that ancient Egypt lacked urbanism, in fact several types of urban settlements were found in ancient times. The Indus Valley Civilization and China are two other areas of the Old World with major indigenous urban traditions. Among the early Old World cities, Mohenjo-daro of the Indus Valley Civilization was one of the largest, with an estimated population of 40,000 or more. Mohenjo-daro and Harappa, the large Indus capitals, were among the first cities to use grid plans, drainage, flush toilets, urban sanitation systems, and sewage systems. At a somewhat later time, a distinctive urban tradition developed in the Khmer region of Cambodia, where Angkor grew into one of the largest cities (in area) the world has ever seen.

In the ancient New World, early urban traditions developed in Mesoamerica and the Andes. Mesoamerica saw the rise of early urbanism in several cultural regions, including the Classic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec drew on these earlier urban traditions. In the Andes, the first urban centers developed in the Chavin and Moche cultures, followed by major cities in the Huari, Chimu and Inca cultures.

This roster of early urban traditions is notable for its diversity. Excavations at early urban sites show that some cities were sparsely-populated political capitals, others were trade centers, and still other cities had a primarily religious focus. Some cities had large dense populations whereas others carried out urban activities in the realms of politics or religion without having large associated populations. Theories that attempt to explain ancient urbanism by a single factor such as economic benefit fail to capture the range of variation documented by archaeologists (Smith 2002).

The growth of the population of ancient civilizations, the formation of ancient empires concentrating political power, and the growth in commerce and manufacturing led to ever greater capital cities and centres of commerce and industry, with Alexandria, Antioch and Seleucia of the Hellenistic civilization, Pataliputra (now Patna) in India, Chang'an (now Xi'an) in China, Carthage, ancient Rome, its eastern successor Constantinople (later Istanbul), and successive Chinese, Indian and Muslim capitals approaching or exceeding the half-million population level. 

It is estimated that ancient Rome had a population of about a million people by the end of the first century BC, after growing continually during the 3rd, 2nd, and 1st centuries BCE. And it is generally considered the largest city before 19th century London. Alexandria's population was also close to Rome's population at around the same time, the historian Rostovtzeff estimates a total population close to a million based on a census dated from 32 CE that counted 180,000 adult male citizens in Alexandria. Similar administrative, commercial, industrial and ceremonial centres emerged in other areas, most notably Baghdad, which to some urban historians, later became the first city to exceed a population of one million by the 8th century instead of Rome.

Agriculture was practiced in sub-Saharan Africa since the third millennium BCE. Because of this, cities were able to develop as centers of non-agricultural activity. Exactly when this first happened is still a topic of archeological and historical investigation. Western scholarship has tended to focus on cities in Europe and Mesopotamia, but emerging archeological evidence indicates that urbanization occurred south of the Sahara in well before the influence of Arab urban culture. The oldest sites documented thus far are from around 500 CE including Awdaghust, Kumbi-Saleh the ancient capital of Ghana, and Maranda a center located on a trade rout between Egypt and Gao.

During the European Middle Ages, a town was as much a political entity as a collection of houses. City residence brought freedom from customary rural obligations to lord and community: "Stadtluft macht frei" ("City air makes you free") was a saying in Germany. In Continental Europe cities with a legislature of their own were not unheard of, the laws for towns as a rule other than for the countryside, the lord of a town often being another than for surrounding land. In the Holy Roman Empire some cities had no other lord than the emperor. In Italy, Medieval communes had quite a statelike power. 

In exceptional cases like Venice, Genoa or LÃ¼beck, cities themselves became powerful states, sometimes taking surrounding areas under their control or establishing extensive maritime empires. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed a considerable autonomy in late medieval Japan.

While the city-states, or poleis, of the Mediterranean and Baltic Sea languished from the 16th century, Europe's larger capitals benefited from the growth of commerce following the emergence of an Atlantic trade. By the late 18th century, London had become the largest city in the world with a population of over a million, while Paris rivaled the well-developed regionally-traditional capital cities of Baghdad, Beijing, Istanbul and Kyoto. During the Spanish colonization of the Americas the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories, and were bound to several laws about administration, finances and urbanism.

Most towns remained far smaller places, so that in 1500 only some two dozen places in the world contained more than 100,000 inhabitants: as late as 1700 there were fewer than forty, a figure which would rise thereafter to 300 in 1900. A small city of the early modern period might contain as few as 10,000 inhabitants, a town far fewer still.

The growth of modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In the United States from 1860 to 1910, the invention of railroads reduced transportation costs, and large manufacturing centers began to emerge, thus allowing migration from rural to city areas. However, cities during those periods of time were deadly places to live in, due to health problems resulting from contaminated water and air, and communicable diseases. In the Great Depression of the 1930s cities were hard hit by unemployment, especially those with a base in heavy industry. In the U.S. urbanization rate increased forty to eighty percent during 1900-1990. Today the world's population is slightly over half urban, with millions still streaming annually into the growing cities of Asia, Africa and Latin America. There has also been a shift to suburbs, perhaps to avoid crime and traffic, which are two costs of living in an urban area.

Modern cities are known for creating their own microclimates. This is due to the large clustering of heat absorbent surfaces that heat up in sunlight and that channel rainwater into underground ducts. 

Waste and sewage are two major problems for cities, as is air pollution coming from internal combustion engines. The impact of cities on places elsewhere, be it hinterlands or places far away, is considered in the notion of city footprinting (ecological footprint).Other negative external effects include health consequences such as communicable diseases, crime, and high traffic and commuting times. Cities cause more interaction with more people than rural areas, thus a higher probability to contracting contagious diseases. However, many inventions such as inoculations, vaccines, and water filtration systems have also lowered health concerns. Crime is also a concern in the cities. Studies have shown that crime rates in cities are higher and the chance of punishment after getting caught is lower. In cases such as burglary, the higher concentration of people in cities create more items of higher value worth the risk of crime. The high concentration of people also makes using automobiles inconvienint and pedestrin traffic is more prominent in metropolitan areas than a rural or suburban one.

Cities also generate positive external effects. The close physical proximity facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Another positive externality of cities comes from the diverse social opportunities created when people of different backgrounds are brought together. Larger cities typically offer a wider variety of social interests and activities, letting people of all backgrounds find something they can be involved in.

The difference between towns and cities is differently understood in different parts of the world. Indeed, languages other than English often use a single word for both concepts (French ville, German Stadt, etc.). Even within the English-speaking world there is no one standard definition of a city: the term may be used either for a town possessing city status; for an urban locality exceeding an arbitrary population size; for a town dominating other towns with particular regional economic or administrative significance. Although city can refer to an agglomeration including suburban and satellite areas, the term is not usually applied to a conurbation (cluster) of distinct urban places, nor for a wider metropolitan area including more than one city, each acting as a focus for parts of the area. And the word "town" (also "downtown") may mean the center of the city.

In Australia and New Zealand, city is used to refer both to units of local government, and as a synonym for urban area. For instance the City of South Perth is part of the urban area known as Perth, commonly described as a city. 

In New Zealand, according to Statistics New Zealand (the government statistics agency), "A city [...] must have a minimum population of 50,000, be predominantly urban in character, be a distinct entity and a major centre of activity within the region.". For example Gisborne, purported to be the first city to see the sun, has a population of only 44,500 (2006) and is therefore administered by a district council, not a city council.



There is a formal definition of city in China provided by the Chinese government. For an urban area that can be defined as a city, there should be at least 100,000 non-agricultural population. City with less than 200,000 non-agricultural population refers to a Small city, 200,000-500,000 non-agricultural population is a Medium city, 500,000-1,000,000 non-agricultural population is a Large city and >1,000,000 non-agricultural population is an Extra-large city. Also, there is an administrative definition based on the city boundary too and a city has its legal city limits. In 1998, there were 668 cities in China - China has the largest urban population in the world.

Chile's Department of National Statistics defines a city (ciudad in Spanish) as an urban entity with more than 5,000 inhabitants. A town (pueblo), is an urban entity with 2,001 to 5,000 persons, however, if the area has some economic activity, the designation may include populations as small as 1,001. The department also defines Major Cities as provincial or regional capitals with populations of 100,001 to 500,000; Great Urban Areas which are comprised of several entities without any appreciable limit between them and populations which total between 500,001 and 1,000,000. A Metropolis is the largest urban area in the country where there are more than one million inhabitants. The "urban entity" is defined as a concentration of habitations with more than 2,000 persons living in them, or more than 1,000 persons if more than half of those persons are in some way gainfully employed. Tourist and recreation areas with more than 250 living units may be considered as urban areas.

The German word for both "town" and "city" is Stadt, while a town with more than 100,000 inhabitants is called a GroÃŸstadt (major city), which is the most adequate equivalence for city (in terms of differentiating it from a town). On the other hand, most towns are communities belonging to a Landkreis (county), but there are some cities, usually with at least 50,000 inhabitants, that are counties by themselves (kreisfreie StÃ¤dte).

In Italy a city is called cittÃ , an uncount noun derived from the latin civitas. The status of "city" is granted by the President of the Republic with Presidential Decree Law. The largest and most important cities in the country, such as Rome, Milan and Naples, are called aree metropolitane (metropolitan areas) because they include several minor cities and towns in their areas. There is no population limit for a city. In the coat of arms, a golden crown tower stands for a city.

In Poland the word miasto serves for both town and city. There are formal distinctions which generally differentiate larger towns from smaller ones (such as status as a separate powiat or county, or the conferring of the title prezydent on the mayor rather than burmistrz), but none of these is universally recognized as equivalent to the English city/town distinction.

South Korea has a system of dividing into metropolitan cities, provinces, a special city (Seoul) and one specially self-governing province (Jeju). In South Korea, cities should have a population of more than 150,000, and if a city has more than 500,000, it would be divided into 2 districts and then sub-communities follow as a name of dong with similar system of normal cities. Additionally, if a city's population is over 1,000,000, then it would be promoted to metropolitan city.

In the United Kingdom (UK), a city is a town which has been known as a city since time immemorial, or which has received city status by letters patent â€?which is normally granted on the basis of size, importance or royal connection (the traditional test was whether the town had a cathedral). For example Ripon was granted city status in 1836 to coincide with the creation of the Diocese of Ripon, but also in recognition of its long-standing role as a supplier of spurs to royalty. In the United Kingdom, when people talk about cities, they generally include the suburbs in that. Some cathedral cities, such as St David's in Wales and Wells in England, are quite small, and may not be known as cities in common parlance. Preston became England's newest city in the year 2002 to mark the Queen's jubilee, as did Newport in Wales, Stirling in Scotland, and Lisburn and Newry in Northern Ireland.

A Review of Scotland's Cities led to the Fair City of Perth, Scotland, losing city status.

By both legal and traditional definition, a town may be of any size, but must contain a market place. A village must contain a church. A small village without a church is called a hamlet.

In the United States (USA), the definition of cities (and town, villages, townships, etc.) is a matter of state laws and the definitions vary widely by state. A city may, in some places, be run by an elected mayor and city council, while a town is governed by people, select board (or board of trustees), or open town meeting. There are some very large towns (such as Hempstead, New York, with a population of 755,785 in 2004) and some very small cities (such as Lake Angelus, Michigan, with a population of 326 in 2000), and the line between town and city, if it exists at all, varies from state to state. Cities in the United States do have many oddities, like Maza, North Dakota, the smallest city in the country, has only 5 inhabitants, but is still incorporated. It does not have an active government, and the mayoral hand changes frequently (due to the lack of city laws). California has both towns and cities but the terms "town" and "city" are considered synonymous.

In some U.S. states, any incorporated town is also called a city. If a distinction is being made between towns and cities, exactly what that distinction is often depends on the context. The context will differ depending on whether the issue is the legal authority it possesses, the availability of shopping and entertainment, and the scope of the group of places under consideration. Intensifiers such as "small town" and "big city" are also common, though the flip side of each is rarely used. 

Some states make a distinction between villages and other forms of municipalities. In some cases, villages combine with larger other communities to form larger towns; a well-known example of an urban village is New York City's famed Greenwich Village, which started as a quiet country settlement but was absorbed by the growing city. The word has often been co-opted by enterprising developers to make their projects sound welcoming and friendly.

In Illinois, cities must have a minimum population of 2,500 but in Nebraska, cities must have a minimum of only 800 residents. In Idaho, all incorporated municipalities are cities. In Ohio, a municipality automatically becomes a city if it has 5,000 residents counted in a federal census but it reverts to a village if its population drops below 5,000. In Nebraska, 5,000 residents is the minimum for a city of the first class while 800 is the minimum for a city of the second class. 

In all the New England states, city status is conferred by the form of government, not population. Town government has a board of selectmen for the executive branch, and a town meeting for the legislative branch. New England cities, on the other hand, have a mayor for the executive, and a legislature referred to as either the city council or the board of aldermen.

In Virginia, all incorporated municipalities designated as cities are independent of the adjacent or surrounding county while a town is an incorporated municipality which remains a part of an adjacent or surrounding county. The largest incorporated municipalities by population are all cities, although some smaller cities have a smaller population than some towns. For example, the smallest city of Norton has a population of 3,904 and the largest town of Blacksburg has a population of 39,573. Independent cities in other states include Baltimore, Maryland and Carson City, Nevada.

In Pennsylvania any municipality with more than 10 persons can incorporate as a Borough. Any Township or Borough with at least 10,000 population can ask the legislature to charter as a city. In Pennsylvania a village is simply an unincorporated community within a township.

A global city, also known as a world city, is a prominent centre of trade, banking, finance, innovations, and markets. The term "global city", as opposed to megacity, was coined by Saskia Sassen in a seminal 1991 work. Whereas "megacity" refers to any city of enormous size, a global city is one of enormous power or influence. Global cities, according to Sassen, have more in common with each other than with other cities in their host nations. Examples of such cities include London, New York City, Paris and Tokyo.The notion of global cities is rooted in the concentration of power and capabilities within all cities. The city is seen as a container where skills and resources are concentrated: the better able a city is to concentrate its skills and resources, the more successful and powerful the city. This makes the city itself more powerful in the sense that it can influence what is happening around the world. Following this view of cities, it is possible to rank the world's cities hierarchically. Other global cities include Singapore which is a city-state, Los Angeles, Hong Kong, Frankfurt, Milan and Chicago which are all classed as "Alpha World Cities" and San Francisco, Madrid, Sydney, Toronto, ZÃ¼rich, Sao Paulo and Mexico City which are "Beta World Cities". A third tier containing Taipei, Osaka, Buenos Aires, Melbourne, Montreal and Santiago, among others is called "Gamma world cities" . Critics of the notion point to the different realms of power. The term global city is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant. For example, cities like Rome, Delhi, Mumbai, Istanbul, Mecca, Mashhad, Karbala, Jerusalem and Lisbon are powerful in religious and historical terms but would not be considered "global cities." Additionally, it has been questioned whether the city itself can be regarded as an actor.

In 1995, Kanter argued that successful cities can be identified by three elements. To be successful, a city needs to have good thinkers (concepts), good makers (competence) or good traders (connections). The interplay of these three elements, Kanter argued, means that good cities are not planned but managed.



In the United States, United Kingdom and Ireland, the term "inner city" is sometimes used with the connotation of being an area, perhaps a ghetto, where people are less wealthy and where there is more crime. These connotations are less common in other Western countries, as deprived areas are located in varying parts of other Western cities. In fact, with the gentrification of some formerly run-down central city areas the reverse connotation can apply. In Australia, for example, the term "outer suburban" applied to a person implies a lack of sophistication. In Paris, the inner city is the richest part of the metropolitan area, where housing is the most expensive, and where elites and high-income individuals dwell. In the developing world, economic modernization brings poor newcomers from the countryside to build haphazardly at the edge of current settlement (see favelas, shacks and shanty towns). 

The United States, in particular, has a culture of anti-urbanism that dates back to colonial times. The American City Beautiful architecture movement of the late 1800s was a reaction to perceived urban decay and sought to provide stately civic buildings and boulevards to inspire civic pride in the motley residents of the urban core. Modern anti-urban attitudes are to be found in America in the form of a planning profession that continues to develop land on a low-density suburban basis, where access to amenities, work and shopping is provided almost exclusively by car rather than on foot.

However, there is a growing movement in North America called "New Urbanism" that calls for a return to traditional city planning methods where mixed-use zoning allows people to walk from one type of land-use to another. The idea is that housing, shopping, office space, and leisure facilities are all provided within walking distance of each other, thus reducing the demand for road-space and also improving the efficiency and effectiveness of mass transit.













The Mississippi River is the second longest river in the United States, with a length of 2,320Â mi (3,734Â km) from its source in Lake Itasca in Minnesota to its mouth in Gulf of Mexico. The longest is its tributary the Missouri River measuring 2,341Â mi (3,767Â km). 

The Mississippi River is part of the Jefferson-Missouri-Mississippi river system, which is the largest river system in North America and among the largest in the world: by length (3,900Â mi or 6,275Â km), it is the fourth longest, and by average discharge of 572,000Â cuÂ ft/s (16,200Â mÂ³/s), it is the tenth largest. 

The longest of the many long Mississippi tributaries is the Missouri River with the Arkansas River as second longest. Measured by water volume, the largest of all Mississippi tributaries is the Ohio River. 

The name Mississippi is derived from the Ojibwe word misi-ziibi ("Great River") or gichi-ziibi ("Big River") at its headwaters.

The widest point of the Mississippi River is Lake Onalaska, near La Crosse, Wisconsin, where the river is over 4Â mi (6.4Â km) wide. Since Lake Onalaska was created by Lock and Dam No. 7, Lake Pepin is historically the widest natural spot at more than two miles wide. These areas however are reservoir rather than free flowing river. In areas where the Mississippi is a real river, it exceeds one mile in width in several places in its lower course.

The river is divided into the upper Mississippi, from its source south to the Ohio River, and the lower Mississippi, from the Ohio to its mouth near New Orleans, Louisiana. 

A series of 29 locks and dams on the upper Mississippi, most of which were built in the 1930s, is designed primarily to maintain a 9Â ft (2.7Â m) deep channel for commercial barge traffic. The lakes formed are also used for recreational boating and fishing. The dams make the river deeper and wider but do not stop it. No flood control is intended. During periods of high flow, the gates, some of which are submersible, are completely opened and the dams simply cease to function. Below St. Louis, Missouri, the Mississippi is relatively free-flowing, although it is constrained by numerous levees and directed by numerous wing dams.

The Mississippi River runs through 10 states and was used to define portions of these states' borders. The middle of the riverbed at the time the borders were established was the line to define the borders between states. The river has since shifted, but the state borders of Wisconsin, Iowa, Illinois, Missouri, Kentucky, Arkansas, Tennessee, and Mississippi have not changed; they still follow the former bed of the MIssissippi River as of their establishment. 

The Mississippi River has the third largest drainage basin (or catchment) in the world, exceeded in size only by the watersheds of the Amazon River and Congo River. It drains 41% of the 48 contiguous states of the United States. The basin covers more than 1,245,000Â sq mi (3,225,000Â kmÂ²), including all or parts of 31 states and two Canadian provinces. The drainage basin empties into the Gulf of Mexico.

Major tributaries of the Mississippi:

The Mississippi drains most of the area between the Rocky Mountains and the Appalachian Mountains, except for the areas drained by Hudson Bay via the Red River of the North, the Great Lakes and the Rio Grande. 

The river empties into the Gulf of Mexico about 100Â mi (160Â km) downstream from Louisiana. Measurements of the length of the Mississippi from Lake Itasca to the Gulf of Mexico vary, but the EPA's number is 2,320Â mi (3,733Â km). The retention time from Lake Itasca to the Gulf is about 90 days.



The upper Mississippi is divided into three sections: the headwaters, from the source to Saint Anthony Falls, a series of man-made lakes between Minneapolis and St. Louis, Missouri, and the middle Mississippi, a relatively free-flowing river downstream of the confluence with the Missouri River at St. Louis.

From its source at Lake Itasca, 1,475Â ft (450Â m) above sea level in Itasca State Park located in Clearwater County, Minnesota, the river falls to  prior to St. Anthony Falls in Minneapolis. There it drops to 725Â feet (220Â m), creating the only waterfall along the river's course. 

The Mississippi is joined by the Minnesota River in the Twin Cities, the Wisconsin River in Prairie du Chien, Wisconsin, the Des Moines River in Keokuk, Iowa, the Illinois River and the Missouri River near St. Louis, Missouri, and by the Ohio River at Cairo, Illinois.

Major sub-tributaries include the Tennessee River (a tributary of the Ohio River) and the Platte River (a tributary of the Missouri River). The Arkansas River joins the Mississippi in the state of Arkansas. The Atchafalaya River in Louisiana is a major distributary of the Mississippi.

Fresh river water flowing from the Mississippi into the Gulf of Mexico does not mix into the salt water immediately. The images from NASA's MODIS to the right show a large plume of fresh water, which appears as a dark ribbon against the lighter-blue surrounding waters.

The images demonstrate that the plume did not mix with the surrounding sea water immediately. Instead, it stayed intact as it flowed through the Gulf of Mexico, into the Straits of Florida, and entered the Gulf Stream. The Mississippi River water rounded the tip of Florida and traveled up the southeast coast to the latitude of Georgia before finally mixing in so thoroughly with the ocean that it could no longer be detected by MODIS.

The Mississippi river discharges at an annual average rate of between 200,000 and 700,000Â cuÂ ft/s (7,000 to 20,000Â mÂ³/s). 

Although it is the 5th largest river in the world by volume, this flow is a mere fraction of the output of the Amazon, which moves nearly 7 million cuÂ ft/s (200,000Â mÂ³/s) during wet seasons. 

On average the Mississippi has only 1/11th the flow of the Amazon River, but is nearly twice that of the Columbia River and almost 6 times the volume of the Colorado River.

Through a natural process known as delta switching the lower Mississippi River has shifted its final course to the ocean every thousand years or so. This occurs because the deposits of silt and sediment begin to clog its channel, raising the river's level and causing it to eventually find a steeper, more direct route to the Gulf of Mexico.

The abandoned distributary diminishes in volume and forms what are known as bayous. This process has, over the past 5,000 years, caused the coastline of south Louisiana to advance toward the Gulf from 15 to 50Â mi (25 to 80Â km).

The Illinoian Glacier, about 200,000 to 125,000 years before present, blocked the Mississippi near Rock Island, diverting it to its present channel farther to the west, the current western border of Illinois. 

The Hennepin Canal roughly follows the ancient channel of the Mississippi downstream from Rock Island to Hennepin. South of Hennepin, the current Illinois River is actually following the ancient channel of the Mississippi River to Alton before the Illinoian glaciation.

Other changes in the course of the river have occurred because of earthquakes along the New Madrid Fault Zone, which lies between the cities of Memphis and St. Louis. Three earthquakes in 1811 and 1812, estimated at approximately 8 on the Richter Scale, were said to have temporarily reversed the course of the Mississippi.

The settlement of Reverie, Tennessee was cut off from Tipton County, Tennessee during the 1811 and 1812 earthquakes and placed on the western side of the Mississippi River, the Arkansas side.

These earthquakes also created Reelfoot Lake in Tennessee from the altered landscape near the river. The faulting is related to an aulacogen (geologic term for a failed rift) that formed at the same time as the Gulf of Mexico.

U.S. government scientists determined in the 1950s that the Mississippi River was starting to switch to the Atchafalaya River channel because of its much steeper path to the Gulf of Mexico. Eventually the Atchafalaya River would capture the Mississippi River and become its main channel to the Gulf of Mexico, leaving New Orleans on a side channel. As a result, the U.S. Congress authorized a project called the Old River Control Structure, which has prevented the Mississippi River from leaving its current channel that drains into the Gulf via New Orleans.

Because of the large scale of high energy water flow through the Old River Control Structure threatening to damage the structure, an auxiliary flow control station was built adjacent to the standing control station. This US$300 million project was completed in 1986 by the U.S. Army Corps Of Engineers.

The longest named river in North America is the Missouri River, with a length of 2,341Â mi (3,767Â km) from the confluence of the Jefferson, Madison, and Gallatin to the Mississippi River. Taken together, the Jefferson, Missouri, and Mississippi form the largest river system in North America.

If measured from the source of the Jefferson at Brower's Spring to the Gulf of Mexico, the length of the Mississippi-Missouri-Jefferson combination is approximately 3,900Â mi (6,275Â km), making the combination the 4th longest river in the world. The uppermost 207Â mi (333Â km) of this combined river are called the Jefferson, the lowest 1,352Â mi (2,175Â km) are part of the Mississippi, and the intervening 2,341Â mi (3,767Â km) are called the Missouri.

The word Mississippi comes from Messipi, the French rendering of the Anishinaabe (Ojibwe or Algonquin) name for the river, Misi-ziibi ("Great River"). 

The Ojibwa called Lake Itasca Omashkoozo-zaaga'igan ("Elk Lake") and the river flowing out of it Omashkoozo-ziibi ("Elk River"). After flowing into Lake Bemidji, the Ojibwe called the river Bemijigamaa-ziibi ("River from the Traversing Lake"). After flowing into Cass Lake, the name of the river again changed to Miskwaawaakokaa-ziibi ("Red Cedar River") and then to Gichi-ziibi ("Great River") after flowing into Lake Winnibigoshish. The Ojibwe name Misi-ziibi applied only to the portion below the Crow Wing River, but the ever-changing names of the river seemed illogical to the English speakers.

After the expeditions by Giacomo Costantino Beltrami and Henry Schoolcraft, the longest stream above the juncture of the Crow Wing River and Gichi-ziibi was named Mississippi River.

In Cheyenne the Mississippi River is called MÃ¡Ë€xe-Ã©Ë€ometaaË€e ("Big Greasy River").

The area of the Mississippi valley was first settled by Native American tribes such as the Ojibwa, the Cheyenne, the Mississippian Culture and the Chickasaw.

On May 8, 1541, Hernando de Soto became the first recorded European to reach the Mississippi River, which he called Rio de Espiritu Santo ("River of the Holy Spirit"). The river is now called Rio Misisipi in Spanish. 

French explorers Louis Joliet and Jacques Marquette began exploring the Mississippi. He traveled with a Sioux named Ne Tongo ("Big river" in Sioux language) in 1673. Marquette proposed calling it the River of the Immaculate Conception. 

In 1682, RenÃ© Robert Cavelier, Sieur de La Salle and Henri de Tonty claimed the entire Mississippi River Valley for France, calling the river Colbert River after Jean-Baptiste Colbert and the region La Louisiane, for King Louis XIV. On March 2, 1699, Pierre Le Moyne d'Iberville rediscovered the mouth of the Mississippi, following the death of La Salle.

In 1718, New Orleans was established along the river crescent, by Jean-Baptiste Le Moyne de Bienville, with construction patterned after the 1711 resettlement on Mobile Bay of Mobile, the capital of French Louisiana at the time.

France lost all its territories on the North American mainland as a result of the French and Indian War. 

The Treaty of Paris (1763) gave the Kingdom of Great Britain rights to all land in the valley east of the Mississippi and Spain rights to land west of the Mississippi. Spain also ceded Florida to England to regain Cuba, which the English occupied during the war. Britain then divided the territory into East Florida and West Florida.

Article 8 of the Treaty of Paris (1783) states, "The navigation of the river Mississippi, from its source to the ocean, shall forever remain free and open to the subjects of Great Britain and the citizens of the United States." With this treaty, which ended the American Revolution, Britain also ceded West Florida back to Spain to regain the Bahamas, which Spain had occupied during the war. Spain then had control over the river south of 32Â°30' north latitude and, in what is known as the Spanish Conspiracy, hoped to gain greater control of Louisiana and all of the west. These hopes ended when Spain was pressured into signing Pinckney's Treaty in 1795. 

France reacquired 'Louisiana' from Spain in the secret Treaty of San Ildefonso in 1800. The United States bought the territory from France in the Louisiana Purchase of 1803.

Mark Twain's book Life on the Mississippi extensively covered the steamboat commerce which took place from 1830 to 1870 on the river before more modern boating methods replaced the steamer. It was published first in serial form in Harper's Weekly in seven parts in 1875. The full version, including a passage from the unfinished Huckleberry Finn and works from other authors, was published by James R. Osgood & Co. in 1885. 

The first steamboat to travel the full length of the Mississippi from the Ohio River to New Orleans, Louisiana, was the New Orleans in December 1811. Its maiden voyage occurred during the series of New Madrid earthquakes in 1811â€?2. 

Steamboat transport remained a viable industry (both in terms of passengers and freight) until the end of the first decade of the 20th century. Among the several Mississippi River system steamboat companies was the noted Anchor Line, which from 1859 to 1898 operated a luxurious fleet of steamers between St. Louis and New Orleans.

In 1815, America defeated Britain at the Battle of New Orleans, part of the War of 1812.

The river played a decisive role in the American Civil War. The Union's Vicksburg Campaign called for Union control of the lower Mississippi River. The Union victory at the Battle of Vicksburg in Warren County, Mississippi in 1863 was pivotal to the Union's final victory of the Civil War.

In 1848, the Illinois and Michigan Canal was built to connect the Mississippi River to Lake Michigan via the Illinois River near Peru, Illinois. In 1900, the canal was replaced by the Chicago Sanitary and Ship Canal. The canal allowed Chicago to address specific health issues (typhoid, cholera and other waterborne diseases) by sending its waste down the Illinois and Mississippi river systems rather than polluting its water source of Lake Michigan. The canal also provided a shipping route between the Great Lakes and the Mississippi.

The Mississippi River was noted for the number of bandits which called its islands and shores home, including John Murrell who was a well-known murderer, horse stealer and slave "re-trader". His notoriety was such that author Mark Twain devoted an entire chapter to him in his book Life on the Mississippi, and Murrell was rumored to have an island headquarters on the river at Island 37.

The Mississippi River has a certain historical tie to the music genre Jazz. Author DuBose Heyward states in one of his books that the music genre Jazz has taken its name from a black itinerant musician called Jazbo Brown. Around the turn of the 19th century the semi-legendary Jazbo Brown is said to have played on boats (as suggested in "Jazzbo Brown from Memphis Town", performed by Bessie Smith) and along the Mississippi River.

The sport of water skiing was invented on the river in a wide region between Minnesota and Wisconsin known as Lake Pepin. Ralph Samuelson of Lake City, Minnesota, created and refined his skiing technique in late June and early July 1922. He later performed the first water ski jump in 1925 and was pulled along at 80Â MPH (128Â km/h) by a Curtiss flying boat later that year.

In the spring of 1927, the river broke out of its banks in 145 places during the Great Mississippi Flood of 1927 and inundated 27,000Â sqÂ mi (70,000Â kmÂ²) to a depth of up to 30Â ft (10Â m).

On October 20, 1976, the automobile ferry MV George Prince was struck by a ship traveling upstream as the ferry attempted to cross from Destrehan, Louisiana, to Luling, Louisiana. Seventy-eight passengers and crew died, only eighteen survived the accident.

The Great Flood of 1993 was another significant flood, although it primarily affected the Mississippi above its confluence with the Ohio River at Cairo, Illinois.

Two portions of the Mississippi were designated as some of the American Heritage Rivers in 1997: The lower portion around Louisiana and Tennessee, and the upper portion around Iowa, Illinois, Minnesota and Missouri.

In 2002 the Slovenian long-distance swimmer Martin Strel swam the entire length of the river, from Minnesota to Louisiana, over the course of 68 days. 

In 2005, the Source to Sea Expedition (http://sourcetosea.net) paddled the Mississippi and Atchafalaya rivers to benefit the Audubon Society's Upper Mississippi River Campaign.

On August 1, 2007, the I-35W Mississippi River bridge in Minneapolis, Minnesota collapsed during the evening rush hour.

Also in 2007, it is expected that more than 150 pleasure boats will travel down the river from Grafton to Cairo while participating in the Great loop, which is circumnavigation of Eastern North America by water.

A clear channel is needed for the barges and other vessels that make the mainstem Mississippi one of the great commercial waterways of the world. 

The task of maintaining a navigation channel is the responsibility of the U.S. Army Corps Of Engineers, which was established in 1802. Earlier projects began as early as 1829 to remove snags, close off secondary channels and excavate rocks and sandbars. 

In 1829, there were surveys of the two major obstacles on the upper Mississippi, the Des Moines Rapids and the Rock Island Rapids, where the river was shallow and the riverbed was rock. The Des Moines Rapids were about 11Â mi (18Â km) long and just above the mouth of the Des Moines River at Keokuk, Iowa. The Rock Island Rapids were between Rock Island and Moline, Illinois. Both rapids were considered virtually impassable.

On a side note, it is at this Quad Cities area of the Mississippi River that the river flows East to West as opposed to its normal course North to South.

The Corps recommended excavation of a 5Â ft (1.5Â m) deep channel at the Des Moines Rapids, but work did not begin until after Lieutenant Robert E. Lee endorsed the project in 1837. The Corps later also began excavating the Rock Island Rapids. By 1866, it had become evident that excavation was impractical, and it was decided to build a canal around the Des Moines Rapids. The canal opened in 1877, but the Rock Island Rapids remained an obstacle.

In 1878, Congress authorized the Corps to establish a 4.5Â ft (1.4Â m) deep channel to be obtained by building wing dams which direct the river to a narrow channel causing it to cut a deeper channel, by closing secondary channels and by dredging. The channel project was complete when the Moline Lock, which bypassed the Rock Island Rapids, opened in 1907.

To improve navigation between St. Paul, Minnesota, and Prairie du Chien, Wisconsin, the Corps constructed several dams on lakes in the headwaters area, including Lake Winnibigoshish and Lake Pokegama. The dams, which were built beginning in the 1880s, stored spring run-off which was released during low water to help maintain channel depth.

In 1907, Congress authorized a 6Â foot (1.8Â m) deep channel project on the Mississippi, which was not complete when it was abandoned in the late 1920s in favor of the 9Â foot (2.7Â m) deep channel project.

In 1913, construction was complete on a dam at Keokuk, Iowa, the first dam below St. Anthony Falls. Built by a private power company to generate electricity, the Keokuk dam was one of the largest hydro-electric plants in the world at the time. The dam also eliminated the Des Moines Rapids.

Lock and Dam No. 1 was completed in Minneapolis, Minnesota in 1917.

Lock and Dam No. 2 at Hastings, Minnesota, was completed in 1930.

Prior to the 1927 flood, the Corps' primary strategy was to close off as many side channels as possible to increase the flow in the main river. It was thought that the river's velocity would scour off bottom sediments, deepening the river and decreasing the possibility of flooding. 

The 1927 flood proved this so wrong that communities threatened by the flood began to make their own levee breaks to relieve the tension of the rising river.

The Rivers and Harbors Act of 1930 authorized the 9Â ft (2.7Â m) channel project, which called for a navigation channel 9Â ft (2.7Â m) deep and 400Â ft (120Â m) wide to accommodate multiple-barge tows.

This was achieved by a series of locks and dams, and by dredging. Twenty-three new locks and dams were built on the upper Mississippi in the 1930s in addition to the three already in existence. 

Two new locks were built north of Lock and Dam No. 1 at Saint Anthony Falls in the 1960s, extending the head of navigation for commercial traffic several miles, but few barges go past the city of Saint Paul, Minnesota today.

Until the 1950s, there was no dam below Lock and Dam 26 at Alton, Illinois. Lock and Dam 27, which consists of a low-water dam and an 8.4Â mi (13.5Â km) long canal, was added in 1953 just below the confluence with the Missouri River, primarily to bypass a series of rock ledges at St. Louis. It also serves to protect the St. Louis city water intakes during times of low water.

Beginning in the 1970s, the Corps applied hydrology transport models to analyze flood flow and water quality of the Mississippi.

Dam 26 at Alton, Illinois, which had structural problems, was replaced by the Mel Price Lock and Dam in 1990. The original Lock and Dam 26 was demolished.

The Corps now actively creates floodways to divert periodic water surges into backwater channels and lakes. The main floodways are the Birds Point-New Madrid Floodway, the Morganza Floodway, which directs floodwaters down the Atchafalaya River and the Bonnet CarrÃ© Spillway which directs water to Lake Pontchartrain. 

The Old River Control structure also serve as a major floodgates that can be opened to prevent flooding. Some of the pre-1927 strategy is still in use today, the Corps actively cuts the necks of horseshoe bends, allowing the water to move faster and reducing flood heights.



The cities below have either historic significance or cultural lore connecting them to the Mississippi River. They are ordered from the beginning of the river to its end.



The first bridge across the Mississippi River was built in 1855. It spanned the river in Minneapolis, Minnesota where the current Hennepin Avenue Bridge is located. The first railroad bridge across the Mississippi was built in 1856. It spanned the river between Arsenal Island at Rock Island, Illinois and Davenport, Iowa. 

Steamboat captains of the day, fearful of competition from the railroads, considered the new bridge "a hazard to navigation". Two weeks after the bridge opened, the steamboat Effie Afton rammed part of the bridge and started it on fire. Legal proceedings ensued - with a young lawyer named Abraham Lincoln defending the railroad. The lawsuit went all the way up to the Supreme Court, and was eventually ruled in favor of Lincoln and the railroad. 

Below is a general overview of bridges over the Mississippi which have notable engineering or landmark significance with its city. They are ordered from the source to the mouth.





William Faulkner uses the Mississippi River and Delta as the setting for many hunts throughout his novels. It has been proposed that in Faulkner's famous story, The Bear, young Ike first begins his transformation into a man, thus relinquishing his birthright to land in Yoknapatawpha County due to his realizations found within the woods surrounding the Mississippi River.

Many of the works of Mark Twain deal with or take place near the Mississippi River. One of his first major works, Life on the Mississippi, is in part a history of the river, in part a memoir of Twain's experiences on the river, and a collection of tales that either take place on or are associated with the river. Twain's most famous work, Adventures of Huckleberry Finn, is largely a journey down the river. The novel works as an episodic meditation on American culture with the river having multiple different meanings including independence, escape, freedom, and adventureHerman Melville's novel The Confidence-Man portrayed a ''Canterbury Tales''-style group of steamboat passengers whose interlocking stories are told as they travel down the Mississippi River. The novel is written both as cultural satire and a metaphysical treatise. Like Huckleberry Finn, it uses the Mississippi River as a metaphor for the larger aspects of American and human identity that unify the otherwise disparate characters. The river's fluidity is reflected by the often shifting personalities and identities of Melville's "confidence man".

The second chapter of Don Rosa's famous comic book The Life and Times of Scrooge McDuck is named "The Master of the Mississippi", it is set on the Mississippi River. Scrooge works here for his Uncle Angus "Pothole" McDuck on a wheel steamer and has his first encounter with The Beagle Boys.

The stage and movie musical Show Boat's central musical piece is the spiritual-influenced ballad Ol' Man River.

Ferde Grofe composed a set of movements for symphony orchestra based on the lands the river travels through in his Mississippi Suite.

The Johnny Cash song Big River is about the Mississippi River.

The song When the Levee Breaks, made famous in the version performed by Led Zeppelin on the album Led Zeppelin IV, was composed by Memphis Minnie McCoy in 1929 after the Great Mississippi Flood of 1927. Another song about the flood was Louisiana 1927 by Randy Newman for the album Good Old Boys.

Moon River from the 1961 film Breakfast at Tiffany's refers to the Mississippi River.

The Mississippi River is a commonly cited natural boundary for purposes of dividing the United States into eastern and western sections, with places often being described as east or west "of the Mississippi".

Due to its size and historical significance, the Mississippi has many nicknames. Among these are:











The Bible is

The Bible refers to the collections of religious writings of Judaism and of Christianity.The books that are considered to be part of the Biblical canon vary depending upon the historic tradition using or defining it. These variations are a reflection of the range of traditions and councils that have convened on the subject.

The Jewish version of the Bible, the Tanakh, is divided into three parts: the Teaching, the Prophets, and the Writings. The Christian version of the Bible includes books of the Tanakh, but includes additional books and reorganizes them into two parts: the books of the Old Testament primarily sourced from the Tanakh (with some variations), and the 27 books of the New Testament containing books originally written primarily in Greek. Some versions of the Christian Bible have a separate Apocrypha section for the books not considered canonical by the tradition or sometimes the publisher.

By 2007, the Bible was translated into 429 languages, with portions of it translated in 2,426 languages. 

According to the Online Etymology Dictionary, the word bible is from Anglo-Latin biblia, traced from the same word through Medieval Latin and Late Latin, as used in the phrase biblia sacra ("holy book" - "In the Latin of the Middle Ages, the neuter plural for Biblia (gen. bibliorum) gradually came to be regarded as a feminine singular noun (biblia, gen. bibliae, in which singular form the word has passed into the languages of the Western world." ). This stemmed from the term (Greek:  Ta biblia ta hagia, "the holy books"), which derived from biblion ("paper" or "scroll," the ordinary word for "book"), which was originally a diminutive of byblos ("Egyptian papyrus"), possibly so called from the name of the Phoenician port Byblos from which Egyptian papyrus was exported to Greece. Bible A Transliterated Torah was published in 2001 and re-published in 2008. This publication TIV transliterated Interlinear Version opened up Ancient Biblical Hebrew language to everyone. It was designed verse by verse with Ancient Hebrew, Modern Hebrew, Transliteration, Interlinear English-Hebrew and Spanish. This work was compiled by Clayton Rickert over a seven year period. This title may be accessed from the National Library of Australia.

Biblical scholar Mark Hamilton states that the Greek phrase Ta biblia ("the books") was "an expression Hellenistic Jews used to describe their sacred books several centuries before the time of Jesus," and would have referred to the Septuagint. The Online Etymology Dictionary states, "The Christian scripture was referred to in Greek as Ta Biblia as early as c.223."

The Tanakh (Hebrew: ) consists of 24 books. Tanakh is an acronym for the three parts of the Hebrew Bible: the Torah ("Teaching/Law" also known as the Pentateuch), Nevi'im ("Prophets"), and Ketuvim ("Writings," or Hagiographa), and is used commonly by Jews but unfamiliar to many English speakers and others .(See Table of books of Judeo-Christian Scripture).

 The Torah, or "Instruction," is also known as the "Five Books" of Moses, thus Chumash from Hebrew meaning "fivesome," and Pentateuch from Greek meaning "five scroll-cases."

The Torah comprises the following five books:

The Hebrew book titles come from the first words in the respective texts. The Hebrew title for Numbers, however, comes from the fifth word of that text.

The Torah focuses on three moments in the changing relationship between God and people. The first eleven chapters of Genesis provide accounts of the creation (or ordering) of the world, and the history of God's early relationship with humanity. The remaining thirty-nine chapters of Genesis provide an account of God's covenant with the Hebrew patriarchs, Abraham, Isaac and Jacob (also called Israel), and Jacob's children (the "Children of Israel"), especially Joseph. It tells of how God commanded Abraham to leave his family and home in the city of Ur, eventually to settle in the land of Canaan, and how the Children of Israel later moved to Egypt. The remaining four books of the Torah tell the story of Moses, who lived hundreds of years after the patriarchs. His story coincides with the story of the liberation of the Children of Israel from slavery in Ancient Egypt, to the renewal of their covenant with God at Mount Sinai, and their wanderings in the desert until a new generation would be ready to enter the land of Canaan. The Torah ends with the death of Moses.

The Torah contains the commandments, of God, revealed at Mount Sinai (although there is some debate amongst Jewish scholars, if this was written down completely in one moment, or if it was spread out during the 40 years in the wandering in the desert). These commandments provide the basis for Halakha (Jewish religious law). Tradition states that the number of these is equal to 613 Mitzvot or 613 commandments. There is some dispute as to how to divide these up (mainly between the Ramban and Rambam). Everyone agrees though that there are 613. 

The Torah is divided into fifty-four portions which are read in turn in Jewish liturgy, from the beginning of Genesis to the end of Deuteronomy, each Sabbath. The cycle ends and recommences at the end of Sukkot, which is called Simchat Torah.

The Nevi'im, or "Prophets," tell the story of the rise of the Hebrew monarchy, its division into two kingdoms, and the prophets who, in God's name, warned the kings and the Children of Israel about the punishment of God. It ends with the conquest of the Kingdom of Israel by the Assyrians and the conquest of the Kingdom of Judah by the Babylonians, and the destruction of the Temple in Jerusalem. Portions of the prophetic books are read by Jews on the Sabbath (Shabbat). The Book of Jonah is read on Yom Kippur. 

According to Jewish tradition, Nevi'im is divided into eight books. Contemporary translations subdivide these into seventeen books.

The Nevi'im comprise the following eight books:



The Ketuvim, or "Writings" or "Scriptures," may have been written during or after the Babylonian Exile but no one can be sure. According to Rabbinic tradition, many of the psalms in the book of Psalms are attributed to David; King Solomon is believed to have written Song of Songs in his youth, Proverbs at the prime of his life, and Ecclesiastes at old age; and the prophet Jeremiah is thought to have written Lamentations. The Book of Ruth is the only biblical book that centers entirely on a non-Jew. The book of Ruth tells the story of a non-Jew (specifically, a Moabite) who married a Jew and, upon his death, followed in the ways of the Jews; according to the Bible, she was the great-grandmother of King David. Five of the books, called "The Five Scrolls" (Megilot), are read on Jewish holidays: Song of Songs on Passover; the Book of Ruth on Shavuot; Lamentations on the Ninth of Av; Ecclesiastes on Sukkot; and the Book of Esther on Purim. Collectively, the Ketuvim contain lyrical poetry, philosophical reflections on life, and the stories of the prophets and other Jewish leaders during the Babylonian exile. It ends with the Persian decree allowing Jews to return to Jerusalem to rebuild the Temple.

The Ketuvim comprise the following eleven books:



The Tanakh was mainly written in Biblical Hebrew, with some portions (notably in Daniel and Ezra) in Biblical Aramaic.

Some time in the 2nd or 3rd century BC, the Torah was translated into Koine Greek, and over the next century, other books were translated (or composed) as well. This translation became known as the Septuagint and was widely used by Greek-speaking Jews, and later by Christians. It differs somewhat from the later standardized Hebrew (Masoretic Text). This translation was promoted by way of a legend (primarily recorded as the Letter of Aristeas) that seventy (or in some sources, seventy-two) separate translators all produced identical texts.

From the 800s to the 1400s, Jewish scholars today known as Masoretes compared the text of all known biblical manuscripts in an effort to create a unified, standardized text. A series of highly similar texts eventually emerged, and any of these texts are known as Masoretic Texts (MT). The Masoretes also added vowel points (called niqqud) to the text, since the original text only contained consonant letters. This sometimes required the selection of an interpretation, since some words differ only in their vowelsâ€”their meaning can vary in accordance with the vowels chosen. In antiquity, variant Hebrew readings existed, some of which have survived in the Samaritan Pentateuch, the Dead Sea scrolls, and other ancient fragments, as well as being attested in ancient versions in other languages.

Versions of the Septuagint contain several passages and whole books beyond what was included in the Masoretic texts of the Tanakh. In some cases these additions were originally composed in Greek, while in other cases they are translations of Hebrew books or variants not present in the Masoretic texts. Recent discoveries have shown that more of the Septuagint additions have a Hebrew origin than was once thought. While there are no complete surviving manuscripts of the Hebrew texts on which the Septuagint was based, many scholars believe that they represent a different textual tradition ("Vorlage") from the one that became the basis for the Masoretic texts.

Jews also produced non-literal translations or paraphrases known as targums, primarily in Aramaic. They frequently expanded on the text with additional details taken from Rabbinic oral tradition.

Orthodox Judaism, as epitomized in the Pharisee sect, rejects any notion that the Written Torah and Oral Torah are distinct entities. The Written Torah (the Five Books of Moses), Prophets and Writings, form the corpus of what is God's word in written form. This body, is completely incomprehensible without an Oral Tradition. For example, a Torah scroll contains no vowels, and no punctuation. Were it not for an Oral Tradition, the meaning of words would be unknown, as well as the sentence structure, where to begin and end verses, sections etc. are all reliant on an oral tradition. This is extended into what Orthodox Judaism classifies in the Legal parts of the Oral Tradition, as the rules of Biblical Exegesis, which defines how to interpret the text, which is also transmitted orally. The Oral Tradition, however, is much more broad. It includes Midrash, Halachic and Aggadic, Kabbalah, interpretation, and the legal portions, which are codified to some extent in the Mishnah, Tosefta, Sifre, Sifra, Mechilta, Talmuds (both Babylonian and Jerusalem).

The Sadducees were a minority group, that had some sway during the Hellenistic period, were the inheritors of their leader Zadok who believed that there was only a minimal oral tradition of interpreting the words of the Torah, and did not extend into extended biblical interpretation. They argued against the Rabbis in mostly legal matters, threatening the very existence of Judaism. The Sadducees became corrupted and took over positions in the Priestly service, some becoming the High Priest. Others incited unrest by subverting the Sanhedrin, by providing false testimony in which the new-moon would be declared, and hence the dates of the year for the festivals would be altered.

Masorti and Conservative Judaism state that the Oral Tradition is to some degree Divinely inspired, but disregard its legal elements in varying degrees. Reform Judaism also gives some credence to the Talmud containing the Legal elements of the Oral Torah, but, as with the written Torah, asserts that both were inspired by, but not dictated by, God. Reconstructionist Judaism, denies any connection of the Torah, Written or Oral with God.

The Christian Bible consists of the Hebrew scriptures, which have been called the Old Testament, and some later writings known as the New Testament. Some groups within Christianity include additional books as part one or both of these sections of their sacred writings â€?most prominent among which are the biblical apocrypha or deuterocanonical books.

In Judaism, the term Christian Bible is commonly used to identify only those books like the New Testament which have been added by Christians to the Masoretic Text, and excludes any reference to an Old Testament.

The Old Testament is the collection of books written prior to the life of Jesus but accepted by Christians as scripture. Broadly speaking, it is the same as the Hebrew Bible, however it divides and orders them differently, and varies from Judaism in interpretation and emphasis, see for example . Several Christian denominations also incorporate additional books into their canons of the Old Testament. A few groups consider particular translations to be divinely inspired, notably the Greek Septuagint, the Aramaic Peshitta, and the English King James Version.

The Septuagint (Greek translation, from Alexandria in Egypt under the Ptolemies) was generally abandoned in favour of the Masoretic text as the basis for translations of the Old Testament into Western languages from St. Jerome's Bible (the Vulgate) to the present day. In Eastern Christianity, translations based on the Septuagint still prevail. Some modern Western translations make use of the Septuagint to clarify passages in the Masoretic text, where the Septuagint may preserve a variant reading of the Hebrew text. They also sometimes adopt variants that appear in other texts e.g. those discovered among the Dead Sea Scrolls.

A number of books which are part of the Greek Septuagint but are not found in the Hebrew (Rabbinic) Bible are often referred to as deuterocanonical books by Catholics referring to a later secondary (i.e. deutero) canon. Most Protestants term these books as apocrypha. Evangelicals and those of the Modern Protestant traditions do not accept the deuterocanonical books as canonical, although Protestant Bibles included them in Apocrypha sections until around the 1820s. However, the Catholic, Eastern Orthodox, and Oriental Orthodox Churches include these books as part of their Old Testament. The Catholic Church recognizes seven such books (Tobit, Judith, 1 Maccabees, 2 Maccabees, Wisdom of Solomon, Ecclesiasticus, and Baruch), as well as some passages in Esther and Daniel. Various Orthodox Churches include a few others, typically 3 Maccabees, 1 Esdras, Odes, Psalms of Solomon, and the Prayer of Manasseh and their Psalter has an additional psalm. The Anglican Church uses the Apocryphal books liturgically, but not to establish doctrine. Therefore, editions of the Bible intended for use in the Anglican Church include these books, plus 1 Esdras, 2 Esdras and the Prayer of Manasseh.

The Bible as used by the majority of Christians includes the Rabbinic Hebrew Scripture and the New Testament, which relates the life and teachings of Jesus, the letters of the Apostle Paul and other disciples to the early church and the Book of Revelation. 



The New Testament is a collection of 27 books, of 4 different genres of Christian literature (Gospels, one account of the Acts of the Apostles, Epistles and an Apocalypse). Jesus is its central figure. The New Testament was written primarily in Koine Greek in the early Christian period, though a minority argue for Aramaic primacy. Nearly all Christians recognize the New Testament (as stated below) as canonical scripture. These books can be grouped into:

The Gospels

Pauline Epistles

General Epistles, also called Jewish Epistles

The order of these books varies according to Church tradition. The New Testament books are ordered differently in the Catholic/Protestant tradition, the Lutheran tradition, the Slavonic tradition, the Syriac tradition and the Ethiopian tradition.

The books of the New Testament were likely written in Koine Greek, the language of the earliest extant manuscripts, even though some authors often included translations from Hebrew and Aramaic texts. Certainly the Pauline Epistles were written in Greek for Greek-speaking audiences. See Greek primacy. Some scholars believe that some books of the Greek New Testament (in particular, the Gospel of Matthew) are actually translations of a Hebrew or Aramaic original. Of these, a small number accept the Syriac Peshitta as representative of the original. See Aramaic primacy.





When ancient scribes copied earlier books, they wrote notes on the margins of the page (marginal glosses) to correct their textâ€”especially if a scribe accidentally omitted a word or lineâ€”and to comment about the text. When later scribes were copying the copy, they were sometimes uncertain if a note was intended to be included as part of the text. See textual criticism. Over time, different regions evolved different versions, each with its own assemblage of omissions and additions.

The autographs, the Greek manuscripts written by the original authors, have not survived. Scholars surmise the original Greek text from the versions that do survive. The three main textual traditions of the Greek New Testament are sometimes called the Alexandrian text-type (generally minimalist), the Byzantine text-type (generally maximalist), and the Western text-type (occasionally wild). Together they comprise most of the ancient manuscripts.

There are also several ancient translations, most important of which are in the Syriac dialect of Aramaic (including the Peshitta and the Diatessaron gospel harmony), in the Ethiopian language of Ge'ez, and in Latin (both the Vetus Latina and the Vulgate).

In 331, the Emperor Constantine commissioned Eusebius to deliver fifty Bibles for the Church of Constantinople. Athanasius (Apol. Const. 4) recorded Alexandrian scribes around 340 preparing Bibles for Constans. Little else is known, though there is plenty of speculation. For example, it is speculated that this may have provided motivation for canon lists, and that Codex Vaticanus, Codex Sinaiticus and Codex Alexandrinus are examples of these Bibles. Together with the Peshitta, these are the earliest extant Christian Bibles.

The earliest surviving complete manuscript of the entire Bible is the Codex Amiatinus, a Latin Vulgate edition produced in eighth century England at the double monastery of Wearmouth-Jarrow.

The earliest printed edition of the Greek New Testament appeared in 1516 from the Froben press, by Desiderius Erasmus, who reconstructed its Greek text from several recent manuscripts of the Byzantine text-type. He occasionally added a Greek translation of the Latin Vulgate for parts that did not exist in the Greek manuscripts. He produced four later editions of this text. Erasmus was Roman Catholic, but his preference for the Byzantine Greek manuscripts rather than the Latin Vulgate led some church authorities to view him with suspicion.

The first printed edition with critical apparatus (noting variant readings among the manuscripts) was produced by the printer Robert Estienne of Paris in 1550. The Greek text of this edition and of those of Erasmus became known as the Textus Receptus (Latin for "received text"), a name given to it in the Elzevier edition of 1633, which termed it as the text nunc ab omnibus receptum ("now received by all").

The churches of the Protestant Reformation translated the Greek of the Textus Receptus to produce vernacular Bibles, such as the German Luther Bible and the English King James Bible.

The discovery of older manuscripts, which belong to the Alexandrian text-type, including the 4th century Codex Vaticanus and Codex Sinaiticus, led scholars to revise their view about the original Greek text. Attempts to reconstruct the original text are called critical editions. Karl Lachmann based his critical edition of 1831 on manuscripts dating from the 4th century and earlier, to demonstrate that the Textus Receptus must be corrected according to these earlier texts.

Later critical editions incorporate ongoing scholarly research, including discoveries of Greek papyrus fragments from near Alexandria, Egypt, that date in some cases within a few decades of the original New Testament writings. Today, most critical editions of the Greek New Testament, such as UBS4 and NA27, consider the Alexandrian text-type corrected by papyri, to be the Greek text that is closest to the original autographs. Their apparatus includes the result of votes among scholars, ranging from certain {A} to doubtful {E}, on which variants best preserve the original Greek text of the New Testament.

Most variants among the manuscripts are minor, such as alternate spelling, alternate word order, the presence or absence of an optional definite article ("the"), and so on. Occasionally, a major variant happens when a portion of a text was accidentally omitted (or perhaps even censored), or was added from a marginal gloss. Fortunately, major variants tend to be easier to correct. Examples of major variants are the endings of Mark, the Pericope AdulterÃ¦, the Comma Johanneum, and the Western version of Acts.

Critical editions that rely primarily on the Alexandrian text-type inform nearly all modern translations (and revisions of older translations).

However for reasons of tradition, especially the doctrine of the inerrancy of the King James Bible, some modern scholars prefer to use the Textus Receptus for the Greek text, or use the Majority Text which is similar to it but is a critical edition that relies on earlier manuscripts of the Byzantine text-type. Among these scholars, some argue that the Byzantine tradition contains scribal additions, but these later interpolations preserve the orthodox interpretations of the biblical textâ€”as part of the ongoing Christian experienceâ€”and in this sense are authoritative.

While individual books within the Christian Bible present narratives set in certain historical periods, most Christian denominations teach that the Bible itself has an overarching message. 

There are among Christians wide differences of opinion as to how particular incidents as described in the Bible are to be interpreted and as to what meaning should be attached to various prophecies. However, Christians in general are in agreement as to the Bible's basic message. A general outline, as described by C. S. Lewis, is as follows:

Many Christians, Muslims, and Jews regard the Bible as inspired by God yet written by a variety of imperfect men over thousands of years. Many others, who identify themselves as Bible-believing Christians, regard both the New and Old Testament as the undiluted Word of God, spoken by God and written down in its perfect form by humans. Still others hold the Biblical infallibility perspective, that the Bible is free from error in spiritual but not scientific matters.

Belief in sacred texts is attested to in Jewish antiquity, and this belief can also be seen in the earliest of Christian writings. Various texts of the Bible mention Divine agency in relation to prophetic writings, the most explicit being : "All scripture is breathed out by God and profitable for teaching, for reproof, for correction, and for training in righteousness." 

In their book A General Introduction to the Bible, Norman Geisler and William Nix wrote: "The process of inspiration is a mystery of the providence of God, but the result of this process is a verbal, plenary, inerrant, and authoritative record."Some biblical scholars associate inspiration with only the original text; for example some American Protestants adhere to the 1978 Chicago Statement on Biblical Inerrancy which asserted that inspiration applied only to the autographic text of Scripture. Others, including some adherents to the King James Only view, attribute inerrancy to particular translations.

The word "canon" etymologically means cane or reed. In early Christianity "canon" referred to a list of books approved for public reading. Books not on the list were referred to as "apocryphal" â€?meaning they were for private reading only. Under Latin usage from the fourth century on, canon came to stand for a closed and authoritative list in the sense of rule or norm.

The New Testament refers to the threefold division of the Hebrew Scriptures: the law, the prophets, and the writings.  refers to the "law of Moses" (Pentateuch), the "prophets" which include certain historical books in addition to the books now called "prophets," and the psalms (the "writings" designated by its most prominent collection). The Hebrew Bible probably was canonized in these three stages: the law canonized before the Exile, the prophets by the time of the Syrian persecution of the Jews, and the writings shortly after AD 70 (the fall of Jerusalem). About that time, early Christian writings began being accepted by Christians as "scripture." These events, taken together, may have caused the Jews to close their "canon." They listed their own recognized Scriptures and also excluded both Christian and Jewish writings considered by them to be "apocryphal." In this canon the thirty-nine books found in the Old Testament of today's Christian Bibles were grouped together as twenty-two books, equaling the number of letters in the Hebrew alphabet. This canon of Jewish scripture is attested to by Philo, Josephus, the New Testament (, ), and the Talmud.

The New Testament writers assumed the inspiration of the Old Testament, probably earliest stated in  which may be rendered "All Scripture is inspired of God" or "Every God-inspired Scripture is profitable for teaching." Both translations consider inspiration as a fact.

The Old Testament canon entered into Christian use in the Greek Septuagint translations and original books, and their differing lists of texts. In addition to the Septuagint, Christianity subsequently added various writings that would become the New Testament. Somewhat different lists of accepted works continued to develop in antiquity. In the fourth century a series of synods produced a list of texts equal to the 46-book canon of the Old Testament and to the 27-book canon of the New Testament that would be subsequently used to today, most notably the Synod of Hippo in AD 393. Also c. 400, Jerome produced a definitive Latin edition of the Bible (see Vulgate), the canon of which, at the insistence of the Pope, was in accord with the earlier Synods. With the benefit of hindsight it can be said that this process effectively set the New Testament canon, although there are examples of other canonical lists in use after this time. A definitive list did not come from an Ecumenical Council until the Council of Trent (1545â€?3).

During the Protestant Reformation, certain reformers proposed different canonical lists than what was currently in use. Though not without debate, see Antilegomena, the list of New Testament books would come to remain the same; however, the Old Testament texts present in the Septuagint, but not included in the Jewish canon, fell out of favor. In time they would come to be removed from most Protestant canons. Hence, in a Catholic context these texts are referred to as deuterocanonical books, whereas in a Protestant context they are referred to as Apocrypha, the label applied to all texts excluded from the biblical canon which were in the Septuagint. It should also be noted, that Catholics and Protestants both describe certain other books, such as the Acts of Peter, as apocryphal. 

Thus, the Protestant Old Testament of today has a 39-book canonâ€”the number varies from that of the books in the Tanakh (though not in content) because of a different method of divisionâ€”while the Roman Catholic Church recognizes 46 books as part of the canonical Old Testament. The term "Hebrew Scriptures" is only synonymous with the Protestant Old Testament, not the Catholic, which contains the Hebrew Scriptures and additional texts. Both Catholics and Protestants have the same 27-book New Testament Canon.

The Canon of the Ethiopian Orthodox Tewahedo Church is wider than for most other Christian groups. The Ethiopian Old Testament Canon includes the books found in the Septuagint accepted by other Orthodox Christians, in addition to Enoch and Jubilees which are ancient Jewish books that only survived in Ge'ez but are quoted in the New Testament, also First and 2 Esdras (the latter also known as the Apocalypse of Ezra), 3 books of Meqabyan, and Psalm 151 at the end of the Psalter. The three books of Meqabyan are not be confused with the books of Maccabees. The order of the other books is somewhat different from other groups', as well.

In scholarly writing, ancient translations are frequently referred to as "versions," with the term "translation" being reserved for medieval or modern translations. Bible versions are discussed below, while Bible translations can be found on a separate page.

The original texts of the Tanakh were in Hebrew, although some portions were in Aramaic. In addition to the authoritative Masoretic Text, Jews still refer to the Septuagint, the translation of the Hebrew Bible into Greek, and the Targum Onkelos, an Aramaic version of the Bible. There are several different ancient versions of the Tanakh in Hebrew, mostly differing by spelling, and the traditional Jewish version is based on the version known as Aleppo Codex. Even in this version by itself, there are words which are traditionally read differently than written (sometimes one word is written and another is read), because the oral tradition is considered more fundamental than the written one, and presumably mistakes had been made in copying the text over the generations.

The primary biblical text for early Christians was the Septuagint or (LXX). In addition they translated the Hebrew Bible into several other languages. Translations were made into Syriac, Coptic, Ge'ez and Latin, among other languages. The Latin translations were historically the most important for the Church in the West, while the Greek-speaking East continued to use the Septuagint translations of the Old Testament and had no need to translate the New Testament.

The earliest Latin translation was the Old Latin text, or Vetus Latina, which, from internal evidence, seems to have been made by several authors over a period of time. It was based on the Septuagint, and thus included books not in the Hebrew Bible.

Pope Damasus I assembled the first list of books of the Bible at the Council of Rome in 382 AD. He commissioned Saint Jerome to produce a reliable and consistent text by translating the original Greek and Hebrew texts into Latin. This translation became known as the Latin Vulgate Bible and in 1546 at the Council of Trent was declared by the Church to be the only authentic and official Bible in the Latin rite.

Bible translations for many languages have been made through the various influences of Catholicism, Orthodox, Protestant, etc especially since the Protestant Reformation. The Bible has seen a notably large number of English language translations.

The work of Bible translation continues, including by Christian organisations such as Wycliffe Bible Translators (wycliffe.net), New Tribes Missions (ntm.org) and the Bible Societies (biblesociety.org). Of the world's 6,900 languages, 2,400 have some or all of the Bible, 1,600 (spoken by more than a billion people) have translation underway, and some 2,500 (spoken by 270 million people) are judged as needing translation to begin.



As Hebrew and Greek, the original languages of the Bible, have idioms and concepts not easily translated, there is an on going critical tension about whether it is better to give a word for word translation or to give a translation that gives a parallel idiom in the target language. For instance, in the English language Catholic translation, the New American Bible, as well as the Protestant translations of the Christian Bible, translations like the King James Version, the New Revised Standard Version and the New American Standard Bible are seen as fairly literal translations (or "word for word"), whereas translations like the New International Version and New Living Translation attempt to give relevant parallel idioms. The Living Bible and The Message are two paraphrases of the Bible that try to convey the original meaning in contemporary language. The further away one gets from word to word translation, the text becomes easier to read while relying more on the theological, linguistic or cultural understanding of the translator, which one would not normally expect a lay reader to require.

Traditionally, English masculine pronouns have been used interchangeably to refer to the male gender and to all people. For instance, "All men are mortal" is not intended to imply that males are mortal but females are immortal. English language readers and hearers have had to interpret masculine pronouns (and such words as "man" and "mankind") based on context. Further, both Hebrew and Greek, like some of the Latin-origin languages, use the male gender of nouns and pronouns to refer to groups that contain both sexes. This creates some difficulty in determining whether a noun or pronoun should be translated using terms that refer to men only, or generically to men and women inclusively. Context sometimes, but not always, helps determine whether to decode them in a gender-insensitive or gender-specific way.

Contemporary language has changed in many cases to reflect criticism of the use of the masculine gender, which has been characterized as discriminatory. Current style guides, such as APA, MLA, NCTE, and others, have published statements encouraging, and in some cases requiring, the use of inclusive language, which avoids language this approach regards as sexist or class-distinctive.

Until recently, virtually all English translations of the Bible have used masculine nouns and pronouns both specifically (to refer to males) and generically (when the reference is not necessarily gender-specific). Recent examples of translations which incorporate gender-inclusive language include the New Revised Standard Version, the Revised English Bible, and Today's New International Version. 



The Hebrew Masoretic text contains verse endings as an important feature. According to the Talmudic tradition, the verse endings are of ancient origin. The Masoretic textual tradition also contains section endings called parashiyot, which are indicated by a space within a line (a "closed" section") or a new line beginning (an "open" section). The division of the text reflected in the parashiyot is usually thematic. The parashiyot are not numbered.

In early manuscripts (most importantly in Tiberian Masoretic manuscripts, such as the Aleppo codex) an "open" section may also be represented by a blank line, and a "closed" section by a new line that is slightly indented (the preceding line may also not be full). These latter conventions are no longer used in Torah scrolls and printed Hebrew Bibles. In this system the one rule differentiating "open" and "closed" sections is that "open" sections must always begin at the beginning of a new line, while "closed" sections never start at the beginning of a new line.

Another related feature of the Masoretic text is the division of the sedarim. This division is not thematic, but is almost entirely based upon the quantity of text.

The Byzantines also introduced a chapter division of sorts, called Kephalaia. It is not identical to the present chapters.

The current division of the Bible into chapters and the verse numbers within the chapters has no basis in any ancient textual tradition. Rather, they are medieval Christian inventions. They were later adopted by many Jews as well, as technical references within the Hebrew text. Such technical references became crucial to medieval rabbis in the historical context of forced debates with Christian clergy (who used the chapter and verse numbers), especially in late medieval Spain. Chapter divisions were first used by Jews in a 1330 manuscript and for a printed edition in 1516. However, for the past generation, most Jewish editions of the complete Hebrew Bible have made a systematic effort to relegate chapter and verse numbers to the margins of the text.

The division of the Bible into chapters and verses has often elicited severe criticism from traditionalists and modern scholars alike. Critics charge that the text is often divided into chapters in an incoherent way, or at inappropriate rhetorical points, and that it encourages citing passages out of context, in effect turning the Bible into a kind of textual quarry for clerical citations. Nevertheless, the chapter divisions and verse numbers have become indispensable as technical references for Bible study.

Stephen Langton is reputed to have been the first to put the chapter divisions into a Vulgate edition of the Bible, in 1205. They were then inserted into Greek manuscripts of the New Testament in the 1400s. Robert Estienne (Robert Stephanus) was the first to number the verses within each chapter, his verse numbers entering printed editions in 1551 (New Testament) and 1571 (Hebrew Bible).

Biblical criticism refers to the investigation of the Bible as a text, and addresses questions such as authorship, dates of composition, and authorial intention.

The traditional view of the Mosaic authorship of the Torah came under sporadic criticism from medieval scholars including Isaac ibn Yashush, Abraham ibn Ezra, Bonfils of Damascus and bishop Tostatus of Avila, who pointed to passages such as the description of the death of Moses in Deuteronomy as evidence that some portions, at least, could not have been written by Moses. In the 17th century Thomas Hobbes collected the current evidence and became the first scholar to conclude outright that Moses could not have written the bulk of the Torah. Shortly afterwards the philosopher Baruch Spinoza published a unified critical analysis, demonstrating that the problematic passages were not isolated cases that could be explained away one by one, but pervasive throughout the five books, concluding that it was "clearer than the sun at noon that the Pentateuch was not written by Mosesâ€?" Despite determined opposition from the Church, both Catholic and Protestant, the views of Hobbes and Spinoza gained increasing acceptance amongst scholars.

Scholars intrigued by the hypothesis that Moses had not written the Pentateuch considered other authors. Independent but nearly simultaneous proposals by H. B. Witter, Jean Astruc, and Johann Gottfried Eichhorn separated the Pentateuch into two original documentary components, both dating from after the time of Moses. Others hypothesized the presence of two additional sources. The four documents were given working titles: J (or Yahwist), E (Elohist), P (Priestly), and D (Deuteronomist), each was discernible by its own characteristic language, and each, when read in isolation, presented a unified, coherent narrative.

Subsequent scholars, notably Eduard Reuss, Karl Heinrich Graf and Wilhelm Vatke, turned their attention to the order in which the documents had been composed (which they deduced from internal clues) and placed them in the context of a theory of the development of ancient Israelite religion, suggesting that much of the Laws and the narrative of the Pentateuch were unknown to the Israelites in the time of Moses. These were synthesized by Julius Wellhausen (1844-1918), who suggested a historical framework for the composition of the documents and their redaction (combination) into the final document known as the Pentateuch. This hypothesis was challenged by William Henry Green in his The Mosaic Origins of the Pentateuchal Codes (available online). Nonetheless, according to contemporary Torah scholar Richard Elliott Friedman, Wellhausen's model of the documentary hypothesis continues to dominate the field of biblical scholarship: "To this day, if you want to disagree, you disagree with Wellhausen. If you want to pose a new model, you compare its merits with those of Wellhausen's model."

The documentary hypothesis is important in the field of biblical studies not only because it claims that the Torah was written by different people at different timesâ€”generally long after the events it describesâ€?but it also proposed what was at the time a radically new way of reading the Bible. Many proponents of the documentary hypothesis view the Bible more as a body of literature than a work of history, believing that the historical value of the text lies not in its account of the events that it describes, but in what critics can infer about the times in which the authors lived (as critics may read Hamlet to learn about seventeenth-century England, but will not read it to learn about seventh-century Denmark).

The critical analysis of authorship now encompasses every book of the Bible. Every book in turn has been hypothesized to bear traces of multiple authorship, even the book of Obadiah, which is only a single page. In some cases the traditional view on authorship has been overturned; in others, additional support, at least in part has been found.

The development of the hypothesis has not stopped with Wellhausen. Wellhausen's hypothesis, for example, proposed that the four documents were composed in the order J-E-D-P, with P, containing the bulk of the Jewish law, dating from the post-Exilic Second Temple period (i.e., after 515 BC); but the contemporary view is that P is earlier than D, and that all four books date from the First Temple period (i.e., prior to 587 BC). The documentary hypothesis has more recently been refined by later scholars such as Martin Noth (who in 1943 provided evidence that Deuteronomy plus the following six books make a unified history from the hand of a single editor), Harold Bloom, Frank Moore Cross and Richard Elliot Friedman.

The documentary hypothesis, at least in the four-document version advanced by Wellhausen, has been controversial since its formulation. The direction of this criticism is to question the existence of separate, identifiable documents, positing instead that the biblical text is made up of almost innumerable strands so interwoven as to be hardly untangleableâ€”the J document, in particular, has been subjected to such intense dissection that it seems in danger of disappearing. 

Although biblical archaeology has confirmed the existence of many people, places, and events mentioned in the Bible, many critical scholars have argued that the Bible be read not as an accurate historical document, but rather as a work of literature and theology that often draws on historical eventsâ€”as well as upon non-Hebrew mythologyâ€”as primary source material(see The Bible and history). For these scholars, the Bible reveals much about the lives and times of its authors and compilers. The relevance of these ideas to contemporary religious life is left to clerics and adherents of contemporary religions to decide.

The claim that the Torahâ€?the Five Books of Moses"â€”were not written by Moses, but by many authors long after Moses was said to have lived, directly challenged Jewish orthodoxy. For most, this claim implies that the Torah itselfâ€”especially its account of God's revelation at Mt. Sinaiâ€”is not historically reliable. Although many Orthodox scholars have rejected this "Higher Criticism", most Conservative and virtually all Reform Jewish scholars have accepted it. Consequently, there has been considerable debate among Jewish scholars as to the nature of revelation and the divine nature of the Torah. Conservative Jewish philosopher Elliot Dorff has categorized five distinct major Jewish positions in these debates within Conservative Judaism in the 20th century:

In addition to the 5 categories described by Elliott, other positions have been adopted:

In 1943 Pope Pius XII's encyclical Divino Afflante Spiritu gave the Vatican's imprimatur to textual criticism.

According to recent theories, linguistic as well as archaeological, the global structure of the texts in the Hebrew Bible were compiled during the reign of King Josiah in the 7th century BC. Even though the components are derived from more ancient writings, the final form of the books is believed to have been set somewhere between the 1st century BC and the 4th century AD. 

With regard to the Exodus and the 40-year sojourn in the desert, archaeological digs in possible Biblical locations have been unsuccessful so far. There is also no archaeological evidence of a conquest of the land and cities of Canaan of the kind recounted in the Book of Joshua.

However, after the split of the Kingdom of Israel in the second half of the 9th century BC, archaeological findings fit the Biblical chronology. 

The ancestors of the Hebrews and the Jews are believed to be either nomads who have become sedentary, or people from the plains of Canaan, who fled to the highlands to escape the control of the cities. These positions are held by Israel Finkelstein and Neil Silberman in The Bible Unearthed, by the American archaeologist William Dever in Who Were the Early Israelites and Where Did They Come from?, and by Jean-Marie Husser, professor at Marc Bloch University in Strasbourg, France.



See Biblical exegesis.













Modern Standard Hindi, also known as Standard Hindi, High Hindi, Nagari Hindi or Literary Hindi, and in some contexts simply Hindi (DevanÄgarÄ«:  or , IAST: , : ) is a standardized register of Hindi. It is one of the 22 official languages of India, and is used, along with English, for administration of the central government. 

The regulating authority for Standard Hindi is the Central Hindi Directorate.

According to SIL International's Ethnologue, about 180 million people in India regarded Standard Hindi (Khariboli) as their mother tongue in 1991, and another 120 million use it as a second language. Of the 8 million "Hindi" speakers in Nepal, 100,000 speak Standard Hindi. 

The Constitution of India, adopted in 1950, declares Hindi in the Devanagari script as the official language() of the Union (Article 343(1)). Hindi is also enumerated as one of the twenty-two languages of the Eighth Schedule of the Constitution of India, which entitles it to representation on the Official Language Commission. The Constitution of India has stipulated the usage of Hindi and English to be the two languages of communication for the Central Government.

It was envisioned that Hindi would become the sole working language of the central government by 1965 (per directives in Article 344 (2) and Article 351), with state governments being free to function in languages of their own choice. However, widespread resistance movements to the imposition of Hindi on non-native speakers (such as the Anti-Hindi agitations in Tamil Nadu) lead to the passage of the Official Languages Act (1963), which provided for the continued use of English, indefinitely, for all official purposes. However, the constitutional directive to the central government to champion the spread of Hindi was retained and has strongly influenced the policies of the Union government.

At the state level, Hindi is the official language of the following states in India: Bihar, Jharkhand, Uttarakhand, Madhya Pradesh, Rajasthan, Uttar Pradesh, Chattisgarh, Himachal Pradesh, Haryana, and Delhi. Each of these states may also designate a "co-official language"; in Uttar Pradesh for instance, depending on the political formation in power, sometimes this language is Urdu. Similarly, Hindi is accorded the status of co-official language in several states.



The dialect upon which Standard Hindi is based is khari boli, the vernacular of the Delhi region. This dialect acquired linguistic prestige in the Mughal period (17th century) and became known as Urdu, "[the language] of the court". 

After independence, the Government of India worked on standardizing Hindi, instituting the following changes:



Standard Hindi derives much of its formal and technical vocabulary from Sanskrit. Standard or shuddh ("pure") Hindi is used only in public addresses and radio or TV news, while the everyday spoken language in most areas is one of several varieties of Hindustani, whose vocabulary contains words drawn from Persian and Arabic. In addition, spoken Hindi includes words from English and other languages as well.

Vernacular Urdu and Hindi share the same grammar and core vocabulary and so are practically indistinguishable. However, the literary registers differ substantially in borrowed vocabulary; in highly formal situations, the languages are barely intelligible to speakers of the other. Hindi has looked to Sanskrit for borrowings from at least the 19th century, and Urdu has looked to Persian and Arabic for borrowings from the eighteenth century. On another dimension, Hindi has been associated with the Hindu community and Urdu with the Muslim community.

There are five principal categories of words in Standard Hindi: 

Similarly, Urdu treats its own vocabulary, borrowed directly from Persian and Arabic, as a separate category for morphological purposes.

Hindi from which most of the Persian, Arabic and English words have been ousted and replaced by tatsam words is called Shuddha Hindi (pure Hindi). Chiefly, the proponents of Hindutva ideology ("Hindu-ness") are vociferous supporters of Shuddha Hindi.

Excessive use of tatsam words sometimes creates problems for most native speakers. Strictly speaking, the tatsam words are words of Sanskrit and not of Hindiâ€”thus they have complicated consonantal clusters which are not linguistically valid in Hindi. The educated middle class population of India can pronounce these words with ease, but people of rural backgrounds have much difficulty in pronouncing them. Similarly, vocabulary borrowed from Persian and Arabic also brings in its own consonantal clusters and "foreign" sounds, which may again cause difficulty in speaking them.





Hindi is written in the Devanagari script, an abugida which is written from left to right.

The standard transliteration of Hindi into the Roman alphabet is usually the IAST scheme, whereby the retroflex consonants (retroflex t, d, their aspirates, n, vowel-like r) and the breath h are shown with a dot beneath; the long vowels are shown with a macron or a bar (as  above); aspiration of a plosive is shown with a following h; and elided a's are removed for a truer correspondence to speech. Other alphabet characters are pronounced as in normal English. Another transliteration (ITRANS) uses capital letters of English to transcribe the long vowels and retroflex consonants. However, since English is a lingua franca of the educated Indians, and since computer keyboards do not have features for typing the IAST characters, Indians today use a casual transliteration into English for Hindi words; in such a casual transliteration, used especially in online chatting, the retroflex and dental consonants are not differentiated, and neither the short and the long vowels (except that sometimes people double the alphabet to indicate a long vowel).



Despite Hindi and English both being Indo-European languages, Hindi grammar is different in many ways from what English speakers are used to. Most notably, Hindi is a subject-object-verb language, meaning that verbs usually fall at the end of the sentence rather than before the object (as in English). Hindi also shows mixed ergativity so that, in some cases, verbs agree with the object of a sentence rather than the subject. Unlike English, Hindi has no definite article (the). The numeral ek might be used as the indefinite singular article (a/an) if this needs to be stressed.

In addition, Hindi uses postpositions (so called because they are placed after nouns) where English uses prepositions. Other differences include gender, honorifics, interrogatives, use of cases, and different tenses. While being complicated, Hindi grammar is fairly regular, with irregularities being relatively limited. Despite differences in vocabulary and writing, Hindi grammar is nearly identical with . The concept of punctuation having been entirely unknown before the advent of the Europeans, Hindi punctuation uses western conventions for commas, exclamation points, and question marks. Periods are sometimes used to end a sentence, though the traditional "full stop" (a vertical line) is also used.

In Hindi, there are only two genders for nouns. All male human beings and male animals (or those animals and plants which are perceived to be "masculine") are masculine. All female human beings and female animals (or those animals and plants which are perceived to be "feminine") are feminine. Things, inanimate articles and abstract nouns are also either masculine or feminine according to convention, which must be memorised by non-Hindi speakers if they wish to learn correct Hindi. While this is the same as  and similar to many other Indo-European languages such as Italian, French and Spanish, it is a challenge for those who are used to only the English language, which although an Indo-European language, has dropped nearly all of its gender inflection.

The ending of a word, if a vowel, usually helps in this gender classification. Among tatsam words, the masculine words of Sanskrit remain masculine in Hindi, and same is the case for the feminine. Sanskrit neuter nouns usually become masculine in Hindi. Among the tadbhav words, if a word end in long , it is normally masculine. If a word ends in  or , it is normally feminine. The gender of words borrowed from Arabic and Persian is determined either by phonology (usually the last vowel in the word) or by the gender of the nearest Hindi equivalent. The gender assignment of Hindi words directly borrowed from English (which are numerous) is also usually determined by the gender of the nearest Hindi "synonym" or by the ending. Most adjectives ending in a vowel are inflected to agree with the gender of the noun:  'my daughter' vs.  'my son'.

Besides the standard interrogative terms of who (à¤•à¥Œà¤?kaun), what (à¤•à¥à¤¯à¤¾ kyaa), why (à¤•à¤¯à¥‹à¤‚ kyÃµ), when (à¤•à¤¬ kab), where (à¤•à¤¹à¤¾à¤ kahÃ£), how and what type (à¤•à¥ˆà¤¸à¤¾ kaisaa), how many (à¤•à¤¿à¤¤à¤¨à¤?kitnaa), etc, the Hindi word kyaa (à¤•à¥à¤¯à¤¾) can be used as a generic interrogative often placed at the beginning of a sentence to turn a statement into a Yes/No question. This makes it clear when a question is being asked. Questions can also be formed simply by modifying intonation, exactly as some questions are in English.

Hindi has pronouns in the first, second and third person for one gender only. Thus, unlike English, there is no difference between he or she. More strictly speaking, the third person of the pronoun is actually the same as the demonstrative pronoun (this / that). The verb, upon conjugation, usually indicates the difference in the gender. The pronouns have additional cases of accusative and genitive, but no vocative. There may also be binary ways of inflecting the pronoun in the accusative case. Note that for the second person of the pronoun (you), Hindi has three levels of honorifics:

Imperatives (requests and commands) correspond in form to the level of honorific being used, and the verb inflects to show the level of respect and politeness desired. Because imperatives can already include politeness, the word "kripayÄ", which can be translated as "please", is much less common than in spoken English; it is generally only used in writing or announcements, and its use in common speech may even reflect mockery.

The standard word order in Hindi is, in general, Subject Object Verb, but where different emphasis or more complex structure is needed, this rule is very easily set aside (provided that the nouns/pronouns are always followed by their postpositions or case markers). More specifically, the standard order is 1. Subject 2. Adverbs (in their standard order) 3. Indirect object and any of its adjectives 4. Direct object and any of its adjectives 5. Negation term or interrogative, if any, and finally the 6. Verb and any auxiliary verbs. (Snell, p93) The standard order can be modified in various ways to impart emphasis on particular parts of the sentence. Negation is formed by adding the word à¤¨à¤¹à¥€à¤?(nahÄ«Ìƒ, "no"), in the appropriate place in the sentence, or by utilizing à¤?(na) or à¤®à¤¤ (mat) in some cases. Note that in Hindi, the adjectives precede the nouns they qualify. The auxiliaries always follow the main verb. Also, Hindi speakers or writers enjoy considerable freedom in placing words to achieve stylistic and other socio-psychological effects, though not as much freedom as in heavily inflected languages.

Hindi verbal structure is focused on aspect with distinctions based on tense usually shown through use of the verb honÄ (to be) as an auxiliary. There are three aspects: habitual (imperfect), progressive (also known as continuous) and perfective. Verbs in each aspect are marked for tense in almost all cases with the proper inflected form of honÄ. Hindi has four simple tenses, present, past, future (presumptive), and subjunctive (referred to as a mood by many linguists). Verbs are conjugated not only to show the number and person (1st, 2nd, 3rd) of their subject, but also its gender. Additionally, Hindi has imperative and conditional moods. The verbs must agree with the person, number and gender of the subject if and only if the subject is not followed by any postposition. If this condition is not met, the verb must agree with the number and gender of the object (provided the object does not have any postposition). If this condition is also not met, the verb agrees with neither. It is this kind of phenomenon that is called mixed ergativity.

Hindi is a weakly inflected language for case; the relationship of a noun in a sentence is usually shown by postpositions (i.e., prepositions that follow the noun). Hindi has three cases for nouns. The Direct case is used for nouns not followed by any postpositions, typically for the subject case. The Oblique case is used for any nouns that is followed by a postposition. Adjectives modifying nouns in the oblique case will inflect that same way. Some nouns have a separate Vocative case. Hindi has two numbers: singular and pluralâ€”but they may not be shown distinctly in all declinations.



The following is a sample text in High Hindi, of the Article 1 of the Universal Declaration of Human Rights (by the United Nations):

Transliteration (IAST):

Transcription (IPA):
